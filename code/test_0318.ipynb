{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"C:/wanted/Lang/lang/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1:\n",
      "Text: 딥러닝 기반 아동 미술 심리 진단 \n",
      "김지민, 박형빈, 정재식\n",
      "DeepPrint\n",
      "Team D.P.\n",
      "Wanted PotenUp\n",
      "3rd Project\n",
      "2025.03.05.\n",
      "--------------------------------------------------\n",
      "Page 2:\n",
      "Text: 오늘의 발표 내용은?\n",
      "01. 프로젝트 개요\n",
      "02. Project Flow\n",
      "03. 서비스 구현\n",
      "--------------------------------------------------\n",
      "Page 3:\n",
      "Text: HTP 검사란?\n",
      "주제 소개\n",
      "House(집), Tree(나무), Person(사람)의 세\n",
      "가지 그림을 그리게 하여 개인의 성격, 정서 상\n",
      "태, 대인 관계 등을 평가하는 투사적 심리 검사\n",
      "특히 아동 및 청소년의 심리 상태를 파악하거나,\n",
      "성인의 무의식적 감정과 스트레스를 이해하는\n",
      "데 유용하게 사용\n",
      "01. 프로젝트 개요\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HTP\n",
      "田田\n",
      "HTP\n",
      "--------------------------------------------------\n",
      "Page 4:\n",
      "Text: HTP 검사의 한계\n",
      "주제 선정 배경\n",
      "주관적 해석의 가능성\n",
      "표준화 부족\n",
      "문화적 차이\n",
      "피검사자의 의도적 왜곡\n",
      "기술적 한계\n",
      "01. 프로젝트 개요\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HTPTESTDRAWING\n",
      "AISGRXNONT\n",
      "田田\n",
      "HOUSE-TREE - PERSON\n",
      "--------------------------------------------------\n",
      "Page 5:\n",
      "Text: Project Work Flow\n",
      "학습 데이터 수집 및 정제\n",
      "1\n",
      "2\n",
      "3\n",
      "모델 학습 및 평가\n",
      "결과 해석\n",
      "AI Hub\n",
      ": AI 기반 아동 미술심리 진단을 위\n",
      "한 그림 데이터 구축\n",
      "Ultralytics YOLO\n",
      ": yolo11n, yolo11s, ...\n",
      "Detection 모델 활용\n",
      "02. Project Flow\n",
      "해석 가능한 요소\n",
      ": 그려진 그림 (탐지된 개체), 그림\n",
      "의 상대적 크기, 그림의 위치 등.\n",
      "--------------------------------------------------\n",
      "Page 6:\n",
      "Text: AI Hub 데이터\n",
      "학습 데이터 수집 (Image)\n",
      "AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축\n",
      "다양한 연령대(7~13)와 성별의 아동 7,000명으로부터 수집한\n",
      "4개 HTP 분류(집, 나무, 여자사람, 남자사람) 그림\n",
      "심리 해석이 포함된 의료 데이터가 아닌 객체 인식을 위한 그림\n",
      "데이터\n",
      "각 HTP 분류 별 Train (11200개), Validation (1400개) 데\n",
      "이터를 포함하고 있고, Test 셋은 정책 상 다운로드 불가능.\n",
      "02. Project Flow\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "--------------------------------------------------\n",
      "Page 7:\n",
      "Text: 1. House (집)\n",
      "2. Tree (나무)\n",
      "3. Person\n",
      "(남자사람, 여자사람)\n",
      "총 15 가지\n",
      "['집전체', '지붕', '집벽', '문', '창문',\n",
      "'굴뚝', '연기', '울타리', '길', '연못',\n",
      "'산', '나무', '꽃', '잔디', '태양']\n",
      "총 14 가지\n",
      "['나무전체', '기둥', '수관', '가지', '뿌\n",
      "리', '나뭇잎', '꽃', '열매', '그네', '새',\n",
      "'다람쥐', '구름', '달', '별']\n",
      "총 18 가지\n",
      "['사람전체', '머리', '얼굴', '눈', '코',\n",
      "'입', '귀', '머리카락', '목', '상체', '팔',\n",
      "'손', '다리', '발', '단추', '주머니', '운동\n",
      "화', '남자구두/여자구두']\n",
      "02. Project Flow\n",
      "학습 데이터 수집 (Label)\n",
      "--------------------------------------------------\n",
      "Page 8:\n",
      "Text: 학습 데이터 정제 (JSON → Label)\n",
      "원본 (JSON)\n",
      "YOLO Format\n",
      "meta\n",
      "데이터 정보 (code, age, sex)\n",
      "annotations\n",
      "bbox (x, y, w, h)\n",
      "shape_description\n",
      "prop_obj_img \n",
      "prop_obj_cls\n",
      "02. Project Flow\n",
      "class_index\n",
      "xc = (x + w/2) / image_width\n",
      "yc = (y + h/2) / image_height\n",
      "width = w / image_width\n",
      "height = h / image_height\n",
      "--------------------------------------------------\n",
      "Page 9:\n",
      "Text: HTP 항목별 모델 구축\n",
      "YOLO11n  학습\n",
      "mAP (mean Average Precision)\n",
      "02. Project Flow\n",
      "모델 학습 및 평가\n",
      "--------------------------------------------------\n",
      "Page 10:\n",
      "Text: 평가 지표\n",
      "IoU가 0.5 이상일 때의 평균 정밀도(Average Precision, AP)를 계산한\n",
      "값이다.\n",
      "즉, 검출한 바운딩 박스가 실제 객체와 2/3 이상 겹치면 정답으로 간주하고\n",
      "계산한다.\n",
      "객체 검출 성능을 대략적으로 평가할 때 많이 사용된다\n",
      "IoU 임계값을 0.50에서 0.95까지 0.05 간격(0.50, 0.55, ..., 0.95) 으로\n",
      "변화시키면서 각각의 AP를 구하고, 그 평균을 낸 값이다.\n",
      "즉, mAP50(B), mAP55(B), ..., mAP95(B) 의 평균이 된다.\n",
      "높은 IoU 임계값(예: 0.90, 0.95)에서는 더 정확한 바운딩 박스를 요구하므\n",
      "로, 모델이 실제로 얼마나 정밀하게 검출하는지를 더 엄격하게 평가하는 지\n",
      "표이다.\n",
      "mAP50 (mean Average Precision)\n",
      "mAP50-95 (mean Average Precision)\n",
      "02. Project Flow\n",
      "IoU :  Intersection over Union\n",
      "=> 실제와 탐지 영역의 교집합 / 합집합\n",
      "--------------------------------------------------\n",
      "Page 11:\n",
      "Text: 성능 개선\n",
      "THEMA\n",
      "imgsz = 640,  epochs = 50\n",
      "mAP50(B)\n",
      "mAP50-95(B)\n",
      "Female\n",
      "0.975\n",
      "0.808\n",
      "Male\n",
      "0.974\n",
      "0.811\n",
      "House\n",
      "0.975\n",
      "0.884\n",
      "Tree\n",
      "0.973\n",
      "0.844\n",
      "THEMA\n",
      "imgsz = 1280,  epochs = 30\n",
      "mAP50(B)\n",
      "mAP50-95(B)\n",
      "Female\n",
      "0.991 (+0.016)\n",
      "0.864 (+0.056)\n",
      "Male\n",
      "0.991 (+0.017)\n",
      "0.862 (+0.051)\n",
      "House\n",
      "0.988 (+0.013)\n",
      "0.925 (+0.041)\n",
      "Tree\n",
      "0.983 (+0.010)\n",
      "0.874 (+0.030)\n",
      "02. Project Flow\n",
      "imgsz 640 → 1280으로 증가시켜 성능을 향상시킴\n",
      "epochs를 50 → 30으로 낮추어 학습속도를 향상시킴\n",
      "--------------------------------------------------\n",
      "Page 12:\n",
      "Text: 성능 개선\n",
      "02. Project Flow\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mountain 0.97\n",
      "House 0.96]\n",
      "Sun 0.97\n",
      "Wall 0.97\n",
      "Pond 0.98\n",
      "Winddw\n",
      "0.88\n",
      "Door 0.88\n",
      "ence0.5\n",
      "Fence 0.89\n",
      "-D-D-0-D0\n",
      "Tree0.97\n",
      "Flover 0.92\n",
      "Mountain 0.96\n",
      "House 0.967\n",
      "Grass\n",
      "Sun0.95\n",
      "Wall0.96\n",
      "Pond0.97\n",
      "Winddw\n",
      "0.87\n",
      "Door 0.93\n",
      "Gruss 0.87\n",
      "rass0.83\n",
      "ence\n",
      "0.95\n",
      "Fence 0.87\n",
      "Fence0.86\n",
      "Tree0.96\n",
      "Flolver 0.92\n",
      "--------------------------------------------------\n",
      "Page 13:\n",
      "Text: 특징 추출 및 해석\n",
      "결과 해석\n",
      "어떤 개체(클래스)가 그려졌는지\n",
      "몇 개나 그려졌는지\n",
      "전체 이미지 크기 대비 어느정도인지\n",
      "해당 개체가 오브젝트에서 어느정도 크기인지\n",
      "위 결과들을 해석하는 레퍼런스 탐색 및 구현\n",
      "02. Project Flow\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HONG NGT\n",
      "rCDIDTOGTG\n",
      "NOROO\n",
      "口\n",
      "门\n",
      "?\n",
      "HTP\n",
      "HTP\n",
      "HOUSE-TREE-PERSON\n",
      "-TEEEPLASCN\n",
      "DEMILED ANALYSIS\n",
      "p:\n",
      "mdrhn\n",
      "DTtoloto  e D s 1\n",
      "LJB\n",
      "Oronloy\n",
      "OVER\n",
      "OVERVIEW\n",
      "DETAILEDANALYSISDEATEDANALYSIS\n",
      "Beenem\n",
      "   \n",
      "ree beitmpto\n",
      "me\n",
      "c ae srsmg wrroanestrgarte\n",
      "te omp tro\n",
      "0 1 p0m 6\n",
      "DETALEDANALYSIS\n",
      "OCMNRGUNRROKT\n",
      "2\n",
      "?\n",
      "OVGERIFEIUW\n",
      "-COIPATELAKALYCITS\n",
      "COLPARATVE\n",
      "REPORT\n",
      "USERFEETBACK\n",
      "Lm10c001\n",
      "--------------------------------------------------\n",
      "Page 14:\n",
      "Text: 결과 해석 예시\n",
      "House (집)\n",
      "Male (남자사람)\n",
      "✔ 집전체에 대한 평가\n",
      "그림에서 집전체 의 위치가 하단에 치우쳐져 있다. 이는 우\n",
      "울, 자존감 문제, 정서불안, 열등감을 표현한 것으로 볼 수 있\n",
      "다.\n",
      "✔ 연기에 대한 평가\n",
      "그림에서 Smoke 의 존재 여부가 TRUE. 이는 우울, 자존감\n",
      "문제을 표현한 것으로 볼 수 있다.\n",
      "✔ 길에 대한 평가\n",
      "그림에서 Fence 의 존재 여부가 TRUE. 이는 사회불안,\n",
      "Avoidance, 자존감 문제, 정서불안, 열등감을 표현한 것으\n",
      "로 볼 수 있다.\n",
      "✔ 사람전체에 대한 평가\n",
      "그림에서 사람전체 의 위치가 하단에 치우쳐져 있다. 이는 사\n",
      "회 불안, 우울, 자존감 문제, 정서불안, 열등감을 표현한 것으로\n",
      "볼 수 있다.\n",
      "✔ 상체에 대한 평가\n",
      "현재 버전에서는 이 항목에 대한 평가 정보가 없습니다.\n",
      "✔ 단추에 대한 평가\n",
      "그림에서 Button 의 크기가 지나치게 크다. 이는 애정결핍,\n",
      "퇴행을 표현한 것으로 볼 수 있다.\n",
      "02. Project Flow\n",
      "--------------------------------------------------\n",
      "Page 15:\n",
      "Text: 기대 효과\n",
      "객관적이고 일관된 분석 가능\n",
      "자동화된 이미지 해석을 통한 시간 단축 \n",
      "데이터 기반 해석 가능\n",
      "전문가와 보완적인 역할 기대\n",
      "03. 서비스 구현\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HTP\n",
      "House-Tree-Person\n",
      "House\n",
      "Person\n",
      "A\n",
      "--------------------------------------------------\n",
      "Page 16:\n",
      "Text: YOLO 모델 \n",
      "HTP 검사 결과를 \n",
      "채점 기준별 점수화\n",
      "Gemini API 활용 과 RAG 적용\n",
      "03. 서비스 구현\n",
      "Gemini API \n",
      "점수화된 데이터를\n",
      "자연스럽고 \n",
      "이해하기 쉬운 \n",
      "문장으로 변환\n",
      "RAG 적용 \n",
      "더 신뢰도 높은 심리\n",
      "평가 문장 생성\n",
      "--------------------------------------------------\n",
      "Page 17:\n",
      "Text: 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[페이지 진입 시 초기 화면]\n",
      "[테마 선택 → 이미지 업로드]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "localhost:8501\n",
      "M Gmail Web DeepL :Aol..\n",
      "\"elohly alol pnis nxleea lool Axaldead 榮Id9ley \n",
      "Deploy\n",
      "DeePrint：AIHTP召人H|\n",
      "?\n",
      "localhost:8501\n",
      "M Gmail Web> DeepL e:AAol.\n",
      "\"elohlyi alolh Apnis nylelea lool Aixaldad 榮ldsieyp \n",
      "Deploy\n",
      "DeePrint：AIHTP召人人H|\n",
      "(jpg/jpeg/.png)\n",
      "Drag and drop file here\n",
      "Browsefiles\n",
      "Limit 200MB per file · JPG, PNG, JPEG\n",
      "--------------------------------------------------\n",
      "Page 18:\n",
      "Text: 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[탐지 모드 설정]\n",
      "[선택 보기 → 탐색 객체 선택]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?\n",
      "localhost:8501\n",
      "品丨M Gmail Web DeepL :4Ao\n",
      "\"elohly alol pnis nxleea lool Axaldead 榮Id9ley \n",
      "Deploy\n",
      "(jpg/jpeg /.png)\n",
      "Drag and drop file here\n",
      "Browse files\n",
      "Limit 200MB per file ·JPG, PNG, JPEG\n",
      "House_8_F_01855.jpg 87.3KB\n",
      "?\n",
      "localhost:8501\n",
      "K)\n",
      "品丨M Gmail Web DeepL :4Ao\n",
      "ChatGPTPerplexityTool Dataiku StudyO目 L\n",
      "Deploy\n",
      "(jpg /.jpeg/.png)\n",
      "Drag and drop file here\n",
      "Browse files\n",
      "Limit 200MB per file ·JPG, PNG, JPEG\n",
      "House_8_F_01855.jpg 87.3KB\n",
      "×\n",
      "×\n",
      "：[，，]\n",
      "--------------------------------------------------\n",
      "Page 19:\n",
      "Text: 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[모두 보기 모드]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "app\n",
      "C\n",
      "?\n",
      "localhost:8501\n",
      "K)\n",
      "品\n",
      "\"elohlyb aloh Apnis nxleea lool Axaldlad 榮ld9ieyp \n",
      "“hh号北百化是最b1lo号YoA区\n",
      "“hh号北百化是最b1lo号YoAx\n",
      "-chineyTRUE，对量双\n",
      "SkeTRUE.，对\n",
      "F.，Avoace，\n",
      "F.ae，\n",
      "-对MountainTRUE.0是,Avoidance,对,是\n",
      "Hh\n",
      "TUE，vol是，\n",
      "“h号得亿百化lo号区\n",
      "--------------------------------------------------\n",
      "Page 20:\n",
      "Text: 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[선택 모드]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "app\n",
      "C\n",
      "?\n",
      "localhost:8501\n",
      "K)\n",
      "品丨M Gmail Web DeepL :47o\n",
      "\"elohly aloh Apnis nxleea lool Axaldlad 榮ld9ieyp \n",
      "xxxxxx\n",
      "HRbORRA\n",
      "“h号8北百化Rh10lo号YoxR\n",
      "-对 Chmney难0早TRUE 0辛鲁,对香是对是是今双。\n",
      "S\n",
      "TRUE，,Avodnce\n",
      "h\n",
      "--------------------------------------------------\n",
      "Page 21:\n",
      "Text: 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[전체 요약 보기]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "app\n",
      "/X\n",
      "Smart Switch1&人|\n",
      "?\n",
      "localhost:8501\n",
      "器丨M Gmail Web DeepL :A7\n",
      "ChatGPTPerplexity Tool Dataiku StudyO目 Lo\n",
      "Deploy\n",
      "Avoidance\n",
      "h百七Il0号Y-\n",
      "h百l0号Y-\n",
      "七品\n",
      "h百l0号Y\n",
      "--------------------------------------------------\n",
      "Page 22:\n",
      "Text: Q & A\n",
      "--------------------------------------------------\n",
      "Page 23:\n",
      "Text: 4월에 다시 만나요\n",
      "  다음 발표는?\n",
      "언제였지...?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path, extract_images=True)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 정보 출력\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"Page {idx + 1}:\")\n",
    "    print(\"Text:\", doc.page_content)  # 페이지의 텍스트 출력\n",
    "    if \"images\" in doc.metadata:\n",
    "        print(\"Images:\", doc.metadata[\"images\"])  # 이미지 정보 출력\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'page': 1, 'text': '딥러닝 기반 아동 미술 심리 진단 \\n김지민, 박형빈, 정재식\\nDeepPrint\\nTeam D.P.\\nWanted PotenUp\\n3rd Project\\n2025.03.05.\\n', 'images': [{'image_id': 746, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 2, 'text': '오늘의 발표 내용은?\\n01. 프로젝트 개요\\n02. Project Flow\\n03. 서비스 구현\\n', 'images': [{'image_id': 772, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 3, 'text': 'HTP 검사란?\\n주제 소개\\nHouse(집), Tree(나무), Person(사람)의 세\\n가지 그림을 그리게 하여 개인의 성격, 정서 상\\n태, 대인 관계 등을 평가하는 투사적 심리 검사\\n특히 아동 및 청소년의 심리 상태를 파악하거나,\\n성인의 무의식적 감정과 스트레스를 이해하는\\n데 유용하게 사용\\n01. 프로젝트 개요\\n', 'images': [{'image_id': 60, 'x': 42.33396911621094, 'y': 253.16091918945312}, {'image_id': 798, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 4, 'text': 'HTP 검사의 한계\\n주제 선정 배경\\n주관적 해석의 가능성\\n표준화 부족\\n문화적 차이\\n피검사자의 의도적 왜곡\\n기술적 한계\\n01. 프로젝트 개요\\n', 'images': [{'image_id': 71, 'x': 42.33396911621094, 'y': 253.16091918945312}, {'image_id': 827, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 5, 'text': 'Project Work Flow\\n학습 데이터 수집 및 정제\\n1\\n2\\n3\\n모델 학습 및 평가\\n결과 해석\\nAI Hub\\n: AI 기반 아동 미술심리 진단을 위\\n한 그림 데이터 구축\\nUltralytics YOLO\\n: yolo11n, yolo11s, ...\\nDetection 모델 활용\\n02. Project Flow\\n해석 가능한 요소\\n: 그려진 그림 (탐지된 개체), 그림\\n의 상대적 크기, 그림의 위치 등.\\n', 'images': [{'image_id': 854, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 6, 'text': 'AI Hub 데이터\\n학습 데이터 수집 (Image)\\nAI 기반 아동 미술심리 진단을 위한 그림 데이터 구축\\n다양한 연령대(7~13)와 성별의 아동 7,000명으로부터 수집한\\n4개 HTP 분류(집, 나무, 여자사람, 남자사람) 그림\\n심리 해석이 포함된 의료 데이터가 아닌 객체 인식을 위한 그림\\n데이터\\n각 HTP 분류 별 Train (11200개), Validation (1400개) 데\\n이터를 포함하고 있고, Test 셋은 정책 상 다운로드 불가능.\\n02. Project Flow\\n', 'images': [{'image_id': 100, 'x': 98.22364807128906, 'y': 134.46978759765625}, {'image_id': 902, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 7, 'text': \"1. House (집)\\n2. Tree (나무)\\n3. Person\\n(남자사람, 여자사람)\\n총 15 가지\\n['집전체', '지붕', '집벽', '문', '창문',\\n'굴뚝', '연기', '울타리', '길', '연못',\\n'산', '나무', '꽃', '잔디', '태양']\\n총 14 가지\\n['나무전체', '기둥', '수관', '가지', '뿌\\n리', '나뭇잎', '꽃', '열매', '그네', '새',\\n'다람쥐', '구름', '달', '별']\\n총 18 가지\\n['사람전체', '머리', '얼굴', '눈', '코',\\n'입', '귀', '머리카락', '목', '상체', '팔',\\n'손', '다리', '발', '단추', '주머니', '운동\\n화', '남자구두/여자구두']\\n02. Project Flow\\n학습 데이터 수집 (Label)\\n\", 'images': [{'image_id': 930, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 8, 'text': '학습 데이터 정제 (JSON → Label)\\n원본 (JSON)\\nYOLO Format\\nmeta\\n데이터 정보 (code, age, sex)\\nannotations\\nbbox (x, y, w, h)\\nshape_description\\nprop_obj_img \\nprop_obj_cls\\n02. Project Flow\\nclass_index\\nxc = (x + w/2) / image_width\\nyc = (y + h/2) / image_height\\nwidth = w / image_width\\nheight = h / image_height\\n', 'images': [{'image_id': 976, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 9, 'text': 'HTP 항목별 모델 구축\\nYOLO11n  학습\\nmAP (mean Average Precision)\\n02. Project Flow\\n모델 학습 및 평가\\n', 'images': [{'image_id': 1005, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 10, 'text': '평가 지표\\nIoU가 0.5 이상일 때의 평균 정밀도(Average Precision, AP)를 계산한\\n값이다.\\n즉, 검출한 바운딩 박스가 실제 객체와 2/3 이상 겹치면 정답으로 간주하고\\n계산한다.\\n객체 검출 성능을 대략적으로 평가할 때 많이 사용된다\\nIoU 임계값을 0.50에서 0.95까지 0.05 간격(0.50, 0.55, ..., 0.95) 으로\\n변화시키면서 각각의 AP를 구하고, 그 평균을 낸 값이다.\\n즉, mAP50(B), mAP55(B), ..., mAP95(B) 의 평균이 된다.\\n높은 IoU 임계값(예: 0.90, 0.95)에서는 더 정확한 바운딩 박스를 요구하므\\n로, 모델이 실제로 얼마나 정밀하게 검출하는지를 더 엄격하게 평가하는 지\\n표이다.\\nmAP50 (mean Average Precision)\\nmAP50-95 (mean Average Precision)\\n02. Project Flow\\nIoU :  Intersection over Union\\n=> 실제와 탐지 영역의 교집합 / 합집합\\n', 'images': [{'image_id': 1030, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 11, 'text': '성능 개선\\nTHEMA\\nimgsz = 640,  epochs = 50\\nmAP50(B)\\nmAP50-95(B)\\nFemale\\n0.975\\n0.808\\nMale\\n0.974\\n0.811\\nHouse\\n0.975\\n0.884\\nTree\\n0.973\\n0.844\\nTHEMA\\nimgsz = 1280,  epochs = 30\\nmAP50(B)\\nmAP50-95(B)\\nFemale\\n0.991 (+0.016)\\n0.864 (+0.056)\\nMale\\n0.991 (+0.017)\\n0.862 (+0.051)\\nHouse\\n0.988 (+0.013)\\n0.925 (+0.041)\\nTree\\n0.983 (+0.010)\\n0.874 (+0.030)\\n02. Project Flow\\nimgsz 640 → 1280으로 증가시켜 성능을 향상시킴\\nepochs를 50 → 30으로 낮추어 학습속도를 향상시킴\\n', 'images': [{'image_id': 1060, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 12, 'text': '성능 개선\\n02. Project Flow\\n', 'images': [{'image_id': 170, 'x': 140.6327667236328, 'y': 251.24545288085938}, {'image_id': 171, 'x': 783.7365112304688, 'y': 251.24545288085938}, {'image_id': 172, 'x': 832.3502197265625, 'y': 380.1793212890625}, {'image_id': 1086, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 13, 'text': '특징 추출 및 해석\\n결과 해석\\n어떤 개체(클래스)가 그려졌는지\\n몇 개나 그려졌는지\\n전체 이미지 크기 대비 어느정도인지\\n해당 개체가 오브젝트에서 어느정도 크기인지\\n위 결과들을 해석하는 레퍼런스 탐색 및 구현\\n02. Project Flow\\n', 'images': [{'image_id': 183, 'x': 42.33396911621094, 'y': 253.16091918945312}, {'image_id': 1114, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 14, 'text': '결과 해석 예시\\nHouse (집)\\nMale (남자사람)\\n✔ 집전체에 대한 평가\\n그림에서 집전체 의 위치가 하단에 치우쳐져 있다. 이는 우\\n울, 자존감 문제, 정서불안, 열등감을 표현한 것으로 볼 수 있\\n다.\\n✔ 연기에 대한 평가\\n그림에서 Smoke 의 존재 여부가 TRUE. 이는 우울, 자존감\\n문제을 표현한 것으로 볼 수 있다.\\n✔ 길에 대한 평가\\n그림에서 Fence 의 존재 여부가 TRUE. 이는 사회불안,\\nAvoidance, 자존감 문제, 정서불안, 열등감을 표현한 것으\\n로 볼 수 있다.\\n✔ 사람전체에 대한 평가\\n그림에서 사람전체 의 위치가 하단에 치우쳐져 있다. 이는 사\\n회 불안, 우울, 자존감 문제, 정서불안, 열등감을 표현한 것으로\\n볼 수 있다.\\n✔ 상체에 대한 평가\\n현재 버전에서는 이 항목에 대한 평가 정보가 없습니다.\\n✔ 단추에 대한 평가\\n그림에서 Button 의 크기가 지나치게 크다. 이는 애정결핍,\\n퇴행을 표현한 것으로 볼 수 있다.\\n02. Project Flow\\n', 'images': [{'image_id': 1141, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 15, 'text': '기대 효과\\n객관적이고 일관된 분석 가능\\n자동화된 이미지 해석을 통한 시간 단축 \\n데이터 기반 해석 가능\\n전문가와 보완적인 역할 기대\\n03. 서비스 구현\\n', 'images': [{'image_id': 205, 'x': 42.33396911621094, 'y': 253.16091918945312}, {'image_id': 1172, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 16, 'text': 'YOLO 모델 \\nHTP 검사 결과를 \\n채점 기준별 점수화\\nGemini API 활용 과 RAG 적용\\n03. 서비스 구현\\nGemini API \\n점수화된 데이터를\\n자연스럽고 \\n이해하기 쉬운 \\n문장으로 변환\\nRAG 적용 \\n더 신뢰도 높은 심리\\n평가 문장 생성\\n', 'images': [{'image_id': 217, 'x': 437.92742919921875, 'y': 226.09039306640625}, {'image_id': 1203, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 17, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[페이지 진입 시 초기 화면]\\n[테마 선택 → 이미지 업로드]\\n', 'images': [{'image_id': 235, 'x': 62.6187744140625, 'y': 121.1241455078125}, {'image_id': 236, 'x': 81.0, 'y': 342.8093566894531}, {'image_id': 1247, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 18, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[탐지 모드 설정]\\n[선택 보기 → 탐색 객체 선택]\\n', 'images': [{'image_id': 247, 'x': 81.0, 'y': 64.85543823242188}, {'image_id': 248, 'x': 81.0, 'y': 319.9941101074219}, {'image_id': 1273, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 19, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[모두 보기 모드]\\n', 'images': [{'image_id': 259, 'x': 81.0, 'y': 191.14779663085938}, {'image_id': 1299, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 20, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[선택 모드]\\n', 'images': [{'image_id': 270, 'x': 81.0, 'y': 191.14779663085938}, {'image_id': 1325, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 21, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[전체 요약 보기]\\n', 'images': [{'image_id': 281, 'x': 81.0, 'y': 226.31155395507812}, {'image_id': 1351, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 22, 'text': 'Q & A\\n', 'images': [{'image_id': 1377, 'x': -422.6431579589844, 'y': -220.075439453125}]}, {'page': 23, 'text': '4월에 다시 만나요\\n  다음 발표는?\\n언제였지...?\\n', 'images': [{'image_id': 1401, 'x': -422.6431579589844, 'y': -220.075439453125}]}]\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "page_data = []\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    text = page.get_text(\"text\")  # 텍스트 추출\n",
    "    images = page.get_images(full=True)  # 이미지 추출\n",
    "\n",
    "    image_data = []\n",
    "    for img in images:\n",
    "        xref = img[0]\n",
    "        img_rects = page.get_image_rects(xref)\n",
    "        if img_rects:\n",
    "            x0, y0, x1, y1 = img_rects[0]\n",
    "            image_data.append({\"image_id\": xref, \"x\": x0, \"y\": y0})\n",
    "\n",
    "    page_data.append({\"page\": page_num + 1, \"text\": text, \"images\": image_data})\n",
    "\n",
    "print(page_data)\n",
    "print(type(page_data[0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'page': 1, 'text': '딥러닝 기반 아동 미술 심리 진단 \\n김지민, 박형빈, 정재식\\nDeepPrint\\nTeam D.P.\\nWanted PotenUp\\n3rd Project\\n2025.03.05.\\n', 'images': [{'image_id': 746, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '딥러닝 기반 아동 미술 심리 진단 '}]}, {'page': 2, 'text': '오늘의 발표 내용은?\\n01. 프로젝트 개요\\n02. Project Flow\\n03. 서비스 구현\\n', 'images': [{'image_id': 772, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '오늘의 발표 내용은?'}]}, {'page': 3, 'text': 'HTP 검사란?\\n주제 소개\\nHouse(집), Tree(나무), Person(사람)의 세\\n가지 그림을 그리게 하여 개인의 성격, 정서 상\\n태, 대인 관계 등을 평가하는 투사적 심리 검사\\n특히 아동 및 청소년의 심리 상태를 파악하거나,\\n성인의 무의식적 감정과 스트레스를 이해하는\\n데 유용하게 사용\\n01. 프로젝트 개요\\n', 'images': [{'image_id': 60, 'x': 42.33396911621094, 'y': 253.16091918945312, 'matched_text': 'HTP 검사란?'}, {'image_id': 798, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': 'HTP 검사란?'}]}, {'page': 4, 'text': 'HTP 검사의 한계\\n주제 선정 배경\\n주관적 해석의 가능성\\n표준화 부족\\n문화적 차이\\n피검사자의 의도적 왜곡\\n기술적 한계\\n01. 프로젝트 개요\\n', 'images': [{'image_id': 71, 'x': 42.33396911621094, 'y': 253.16091918945312, 'matched_text': 'HTP 검사의 한계'}, {'image_id': 827, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': 'HTP 검사의 한계'}]}, {'page': 5, 'text': 'Project Work Flow\\n학습 데이터 수집 및 정제\\n1\\n2\\n3\\n모델 학습 및 평가\\n결과 해석\\nAI Hub\\n: AI 기반 아동 미술심리 진단을 위\\n한 그림 데이터 구축\\nUltralytics YOLO\\n: yolo11n, yolo11s, ...\\nDetection 모델 활용\\n02. Project Flow\\n해석 가능한 요소\\n: 그려진 그림 (탐지된 개체), 그림\\n의 상대적 크기, 그림의 위치 등.\\n', 'images': [{'image_id': 854, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': 'Project Work Flow'}]}, {'page': 6, 'text': 'AI Hub 데이터\\n학습 데이터 수집 (Image)\\nAI 기반 아동 미술심리 진단을 위한 그림 데이터 구축\\n다양한 연령대(7~13)와 성별의 아동 7,000명으로부터 수집한\\n4개 HTP 분류(집, 나무, 여자사람, 남자사람) 그림\\n심리 해석이 포함된 의료 데이터가 아닌 객체 인식을 위한 그림\\n데이터\\n각 HTP 분류 별 Train (11200개), Validation (1400개) 데\\n이터를 포함하고 있고, Test 셋은 정책 상 다운로드 불가능.\\n02. Project Flow\\n', 'images': [{'image_id': 100, 'x': 98.22364807128906, 'y': 134.46978759765625, 'matched_text': 'AI Hub 데이터'}, {'image_id': 902, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': 'AI Hub 데이터'}]}, {'page': 7, 'text': \"1. House (집)\\n2. Tree (나무)\\n3. Person\\n(남자사람, 여자사람)\\n총 15 가지\\n['집전체', '지붕', '집벽', '문', '창문',\\n'굴뚝', '연기', '울타리', '길', '연못',\\n'산', '나무', '꽃', '잔디', '태양']\\n총 14 가지\\n['나무전체', '기둥', '수관', '가지', '뿌\\n리', '나뭇잎', '꽃', '열매', '그네', '새',\\n'다람쥐', '구름', '달', '별']\\n총 18 가지\\n['사람전체', '머리', '얼굴', '눈', '코',\\n'입', '귀', '머리카락', '목', '상체', '팔',\\n'손', '다리', '발', '단추', '주머니', '운동\\n화', '남자구두/여자구두']\\n02. Project Flow\\n학습 데이터 수집 (Label)\\n\", 'images': [{'image_id': 930, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '1. House (집)'}]}, {'page': 8, 'text': '학습 데이터 정제 (JSON → Label)\\n원본 (JSON)\\nYOLO Format\\nmeta\\n데이터 정보 (code, age, sex)\\nannotations\\nbbox (x, y, w, h)\\nshape_description\\nprop_obj_img \\nprop_obj_cls\\n02. Project Flow\\nclass_index\\nxc = (x + w/2) / image_width\\nyc = (y + h/2) / image_height\\nwidth = w / image_width\\nheight = h / image_height\\n', 'images': [{'image_id': 976, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '학습 데이터 정제 (JSON → Label)'}]}, {'page': 9, 'text': 'HTP 항목별 모델 구축\\nYOLO11n  학습\\nmAP (mean Average Precision)\\n02. Project Flow\\n모델 학습 및 평가\\n', 'images': [{'image_id': 1005, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': 'HTP 항목별 모델 구축'}]}, {'page': 10, 'text': '평가 지표\\nIoU가 0.5 이상일 때의 평균 정밀도(Average Precision, AP)를 계산한\\n값이다.\\n즉, 검출한 바운딩 박스가 실제 객체와 2/3 이상 겹치면 정답으로 간주하고\\n계산한다.\\n객체 검출 성능을 대략적으로 평가할 때 많이 사용된다\\nIoU 임계값을 0.50에서 0.95까지 0.05 간격(0.50, 0.55, ..., 0.95) 으로\\n변화시키면서 각각의 AP를 구하고, 그 평균을 낸 값이다.\\n즉, mAP50(B), mAP55(B), ..., mAP95(B) 의 평균이 된다.\\n높은 IoU 임계값(예: 0.90, 0.95)에서는 더 정확한 바운딩 박스를 요구하므\\n로, 모델이 실제로 얼마나 정밀하게 검출하는지를 더 엄격하게 평가하는 지\\n표이다.\\nmAP50 (mean Average Precision)\\nmAP50-95 (mean Average Precision)\\n02. Project Flow\\nIoU :  Intersection over Union\\n=> 실제와 탐지 영역의 교집합 / 합집합\\n', 'images': [{'image_id': 1030, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '평가 지표'}]}, {'page': 11, 'text': '성능 개선\\nTHEMA\\nimgsz = 640,  epochs = 50\\nmAP50(B)\\nmAP50-95(B)\\nFemale\\n0.975\\n0.808\\nMale\\n0.974\\n0.811\\nHouse\\n0.975\\n0.884\\nTree\\n0.973\\n0.844\\nTHEMA\\nimgsz = 1280,  epochs = 30\\nmAP50(B)\\nmAP50-95(B)\\nFemale\\n0.991 (+0.016)\\n0.864 (+0.056)\\nMale\\n0.991 (+0.017)\\n0.862 (+0.051)\\nHouse\\n0.988 (+0.013)\\n0.925 (+0.041)\\nTree\\n0.983 (+0.010)\\n0.874 (+0.030)\\n02. Project Flow\\nimgsz 640 → 1280으로 증가시켜 성능을 향상시킴\\nepochs를 50 → 30으로 낮추어 학습속도를 향상시킴\\n', 'images': [{'image_id': 1060, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '성능 개선'}]}, {'page': 12, 'text': '성능 개선\\n02. Project Flow\\n', 'images': [{'image_id': 170, 'x': 140.6327667236328, 'y': 251.24545288085938, 'matched_text': '성능 개선'}, {'image_id': 171, 'x': 783.7365112304688, 'y': 251.24545288085938, 'matched_text': '성능 개선'}, {'image_id': 172, 'x': 832.3502197265625, 'y': 380.1793212890625, 'matched_text': '성능 개선'}, {'image_id': 1086, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '성능 개선'}]}, {'page': 13, 'text': '특징 추출 및 해석\\n결과 해석\\n어떤 개체(클래스)가 그려졌는지\\n몇 개나 그려졌는지\\n전체 이미지 크기 대비 어느정도인지\\n해당 개체가 오브젝트에서 어느정도 크기인지\\n위 결과들을 해석하는 레퍼런스 탐색 및 구현\\n02. Project Flow\\n', 'images': [{'image_id': 183, 'x': 42.33396911621094, 'y': 253.16091918945312, 'matched_text': '특징 추출 및 해석'}, {'image_id': 1114, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '특징 추출 및 해석'}]}, {'page': 14, 'text': '결과 해석 예시\\nHouse (집)\\nMale (남자사람)\\n✔ 집전체에 대한 평가\\n그림에서 집전체 의 위치가 하단에 치우쳐져 있다. 이는 우\\n울, 자존감 문제, 정서불안, 열등감을 표현한 것으로 볼 수 있\\n다.\\n✔ 연기에 대한 평가\\n그림에서 Smoke 의 존재 여부가 TRUE. 이는 우울, 자존감\\n문제을 표현한 것으로 볼 수 있다.\\n✔ 길에 대한 평가\\n그림에서 Fence 의 존재 여부가 TRUE. 이는 사회불안,\\nAvoidance, 자존감 문제, 정서불안, 열등감을 표현한 것으\\n로 볼 수 있다.\\n✔ 사람전체에 대한 평가\\n그림에서 사람전체 의 위치가 하단에 치우쳐져 있다. 이는 사\\n회 불안, 우울, 자존감 문제, 정서불안, 열등감을 표현한 것으로\\n볼 수 있다.\\n✔ 상체에 대한 평가\\n현재 버전에서는 이 항목에 대한 평가 정보가 없습니다.\\n✔ 단추에 대한 평가\\n그림에서 Button 의 크기가 지나치게 크다. 이는 애정결핍,\\n퇴행을 표현한 것으로 볼 수 있다.\\n02. Project Flow\\n', 'images': [{'image_id': 1141, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '결과 해석 예시'}]}, {'page': 15, 'text': '기대 효과\\n객관적이고 일관된 분석 가능\\n자동화된 이미지 해석을 통한 시간 단축 \\n데이터 기반 해석 가능\\n전문가와 보완적인 역할 기대\\n03. 서비스 구현\\n', 'images': [{'image_id': 205, 'x': 42.33396911621094, 'y': 253.16091918945312, 'matched_text': '기대 효과'}, {'image_id': 1172, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '기대 효과'}]}, {'page': 16, 'text': 'YOLO 모델 \\nHTP 검사 결과를 \\n채점 기준별 점수화\\nGemini API 활용 과 RAG 적용\\n03. 서비스 구현\\nGemini API \\n점수화된 데이터를\\n자연스럽고 \\n이해하기 쉬운 \\n문장으로 변환\\nRAG 적용 \\n더 신뢰도 높은 심리\\n평가 문장 생성\\n', 'images': [{'image_id': 217, 'x': 437.92742919921875, 'y': 226.09039306640625, 'matched_text': 'YOLO 모델 '}, {'image_id': 1203, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': 'YOLO 모델 '}]}, {'page': 17, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[페이지 진입 시 초기 화면]\\n[테마 선택 → 이미지 업로드]\\n', 'images': [{'image_id': 235, 'x': 62.6187744140625, 'y': 121.1241455078125, 'matched_text': '이미지 업로드 및  해석 화면'}, {'image_id': 236, 'x': 81.0, 'y': 342.8093566894531, 'matched_text': '이미지 업로드 및  해석 화면'}, {'image_id': 1247, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '이미지 업로드 및  해석 화면'}]}, {'page': 18, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[탐지 모드 설정]\\n[선택 보기 → 탐색 객체 선택]\\n', 'images': [{'image_id': 247, 'x': 81.0, 'y': 64.85543823242188, 'matched_text': '이미지 업로드 및  해석 화면'}, {'image_id': 248, 'x': 81.0, 'y': 319.9941101074219, 'matched_text': '이미지 업로드 및  해석 화면'}, {'image_id': 1273, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '이미지 업로드 및  해석 화면'}]}, {'page': 19, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[모두 보기 모드]\\n', 'images': [{'image_id': 259, 'x': 81.0, 'y': 191.14779663085938, 'matched_text': '이미지 업로드 및  해석 화면'}, {'image_id': 1299, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '이미지 업로드 및  해석 화면'}]}, {'page': 20, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[선택 모드]\\n', 'images': [{'image_id': 270, 'x': 81.0, 'y': 191.14779663085938, 'matched_text': '이미지 업로드 및  해석 화면'}, {'image_id': 1325, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '이미지 업로드 및  해석 화면'}]}, {'page': 21, 'text': '이미지 업로드 및  해석 화면\\n03. 서비스 구현\\n[전체 요약 보기]\\n', 'images': [{'image_id': 281, 'x': 81.0, 'y': 226.31155395507812, 'matched_text': '이미지 업로드 및  해석 화면'}, {'image_id': 1351, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '이미지 업로드 및  해석 화면'}]}, {'page': 22, 'text': 'Q & A\\n', 'images': [{'image_id': 1377, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': 'Q & A'}]}, {'page': 23, 'text': '4월에 다시 만나요\\n  다음 발표는?\\n언제였지...?\\n', 'images': [{'image_id': 1401, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '4월에 다시 만나요'}]}]\n",
      "[{'image_id': 746, 'x': -422.6431579589844, 'y': -220.075439453125, 'matched_text': '딥러닝 기반 아동 미술 심리 진단 '}]\n"
     ]
    }
   ],
   "source": [
    "for page in page_data:\n",
    "    for img in page[\"images\"]:\n",
    "        closest_text = None\n",
    "        min_distance = float(\"inf\")\n",
    "\n",
    "        for line in page[\"text\"].split(\"\\n\"):\n",
    "            # 이미지와 가장 가까운 텍스트 찾기\n",
    "            distance = abs(100 - img[\"y\"])  # 이미지의 Y 좌표와 비교 (실제 좌표는 다를 수 있음)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_text = line\n",
    "\n",
    "        img[\"matched_text\"] = closest_text if closest_text else \"No text found\"\n",
    "\n",
    "print(page_data)\n",
    "print(page_data[0]['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8844\\112685771.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
      "c:\\wanted\\Lang\\lang\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\wanted\\Lang\\lang\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--jhgan--ko-sbert-nli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 임베딩 생성\n",
    "embedding_model_name = \"jhgan/ko-sbert-nli\"  # 임베딩 모델 선택\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 벡터 스토어에 문서 추가\n",
    "vector_store = FAISS.from_texts([page[\"text\"] for page in page_data], embedding)\n",
    "\n",
    "query = \"이 문서의 핵심 내용을 설명해줘.\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=5)  # 상위 5개 문서 검색\n",
    "\n",
    "# 검색된 문서의 내용을 하나로 합치기\n",
    "retrieved_text = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def convert_to_base64(pil_image):\n",
    "#     '''pil이미지를 받아서 base64문자열로 바꾸어주는 함수'''\n",
    "#     buffer = BytesIO()\n",
    "#     pil_image.save(buffer, format='jpeg')\n",
    "#     img_str = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "#     return img_str\n",
    "\n",
    "# def plt_img_base64(img_base64):\n",
    "#     image = f'<img sec=\"data:image/jpeg;base64,{img_base64}\">'\n",
    "#     display(HTML(image))\n",
    "\n",
    "# def make_prompt(data):\n",
    "#     text = data['text']\n",
    "#     image = data['image']\n",
    "\n",
    "#     image_data = {\n",
    "#         'type' : 'image_url',\n",
    "#         'image_url' : f\"data:image/jpeg;base64,{image}\"\n",
    "#     }\n",
    "\n",
    "#     text_data = {'type' : 'text', 'text':text}\n",
    "\n",
    "#     content = []\n",
    "\n",
    "#     content.append(image_data)\n",
    "#     content.append(text_data)\n",
    "#     return [HumanMessage(content=content)]  # 보내줄때 현업에서 이따위로 보내주는게 국룰이라 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 문서는 HTP 검사(집-나무-사람 그림 검사) 이미지를 분석하여 심리 평가 보고서를 생성하는 서비스 구현에 대한 발표 내용을 담고 있습니다.  주요 내용은 다음과 같습니다.\n",
      "\n",
      "* **프로젝트 목표:** HTP 검사 결과를 자동으로 분석하고, 사용자에게 이해하기 쉬운 심리 평가 보고서를 제공하는 서비스 개발.\n",
      "* **핵심 기술:** \n",
      "    * **YOLO 모델:** 업로드된 HTP 그림에서 집, 나무, 사람 이미지를 객체 탐지하여 위치 및 크기 정보 추출.\n",
      "    * **Gemini API:** 추출된 정보와 채점 기준을 바탕으로 점수화된 데이터를 자연어 문장으로 변환.\n",
      "    * **RAG (Retrieval Augmented Generation):**  신뢰도 높은 심리 평가 문장 생성을 위해 관련 정보 검색 및 활용.\n",
      "* **데이터 처리:** JSON 형식의 원본 데이터를 YOLO 모델 학습에 적합한 형태로 변환 (bbox, class_index 등).\n",
      "* **서비스 구현:** 사용자가 이미지를 업로드하고, 테마를 선택하면 분석 결과를 제공하는 화면 구성.\n",
      "\n",
      "요약하자면, 사용자가 그림을 업로드하면 YOLO 모델이 그림을 분석하고, Gemini API와 RAG 기술을 활용하여 심리 평가 보고서를 생성하는 서비스를 구현하는 과정을 설명하고 있습니다.  발표는 프로젝트 개요, 워크플로우, 서비스 구현 순서로 진행되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import time\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(model = 'gemini-1.5-pro')\n",
    "ollama = ChatOllama(model='EEVE-korean-10.8B')\n",
    "\n",
    "# LLM에 검색된 내용만 전달\n",
    "full_prompt = f\"\"\"\n",
    "이 문서의 주요 내용을 설명해줘. 관련된 내용:\n",
    "{retrieved_text}\n",
    "\"\"\"\n",
    "\n",
    "response = gemini.invoke([\n",
    "    SystemMessage(content=\"당신은 PDF 문서를 해석하는 AI입니다.\"),\n",
    "    HumanMessage(content=full_prompt)\n",
    "])\n",
    "\n",
    "full_explanation = response.content\n",
    "\n",
    "print(full_explanation)  # 최종 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제공된 정보에는 AI가 분석할 수 있는 주요 내용이나 텍스트가 없습니다. 대신, 이 문서는 \"DeepPrint\" 팀의 프로젝트 제목인 \"딥러닝 기반 아동 미술 심리 진단\"과 관련 세부 정보를 나타내고 있습니다. 해당 팀원으로는 김지민, 박형빈, 정재식이 있으며, 이는 2025년 3월 5일에 수행된 세 번째 프로젝트입니다. 이미지 설명은 이 문서에 언급된 제목을 담고 있습니다.\n",
      "해당 페이지는 발표 내용에 관한 것으로 보입니다. 이 발표는 총 세 가지 주요 주제로 구성되어 있습니다: 프로젝트 개요, Project Flow, 그리고 서비스 구현입니다. 또한 이미지 설명도 포함되어 있으며 해당 이미지의 제목은 \"오늘의 발표 내용은?\"으로 보입니다.\n",
      "해당 페이지의 주요 내용은 HTP 검사에 관한 것으로, 이는 House(집), Tree(나무), Person(사람)이라는 세 가지 그림을 그리게 하여 개인의 성격, 정서 상태, 대인 관계 등을 평가하는 투사적 심리 검사입니다. 아동 및 청소년의 심리 상태를 파악하거나 성인의 무의식 감정과 스트레스를 이해하는 데 유용하게 사용됩니다. 프로젝트 개요는 HTP 검사의 목적과 그 사용에 대해 설명하고 있습니다.\n",
      "제시된 내용은 HTP(한자능력검사)의 한계를 설명하는 것으로 보입니다. 이는 한자에 대한 이해력과 능력을 평가하는 시험입니다. 이 페이지는 주관적 해석, 표준화의 부재, 문화적 차이, 피검사자의 의도적 왜곡, 기술적 한계 등 여러 가지 측면에서의 이러한 한계점을 논의합니다. 또한 HTP 검사를 위한 프로젝트 개요를 다루고 있으며, 이는 주제에 대한 더 포괄적인 정보를 제공하려는 의도를 나타냅니다.\n",
      "해당 페이지는 AI 기반 아동 미술심리 진단 및 그림 데이터 구축을 위한 프로젝트 워크 플로우에 대해 논의하고 있습니다. 주요 내용은 다음과 같습니다:\n",
      "\n",
      "1. 학습 데이터를 수집하고 정제합니다. 이 단계는 고품질의 훈련 세트를 확보하기 위해 필요한 전처리 작업을 포함하며, 여기에는 이미지 수집, 이미지 변환, 원하지 않는 항목 제거 등이 포함됩니다.\n",
      "\n",
      "2. 모델을 학습시키고 평가합니다. 이는 YOLO(You Only Look Once)와 같은 딥러닝 기술을 사용하여 데이터에서 객체를 인식하고 분류하는 것을 말합니다. YOLO 알고리즘의 다양한 버전인 YOLOv1, YOLOv3, YOLOv4 등이 있습니다.\n",
      "\n",
      "3. 결과를 해석하고 분석하여 미술심리 진단과 그림 이해를 위한 통찰력을 도출합니다. 이 과정은 그려진 그림 속 개체(탐지된 객체), 그림의 크기, 위치 등 그림을 구성하는 요소들의 해석을 포함할 수 있습니다.\n",
      "\n",
      "4. AI Hub에서 AI 기반 아동 미술심리 진단 시스템을 구축하여 의료 전문가와 연구원이 아이들의 심리 상태를 이해하고 분석하는데 도움을 줍니다. 이 플랫폼은 이미지 수집 및 분석 과정을 단순화하고 더 나은 의사결정을 가능하게 합니다.\n",
      "\n",
      "5. Ultralytics YOLO를 활용하여 YOLOv1, YOLOv3, YOLOv4 같은 모델을 활용합니다. 이들은 이미지에서 객체를 빠르고 정확하게 감지하는 컴퓨터 비전 기술에 중점을 둔 인기 있는 객체 탐지 알고리즘입니다. 이러한 기술을 사용하면 학습 및 평가 단계를 크게 향상시킬 수 있습니다.\n",
      "\n",
      "프로젝트의 핵심 구성 요소로는 그려진 그림(탐지된 개체), 그림의 상대적 크기, 위치 등이 포함됩니다. 이 정보들은 미술심리 진단에서 필수적이며 아동들의 감정 상태와 심리 상태를 이해하는 데 도움이 됩니다.\n",
      "해당 페이지는 AI 기반 아동 미술심리 진단에 사용될 그림 데이터를 수집한 AI Hub 데이터 세트에 대한 정보를 제공합니다. 데이터에는 다양한 연령과 성별의 아동 7,000명이 제공한 총 4개 HTP(집, 나무, 여자사람, 남자사람) 분류에 해당하는 그림들이 포함되어 있습니다. 데이터는 각 HTP 범주별로 Train (11,200개), Validation (1,400개)으로 구성되어 있으며 Test 셋은 정책상 다운로드할 수 없습니다. 프로젝트 흐름도 제공되며, 이는 데이터 수집 및 준비 과정을 개략적으로 설명합니다.\n",
      "해당 페이지의 주요 내용은 다음과 같습니다:\n",
      "\n",
      "- 주어진 카테고리 목록에는 집, 나무, 사람(남자와 여자)을 포함한 총 15가지 항목이 있습니다. 각 카테고리는 세부적인 부분이나 요소들로 더 세분화되어 있으며, 예를 들어 '집'은 '지붕', '창문', '굴뚝' 등으로 나뉩니다.\n",
      "- 학습 데이터 수집 과정에서 라벨링 작업을 해야 합니다. 이는 이미지 분석에서 중요한 단계로, AI가 이미지의 객체와 맥락을 이해하고 분류할 수 있도록 돕습니다.\n",
      "- 프로젝트 흐름에서는 학습 데이터를 수집하고 라벨링을 한 뒤 다음 단계를 진행할 예정입니다.\n",
      "- 제공된 이미지는 '1. House (집)'라는 텍스트를 포함하고 있으며 'x'와 'y' 좌표를 통해 이미지 내의 특정 위치를 나타냅니다.\n",
      "해당 페이지의 주요 내용은 YOLO(You Only Look Once) 형식의 JSON 데이터를 라벨로 변환하는 과정을 설명하고 있습니다. 원본 JSON 데이터에는 코드, 나이, 성별과 같은 데이터 정보 및 x, y, w, h와 같은 바운딩 박스 좌표, 형태 기술서 등이 포함됩니다. 또한 XC, YC, 넓이 및 높이와 같은 계산된 값들이 제공됩니다. 이 과정은 객체 감지나 분류 작업을 위한 이미지 라벨링에 사용됩니다.\n",
      "해당 페이지의 주요 내용은 HTP 항목별 모델을 구축하는 것과 관련이 있으며, YOLO(You Only Look Once) 모델의 버전인 YOLO11n 학습과 mAP(평균 평균 정밀도) 개념을 언급하고 있습니다. 또한 프로젝트 흐름에서 모델 학습 및 평가에 대해서도 설명하고 있습니다.\n",
      "\n",
      "이미지 설명: 해당 이미지에서 'HTP 항목별 모델 구축'이라는 텍스트가 페이지 상단 좌측 구석에 위치해 있으며, (x: -422.6431579589844, y: -220.075439453125) 좌표와 일치하는 것으로 보입니다.\n",
      "제공된 텍스트의 주요 내용은 객체 검출 성능을 평가하기 위해 사용되는 평균 정밀도(AP)와 그 변형에 대한 것입니다. AP는 검출한 바운딩 박스가 실제 객체와 2/3 이상 겹치면 정답으로 간주하여 계산됩니다. 이는 모델이 얼마나 정확하게 객체를 감지하는지를 평가하는 데 사용됩니다.\n",
      "\n",
      "평균 정밀도의 평균값을 평가하기 위해서는, IoU 임계값(Intersection over Union)을 0.5부터 시작하여 0.95까지 점차 증가시켜 각각의 AP를 계산하고 그 결과들을 평균내어 mAP50, mAP55, ..., mAP95 값을 구합니다. 이 방법은 검출된 바운딩 박스가 얼마나 정확한지를 더 엄격하게 평가하는 것으로, 모델의 정밀도를 나타내는 지표가 됩니다.\n",
      "\n",
      "IoU는 실제 객체와 탐지 영역의 교집합을 합집합으로 나눈 비율입니다. 따라서 IoU 임계값이 높아질수록 검출된 바운딩 박스에 대한 요구되는 정확도가 더 엄격해집니다. 마지막으로, 'mAP50-95'는 모든 mAP 값(예: mAP50, mAP55 등)을 평균내어 가장 높은 IoU 임계값에서 모델의 성능을 평가하는 지표로 사용됩니다.\n",
      "해당 페이지는 객체 감지 및 인식 작업에 대한 성능 향상을 다루고 있습니다. 이 맥락에서 주요 내용은 다음과 같습니다:\n",
      "\n",
      "- THEMA라는 시스템을 사용하여 이미지 크기를 640에서 1280으로 증가시키고 에포크 수를 50에서 30으로 줄임으로써 객체 감지와 인식 작업의 성능이 향상되었습니다.\n",
      "- 성별, 집, 나무와 같은 다양한 범주에서의 의미 있는 평균 픽율 50(mAP50)과 mAP50-95 값이 증가했습니다.\n",
      "- 이미지 크기를 늘림으로써 학습 효율성이 개선된 것으로 나타났으며, 이는 더 큰 이미지에서 학습하는 것이 정확도를 향상시킨다는 것을 보여줍니다.\n",
      "- 에포크 수를 줄임으로써 학습 속도가 빨라졌습니다. 이는 더 적게 많은 학습 주기로 성능이 증가한 것을 의미합니다.\n",
      "\n",
      "해당 페이지에는 이러한 변화가 적용된 후의 향상된 mAP50과 mAP50-95 값을 보여주는 표도 포함되어 있습니다.\n",
      "해당 페이지는 \"Project Flow\" 제목 아래의 성능 향상에 초점을 맞추고 있습니다. 제시된 내용에 따르면 이 프로젝트의 주요 초점은 과정 또는 시스템을 최적화하고 효율화하여 그 성과를 향상시키는 것으로 보입니다. 제공된 이미지 설명은 다양한 위치에 '성능 개선'이라는 텍스트가 포함되어 있음을 보여주며, 이는 전체 문서의 중심 테마를 강조합니다.\n",
      "제공된 텍스트는 AI가 다양한 개체(클래스)의 특징을 인식하고 분석하는 것에 관한 것으로 보입니다. 여기에는 개체의 수, 크기, 전체 이미지와 대비되는 크기가 포함됩니다. 결과 해석은 레퍼런스를 참조하여 수행됩니다. 또한 프로젝트 흐름에 대한 언급이 있습니다.\n",
      "\n",
      "텍스트와 함께 제공된 이미지 설명에는 '특징 추출 및 해석'이라는 문구가 두 번 포함되어 있으며, 해당 텍스트의 위치 좌표가 명시되어 있어 시각 자료에서 이 구절이 어떻게 표시되는지를 보여줍니다.\n",
      "해당 페이지는 그림 속의 다양한 요소들의 해석을 설명하고 있습니다. 결과 해석 예시는 그림 속 집 전체의 하단에 치우친 위치와 연기(smoke)의 존재, 그리고 펜스(fence)가 있음을 보여줍니다. 이는 우울함, 자존감 문제 및 정서불안을 나타냅니다. 또한 사회 불안과 열등감을 표현하고 있습니다. 사람 전반적인 부분의 하단 위치 역시 이러한 감정을 더욱 뒷받침합니다. 단추는 과도하게 크게 나타나 애정 결핍과 퇴행을 시사합니다.\n",
      "해당 페이지는 주로 자동화된 이미지 해석의 맥락에서 기대 효과에 대해 논의하고 있습니다. 주요 내용은 다음과 같습니다:\n",
      "\n",
      "1. 객관적이고 일관된 분석: 이미지 해석은 인간 분석가의 주관성을 줄이고 일관된 결과를 보장하여, 데이터의 보다 정확한 해석을 가능하게 합니다.\n",
      "2. 시간 절약: 이 기술을 통해 수동 프로세스를 간소화하고 데이터 분석 및 해석의 효율성을 높일 수 있어 시간과 노력을 절감할 수 있습니다.\n",
      "3. 데이터 기반 해석: 기계 학습 알고리즘과 같은 자동화된 이미지 해석은 방대한 양의 데이터를 처리하여 의미 있는 인사이트를 도출하고 의사결정을 안내하는데 유용한 정보를 제공합니다.\n",
      "4. 전문가 보완 역할 기대: 이 기술은 전문 지식과 경험을 가진 인간 분석가와 함께 사용되어 상호보완적인 역할을 하며, 결과의 정확성과 신뢰성을 향상시킵니다.\n",
      "5. 서비스 구현: 자동화된 이미지 해석은 다양한 산업과 응용 분야에서 널리 도입되어 효율성 증대와 데이터의 보다 나은 활용을 목표로 하고 있습니다.\n",
      "해당 페이지의 주요 내용은 다음과 같습니다:\n",
      "\n",
      "- YOLO (You Only Look Once) 모델은 HTP 검사 결과를 점수화하기 위한 방법입니다. 이 모델은 Gemini API와 RAG(Red, Amber, Green) 등급을 적용하여 채점 기준별 점수를 부여합니다.\n",
      "- 서비스 구현의 목적은 점수화된 데이터를 자연스럽게 이해하고 해석하기 쉬운 문장으로 변환하는 것입니다. 이를 위해 Gemini API가 활용됩니다.\n",
      "- RAG 등급은 결과의 신뢰도를 기반으로 한 색상 코딩 체계입니다. 이 등급을 적용함으로써 더 신뢰도 높은 심리 평가 문장을 생성할 수 있습니다.\n",
      "- 제공된 이미지는 YOLO 모델과 관련된 키워드를 강조합니다.\n",
      "이 페이지의 주요 내용은 다음과 같습니다:\n",
      "\n",
      "- 서비스를 구현하는 맥락에서, 이미지 업로딩과 해석 화면을 소개하는 소개 페이지입니다.\n",
      "- 사용자는 테마를 선택한 후 이미지를 업로드할 수 있습니다.\n",
      "- 화면에 표시된 이미지의 설명은 '이미지 업로드 및 해석 화면'이라는 문구가 있음을 강조합니다.\n",
      "해당 페이지의 주요 내용은 이미지 업로드를 위한 화면이며, 사용자는 이미지를 탐색하고 해석할 수 있습니다. 이 화면에는 서비스 구현, 모드 설정 옵션(특히 탐지 모드) 및 선택된 옵션을 검토하기 위해 '선택 보기'를 사용하여 탐색 가능한 객체를 선택할 수 있는 기능이 포함되어 있습니다. 또한 이미지 설명이 제공되어 사용자가 시각 자료를 이해하는 데 도움을 주고 있습니다.\n",
      "해당 페이지의 주요 내용은 사용자가 이미지를 업로드하고 해석하는 서비스를 설명하고 있습니다. 서비스의 구현에 대한 언급이 있으며, 전체 내용을 보려면 [모두 보기 모드]를 클릭하라는 안내가 있습니다. 또한 이미지 인식 결과와 해당 좌표 값을 담은 이미지가 포함되어 있어, 페이지 내용에 나타난 텍스트와 일치함을 보여주고 있습니다.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m text = page[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m이 페이지의 주요 내용을 설명해줘. 텍스트 (일부만 제공됨):\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage[\u001b[33m'\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m당신은 PDF 문서를 해석하는 AI입니다.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m page[\u001b[33m\"\u001b[39m\u001b[33mgenerated_explanation\u001b[39m\u001b[33m\"\u001b[39m] = response.content  \u001b[38;5;66;03m# ollama 응답 저장\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(page[\u001b[33m'\u001b[39m\u001b[33mgenerated_explanation\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:285\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    275\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    276\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m     **kwargs: Any,\n\u001b[32m    281\u001b[39m ) -> BaseMessage:\n\u001b[32m    282\u001b[39m     config = ensure_config(config)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    284\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    295\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:861\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    854\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    855\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    858\u001b[39m     **kwargs: Any,\n\u001b[32m    859\u001b[39m ) -> LLMResult:\n\u001b[32m    860\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:691\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    689\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    690\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m         )\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    699\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:926\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    930\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:291\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    269\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    272\u001b[39m     **kwargs: Any,\n\u001b[32m    273\u001b[39m ) -> ChatResult:\n\u001b[32m    274\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[32m    275\u001b[39m \n\u001b[32m    276\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    288\u001b[39m \u001b[33;03m            ])\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    299\u001b[39m         message=AIMessage(content=final_chunk.text),\n\u001b[32m    300\u001b[39m         generation_info=final_chunk.generation_info,\n\u001b[32m    301\u001b[39m     )\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations=[chat_generation])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:222\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    214\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    215\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m     **kwargs: Any,\n\u001b[32m    220\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    221\u001b[39m     final_chunk: Optional[ChatGenerationChunk] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:194\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_chat_stream\u001b[39m(\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    186\u001b[39m     messages: List[BaseMessage],\n\u001b[32m    187\u001b[39m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    188\u001b[39m     **kwargs: Any,\n\u001b[32m    189\u001b[39m ) -> Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    190\u001b[39m     payload = {\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._convert_messages_to_ollama_messages(messages),\n\u001b[32m    193\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:252\u001b[39m, in \u001b[36m_OllamaCommon._create_stream\u001b[39m\u001b[34m(self, api_url, payload, stop, **kwargs)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    247\u001b[39m     request_payload = {\n\u001b[32m    248\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: payload.get(\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    249\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m: payload.get(\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m, []),\n\u001b[32m    250\u001b[39m         **params,\n\u001b[32m    251\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m response.encoding = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\urllib3\\connectionpool.py:493\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\urllib3\\connection.py:445\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers.items():\n\u001b[32m    444\u001b[39m     \u001b[38;5;28mself\u001b[39m.putheader(header, value)\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\http\\client.py:1331\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\http\\client.py:1091\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1089\u001b[39m msg = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._buffer)\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[32m   1096\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1097\u001b[39m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[32m   1098\u001b[39m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[32m   1099\u001b[39m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\http\\client.py:1035\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\urllib3\\connection.py:276\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n\u001b[32m    278\u001b[39m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n\u001b[32m    279\u001b[39m         \u001b[38;5;28mself\u001b[39m._has_connected_to_proxy = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\wanted\\Lang\\lang\\Lib\\site-packages\\urllib3\\util\\connection.py:81\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     79\u001b[39m         err = _\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m             \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\socket.py:501\u001b[39m, in \u001b[36msocket.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_real_close\u001b[39m(\u001b[38;5;28mself\u001b[39m, _ss=_socket.socket):\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[32m    499\u001b[39m     _ss.close(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    502\u001b[39m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._io_refs <= \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for page in page_data:\n",
    "    text = page[\"text\"]\n",
    "    prompt = f\"\"\"\n",
    "    이 페이지의 주요 내용을 설명해줘. 텍스트 (일부만 제공됨):\n",
    "    {text}\n",
    "    \n",
    "    그리고 이미지 설명:\n",
    "    {page['images']}\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.invoke([\n",
    "        SystemMessage(content=\"당신은 PDF 문서를 해석하는 AI입니다.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ])\n",
    "\n",
    "    page[\"generated_explanation\"] = response.content  # ollama 응답 저장\n",
    "    print(page['generated_explanation'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
