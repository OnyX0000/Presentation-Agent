{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# txt 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 필요한 라이브러리 임포트\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "from typing import Optional, List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from lxml import html\n",
    "\n",
    "# 2. HTML 파싱 관련 함수들\n",
    "def parse_book_title(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"책 제목을 파싱\"\"\"\n",
    "    return soup.select_one('h1').text.strip()\n",
    "\n",
    "def parse_toc_links(soup: BeautifulSoup) -> List[Dict[str, str]]:\n",
    "    \"\"\"목차 링크들을 파싱\"\"\"\n",
    "    links = soup.select('div.toc a')\n",
    "    processed_links = []\n",
    "    \n",
    "    for link in links:\n",
    "        href = link.get('href', '')\n",
    "        # JavaScript 함수에서 페이지 번호 추출\n",
    "        if href.startswith('javascript:'):\n",
    "            page_num = re.search(r'page\\((\\d+)\\)', href)\n",
    "            if page_num:\n",
    "                href = f\"/{page_num.group(1)}\"\n",
    "        \n",
    "        processed_links.append({\n",
    "            'text': link.text.strip(),\n",
    "            'href': href\n",
    "        })\n",
    "    \n",
    "    return processed_links\n",
    "\n",
    "def parse_chapter_content(response: requests.Response) -> str:\n",
    "    \"\"\"챕터 내용을 파싱 (XPath 사용)\"\"\"\n",
    "    # HTML 파싱\n",
    "    tree = html.fromstring(response.content)\n",
    "    \n",
    "    # XPath로 본문 내용 추출\n",
    "    main_content = tree.xpath('/html/body/div/div[1]/div[2]/div[5]')\n",
    "    \n",
    "    if not main_content:\n",
    "        return \"\"\n",
    "    \n",
    "    # 본문 내용에서 불필요한 요소 제거\n",
    "    for element in main_content[0].xpath('.//script | .//style | .//iframe | .//noscript'):\n",
    "        element.getparent().remove(element)\n",
    "    \n",
    "    # 텍스트 추출 및 정리\n",
    "    text_content = main_content[0].text_content()\n",
    "    \n",
    "    # 줄바꿈 정리\n",
    "    lines = [line.strip() for line in text_content.split('\\n') if line.strip()]\n",
    "    \n",
    "    # 코드 블록 보존\n",
    "    code_blocks = main_content[0].xpath('.//pre')\n",
    "    for code in code_blocks:\n",
    "        code_text = code.text_content()\n",
    "        if code_text:\n",
    "            # 코드 블록을 ```로 감싸기\n",
    "            code_text = f\"```\\n{code_text}\\n```\"\n",
    "            # 원본 코드 블록을 변환된 텍스트로 교체\n",
    "            code.getparent().text = code_text\n",
    "    \n",
    "    # 이미지 처리\n",
    "    images = main_content[0].xpath('.//img')\n",
    "    for img in images:\n",
    "        alt_text = img.get('alt', '')\n",
    "        img_text = f\"[이미지: {alt_text}]\"\n",
    "        img.getparent().text = img_text\n",
    "    \n",
    "    # 링크 처리\n",
    "    links = main_content[0].xpath('.//a')\n",
    "    for link in links:\n",
    "        href = link.get('href', '')\n",
    "        text = link.text_content().strip()\n",
    "        if href:\n",
    "            link_text = f\"{text} ({href})\"\n",
    "            link.getparent().text = link_text\n",
    "    \n",
    "    # HTML 태그 제거 후 텍스트만 추출\n",
    "    text_content = main_content[0].text_content()\n",
    "    \n",
    "    # 줄바꿈 정리 및 불필요한 공백 제거\n",
    "    lines = []\n",
    "    for line in text_content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and not line.isspace():\n",
    "            lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# 3. HTTP 요청 관련 함수들\n",
    "def create_session() -> requests.Session:\n",
    "    \"\"\"세션 생성\"\"\"\n",
    "    return requests.Session()\n",
    "\n",
    "def get_page_content(session: requests.Session, url: str) -> requests.Response:\n",
    "    \"\"\"페이지 내용을 가져옴\"\"\"\n",
    "    return session.get(url)\n",
    "\n",
    "# 4. 책 데이터 수집 함수들\n",
    "def collect_chapter_data(session: requests.Session, chapter_link: Dict[str, str], pbar: tqdm) -> Dict[str, str]:\n",
    "    \"\"\"개별 챕터 데이터 수집\"\"\"\n",
    "    chapter_url = f\"https://wikidocs.net{chapter_link['href']}\"\n",
    "    response = get_page_content(session, chapter_url)\n",
    "    \n",
    "    # 진행 상태 업데이트\n",
    "    pbar.set_description(f\"크롤링 중: {chapter_link['text'][:30]}...\")\n",
    "    pbar.update(1)\n",
    "    \n",
    "    return {\n",
    "        'title': chapter_link['text'],\n",
    "        'content': parse_chapter_content(response),\n",
    "        'url': chapter_url\n",
    "    }\n",
    "\n",
    "def collect_book_data(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"전체 책 데이터 수집\"\"\"\n",
    "    session = create_session()\n",
    "    response = get_page_content(session, url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # 책 제목과 목차 링크 수집\n",
    "    book_title = parse_book_title(soup)\n",
    "    toc_links = parse_toc_links(soup)\n",
    "    \n",
    "    # tqdm으로 진행 상태 표시\n",
    "    with tqdm(total=len(toc_links), desc=\"챕터 크롤링\", unit=\"챕터\") as pbar:\n",
    "        # 각 챕터 데이터 수집\n",
    "        chapters = []\n",
    "        for link in toc_links:\n",
    "            chapter = collect_chapter_data(session, link, pbar)\n",
    "            chapters.append(chapter)\n",
    "            time.sleep(1)  # 서버 부하 방지\n",
    "    \n",
    "    return {\n",
    "        'title': book_title,\n",
    "        'chapters': chapters,\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "# 5. 파일 저장 관련 함수들\n",
    "def format_book_content(book: Dict[str, Any]) -> str:\n",
    "    \"\"\"책 내용을 텍스트 형식으로 변환\"\"\"\n",
    "    content = [f\"=== {book['title']} ===\\n\"]\n",
    "    \n",
    "    for chapter in book['chapters']:\n",
    "        content.extend([\n",
    "            f\"\\n--- {chapter['title']} ---\\n\",\n",
    "            chapter['content'],\n",
    "            \"\\n\" + \"=\"*50 + \"\\n\"  # 챕터 구분선\n",
    "        ])\n",
    "    \n",
    "    return '\\n'.join(content)\n",
    "\n",
    "def save_book_to_file(content: str, url: str, save_dir: Optional[str] = None) -> str:\n",
    "    \"\"\"책 내용을 파일로 저장\"\"\"\n",
    "    if save_dir is None:\n",
    "        save_dir = os.getcwd()\n",
    "    \n",
    "    book_number = url.split('/')[-1]\n",
    "    filename = f\"wikidocs_book_{book_number}.txt\"\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# 6. 메인 크롤링 함수\n",
    "def crawl_wikidocs_book(url: str, save_dir: Optional[str] = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    위키독스 책의 모든 내용을 크롤링하여 텍스트 파일로 저장\n",
    "    \n",
    "    Args:\n",
    "        url (str): 위키독스 책 URL\n",
    "        save_dir (str, optional): 저장할 디렉토리 경로\n",
    "    \n",
    "    Returns:\n",
    "        Optional[str]: 저장된 파일 경로 또는 에러 발생 시 None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"책 페이지 접속 중: {url}\")\n",
    "        book = collect_book_data(url)\n",
    "        print(f\"책 제목: {book['title']}\")\n",
    "        print(f\"총 {len(book['chapters'])}개의 챕터 발견\")\n",
    "        \n",
    "        print(\"파일 저장 중...\")\n",
    "        content = format_book_content(book)\n",
    "        filepath = save_book_to_file(content, url, save_dir)\n",
    "        \n",
    "        print(f\"\\n저장 완료: {filepath}\")\n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from lxml import html\n",
    "# from reportlab.lib import colors\n",
    "# from reportlab.lib.pagesizes import A4\n",
    "# from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "# from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
    "# from reportlab.pdfbase import pdfmetrics\n",
    "# from reportlab.pdfbase.ttfonts import TTFont\n",
    "# import os\n",
    "# import time\n",
    "# import re\n",
    "# from typing import Dict, List, Optional, Any\n",
    "# from tqdm import tqdm\n",
    "# import io\n",
    "\n",
    "# def clean_html_text(text: str) -> str:\n",
    "#     \"\"\"HTML 태그 제거 및 텍스트 정리\"\"\"\n",
    "#     # HTML 태그 제거\n",
    "#     text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "#     # HTML 엔티티 처리\n",
    "#     text = text.replace('&nbsp;', ' ')\n",
    "#     text = text.replace('&lt;', '<')\n",
    "#     text = text.replace('&gt;', '>')\n",
    "#     text = text.replace('&amp;', '&')\n",
    "#     text = text.replace('&quot;', '\"')\n",
    "#     text = text.replace('&apos;', \"'\")\n",
    "    \n",
    "#     # 불필요한 공백 제거\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     text = text.strip()\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# def download_image(session: requests.Session, img_url: str) -> Optional[bytes]:\n",
    "#     \"\"\"이미지 다운로드\"\"\"\n",
    "#     try:\n",
    "#         response = session.get(img_url)\n",
    "#         if response.status_code == 200:\n",
    "#             return response.content\n",
    "#     except Exception as e:\n",
    "#         print(f\"이미지 다운로드 실패: {str(e)}\")\n",
    "#     return None\n",
    "\n",
    "# def parse_book_title(response: requests.Response) -> str:\n",
    "#     \"\"\"책 제목 파싱\"\"\"\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     title = soup.find('h1', class_='book_title')\n",
    "#     return title.text.strip() if title else \"제목 없음\"\n",
    "\n",
    "# def parse_toc_links(response: requests.Response) -> List[Dict[str, str]]:\n",
    "#     \"\"\"목차 링크 파싱\"\"\"\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     toc = soup.find('div', class_='toc')\n",
    "#     if not toc:\n",
    "#         return []\n",
    "    \n",
    "#     links = []\n",
    "#     for link in toc.find_all('a'):\n",
    "#         href = link.get('href', '')\n",
    "#         # JavaScript URL 처리\n",
    "#         if href.startswith('javascript:'):\n",
    "#             # page(숫자) 형식에서 숫자 추출\n",
    "#             page_num = re.search(r'page\\((\\d+)\\)', href)\n",
    "#             if page_num:\n",
    "#                 href = f\"/page/{page_num.group(1)}\"\n",
    "        \n",
    "#         links.append({\n",
    "#             'text': link.text.strip(),\n",
    "#             'href': href\n",
    "#         })\n",
    "#     return links\n",
    "\n",
    "# def parse_chapter_content(response: requests.Response, session: requests.Session) -> Dict[str, Any]:\n",
    "#     \"\"\"챕터 내용을 파싱 (XPath 사용)\"\"\"\n",
    "#     tree = html.fromstring(response.content)\n",
    "#     main_content = tree.xpath('/html/body/div/div[1]/div[2]/div[5]')\n",
    "    \n",
    "#     if not main_content:\n",
    "#         return {\"text\": \"\", \"images\": []}\n",
    "    \n",
    "#     # 불필요한 요소 제거\n",
    "#     for element in main_content[0].xpath('.//script | .//style | .//iframe | .//noscript'):\n",
    "#         element.getparent().remove(element)\n",
    "    \n",
    "#     # 텍스트 추출 및 정리\n",
    "#     text_content = main_content[0].text_content()\n",
    "#     cleaned_text = clean_html_text(text_content)\n",
    "#     lines = [line.strip() for line in cleaned_text.split('\\n') if line.strip()]\n",
    "    \n",
    "#     # 이미지 처리\n",
    "#     images = []\n",
    "#     img_elements = main_content[0].xpath('.//img')\n",
    "#     for img in img_elements:\n",
    "#         img_url = img.get('src', '')\n",
    "#         if img_url:\n",
    "#             if not img_url.startswith('http'):\n",
    "#                 img_url = f\"https://wikidocs.net{img_url}\"\n",
    "            \n",
    "#             img_data = download_image(session, img_url)\n",
    "#             if img_data:\n",
    "#                 alt_text = img.get('alt', '')\n",
    "#                 images.append({\n",
    "#                     'data': img_data,\n",
    "#                     'alt': alt_text\n",
    "#                 })\n",
    "    \n",
    "#     # 코드 블록 보존\n",
    "#     code_blocks = main_content[0].xpath('.//pre')\n",
    "#     for code in code_blocks:\n",
    "#         code_text = code.text_content()\n",
    "#         if code_text:\n",
    "#             code_text = f\"```\\n{code_text}\\n```\"\n",
    "#             code.getparent().text = code_text\n",
    "    \n",
    "#     # 링크 처리\n",
    "#     links = main_content[0].xpath('.//a')\n",
    "#     for link in links:\n",
    "#         href = link.get('href', '')\n",
    "#         text = link.text_content().strip()\n",
    "#         if href:\n",
    "#             link_text = f\"{text} ({href})\"\n",
    "#             link.getparent().text = link_text\n",
    "    \n",
    "#     return {\n",
    "#         \"text\": '\\n'.join(lines),\n",
    "#         \"images\": images\n",
    "#     }\n",
    "\n",
    "# def register_fonts() -> bool:\n",
    "#     \"\"\"한글 폰트 등록\"\"\"\n",
    "#     try:\n",
    "#         # Windows의 경우\n",
    "#         if os.name == 'nt':\n",
    "#             pdfmetrics.registerFont(TTFont('MalgunGothic', 'C:/Windows/Fonts/malgun.ttf'))\n",
    "#             pdfmetrics.registerFont(TTFont('MalgunGothicBold', 'C:/Windows/Fonts/malgunbd.ttf'))\n",
    "#             return True\n",
    "#         # macOS의 경우\n",
    "#         elif os.name == 'posix':\n",
    "#             pdfmetrics.registerFont(TTFont('AppleGothic', '/System/Library/Fonts/AppleGothic.ttf'))\n",
    "#             return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"폰트 등록 실패: {str(e)}\")\n",
    "#     return False\n",
    "\n",
    "# def create_pdf_styles() -> Dict[str, ParagraphStyle]:\n",
    "#     \"\"\"PDF 스타일 생성\"\"\"\n",
    "#     styles = getSampleStyleSheet()\n",
    "    \n",
    "#     # 한글 폰트 사용 가능 여부 확인\n",
    "#     has_korean_font = register_fonts()\n",
    "#     default_font = 'MalgunGothic' if has_korean_font else 'Helvetica'\n",
    "#     default_bold_font = 'MalgunGothicBold' if has_korean_font else 'Helvetica-Bold'\n",
    "    \n",
    "#     # 책 제목 스타일\n",
    "#     styles.add(ParagraphStyle(\n",
    "#         name='BookTitle',\n",
    "#         parent=styles['Heading1'],\n",
    "#         fontSize=24,\n",
    "#         spaceAfter=30,\n",
    "#         fontName=default_bold_font\n",
    "#     ))\n",
    "    \n",
    "#     # 챕터 제목 스타일\n",
    "#     styles.add(ParagraphStyle(\n",
    "#         name='WikiChapterTitle',\n",
    "#         parent=styles['Heading2'],\n",
    "#         fontSize=18,\n",
    "#         spaceAfter=20,\n",
    "#         fontName=default_bold_font\n",
    "#     ))\n",
    "    \n",
    "#     # 본문 스타일\n",
    "#     styles.add(ParagraphStyle(\n",
    "#         name='WikiBodyText',\n",
    "#         parent=styles['Normal'],\n",
    "#         fontSize=12,\n",
    "#         leading=18,\n",
    "#         fontName=default_font\n",
    "#     ))\n",
    "    \n",
    "#     # 코드 블록 스타일\n",
    "#     styles.add(ParagraphStyle(\n",
    "#         name='WikiCodeBlock',\n",
    "#         parent=styles['Normal'],\n",
    "#         fontSize=10,\n",
    "#         leading=14,\n",
    "#         fontName='Courier',\n",
    "#         textColor=colors.darkblue,\n",
    "#         backColor=colors.lightgrey,\n",
    "#         borderWidth=1,\n",
    "#         borderColor=colors.grey,\n",
    "#         borderPadding=5\n",
    "#     ))\n",
    "    \n",
    "#     return styles\n",
    "\n",
    "# def create_pdf_content(book: Dict[str, Any], styles) -> List:\n",
    "#     \"\"\"PDF 내용 생성\"\"\"\n",
    "#     content = []\n",
    "    \n",
    "#     # 책 제목\n",
    "#     title = clean_html_text(book['title'])\n",
    "#     content.append(Paragraph(title, styles['BookTitle']))\n",
    "#     content.append(Spacer(1, 30))\n",
    "    \n",
    "#     # 각 챕터\n",
    "#     for chapter in book['chapters']:\n",
    "#         # 챕터 제목\n",
    "#         chapter_title = clean_html_text(chapter['title'])\n",
    "#         content.append(Paragraph(chapter_title, styles['WikiChapterTitle']))\n",
    "#         content.append(Spacer(1, 10))\n",
    "        \n",
    "#         # 본문 내용\n",
    "#         paragraphs = chapter['content'].split('\\n')\n",
    "#         for para in paragraphs:\n",
    "#             if para.strip():\n",
    "#                 if para.startswith('```'):\n",
    "#                     # 코드 블록\n",
    "#                     content.append(Paragraph(para, styles['WikiCodeBlock']))\n",
    "#                 else:\n",
    "#                     # 일반 텍스트\n",
    "#                     cleaned_para = clean_html_text(para)\n",
    "#                     if cleaned_para:\n",
    "#                         content.append(Paragraph(cleaned_para, styles['WikiBodyText']))\n",
    "        \n",
    "#         # 이미지 추가\n",
    "#         for img in chapter['images']:\n",
    "#             try:\n",
    "#                 # 이미지 데이터를 ReportLab Image 객체로 변환\n",
    "#                 img_data = io.BytesIO(img['data'])\n",
    "#                 img_width = 400  # 이미지 너비 설정\n",
    "#                 img_height = 300  # 이미지 높이 설정\n",
    "#                 img = Image(img_data, width=img_width, height=img_height)\n",
    "#                 content.append(img)\n",
    "#                 content.append(Spacer(1, 10))\n",
    "                \n",
    "#                 # 이미지 설명 추가\n",
    "#                 if img['alt']:\n",
    "#                     content.append(Paragraph(f\"[이미지 설명: {img['alt']}]\", styles['WikiBodyText']))\n",
    "#                     content.append(Spacer(1, 10))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"이미지 처리 실패: {str(e)}\")\n",
    "        \n",
    "#         content.append(Spacer(1, 20))\n",
    "    \n",
    "#     return content\n",
    "\n",
    "# def save_to_pdf(book: Dict[str, Any], save_dir: str):\n",
    "#     \"\"\"책 내용을 PDF로 저장\"\"\"\n",
    "#     # URL에서 책 번호 추출\n",
    "#     book_number = book['url'].split('/')[-1]\n",
    "#     pdf_path = os.path.join(save_dir, f'wikidocs_{book_number}.pdf')\n",
    "    \n",
    "#     # PDF 생성\n",
    "#     doc = SimpleDocTemplate(\n",
    "#         pdf_path,\n",
    "#         pagesize=A4,\n",
    "#         rightMargin=72,\n",
    "#         leftMargin=72,\n",
    "#         topMargin=72,\n",
    "#         bottomMargin=72\n",
    "#     )\n",
    "    \n",
    "#     # 스타일 생성\n",
    "#     styles = create_pdf_styles()\n",
    "    \n",
    "#     # 내용 생성\n",
    "#     content = create_pdf_content(book, styles)\n",
    "    \n",
    "#     # PDF 저장\n",
    "#     doc.build(content)\n",
    "#     print(f\"PDF 저장 완료: {pdf_path}\")\n",
    "\n",
    "# def create_session() -> requests.Session:\n",
    "#     \"\"\"세션 생성\"\"\"\n",
    "#     session = requests.Session()\n",
    "#     session.headers.update({\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "#     })\n",
    "#     return session\n",
    "\n",
    "# def get_page_content(session: requests.Session, url: str) -> requests.Response:\n",
    "#     \"\"\"페이지 내용 가져오기\"\"\"\n",
    "#     try:\n",
    "#         response = session.get(url)\n",
    "#         response.raise_for_status()\n",
    "#         time.sleep(1)  # 서버 부하 방지\n",
    "#         return response\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"페이지 요청 실패: {str(e)}\")\n",
    "#         raise\n",
    "\n",
    "# def collect_chapter_data(session: requests.Session, chapter_link: Dict[str, str], pbar: tqdm) -> Dict[str, Any]:\n",
    "#     \"\"\"개별 챕터 데이터 수집\"\"\"\n",
    "#     # URL 구성 수정\n",
    "#     if chapter_link['href'].startswith('/page/'):\n",
    "#         chapter_url = f\"https://wikidocs.net{chapter_link['href']}\"\n",
    "#     else:\n",
    "#         chapter_url = f\"https://wikidocs.net{chapter_link['href']}\"\n",
    "    \n",
    "#     response = get_page_content(session, chapter_url)\n",
    "    \n",
    "#     pbar.set_description(f\"크롤링 중: {chapter_link['text'][:30]}...\")\n",
    "#     pbar.update(1)\n",
    "    \n",
    "#     content = parse_chapter_content(response, session)\n",
    "    \n",
    "#     return {\n",
    "#         'title': chapter_link['text'],\n",
    "#         'content': content['text'],\n",
    "#         'images': content['images'],\n",
    "#         'url': chapter_url\n",
    "#     }\n",
    "\n",
    "# def crawl_wikidocs_book(url: str, save_dir: str):\n",
    "#     \"\"\"위키독스 책 크롤링 및 PDF 저장\"\"\"\n",
    "#     try:\n",
    "#         # 저장 디렉토리 생성\n",
    "#         os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "#         # 세션 생성\n",
    "#         session = create_session()\n",
    "        \n",
    "#         # 메인 페이지 요청\n",
    "#         response = get_page_content(session, url)\n",
    "        \n",
    "#         # 책 정보 수집\n",
    "#         book = {\n",
    "#             'title': parse_book_title(response),\n",
    "#             'url': url,\n",
    "#             'chapters': []\n",
    "#         }\n",
    "        \n",
    "#         # 목차 링크 수집\n",
    "#         toc_links = parse_toc_links(response)\n",
    "        \n",
    "#         # 진행률 표시\n",
    "#         with tqdm(total=len(toc_links), desc=\"챕터 크롤링\") as pbar:\n",
    "#             # 각 챕터 데이터 수집\n",
    "#             for chapter_link in toc_links:\n",
    "#                 chapter_data = collect_chapter_data(session, chapter_link, pbar)\n",
    "#                 book['chapters'].append(chapter_data)\n",
    "        \n",
    "#         # PDF 저장\n",
    "#         save_to_pdf(book, save_dir)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"크롤링 실패: {str(e)}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "크롤링 중: 01. StreamEvent 타입별 정리...: 100%|██████████| 184/184 [04:24<00:00,  1.44s/it]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 저장 완료: ../../data/pdf\\wikidocs_14314.pdf\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 결과 저장 경로\n",
    "save_dir = r\"../../data/txt\"\n",
    "\n",
    "# 첫번째 책 크롤링\n",
    "url1 = \"https://wikidocs.net/book/14314\"  # 첫 번째 책\n",
    "filepath = crawl_wikidocs_book(url1, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "책 페이지 접속 중: https://wikidocs.net/book/2155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "크롤링 중: 24. 교육 문의...: 100%|██████████| 179/179 [03:24<00:00,  1.14s/챕터]                    ]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "책 제목: 딥 러닝을 이용한 자연어 처리 입문\n",
      "총 179개의 챕터 발견\n",
      "파일 저장 중...\n",
      "\n",
      "저장 완료: ../../data/txt\\wikidocs_book_2155.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 결과 저장 경로\n",
    "save_dir = r\"../../data/txt\"\n",
    "\n",
    "# 두번째 책 크롤링\n",
    "url2 = \"https://wikidocs.net/book/2155\"  # 두 번째 책\n",
    "filepath2 = crawl_wikidocs_book(url2, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "책 페이지 접속 중: https://wikidocs.net/book/2788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "크롤링 중: 구버전) IMDB 리뷰 감성 분류하기(IMDB Movi...: 100%|██████████| 132/132 [02:27<00:00,  1.12s/챕터]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "책 제목: 딥 러닝 파이토치 교과서 - 입문부터 파인튜닝까지\n",
      "총 132개의 챕터 발견\n",
      "파일 저장 중...\n",
      "\n",
      "저장 완료: ../../data/txt\\wikidocs_book_2788.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 결과 저장 경로\n",
    "save_dir = r\"../../data/txt\"\n",
    "\n",
    "# 세번째 책 크롤링\n",
    "url3 = \"https://wikidocs.net/book/2788\"  # 세 번째 책\n",
    "filepath3 = crawl_wikidocs_book(url3, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
