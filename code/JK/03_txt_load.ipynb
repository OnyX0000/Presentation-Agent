{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_chapter_meta(chapter_num: str, chapter_title: str) -> dict:\n",
    "    \"\"\"\n",
    "    chapter_num이 '01' → level 1, parent 없음\n",
    "    chapter_num이 '01-02' → level 2, parent '01'\n",
    "    \"\"\"\n",
    "    if \"-\" in chapter_num:\n",
    "        parent = chapter_num.split(\"-\")[0]\n",
    "        level = 2\n",
    "    else:\n",
    "        parent = None\n",
    "        level = 1\n",
    "    return {\n",
    "        \"chapter\": chapter_num,\n",
    "        \"title\": chapter_title,\n",
    "        \"level\": level,\n",
    "        \"parent\": parent\n",
    "    }\n",
    "\n",
    "def split_by_chapter(text: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Wikidocs 형식 텍스트를 챕터별로 나누고 계층 정보를 포함한 Document로 변환\n",
    "    \"\"\"\n",
    "    pattern = r\"(?=^---\\s+(\\d{2}(?:-\\d{2})?)\\.\\s+(.*?)\\s+---)\"\n",
    "    matches = list(re.finditer(pattern, text, flags=re.MULTILINE))\n",
    "\n",
    "    documents = []\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "        chapter_num = match.group(1)\n",
    "        chapter_title = match.group(2).strip()\n",
    "        chapter_text = text[start:end].strip()\n",
    "\n",
    "        metadata = extract_chapter_meta(chapter_num, chapter_title)\n",
    "\n",
    "        doc = Document(page_content=chapter_text, metadata=metadata)\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def load_and_split_wikidocs(path: str) -> List[Document]:\n",
    "    text = load_txt(path)\n",
    "    return split_by_chapter(text)\n",
    "\n",
    "def filter_chapters(\n",
    "    documents: List[Document],\n",
    "    level: Optional[int] = None,\n",
    "    parent: Optional[str] = None,\n",
    "    contains_title: Optional[str] = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    챕터 리스트에서 조건에 맞는 챕터만 필터링\n",
    "    - level: 1 (대챕터), 2 (소챕터)\n",
    "    - parent: '01' 등 상위 챕터 번호\n",
    "    - contains_title: 제목 키워드 포함 여부\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for doc in documents:\n",
    "        meta = doc.metadata\n",
    "        if level and meta[\"level\"] != level:\n",
    "            continue\n",
    "        if parent and meta[\"parent\"] != parent:\n",
    "            continue\n",
    "        if contains_title and contains_title not in meta[\"title\"]:\n",
    "            continue\n",
    "        filtered.append(doc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 총 문서 수: 165\n",
      "🔹 소챕터 수: 0\n",
      "🔹 02번 챕터 하위 수: 0\n",
      "🔹 '설치' 포함 제목 수: 1\n"
     ]
    }
   ],
   "source": [
    "first_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt\")\n",
    "\n",
    "# 소챕터만 추출 (level 2)\n",
    "subchapters = filter_chapters(first_docs, level=2)\n",
    "\n",
    "# 대챕터 02의 모든 소챕터 추출\n",
    "chap02_children = filter_chapters(first_docs, parent=\"02\")\n",
    "\n",
    "# \"설치\"라는 단어를 포함하는 챕터만 추출\n",
    "install_sections = filter_chapters(first_docs, contains_title=\"설치\")\n",
    "\n",
    "print(f\"📌 총 문서 수: {len(first_docs)}\")\n",
    "print(f\"🔹 소챕터 수: {len(subchapters)}\")\n",
    "print(f\"🔹 02번 챕터 하위 수: {len(chap02_children)}\")\n",
    "print(f\"🔹 '설치' 포함 제목 수: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 총 문서 수: 27\n",
      "🔹 소챕터 수: 0\n",
      "🔹 02번 챕터 하위 수: 0\n",
      "🔹 '설치' 포함 제목 수: 0\n"
     ]
    }
   ],
   "source": [
    "second_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_02.txt\")\n",
    "\n",
    "# 소챕터만 추출 (level 2)\n",
    "subchapters = filter_chapters(second_docs, level=2)\n",
    "\n",
    "# 대챕터 02의 모든 소챕터 추출\n",
    "chap02_children = filter_chapters(second_docs, parent=\"02\")\n",
    "\n",
    "# \"설치\"라는 단어를 포함하는 챕터만 추출\n",
    "install_sections = filter_chapters(second_docs, contains_title=\"설치\")\n",
    "\n",
    "print(f\"📌 총 문서 수: {len(second_docs)}\")\n",
    "print(f\"🔹 소챕터 수: {len(subchapters)}\")\n",
    "print(f\"🔹 02번 챕터 하위 수: {len(chap02_children)}\")\n",
    "print(f\"🔹 '설치' 포함 제목 수: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 총 문서 수: 33\n",
      "🔹 소챕터 수: 4\n",
      "🔹 02번 챕터 하위 수: 0\n",
      "🔹 '설치' 포함 제목 수: 0\n"
     ]
    }
   ],
   "source": [
    "third_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_03.txt\")\n",
    "\n",
    "# 소챕터만 추출 (level 2)\n",
    "subchapters = filter_chapters(third_docs, level=2)\n",
    "\n",
    "# 대챕터 02의 모든 소챕터 추출\n",
    "chap02_children = filter_chapters(third_docs, parent=\"02\")\n",
    "\n",
    "# \"설치\"라는 단어를 포함하는 챕터만 추출\n",
    "install_sections = filter_chapters(third_docs, contains_title=\"설치\")\n",
    "\n",
    "print(f\"📌 총 문서 수: {len(third_docs)}\")\n",
    "print(f\"🔹 소챕터 수: {len(subchapters)}\")\n",
    "print(f\"🔹 02번 챕터 하위 수: {len(chap02_children)}\")\n",
    "print(f\"🔹 '설치' 포함 제목 수: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "27\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(first_docs))\n",
    "print(len(second_docs))\n",
    "print(len(third_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문서 청크 수: 4746\n",
      "두 번째 문서 청크 수: 1983\n",
      "세 번째 문서 청크 수: 2964\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 초기화 (하나의 스플리터만 사용)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # 청크 크기\n",
    "    chunk_overlap=50,  # 청크 간 중복\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # 분할 기준\n",
    ")\n",
    "\n",
    "first_splits = []\n",
    "second_splits = []\n",
    "third_splits = []\n",
    "\n",
    "for doc in first_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    first_splits.extend(splits)\n",
    "    # print(first_splits)\n",
    "\n",
    "for doc in second_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    second_splits.extend(splits)\n",
    "    # print(second_splits)\n",
    "\n",
    "for doc in third_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    third_splits.extend(splits)\n",
    "    # print(third_splits)\n",
    "\n",
    "print(f\"첫 번째 문서 청크 수: {len(first_splits)}\")\n",
    "print(f\"두 번째 문서 청크 수: {len(second_splits)}\")\n",
    "print(f\"세 번째 문서 청크 수: {len(third_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4640\\1310670779.py:30: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  first_db.persist()\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수에서 API 키 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings()  # OpenAI 임베딩 모델 지정\n",
    "\n",
    "# DB 디렉토리 존재 여부 확인 및 삭제\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\")\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\")\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\")\n",
    "\n",
    "# 첫 번째 문서 DB 생성\n",
    "first_db = Chroma.from_texts(\n",
    "    texts=first_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\"\n",
    ")\n",
    "first_db.persist()\n",
    "\n",
    "# # 두 번째 문서 DB 생성\n",
    "# second_db = Chroma.from_texts(\n",
    "#     texts=second_splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\"\n",
    "# )\n",
    "# second_db.persist()\n",
    "\n",
    "# # 세 번째 문서 DB 생성\n",
    "# third_db = Chroma.from_texts(\n",
    "#     texts=third_splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\"\n",
    "# )\n",
    "# third_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# 원하는 Pandas DataFrame을 정의합니다.\\ndf = pd.read_csv(\"./data/titanic.csv\")\\ndf.head()'),\n",
       " Document(metadata={}, page_content='PassengerId\\nSurvived\\nPclass\\nName\\nSex\\nAge\\nSibSp\\nParch\\nTicket\\nFare\\nCabin\\nEmbarked\\n1\\n0\\n3\\nBraund, Mr. Owen Harris\\nmale\\n22\\n1\\n0\\nA/5 21171\\n7.25\\nS\\n2\\n1\\n1\\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\\nfemale\\n38\\n1\\n0\\nPC 17599\\n71.2833\\nC85\\nC\\n3\\n1\\n3\\nHeikkinen, Miss. Laina\\nfemale\\nDataFrameLoader\\nPandas는 Python 프로그래밍 언어를 위한 오픈 소스 데이터 분석 및 조작 도구입니다. 이 라이브러리는 데이터 과학, 머신러닝, 그리고 다양한 분야의 데이터 작업에 널리 사용되고 있습니다.\\nimport pandas as pd\\n# CSV 파일 읽기\\ndf = pd.read_csv(\"./data/titanic.csv\")\\n첫 5개 행을 조회합니다.'),\n",
       " Document(metadata={}, page_content=\"연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링\\n판다스 (Pandas)\\nMetadata: {'source': './data/appendix-keywords.txt', 'id': 10, 'relevance_score': 0.9997084}\"),\n",
       " Document(metadata={}, page_content='# !pip install -qU langchain-teddynote\\nfrom langchain_teddynote import logging\\n# 프로젝트 이름을 입력합니다.\\nlogging.langsmith(\"CH15-Agent-Toolkits\")\\nLangSmith 추적을 시작합니다.\\n[프로젝트명]\\nCH15-Agent-Toolkits\\nimport pandas as pd\\ndf = pd.read_csv(\"./data/titanic.csv\")  # CSV 파일을 읽습니다.\\n# df = pd.read_excel(\"./data/titanic.xlsx\") # 엑셀 파일도 읽을 수 있습니다.\\ndf.head()\\n[이미지: ]\\nfrom langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\\nfrom langchain.agents.agent_types import AgentType'),\n",
       " Document(metadata={}, page_content=\"[이미지: ]\\n[도구 호출]\\nTool: python_repl_ast\\nquery: import pandas as pd\\nimport matplotlib.pyplot as plt\\n# 남자와 여자 승객의 생존율 계산\\nsurvival_rate = df.groupby('Sex')['Survived'].mean()\\n# barplot 시각화\\nsurvival_rate.plot(kind='bar', color=['blue', 'pink'])\\nplt.title('Survival Rate by Gender')\\nplt.xlabel('Gender')\\nplt.ylabel('Survival Rate')\\nplt.xticks(rotation=0)\\nplt.show()\\nLog:\"),\n",
       " Document(metadata={}, page_content='# !pip install -qU langchain-teddynote\\nfrom langchain_teddynote import logging\\n# 프로젝트 이름을 입력합니다.\\nlogging.langsmith(\"CH16-Evaluations\")\\nLangSmith 추적을 시작합니다.\\n[프로젝트명]\\nCH16-Evaluations\\n저장한 CSV 파일로부터 로드\\ndata/ragas_synthetic_dataset.csv 파일을 로드합니다.\\nimport pandas as pd\\ndf = pd.read_csv(\"data/ragas_synthetic_dataset.csv\")\\ndf.head()\\n[이미지: ]\\nfrom datasets import Dataset\\ntest_dataset = Dataset.from_pandas(df)\\ntest_dataset'),\n",
       " Document(metadata={}, page_content='아래 코드는 업로드한 HuggingFace Dataset 을 활용하는 예시입니다.\\n(참고) 아래의 주석을 풀고 실행하여 datasets 라이브러리를 업데이트 후 진행해 주세요.\\n# !pip install -qU datasets\\nimport pandas as pd\\nfrom datasets import load_dataset, Dataset\\nimport os\\n# huggingface Dataset에서 repo_id로 데이터셋 다운로드\\ndataset = load_dataset(\\n\"teddylee777/rag-synthetic-dataset\",  # 데이터셋 이름\\ntoken=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # private 데이터인 경우 필요합니다.\\n)\\n# 데이터셋에서 split 기준으로 조회\\nhuggingface_df = dataset[\"korean_v1\"].to_pandas()\\nhuggingface_df.head()\\n[이미지: ]'),\n",
       " Document(metadata={}, page_content='----------------------------------------------------------------------------------------------------\\nDocument 6:\\n판다스 (Pandas)\\n정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.\\n예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리\\nGPT (Generative Pretrained Transformer)\\n정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수 있습니다.\\n예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.'),\n",
       " Document(metadata={}, page_content='```\\n22.19937\\n```PandasDataFrameOutputParser\\nPandas DataFrame은 Python 프로그래밍 언어에서 널리 사용되는 데이터 구조로, 데이터 조작 및 분석을 위해 흔히 사용됩니다. 구조화된 데이터를 다루기 위한 포괄적인 도구 세트를 제공하여, 데이터 정제, 변환 및 분석과 같은 작업에 다양하게 활용될 수 있습니다.\\n이 출력 파서는 사용자가 임의의 Pandas DataFrame을 지정하고 해당 DataFrame에서 데이터를 추출하여 형식화된 사전 형태로 데이터를 조회할 수 있는 LLM을 요청할 수 있게 해줍니다.\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nTrue\\n# LangSmith 추적을 설정합니다. https://smith.langchain.com\\n# !pip install langchain-teddynote\\nfrom langchain_teddynote import logging'),\n",
       " Document(metadata={}, page_content='Document 7:\\n판다스 (Pandas)\\n정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.\\n예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리\\nGPT (Generative Pretrained Transformer)\\n정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수 있습니다.\\n예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝\\nInstructGPT')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10}).invoke('import pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from typing import List\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def load_txt(path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     텍스트 파일을 UTF-8로 로드\n",
    "#     \"\"\"\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return f.read()\n",
    "\n",
    "# def extract_chapter_meta(chapter_num: str, chapter_title: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     개선된 버전: 제목 내 [카테고리] → parent\n",
    "#     \"\"\"\n",
    "#     # [카테고리]가 존재하면 parent로 추정\n",
    "#     category_match = re.match(r\"\\[(.*?)\\]\", chapter_title)\n",
    "#     if category_match:\n",
    "#         parent = category_match.group(1)\n",
    "#     else:\n",
    "#         parent = None\n",
    "\n",
    "#     return {\n",
    "#         \"chapter\": chapter_num,\n",
    "#         \"title\": chapter_title,\n",
    "#         \"level\": 1,  # 실제 파일에는 대챕터만 존재\n",
    "#         \"parent\": parent\n",
    "#     }\n",
    "\n",
    "# def protect_code_blocks(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     ```로 감싸진 코드 블록을 <CODE_BLOCK>으로 감싸 보존\n",
    "#     \"\"\"\n",
    "#     code_pattern = re.compile(r\"```.*?\\n.*?```\", re.DOTALL)\n",
    "#     protected = []\n",
    "#     last_end = 0\n",
    "\n",
    "#     for match in code_pattern.finditer(text):\n",
    "#         start, end = match.span()\n",
    "#         protected.append(text[last_end:start])\n",
    "#         code = match.group()\n",
    "#         protected.append(f\"\\n<CODE_BLOCK>\\n{code}\\n</CODE_BLOCK>\\n\")\n",
    "#         last_end = end\n",
    "\n",
    "#     protected.append(text[last_end:])\n",
    "#     return \"\".join(protected)\n",
    "\n",
    "# def split_by_chapter_with_code(text: str) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     챕터 헤더(--- 01. 제목 ---) 기준으로 분할하되, 각 챕터 내 코드블록 보존\n",
    "#     \"\"\"\n",
    "#     pattern = r\"(?=^---\\s+(\\d{2}(?:-\\d{2})?)\\.\\s+(.*?)\\s+---)\"\n",
    "#     matches = list(re.finditer(pattern, text, flags=re.MULTILINE))\n",
    "\n",
    "#     documents = []\n",
    "#     for i, match in enumerate(matches):\n",
    "#         start = match.start()\n",
    "#         end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "#         chapter_num = match.group(1)\n",
    "#         chapter_title = match.group(2).strip()\n",
    "#         chapter_text = text[start:end].strip()\n",
    "\n",
    "#         protected_text = protect_code_blocks(chapter_text)\n",
    "#         metadata = extract_chapter_meta(chapter_num, chapter_title)\n",
    "\n",
    "#         documents.append(Document(page_content=protected_text, metadata=metadata))\n",
    "\n",
    "#     return documents\n",
    "\n",
    "# def process_wikidocs_files(paths: List[str]) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     여러 Wikidocs 형식의 텍스트 파일을 처리하여 Document 리스트로 반환\n",
    "#     \"\"\"\n",
    "#     all_docs = []\n",
    "#     for path in paths:\n",
    "#         text = load_txt(path)\n",
    "#         docs = split_by_chapter_with_code(text)\n",
    "#         all_docs.extend(docs)\n",
    "#     return all_docs\n",
    "\n",
    "# def extract_and_replace_code_blocks(text: str):\n",
    "#     \"\"\"\n",
    "#     <CODE_BLOCK>...</CODE_BLOCK> 구간을 [[CODE:0]], [[CODE:1]], ...로 치환\n",
    "#     \"\"\"\n",
    "#     code_blocks = []\n",
    "#     pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "\n",
    "#     def replacer(match):\n",
    "#         code_blocks.append(match.group())\n",
    "#         return f\"[[CODE:{len(code_blocks) - 1}]]\"\n",
    "\n",
    "#     modified_text = pattern.sub(replacer, text)\n",
    "#     return modified_text, code_blocks\n",
    "\n",
    "# def split_protected_chunks(docs: List[Document], chunk_size=1000, chunk_overlap=200) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     코드 블록이 잘리지 않도록 보호한 상태로 chunk 분할\n",
    "#     \"\"\"\n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap\n",
    "#     )\n",
    "\n",
    "#     chunked_docs = []\n",
    "#     for doc in docs:\n",
    "#         # 코드 블록을 임시 토큰으로 치환\n",
    "#         mod_text, code_blocks = extract_and_replace_code_blocks(doc.page_content)\n",
    "#         temp_doc = Document(page_content=mod_text, metadata=doc.metadata)\n",
    "\n",
    "#         # 분할\n",
    "#         split_docs = splitter.split_documents([temp_doc])\n",
    "\n",
    "#         # 코드 블록 복원\n",
    "#         for d in split_docs:\n",
    "#             restored_text = d.page_content\n",
    "#             for i, block in enumerate(code_blocks):\n",
    "#                 restored_text = restored_text.replace(f\"[[CODE:{i}]]\", block)\n",
    "#             d.page_content = restored_text\n",
    "#             chunked_docs.append(d)\n",
    "\n",
    "#     return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def protect_code_blocks(text: str) -> str:\n",
    "    code_pattern = re.compile(r\"```.*?\\n.*?```\", re.DOTALL)\n",
    "    protected = []\n",
    "    last_end = 0\n",
    "    for match in code_pattern.finditer(text):\n",
    "        start, end = match.span()\n",
    "        protected.append(text[last_end:start])\n",
    "        code = match.group()\n",
    "        protected.append(f\"\\n<CODE_BLOCK>\\n{code}\\n</CODE_BLOCK>\\n\")\n",
    "        last_end = end\n",
    "    protected.append(text[last_end:])\n",
    "    return \"\".join(protected)\n",
    "\n",
    "def split_by_structure_with_code(text: str) -> List[Document]:\n",
    "    lines = text.splitlines()\n",
    "    documents = []\n",
    "    buffer = []\n",
    "\n",
    "    current_chapter_num = None\n",
    "    current_chapter_title = None\n",
    "    current_section_num = None\n",
    "    current_section_title = None\n",
    "\n",
    "    chapter_pattern = re.compile(r\"^---\\s+CH(\\d+)\\s+(.*?)\\s+---$\")\n",
    "    section_pattern = re.compile(r\"^---\\s+(\\d{2})\\.\\s+(.*?)\\s+---$\")\n",
    "\n",
    "    for line in lines:\n",
    "        chapter_match = chapter_pattern.match(line)\n",
    "        section_match = section_pattern.match(line)\n",
    "\n",
    "        if chapter_match:\n",
    "            # flush 이전 buffer\n",
    "            if buffer:\n",
    "                chunk = \"\\n\".join(buffer).strip()\n",
    "                protected = protect_code_blocks(chunk)\n",
    "                documents.append(Document(\n",
    "                    page_content=protected,\n",
    "                    metadata={\n",
    "                        \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                        \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "                    }\n",
    "                ))\n",
    "                buffer = []\n",
    "\n",
    "            current_chapter_num = chapter_match.group(1)\n",
    "            current_chapter_title = chapter_match.group(2).strip()\n",
    "            current_section_num, current_section_title = None, None\n",
    "\n",
    "        elif section_match:\n",
    "            if buffer:\n",
    "                chunk = \"\\n\".join(buffer).strip()\n",
    "                protected = protect_code_blocks(chunk)\n",
    "                documents.append(Document(\n",
    "                    page_content=protected,\n",
    "                    metadata={\n",
    "                        \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                        \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "                    }\n",
    "                ))\n",
    "                buffer = []\n",
    "\n",
    "            current_section_num = section_match.group(1)\n",
    "            current_section_title = section_match.group(2).strip()\n",
    "\n",
    "        buffer.append(line)\n",
    "\n",
    "    if buffer:\n",
    "        chunk = \"\\n\".join(buffer).strip()\n",
    "        protected = protect_code_blocks(chunk)\n",
    "        documents.append(Document(\n",
    "            page_content=protected,\n",
    "            metadata={\n",
    "                \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def process_wikidocs_files(paths: List[str]) -> List[Document]:\n",
    "    all_docs = []\n",
    "    for path in paths:\n",
    "        text = load_txt(path)\n",
    "        docs = split_by_structure_with_code(text)\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    "def extract_and_replace_code_blocks(text: str):\n",
    "    code_blocks = []\n",
    "    pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "    def replacer(match):\n",
    "        code_blocks.append(match.group())\n",
    "        return f\"[[CODE:{len(code_blocks) - 1}]]\"\n",
    "    modified_text = pattern.sub(replacer, text)\n",
    "    return modified_text, code_blocks\n",
    "\n",
    "def split_protected_chunks(docs: List[Document], chunk_size=2000, chunk_overlap=50) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunked_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        mod_text, code_blocks = extract_and_replace_code_blocks(doc.page_content)\n",
    "        temp_doc = Document(page_content=mod_text, metadata=doc.metadata)\n",
    "        split_docs = splitter.split_documents([temp_doc])\n",
    "\n",
    "        for d in split_docs:\n",
    "            # 실제로 사용된 [[CODE:X]]만 찾아서 복원\n",
    "            used_codes = re.findall(r\"\\[\\[CODE:(\\d+)\\]\\]\", d.page_content)\n",
    "            for code_index in set(used_codes):\n",
    "                code_index = int(code_index)\n",
    "                if code_index < len(code_blocks):\n",
    "                    d.page_content = d.page_content.replace(f\"[[CODE:{code_index}]]\", code_blocks[code_index])\n",
    "            chunked_docs.append(d)\n",
    "\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 원본 챕터 수: 184\n",
      "총 분할된 문서 수 (임베딩용): 1241\n",
      "--- 샘플 ---\n",
      "--- CH01 LangChain 시작하기 ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Wikidocs 텍스트 파일들을 챕터 단위로 로딩 (설명 + 코드 보존)\n",
    "docs = process_wikidocs_files([\n",
    "    \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt\",\n",
    "    # \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_02.txt\",\n",
    "    # \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_03.txt\"\n",
    "])\n",
    "\n",
    "# 2. 임베딩 전용 chunk 단위로 분할 (코드블록 보호됨)\n",
    "chunked_docs = split_protected_chunks(docs)\n",
    "\n",
    "# 3. 확인\n",
    "print(f\"총 원본 챕터 수: {len(docs)}\")\n",
    "print(f\"총 분할된 문서 수 (임베딩용): {len(chunked_docs)}\")\n",
    "print(\"--- 샘플 ---\")\n",
    "print(chunked_docs[1].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document 객체에서 텍스트 내용만 추출\n",
    "text_contents = [doc.page_content for doc in chunked_docs]\n",
    "\n",
    "test_db = Chroma.from_texts(\n",
    "    texts=text_contents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\test_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='--- 03. LangChain Hub ---'),\n",
       " Document(metadata={}, page_content='=== <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 ===\\n\\n=================================================='),\n",
       " Document(metadata={}, page_content='--- CH01 LangChain 시작하기 ---'),\n",
       " Document(metadata={}, page_content='--- 05. 대화내용을 기억하는 RAG 체인 ---'),\n",
       " Document(metadata={}, page_content='--- 08. LCEL Chain 에 메모리 추가 ---')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}).invoke('랭체인')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 메타데이터:\n",
      "  chapter_info: None\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH01 LangChain 시작하기\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH01 LangChain 시작하기\n",
      "  section_info: 01. 설치 영상보고 따라하기\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH01 LangChain 시작하기\n",
      "  section_info: 02. OpenAI API 키 발급 및 테스트\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH01 LangChain 시작하기\n",
      "  section_info: 03. LangSmith 추적 설정\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH01 LangChain 시작하기\n",
      "  section_info: 04. OpenAI API 사용(GPT-4o 멀티모달)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH01 LangChain 시작하기\n",
      "  section_info: 05. LangChain Expression Language(LCEL)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH01 LangChain 시작하기\n",
      "  section_info: 06. LCEL 인터페이스\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH01 LangChain 시작하기\n",
      "  section_info: 07. Runnable\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH02 프롬프트(Prompt)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH02 프롬프트(Prompt)\n",
      "  section_info: 01. 프롬프트(Prompt)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH02 프롬프트(Prompt)\n",
      "  section_info: 02. 퓨샷 프롬프트(FewShotPromptTemplate)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH02 프롬프트(Prompt)\n",
      "  section_info: 03. LangChain Hub\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH02 프롬프트(Prompt)\n",
      "  section_info: 04. 개인화된 프롬프트(Hub에 업로드)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: 01. Pydantic 출력 파서(PydanticOutputParser)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: 02. 콤마 구분자 출력 파서(CommaSeparatedListOutputParser)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: 03. 구조화된 출력 파서(StructuredOuputParser)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: 04. JSON 출력 파서(JsonOutputParser)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: 05. 데이터프레임 출력 파서(PandasDataFrameOutputParser)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: 06. 날짜 형식 출력 파서(DatetimeOutputParser)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: 07. 열거형 출력 파서(EnumOutputParser)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH03 출력 파서(Output Parsers)\n",
      "  section_info: 08. 출력 수정 파서(OutputFixingParser)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 01. 다양한 LLM 모델 활용\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 02. 캐싱(Cache)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 03. 모델 직렬화(Serialization) - 저장 및 불러오기\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 04. 토큰 사용량 확인\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 05. 구글 생성 AI(Google Generative AI)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 06. 허깅페이스 엔드포인트(HuggingFace Endpoints)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 07. 허깅페이스 로컬(HuggingFace Local)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 08. 허깅페이스 파이프라인(HuggingFace Pipeline)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 09. 올라마(Ollama)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 10. GPT4ALL\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH04 모델(Model)\n",
      "  section_info: 11. 비디오(Video) 질의 응답 LLM (Gemini)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 01. 대화 버퍼 메모리(ConversationBufferMemory)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 02. 대화 버퍼 윈도우 메모리(ConversationBufferWindowMemory)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 04. 대화 엔티티 메모리(ConversationEntityMemory)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 05. 대화 지식그래프 메모리(ConversationKGMemory)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 06. 대화 요약 메모리(ConversationSummaryMemory)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 07. 벡터저장소 검색 메모리(VectorStoreRetrieverMemory)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 08. LCEL Chain 에 메모리 추가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 09. SQLite 에 대화내용 저장\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH05 메모리(Memory)\n",
      "  section_info: 10. RunnableWithMessageHistory에 ChatMessageHistory추가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 01. 도큐먼트(Document) 의 구조\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 02. PDF\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 03. 한글(HWP)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 04. CSV\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 05. Excel\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 06. Word\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 07. PowerPoint\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 08. 웹 문서(WebBaseLoader)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 09. 텍스트(TextLoader)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 10. JSON\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 11. Arxiv\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 12. UpstageLayoutAnalysisLoader\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH06 문서 로더(Document Loader)\n",
      "  section_info: 13. LlamaParser\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: 01. 문자 텍스트 분할(CharacterTextSplitter)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: 02. 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: 03. 토큰 텍스트 분할(TokenTextSplitter)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: 04. 시멘틱 청커(SemanticChunker)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: 05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: 06. 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: 07. HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH07 텍스트 분할(Text Splitter)\n",
      "  section_info: 08. 재귀적 JSON 분할(RecursiveJsonSplitter)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH08 임베딩(Embedding)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH08 임베딩(Embedding)\n",
      "  section_info: 01. OpenAIEmbeddings\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH08 임베딩(Embedding)\n",
      "  section_info: 02. 캐시 임베딩(CacheBackedEmbeddings)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH08 임베딩(Embedding)\n",
      "  section_info: 03. 허깅페이스 임베딩(HuggingFace Embeddings)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH08 임베딩(Embedding)\n",
      "  section_info: 04. UpstageEmbeddings\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH08 임베딩(Embedding)\n",
      "  section_info: 05. OllamaEmbeddings\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH08 임베딩(Embedding)\n",
      "  section_info: 06. GPT4ALL 임베딩\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH08 임베딩(Embedding)\n",
      "  section_info: 07. Llama CPP 임베딩\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH09 벡터저장소(VectorStore)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH09 벡터저장소(VectorStore)\n",
      "  section_info: 01. Chroma\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH09 벡터저장소(VectorStore)\n",
      "  section_info: 02. FAISS\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH09 벡터저장소(VectorStore)\n",
      "  section_info: 03. Pinecone\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 01. 벡터스토어 기반 검색기(VectorStore-backed Retriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 02. 문맥 압축 검색기(ContextualCompressionRetriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 03. 앙상블 검색기(EnsembleRetriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 04. 긴 문맥 재정렬(LongContextReorder)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 05. 상위 문서 검색기(ParentDocumentRetriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 06. 다중 쿼리 검색기(MultiQueryRetriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 07. 다중 벡터저장소 검색기(MultiVectorRetriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 08. 셀프 쿼리 검색기(SelfQueryRetriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 09. 시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH10 검색기(Retriever)\n",
      "  section_info: 11. Convex Combination(CC) 적용된 앙상블 검색기(EnsembleRetriever)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH11 리랭커(Reranker)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH11 리랭커(Reranker)\n",
      "  section_info: 01. Cross Encoder Reranker\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH11 리랭커(Reranker)\n",
      "  section_info: 02. Cohere Reranker\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH11 리랭커(Reranker)\n",
      "  section_info: 03. Jina Reranker\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH11 리랭커(Reranker)\n",
      "  section_info: 04. FlashRank Reranker\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 01. PDF 문서 기반 QA(Question-Answer)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 02. 네이버 뉴스기사 QA(Question-Answer)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 03. RAG 의 기능별 다양한 모듈 활용기\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 04. RAPTOR: 긴 문맥 요약(Long Context Summary)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 05. 대화내용을 기억하는 RAG 체인\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 01. RunnablePassthrough\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 02. Runnable 구조(그래프) 검토\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 03. RunnableLambda\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 04. LLM 체인 라우팅(RunnableLambda, RunnableBranch)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 05. RunnableParallel\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 06. 동적 속성 지정(configurable_fields, configurable_alternatives)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 07. @chain 데코레이터로 Runnable 구성\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 08. RunnableWithMessageHistory\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 09. 사용자 정의 제네레이터(generator)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 10. Runtime Arguments 바인딩\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 11. 폴백(fallback) 모델 지정\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH14 체인(Chains)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH14 체인(Chains)\n",
      "  section_info: 01. 문서 요약\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH14 체인(Chains)\n",
      "  section_info: 02. SQL\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH14 체인(Chains)\n",
      "  section_info: 03. 구조화된 출력 체인(with_structered_output)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 01. 합성 테스트 데이터셋 생성(RAGAS)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 02. RAGAS 를 활용한 평가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 03. 생성한 평가용 데이터셋 업로드(HuggingFace Dataset)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 04. LangSmith 데이터셋 생성\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 05. LLM-as-Judge\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 06. 임베딩 기반 평가(embedding_distance)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 07. 사용자 정의(Custom) LLM 평가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 08. Rouge, BLEU, METEOR, SemScore 기반 휴리스틱 평가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 09. 실험(Experiment) 평가 비교\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 10. 요약(Summary) 방식의 평가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 11. Groundedness(할루시네이션) 평가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 12. 실험 비교(Pairwise Evaluation)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 13. 반복 평가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH15 평가(Evaluations)\n",
      "  section_info: 14. 온라인 평가를 활용한 평가 자동화\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 01. 도구(Tools)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 02. 도구 바인딩(Binding Tools)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 03. 에이전트(Agent)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 04. Claude, Gemini, Ollama, Together.ai 를 활용한 Agent\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 05. Iteration 기능과 사람 개입(Human-in-the-loop)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 06. Agentic RAG\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 07. CSVExcel 데이터 분석 Agent\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 08. Toolkits 활용 Agent\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 09. RAG + Image Generator Agent(보고서 작성)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH16 에이전트(Agent)\n",
      "  section_info: 10. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. 핵심 기능\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. LangGraph 에 자주 등장하는 Python 문법이해\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. LangGraph를 활용한 챗봇 구축\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. LangGraph를 활용한 Agent 구축\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. Agent 에 메모리(memory) 추가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. 노드의 단계별 스트리밍 출력\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. Human-in-the-loop(사람의 개입)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. 중간단계 개입  되돌림을 통한 상태 수정과 Replay\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 08. 사람(Human)에게 물어보는 노드 추가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 09. 메시지 삭제(RemoveMessage)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 10. ToolNode 를 사용하여 도구를 호출하는 방법\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 11. 병렬 노드 실행을 위한 분기 생성 방법\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 12. 대화 기록 요약을 추가하는 방법\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 13. 서브그래프 추가 및 사용 방법\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 14. 서브그래프의 입력과 출력을 변환하는 방법\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 15. LangGraph 스트리밍 모드의 모든 것\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. 구조 설계\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. 기본 그래프 생성\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. Naive RAG\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. 관련성 체커(Relevance Checker) 모듈 추가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. 웹 검색 모듈 추가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. 쿼리 재작성 모듈 추가\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. Agentic RAG\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. Adaptive RAG\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. Use Cases\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. 에이전트 대화 시뮬레이션 (고객 응대 시나리오)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. 사용자 요구사항 기반 메타 프롬프트 생성 에이전트\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. CRAG(Corrective RAG)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. Self-RAG\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. 계획 후 실행(Plan-and-Execute)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. 멀티 에이전트 협업 네트워크(Multi-Agent Collaboration Network)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. 멀티 에이전트 감독자(Multi-Agent Supervisor)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams)\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 09. SQL 데이터베이스와 상호작용하는 에이전트\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 10. STORM 개념을 도입한 연구를 위한 멀티 에이전트\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH18 기타 정보\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "문서 메타데이터:\n",
      "  chapter_info: CH18 기타 정보\n",
      "  section_info: 01. StreamEvent 타입별 정리\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(f\"문서 메타데이터:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        if key == 'level':\n",
    "            print(f\"  {'  ' * (int(value) - 1)}└─ {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# def extract_code_blocks_from_documents(docs: List[Document]) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     각 Document 객체 내의 <CODE_BLOCK>...</CODE_BLOCK> 구간만 추출하여 리스트로 반환\n",
    "#     \"\"\"\n",
    "#     code_blocks = []\n",
    "#     pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "\n",
    "#     for doc in docs:\n",
    "#         matches = pattern.findall(doc.page_content)\n",
    "#         code_blocks.extend(matches)\n",
    "\n",
    "#     return code_blocks\n",
    "\n",
    "# chunked_docs = split_protected_chunks(docs)\n",
    "\n",
    "# code_blocks = extract_code_blocks_from_documents(chunked_docs)\n",
    "\n",
    "# # 출력 확인\n",
    "# for i, code in enumerate(code_blocks[100:150]):\n",
    "#     print(f\"🔹 Code Block {i + 1}:\\n{code}\\n{'-'*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 답변:\n",
      " 랭체인(LangChain)은 언어 모델 기반 애플리케이션을 개발하기 위한 프레임워크입니다. 이 프레임워크는 다양한 데이터 소스와 통합하여 복잡한 작업을 처리할 수 있도록 도와주며, 언어 모델의 기능을 맞춤 설정할 수 있는 도구와 컴포넌트를 제공합니다. 랭체인은 대화형 AI, 데이터 처리, 정보 검색 등 다양한 분야에서 활용될 수 있습니다.\n",
      "\n",
      "📎 출처 문서:\n",
      "🔹 정보 없음 > 정보 없음\n",
      "=== <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 ===\n",
      "\n",
      "================================================== \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      "--- 05. 대화내용을 기억하는 RAG 체인 --- \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      ")\n",
      "# 체인 생성\n",
      "chain = prompt | llm | StrOutputParser()\n",
      "# 질의 실행\n",
      "response = chain.invoke({\"question\": \"대한민국의 수도는 어디인가요?\"})\n",
      "대한민국(South Korea)의 수도는 서울입니다.\n",
      "서울은 약 1000만 명의 인구를 가진 대도시로, 한반도 북부에 위치해 있습니다. 세계에서 가장 큰 도시 중 하나로 간주되며 문화, 경제 및 정치 중심지 역할을 하고 있습니다. 도시의 역사는 삼한 시대까지 거슬러 올라가며 이후 백제, 고려 그리고 조선 시대에 중요한 지역으로 \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      "문서: data/SPRI_AI_Brief_2023년12월호_F.pdf (페이지 10)\n",
      "LangSmith: https://smith.langchain.com/public/4449e744-f0a0-42d2-a3df-855bd7f41652/r\n",
      "# 단계 8: 체인 실행(Run Chain)\n",
      "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
      "question = \"삼성 가우스에 대해 설명해주세요\"\n",
      "response = rag_chain.invoke(question)\n",
      "print(response)\n",
      "삼성 가우스는 삼성전자가 개발한 생성 AI 모델로 \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      "<CODE_BLOCK>\n",
      "```\n",
      "- 빌 게이츠는 5년 내에 일상 언어로 모든 작업을 처리할 수 있는 AI 에이전트가 보급될 것이라고 전망하며, 이러한 AI 에이전트가 컴퓨터 사용 방식을 완전히 변화시키고 소프트웨어 산업에도 큰 영향을 미칠 것으로 예상하고 있다. AI 에이전트의 도입은 의료, 교육, 생산성, 엔터테인먼트 및 쇼핑 등 다양한 산업 분야에서 고가의 서비스가 대중화되는 계기가 될 것이다.\n",
      "- 유튜브는 2024년부터 AI가 생성한 콘텐츠에 대한 표시를 의무화할 계획이다.\n",
      "- 영국 과학혁신기술부는 AI 안전 연구소를 설립한다고 \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      "<CODE_BLOCK>\n",
      "```\n",
      "🌌🔬✨ 양자역학: 미시 세계의 신비를 탐구하다!\n",
      "양자역학은 원자와 아원자 입자들의 놀라운 현상을 설명하는 이론이에요. 🧪💫 고전역학과는 다르게, 입자의 위치와 운동량을 동시에 정확히 알 수 없다는 점이 매력적이죠! 🤔🔍\n",
      "여기 몇 가지 핵심 개념을 소개할게요:\n",
      "- 파동-입자 이중성 🌊➡️⚛️\n",
      "- 불확정성 원리 ❓🔄\n",
      "- 양자 얽힘 🔗💖\n",
      "이 모든 것이 현대 물리학과 기술의 기초가 된답니다! 🌍💡 양자 세계의 신비를 함께 탐험해봐요! 🚀🔭 #양자역학 #물리학 #과학의미래 #신비로운세계\n",
      "```\n",
      "</CODE_BL \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      "검색 📚\n",
      "'데이터 강화 생성'에 초점을 맞춘 이 모듈은 생성 단계에서 필요한 데이터를 외부 데이터 소스에서 가져오는 작업을 담당합니다.\n",
      "에이전트 🤖\n",
      "언어 모델이 어떤 조치를 취할지 결정하고, 해당 조치를 실행하며, 관찰하고, 필요한 경우 반복하는 과정을 포함합니다.\n",
      "LangChain을 활용하면, 언어 모델 기반 애플리케이션의 개발을 보다 쉽게 시작할 수 있으며, 필요에 맞게 기능을 맞춤 설정하고, 다양한 데이터 소스와 통합하여 복잡한 작업을 처리할 수 있게 됩니다. \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      "--- 08. LCEL Chain 에 메모리 추가 --- \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      "--- CH15 평가(Evaluations) ---\n",
      "\n",
      "평가 (Evaluations)\n",
      "LLM(Large Language Model) 평가는 인공지능 언어 모델의 성능, 정확성, 일관성 및 기타 중요한 측면을 측정하고 분석하는 과정입니다. 이는 모델의 개선, 비교, 선택 및 응용 프로그램에 적합한 모델 결정에 필수적인 단계입니다.\n",
      "평가 방법\n",
      "LLM 평가는 다양한 방법으로 수행될 수 있으며, 주요 접근 방식은 다음과 같습니다:\n",
      "자동화된 메트릭: BLEU, ROUGE, METEOR, SemScore 등의 지표를 사용합니다.\n",
      "인간 평가: 전 \n",
      "---\n",
      "\n",
      "🔹 정보 없음 > 정보 없음\n",
      "- 광주시: 17살 될 때까지 7400만원 지급\n",
      "LangSmith Trace 보기 (https://smith.langchain.com/public/1a613ee7-6eaa-482f-a45f-8c22b4e60fbf/r)LangSmith Trace 보기\n",
      "answer = rag_chain.stream(\"부영그룹의 임직원 숫자는 몇명인가요?\")\n",
      "stream_response(answer)\n",
      "주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다. \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. 로컬 LLM (gemma:3.12b)\n",
    "# llm = Ollama(model=\"gemma3:12b\")\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# 4. 메타데이터 필드 정의 (사용 필드만)\n",
    "metadata_field_info = [\n",
    "    {\"name\": \"chapter_info\", \"type\": \"string\"},\n",
    "    {\"name\": \"section_info\", \"type\": \"string\"},\n",
    "]\n",
    "\n",
    "# 5. ParentDocumentRetriever 구성\n",
    "retriever = test_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# 6. QA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 7. 질문 실행\n",
    "query = \"랭체인이 뭔지 설명해\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# 8. 출력\n",
    "print(\"🧠 답변:\\n\", result[\"result\"])\n",
    "print(\"\\n📎 출처 문서:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    meta = doc.metadata\n",
    "    print(f\"🔹 {meta.get('chapter_info', '정보 없음')} > {meta.get('section_info', '정보 없음')}\")\n",
    "    print(doc.page_content[:300], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    # 파일을 로드합니다.\n",
    "    TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\wikidocs_01.txt\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    # 로더를 사용하여 문서를 로드하고 docs 리스트에 추가합니다.\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자식 분할기를 생성합니다.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# DB를 생성합니다.\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"test_01\", embedding_function=HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Retriever 를 생성합니다.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 검색기에 추가합니다. docs는 문서 목록이고, ids는 문서의 고유 식별자 목록입니다.\n",
    "retriever.add_documents(docs, ids=None, add_to_docstore=True)\n",
    "\n",
    "# 유사도 검색을 수행합니다.\n",
    "sub_docs = vectorstore.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5ca58a77-98d8-4874-a6b7-bf0e4c74bef0', metadata={'doc_id': '7738ffb4-e36b-43e2-8aa0-b00d465827ae', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='1. **Introduction to LangChain and LCEL**: LangChain offers a way to build complex chains from basic components, supporting functionalities like streaming, parallelism, and logging. LCEL (LangChain Expression Language) simplifies the process of chaining together prompts, models, and output parsers to perform tasks like generating jokes based on a given topic or conducting retrieval-augmented generation.'),\n",
       " Document(id='b20744d0-77c4-404e-a72b-d399c3c12604', metadata={'doc_id': '7738ffb4-e36b-43e2-8aa0-b00d465827ae', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='\"langchain-kr/README.md at main · teddylee777/langchain-kr - GitHub\", \"url\": \"https://github.com/teddylee777/langchain-kr/blob/main/README.md\", \"content\": \"📘 LangChain 한국어 튜토리얼 🌟 LangChain 공식 Document, Cookbook, 그 밖의 실용 예제 를 바탕으로 작성한 한국어 튜토리얼입니다.'),\n",
       " Document(id='c64ee76b-0eff-479f-af01-82abe735b61c', metadata={'doc_id': '7738ffb4-e36b-43e2-8aa0-b00d465827ae', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='```\\n[Query] LangChain 에 대해서 알려주세요.\\n====================================\\n[0] 유사도: 399.644 | LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\\n[1] 유사도: 356.518 | 랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다.\\n[2] 유사도: 322.359 | LangChain simplifies the process of building applications with large language models\\n[3] 유사도: 321.078 | 안녕, 만나서 반가워.'),\n",
       " Document(id='989b1a40-741e-4001-99da-8f7874b47a5d', metadata={'doc_id': '7738ffb4-e36b-43e2-8aa0-b00d465827ae', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='```Document & Document Loaders\\n참고\\nLangChain 에서 사용되는 주요 로더 (https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)LangChain 에서 사용되는 주요 로더\\nLangChain 에서 사용되는 로더 목록 (https://python.langchain.com/v0.1/docs/integrations/document_loaders/)LangChain 에서 사용되는 로더 목록\\n실습에 활용한 문서\\n소프트웨어정책연구소(SPRi) - 2023년 12월호\\n저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\\nhttps://spri.kr/posts/view/23669 (https://spri.kr/posts/view/23669)https://spri.kr/posts/view/23669')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Introduction to LangChain and LCEL**: LangChain offers a way to build complex chains from basic components, supporting functionalities like streaming, parallelism, and logging. LCEL (LangChain Expression Language) simplifies the process of chaining together prompts, models, and output parsers to perform tasks like generating jokes based on a given topic or conducting retrieval-augmented generation.\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 길이: 1800480\n",
      "\n",
      "=====================\n",
      "\n",
      "는 모듈식으로 설계되어, 사용하기 쉽습니다. 이는 개발자가 LangChain 프레임워크를 자유롭게 활용할 수 있게 해줍니다.\n",
      "즉시 사용 가능한 체인 🚀\n",
      "고수준 작업을 수행하기 위한 컴포넌트의 내장 조합을 제공합니다.\n",
      "이러한 체인은 개발 과정을 간소화하고 속도를 높여줍니다.\n",
      "주요 모듈 📌\n",
      "모델 I/O 📃\n",
      "프롬프트 관리, 최적화 및 LLM과의 일반적인 인터페이스와 작업을 위한 유틸리티를 포함합니다.\n",
      "검색 📚\n",
      "'데이터 강화 생성'에 초점을 맞춘 이 모듈은 생성 단계에서 필요한 데이터를 외부 데이터 소스에서 가져오는 작업을 담당합니다.\n",
      "에이전트 🤖\n",
      "언어 모델이 어떤 조치를 취할지 결정하고, 해당 조치를 실행하며, 관찰하고, 필요한 경우 반복하는 과정을 포함합니다.\n",
      "LangChain을 활용하면, 언어 모델 기반 애플리케이션의 개발을 보다 쉽게 시작할 수 있으며, 필요에 맞게 기능을 맞춤 설정하고, 다양한 데이터 소스와 통합하여 복잡한 작업을 처리할 수 있게 됩니다.\n",
      "\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Langchain\")\n",
    "\n",
    "# 검색된 문서의 문서의 페이지 내용의 길이를 출력합니다.\n",
    "print(\n",
    "    f\"문서의 길이: {len(retrieved_docs[0].page_content)}\",\n",
    "    end=\"\\n\\n=====================\\n\\n\",\n",
    ")\n",
    "\n",
    "# 문서의 일부를 출력합니다.\n",
    "print(retrieved_docs[0].page_content[2000:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 부모 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 250, separators=['=================================================='])\n",
    "# 자식 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\n",
    "# 부모보다 작은 문서를 생성해야 합니다.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 100)\n",
    "# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소입니다.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    ")\n",
    "# 부모 문서의 저장 계층입니다.\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    # 벡터 저장소를 지정합니다.\n",
    "    vectorstore=vectorstore,\n",
    "    # 문서 저장소를 지정합니다.\n",
    "    docstore=store,\n",
    "    # 하위 문서 분할기를 지정합니다.\n",
    "    child_splitter=child_splitter,\n",
    "    # 상위 문서 분할기를 지정합니다.\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 ===\n"
     ]
    }
   ],
   "source": [
    "retriever.add_documents(docs)\n",
    "\n",
    "# 유사도 검색을 수행합니다.\n",
    "sub_docs = vectorstore.similarity_search(\"Langchain\")\n",
    "\n",
    "# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 ===\n"
     ]
    }
   ],
   "source": [
    "# 문서를 검색하여 가져옵니다.\n",
    "retrieved_docs = retriever.invoke(\"Langchain\")\n",
    "\n",
    "# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 ===\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- 04. RAPTOR: 긴 문맥 요약(Long Context Summary) ---\n",
      "\n",
      "```\n",
      "Self-querying 방법은 자연어 쿼리를 받아 구조화된 쿼리를 작성하고, 이를 기반으로 VectorStore에 적용하여 문서의 의미적 유사성 비교 및 사용자 쿼리에서 추출한 필터를 메타데이터에 적용하여 실행하는 방식입니다. 예를 들어, Chroma vector store를 사용하여 영화 요약 문서가 포함된 작은 데모 세트를 생성하고, 이를 통해 자체 쿼리 검색기를 인스턴스화할 수 있습니다. 다음은 자체 쿼리 검색기를 사용하는 예시 코드입니다:\n",
      "```python\n",
      "%pip install --upgrade --quiet  lark chromadb\n",
      "from langchain_community.vectorstores import Chroma\n",
      "from langchain_core.documents import Document\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "docs = [\n",
      "Document(\n",
      "page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
      "metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
      "),\n",
      "# Additional documents...\n",
      "]\n",
      "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n",
      "from langchain.chains.query_constructor.base import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "from langchain_openai import ChatOpenAI\n",
      "metadata_field_info = [\n",
      "AttributeInfo(\n",
      "name=\"genre\",\n",
      "description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
      "type=\"string\",\n",
      "),\n",
      "# Additional metadata fields...\n",
      "]\n",
      "document_content_description = \"Brief summary of a movie\"\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "retriever = SelfQueryRetriever.from_llm(\n",
      "llm,\n",
      "vectorstore,\n",
      "document_content_description,\n",
      "metadata_field_info,\n",
      ")\n",
      "# example usage\n",
      "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")\n",
      "```설치\n",
      "pip install -qU langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic\n",
      "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "RAPTOR (https://arxiv.org/pdf/2401.18059.pdf)RAPTOR 논문은 문서의 색인 생성 및 검색에 대한 흥미로운 접근 방식을 제시합니다.\n",
      "테디노트 논문 요약글(노션) (https://teddylee777.notion.site/RAPTOR-e835d306fc664dc2ad76191dee1cd859?pvs=4)테디노트 논문 요약글(노션)\n",
      "leafs는 시작 문서 집합입니다.\n",
      "leafs는 임베딩되어 클러스터링됩니다.\n",
      "그런 다음 클러스터는 유사한 문서들 간의 정보를 더 높은 수준(더 추상적인)으로 요약합니다.\n",
      "이 과정은 재귀적으로 수행되어, 원본 문서(leafs)에서 더 추상적인 요약으로 이어지는 \"트리\"를 형성합니다.\n",
      "이를 다양한 규모에서 적용할 수 있습니다; leafs는 다음과 같을 수 있습니다:\n",
      "단일 문서에서의 텍스트 청크(논문에서 보여준 것처럼)\n",
      "전체 문서(아래에서 보여주는 것처럼)\n",
      "더 긴 컨텍스트의 LLMs를 사용하면, 전체 문서에 대해 이 작업을 수행할 수 있습니다.\n",
      "문서\n",
      "LangChain의 LCEL 문서에 이를 적용해 봅시다.\n",
      "이 경우, 각 doc은 LCEL 문서의 고유한 웹 페이지입니다.\n",
      "콘텍스트는 2,000 토큰 미만에서 10,000 토큰 이상까지 다양합니다.\n",
      "웹 문서에서 텍스트 데이터를 추출하고, 텍스트의 토큰 수를 계산하여 히스토그램으로 시각화하는 과정을 설명합니다.\n",
      "tiktoken 라이브러리를 사용하여 주어진 인코딩 이름에 따라 문자열의 토큰 수를 계산합니다.\n",
      "RecursiveUrlLoader 클래스를 사용하여 지정된 URL에서 웹 문서를 재귀적으로 로드합니다. 이 과정에서 BeautifulSoup를 활용하여 HTML 문서에서 텍스트를 추출합니다.\n",
      "여러 URL에서 문서를 로드하여 모든 텍스트 데이터를 하나의 리스트에 모읍니다.\n",
      "각 문서 텍스트에 대해 num_tokens_from_string 함수를 호출하여 토큰 수를 계산하고, 이를 리스트에 저장합니다.\n",
      "matplotlib를 사용하여 계산된 토큰 수의 분포를 히스토그램으로 시각화합니다. 히스토그램은 토큰 수를 x축에, 해당 토큰 수를 가진 문서의 빈도수를 y축에 나타냅니다.\n",
      "히스토그램은 데이터의 분포를 이해하는 데 도움을 주며, 특히 텍스트 데이터의 길이 분포를 시각적으로 파악할 수 있습니다.\n",
      "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
      "from bs4 import BeautifulSoup as Soup\n",
      "import tiktoken\n",
      "import matplotlib.pyplot as plt\n",
      "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
      "# 주어진 문자열에서 토큰의 개수를 반환합니다.\n",
      "encoding = tiktoken.get_encoding(encoding_name)\n",
      "num_tokens = len(encoding.encode(string))\n",
      "return num_tokens\n",
      "# LCEL 문서 로드\n",
      "url = \"https://python.langchain.com/docs/expression_language/\"\n",
      "loader = RecursiveUrlLoader(\n",
      "url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
      ")\n",
      "docs = loader.load()\n",
      "# PydanticOutputParser를 사용한 LCEL 문서 로드 (기본 LCEL 문서 외부)\n",
      "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
      "loader = RecursiveUrlLoader(\n",
      "url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
      ")\n",
      "docs_pydantic = loader.load()\n",
      "# Self Query를 사용한 LCEL 문서 로드 (기본 LCEL 문서 외부)\n",
      "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
      "loader = RecursiveUrlLoader(\n",
      "url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
      ")\n",
      "docs_sq = loader.load()\n",
      "# 문서 텍스트\n",
      "docs.extend([*docs_pydantic, *docs_sq])\n",
      "docs_texts = [d.page_content for d in docs]\n",
      "# 각 문서에 대한 토큰 수 계산\n",
      "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
      "# 토큰 수의 히스토그램을 그립니다.\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
      "plt.title(\"Token Counts in LCEL Documents\")\n",
      "plt.xlabel(\"Token Count\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "plt.grid(axis=\"y\", alpha=0.75)\n",
      "# 히스토그램을 표시합니다.\n",
      "plt.show\n",
      "문서 텍스트를 정렬하고 연결하여 토큰 수를 계산하는 과정을 설명합니다.\n",
      "문서(docs)를 메타데이터의 \"source\" 키를 기준으로 정렬합니다.\n",
      "정렬된 문서 리스트를 역순으로 뒤집습니다.\n",
      "역순으로 된 문서의 내용을 특정 구분자(\"\\n\\n\\n --- \\n\\n\\n\")를 사용하여 연결합니다.\n",
      "연결된 내용의 토큰 수를 num_tokens_from_string 함수를 사용하여 계산하고, 이를 출력합니다. 이때, \"cl100k_base\" 모델을 사용합니다.\n",
      "# 문서 텍스트를 연결합니다.\n",
      "# 문서를 출처 메타데이터 기준으로 정렬합니다.\n",
      "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
      "d_reversed = list(reversed(d_sorted))  # 정렬된 문서를 역순으로 배열합니다.\n",
      "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
      "[\n",
      "# 역순으로 배열된 문서의 내용을 연결합니다.\n",
      "doc.page_content\n",
      "for doc in d_reversed\n",
      "]\n",
      ")\n",
      "print(\n",
      "\"Num tokens in all context: %s\"  # 모든 문맥에서의 토큰 수를 출력합니다.\n",
      "% num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
      ")\n",
      "[이미지: ]Num tokens in all context: 69074\n",
      "RecursiveCharacterTextSplitter를 사용하여 텍스트를 분할하는 과정을 설명합니다.\n",
      "chunk_size_tok 변수를 설정하여, 각 텍스트 청크의 크기를 2000 토큰으로 지정합니다.\n",
      "RecursiveCharacterTextSplitter의 from_tiktoken_encoder 메소드를 사용하여 텍스트 분할기를 초기화합니다. 여기서 청크 크기(chunk_size)와 청크 간 겹침(chunk_overlap)을 0으로 설정합니다.\n",
      "초기화된 텍스트 분할기의 split_text 메소드를 호출하여, concatenated_content라는 변수에 저장된 연결된 텍스트를 분할합니다. 분할 결과는 texts_split 변수에 저장됩니다.\n",
      "# 텍스트 분할을 위한 코드\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "chunk_size_tok = 2000  # 토큰의 청크 크기를 설정합니다.\n",
      "# 재귀적 문자 텍스트 분할기를 초기화합니다. 토큰 인코더를 사용하여 청크 크기와 중복을 설정합니다.\n",
      "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
      "chunk_size=chunk_size_tok, chunk_overlap=0\n",
      ")\n",
      "texts_split = text_splitter.split_text(\n",
      "concatenated_content\n",
      ")  # 주어진 텍스트를 분할합니다.\n",
      "모델\n",
      "Claude3 (https://www.anthropic.com/news/claude-3-family)Claude3 계열도 포함됩니다.\n",
      "관련 API 키를 설정하는 것을 잊지 마세요.\n",
      "OPENAI_API_KEY, Anthropic 을 사용하는 경우 ANTHROPIC_API_KEY\n",
      "ChatOpenAI 혹은 ChatAnthropic + OpenAIEmbeddings를 사용하여 챗봇 모델을 구현합니다.\n",
      "OpenAIEmbeddings를 인스턴스화하여 OpenAI의 임베딩 기능을 초기화합니다.\n",
      "ChatOpenAI 혹은 ChatAnthropic 을 사용하여 temperature를 0으로 설정하고, 챗봇 모델을 초기화합니다.\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv()\n",
      "True\n",
      "Cache Embedding 을 사용합니다.\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "from langchain.embeddings import CacheBackedEmbeddings\n",
      "from langchain.storage import LocalFileStore\n",
      "store = LocalFileStore(\"./cache/\")\n",
      "# embeddings 인스턴스를 생성합니다.\n",
      "embd = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\n",
      "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
      "embd, store, namespace=embd.model\n",
      ")\n",
      "모델을 초기화 합니다.\n",
      "from langchain_anthropic import ChatAnthropic\n",
      "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
      "from langchain.callbacks.base import BaseCallbackHandler\n",
      "class StreamCallback(BaseCallbackHandler):\n",
      "def on_llm_new_token(self, token: str, **kwargs):\n",
      "print(token, end=\"\", flush=True)\n",
      "# ChatOpenAI 모델을 초기화합니다. 모델은 \"gpt-4-turbo-preview\"를 사용합니다.\n",
      "model = ChatOpenAI(\n",
      "model=\"gpt-4-turbo-preview\",\n",
      "temperature=0,\n",
      "streaming=True,\n",
      "callbacks=[StreamCallback()],\n",
      ")\n",
      "# ChatAnthropic 모델을 초기화합니다. 온도는 0으로 설정하고, 모델은 \"claude-3-opus-20240229\"를 사용합니다.\n",
      "# model = ChatAnthropic(temperature=0, model=\"claude-3-opus-20240229\")\n",
      "트리 구축\n",
      "트리 구축에서의 클러스터링 접근 방식에는 몇 가지 흥미로운 아이디어가 포함되어 있습니다.\n",
      "GMM (가우시안 혼합 모델)\n",
      "다양한 클러스터에 걸쳐 데이터 포인트의 분포를 모델링합니다.\n",
      "모델의 베이지안 정보 기준(BIC)을 평가하여 최적의 클러스터 수를 결정합니다.\n",
      "UMAP (Uniform Manifold Approximation and Projection)\n",
      "클러스터링을 지원합니다.\n",
      "고차원 데이터의 차원을 축소합니다.\n",
      "UMAP은 데이터 포인트의 유사성에 기반하여 자연스러운 그룹화를 강조하는 데 도움을 줍니다.\n",
      "지역 및 전역 클러스터링\n",
      "다양한 규모에서 데이터를 분석하는 데 사용됩니다.\n",
      "데이터 내의 세밀한 패턴과 더 넓은 패턴 모두를 효과적으로 포착합니다.\n",
      "임계값 설정\n",
      "GMM의 맥락에서 클러스터 멤버십을 결정하기 위해 적용됩니다.\n",
      "확률 분포를 기반으로 합니다(데이터 포인트를 ≥ 1 클러스터에 할당).\n",
      "GMM 및 임계값 설정에 대한 코드는 아래 두 출처에서 언급된 Sarthi et al의 것입니다:\n",
      "원본 저장소 (https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py)원본 저장소\n",
      "소소한 조정 (https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/clustering.py)소소한 조정\n",
      "두 저자 모두에게 전적인 공로를 인정합니다.\n",
      "global_cluster_embeddings 함수는 임베딩의 글로벌 차원 축소를 수행하기 위해 UMAP을 사용합니다.\n",
      "입력된 임베딩(embeddings)을 UMAP을 사용하여 지정된 차원(dim)으로 차원 축소합니다.\n",
      "n_neighbors는 각 포인트를 고려할 이웃의 수를 지정하며, 제공되지 않을 경우 임베딩 수의 제곱근으로 기본 설정됩니다.\n",
      "metric은 UMAP에 사용될 거리 측정 기준을 지정합니다.\n",
      "결과로, 지정된 차원으로 축소된 임베딩이 numpy 배열로 반환됩니다.\n",
      "from typing import Dict, List, Optional, Tuple\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import umap\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "from sklearn.mixture import GaussianMixture\n",
      "RANDOM_SEED = 42  # 재현성을 위한 고정된 시드 값\n",
      "### --- 위의 인용된 코드에서 주석과 문서화를 추가함 --- ###\n",
      "def global_cluster_embeddings(\n",
      "embeddings: np.ndarray,\n",
      "dim: int,\n",
      "n_neighbors: Optional[int] = None,\n",
      "metric: str = \"cosine\",\n",
      ") -> np.ndarray:\n",
      "\"\"\"\n",
      "UMAP을 사용하여 임베딩의 전역 차원 축소를 수행합니다.\n",
      "매개변수:\n",
      "- embeddings: numpy 배열로 된 입력 임베딩.\n",
      "- dim: 축소된 공간의 목표 차원.\n",
      "- n_neighbors: 선택 사항; 각 점을 고려할 이웃의 수.\n",
      "제공되지 않으면 임베딩 수의 제곱근으로 기본 설정됩니다.\n",
      "- metric: UMAP에 사용할 거리 측정 기준.\n",
      "반환값:\n",
      "- 지정된 차원으로 축소된 임베딩의 numpy 배열.\n",
      "\"\"\"\n",
      "if n_neighbors is None:\n",
      "n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
      "return umap.UMAP(\n",
      "n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
      ").fit_transform(embeddings)\n",
      "임베딩 데이터에 대해 지역 차원 축소를 수행하는 함수 local_cluster_embeddings를 구현합니다.\n",
      "입력된 임베딩(embeddings)을 UMAP을 사용하여 지정된 차원(dim)으로 차원 축소합니다.\n",
      "차원 축소 과정에서 각 점에 대해 고려할 이웃의 수(num_neighbors)와 거리 측정 메트릭(metric)을 파라미터로 사용합니다.\n",
      "최종적으로 차원이 축소된 임베딩을 numpy 배열로 반환합니다.\n",
      "def local_cluster_embeddings(\n",
      "embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
      ") -> np.ndarray:\n",
      "\"\"\"\n",
      "임베딩에 대해 지역 차원 축소를 수행합니다. 이는 일반적으로 전역 클러스터링 이후에 사용됩니다.\n",
      "매개변수:\n",
      "- embeddings: numpy 배열로서의 입력 임베딩.\n",
      "- dim: 축소된 공간의 목표 차원 수.\n",
      "- num_neighbors: 각 점에 대해 고려할 이웃의 수.\n",
      "- metric: UMAP에 사용할 거리 측정 기준.\n",
      "반환값:\n",
      "- 지정된 차원으로 축소된 임베딩의 numpy 배열.\n",
      "\"\"\"\n",
      "return umap.UMAP(\n",
      "n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
      ").fit_transform(embeddings)\n",
      "get_optimal_clusters 함수는 주어진 임베딩 데이터를 기반으로 최적의 클러스터 수를 결정하는 데 사용됩니다. 이 과정은 가우시안 혼합 모델(Gaussian Mixture Model)을 사용하여 베이지안 정보 기준(Bayesian Information Criterion, BIC)을 계산함으로써 수행됩니다.\n",
      "입력 임베딩(embeddings)은 numpy 배열로 제공됩니다.\n",
      "최대 클러스터 수(max_clusters)는 고려할 클러스터의 최대 수를 지정합니다. 기본값은 50입니다.\n",
      "재현성을 위한 난수 상태(random_state)는 고정된 값을 사용합니다.\n",
      "함수는 입력 임베딩에 대해 여러 클러스터 수를 시도하며 각각에 대한 BIC 값을 계산합니다.\n",
      "최소 BIC 값을 가지는 클러스터 수를 최적의 클러스터 수로 결정하고 반환합니다.\n",
      "이 함수는 클러스터링 문제에서 데이터를 가장 잘 설명하는 클러스터 수를 자동으로 찾는 데 유용하게 사용될 수 있습니다.\n",
      "def get_optimal_clusters(\n",
      "embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
      ") -> int:\n",
      "\"\"\"\n",
      "가우시안 혼합 모델(Gaussian Mixture Model)을 사용하여 베이지안 정보 기준(BIC)을 통해 최적의 클러스터 수를 결정합니다.\n",
      "매개변수:\n",
      "- embeddings: numpy 배열로서의 입력 임베딩.\n",
      "- max_clusters: 고려할 최대 클러스터 수.\n",
      "- random_state: 재현성을 위한 시드.\n",
      "반환값:\n",
      "- 발견된 최적의 클러스터 수를 나타내는 정수.\n",
      "\"\"\"\n",
      "max_clusters = min(\n",
      "max_clusters, len(embeddings)\n",
      ")  # 최대 클러스터 수와 임베딩의 길이 중 작은 값을 최대 클러스터 수로 설정\n",
      "n_clusters = np.arange(1, max_clusters)  # 1부터 최대 클러스터 수까지의 범위를 생성\n",
      "bics = []  # BIC 점수를 저장할 리스트\n",
      "for n in n_clusters:  # 각 클러스터 수에 대해 반복\n",
      "gm = GaussianMixture(\n",
      "n_components=n, random_state=random_state\n",
      ")  # 가우시안 혼합 모델 초기화\n",
      "gm.fit(embeddings)  # 임베딩에 대해 모델 학습\n",
      "bics.append(gm.bic(embeddings))  # 학습된 모델의 BIC 점수를 리스트에 추가\n",
      "return n_clusters[np.argmin(bics)]  # BIC 점수가 가장 낮은 클러스터 수를 반환\n",
      "GMM_cluster 함수는 임베딩을 가우시안 혼합 모델(Gaussian Mixture Model, GMM)을 사용하여 클러스터링합니다. 이 과정은 확률 임계값을 기반으로 합니다.\n",
      "입력된 임베딩(embeddings)은 numpy 배열로 제공됩니다.\n",
      "threshold는 임베딩을 특정 클러스터에 할당하기 위한 확률 임계값입니다.\n",
      "random_state는 결과의 재현성을 위한 시드 값입니다.\n",
      "최적의 클러스터 수를 결정하기 위해 get_optimal_clusters 함수를 호출합니다.\n",
      "결정된 클러스터 수를 바탕으로 가우시안 혼합 모델을 초기화하고, 입력된 임베딩에 대해 학습을 수행합니다.\n",
      "각 임베딩에 대한 클러스터 할당 확률을 계산하고, 이 확률이 주어진 임계값을 초과하는 경우 해당 임베딩을 클러스터에 할당합니다.\n",
      "함수는 최종적으로 임베딩의 클러스터 레이블과 결정된 클러스터 수를 튜플로 반환합니다.\n",
      "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
      "\"\"\"\n",
      "확률 임계값을 기반으로 가우시안 혼합 모델(GMM)을 사용하여 임베딩을 클러스터링합니다.\n",
      "매개변수:\n",
      "- embeddings: numpy 배열로서의 입력 임베딩.\n",
      "- threshold: 임베딩을 클러스터에 할당하기 위한 확률 임계값.\n",
      "- random_state: 재현성을 위한 시드.\n",
      "반환값:\n",
      "- 클러스터 레이블과 결정된 클러스터 수를 포함하는 튜플.\n",
      "\"\"\"\n",
      "n_clusters = get_optimal_clusters(embeddings)  # 최적의 클러스터 수를 구합니다.\n",
      "# 가우시안 혼합 모델을 초기화합니다.\n",
      "gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
      "gm.fit(embeddings)  # 임베딩에 대해 모델을 학습합니다.\n",
      "probs = gm.predict_proba(\n",
      "embeddings\n",
      ")  # 임베딩이 각 클러스터에 속할 확률을 예측합니다.\n",
      "# 임계값을 초과하는 확률을 가진 클러스터를 레이블로 선택합니다.\n",
      "labels = [np.where(prob > threshold)[0] for prob in probs]\n",
      "return labels, n_clusters  # 레이블과 클러스터 수를 반환합니다.\n",
      "perform_clustering 함수는 임베딩에 대해 차원 축소, 가우시안 혼합 모델을 사용한 글로벌 클러스터링, 그리고 각 글로벌 클러스터 내에서의 로컬 클러스터링을 수행하여 클러스터링 결과를 반환합니다.\n",
      "입력된 임베딩(embeddings)에 대해 차원 축소를 수행합니다. 이는 UMAP을 사용하여 지정된 차원(dim)으로 임베딩의 차원을 축소하는 과정을 포함합니다.\n",
      "차원이 축소된 임베딩에 대해 가우시안 혼합 모델(GMM)을 사용하여 글로벌 클러스터링을 수행합니다. 클러스터 할당은 주어진 확률 임계값(threshold)을 기준으로 결정됩니다.\n",
      "각 글로벌 클러스터 내에서 추가적인 로컬 클러스터링을 수행합니다. 이 과정은 글로벌 클러스터링 결과를 바탕으로 각 글로벌 클러스터에 속한 임베딩들만을 대상으로 다시 차원 축소 및 GMM 클러스터링을 진행합니다.\n",
      "최종적으로, 모든 임베딩에 대해 글로벌 및 로컬 클러스터 ID를 할당하여, 각 임베딩이 속한 클러스터 ID를 담은 리스트를 반환합니다. 이 리스트는 임베딩의 순서에 따라 각 임베딩에 대한 클러스터 ID 배열을 포함합니다.\n",
      "이 함수는 고차원 데이터의 클러스터링을 위해 글로벌 및 로컬 차원에서의 클러스터링을 결합한 접근 방식을 제공합니다. 이를 통해 더 세분화된 클러스터링 결과를 얻을 수 있으며, 복잡한 데이터 구조를 보다 효과적으로 분석할 수 있습니다.\n",
      "def perform_clustering(\n",
      "embeddings: np.ndarray,\n",
      "dim: int,\n",
      "threshold: float,\n",
      ") -> List[np.ndarray]:\n",
      "\"\"\"\n",
      "임베딩에 대해 차원 축소, 가우시안 혼합 모델을 사용한 클러스터링, 각 글로벌 클러스터 내에서의 로컬 클러스터링을 순서대로 수행합니다.\n",
      "매개변수:\n",
      "- embeddings: numpy 배열로 된 입력 임베딩입니다.\n",
      "- dim: UMAP 축소를 위한 목표 차원입니다.\n",
      "- threshold: GMM에서 임베딩을 클러스터에 할당하기 위한 확률 임계값입니다.\n",
      "반환값:\n",
      "- 각 임베딩의 클러스터 ID를 포함하는 numpy 배열의 리스트입니다.\n",
      "\"\"\"\n",
      "if len(embeddings) <= dim + 1:\n",
      "# 데이터가 충분하지 않을 때 클러스터링을 피합니다.\n",
      "return [np.array([0]) for _ in range(len(embeddings))]\n",
      "# 글로벌 차원 축소\n",
      "reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
      "# 글로벌 클러스터링\n",
      "global_clusters, n_global_clusters = GMM_cluster(\n",
      "reduced_embeddings_global, threshold\n",
      ")\n",
      "all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
      "total_clusters = 0\n",
      "# 각 글로벌 클러스터를 순회하며 로컬 클러스터링 수행\n",
      "for i in range(n_global_clusters):\n",
      "# 현재 글로벌 클러스터에 속하는 임베딩 추출\n",
      "global_cluster_embeddings_ = embeddings[\n",
      "np.array([i in gc for gc in global_clusters])\n",
      "]\n",
      "if len(global_cluster_embeddings_) == 0:\n",
      "continue\n",
      "if len(global_cluster_embeddings_) <= dim + 1:\n",
      "# 작은 클러스터는 직접 할당으로 처리\n",
      "local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
      "n_local_clusters = 1\n",
      "else:\n",
      "# 로컬 차원 축소 및 클러스터링\n",
      "reduced_embeddings_local = local_cluster_embeddings(\n",
      "global_cluster_embeddings_, dim\n",
      ")\n",
      "local_clusters, n_local_clusters = GMM_cluster(\n",
      "reduced_embeddings_local, threshold\n",
      ")\n",
      "# 로컬 클러스터 ID 할당, 이미 처리된 총 클러스터 수를 조정\n",
      "for j in range(n_local_clusters):\n",
      "local_cluster_embeddings_ = global_cluster_embeddings_[\n",
      "np.array([j in lc for lc in local_clusters])\n",
      "]\n",
      "indices = np.where(\n",
      "(embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
      ")[1]\n",
      "for idx in indices:\n",
      "all_local_clusters[idx] = np.append(\n",
      "all_local_clusters[idx], j + total_clusters\n",
      ")\n",
      "total_clusters += n_local_clusters\n",
      "return all_local_clusters\n",
      "텍스트 문서의 목록에 대한 임베딩을 생성하는 함수 embed를 구현합니다.\n",
      "입력으로 텍스트 문서의 목록(texts)을 받습니다.\n",
      "embd 객체의 embed_documents 메소드를 사용하여 텍스트 문서의 임베딩을 생성합니다.\n",
      "생성된 임베딩을 numpy.ndarray 형태로 변환하여 반환합니다.\n",
      "def embed(texts):\n",
      "# 텍스트 문서 목록에 대한 임베딩을 생성합니다.\n",
      "#\n",
      "# 이 함수는 `embd` 객체가 존재한다고 가정하며, 이 객체는 텍스트 목록을 받아 그 임베딩을 반환하는 `embed_documents` 메소드를 가지고 있습니다.\n",
      "#\n",
      "# 매개변수:\n",
      "# - texts: List[str], 임베딩할 텍스트 문서의 목록입니다.\n",
      "#\n",
      "# 반환값:\n",
      "# - numpy.ndarray: 주어진 텍스트 문서들에 대한 임베딩 배열입니다.\n",
      "text_embeddings = embd.embed_documents(\n",
      "texts\n",
      ")  # 텍스트 문서들의 임베딩을 생성합니다.\n",
      "text_embeddings_np = np.array(text_embeddings)  # 임베딩을 numpy 배열로 변환합니다.\n",
      "return text_embeddings_np  # 임베딩된 numpy 배열을 반환합니다.\n",
      "embed_cluster_texts 함수는 텍스트 목록을 임베딩하고 클러스터링하여, 원본 텍스트, 해당 임베딩, 그리고 할당된 클러스터 라벨을 포함하는 pandas.DataFrame을 반환합니다.\n",
      "주어진 텍스트 목록에 대해 임베딩을 생성합니다.\n",
      "생성된 임베딩을 기반으로 클러스터링을 수행합니다. 이 과정은 사전에 정의된 perform_clustering 함수를 사용합니다.\n",
      "결과를 저장하기 위해 pandas.DataFrame을 초기화합니다.\n",
      "DataFrame에 원본 텍스트, 임베딩 리스트, 클러스터 라벨을 각각 저장합니다.\n",
      "이 함수는 텍스트 데이터의 임베딩 생성과 클러스터링을 하나의 단계로 결합하여, 텍스트 데이터의 구조적 분석과 그룹화를 용이하게 합니다.\n",
      "def embed_cluster_texts(texts):\n",
      "\"\"\"\n",
      "텍스트 목록을 임베딩하고 클러스터링하여, 텍스트, 그들의 임베딩, 그리고 클러스터 라벨이 포함된 DataFrame을 반환합니다.\n",
      "이 함수는 임베딩 생성과 클러스터링을 단일 단계로 결합합니다. 임베딩에 대해 클러스터링을 수행하는 `perform_clustering` 함수의 사전 정의된 존재를 가정합니다.\n",
      "매개변수:\n",
      "- texts: List[str], 처리될 텍스트 문서의 목록입니다.\n",
      "반환값:\n",
      "- pandas.DataFrame: 원본 텍스트, 그들의 임베딩, 그리고 할당된 클러스터 라벨이 포함된 DataFrame입니다.\n",
      "\"\"\"\n",
      "text_embeddings_np = embed(texts)  # 임베딩 생성\n",
      "cluster_labels = perform_clustering(\n",
      "text_embeddings_np, 10, 0.1\n",
      ")  # 임베딩에 대해 클러스터링 수행\n",
      "df = pd.DataFrame()  # 결과를 저장할 DataFrame 초기화\n",
      "df[\"text\"] = texts  # 원본 텍스트 저장\n",
      "df[\"embd\"] = list(text_embeddings_np)  # DataFrame에 리스트로 임베딩 저장\n",
      "df[\"cluster\"] = cluster_labels  # 클러스터 라벨 저장\n",
      "return df\n",
      "fmt_txt 함수는 pandas의 DataFrame에서 텍스트 문서를 단일 문자열로 포맷팅합니다.\n",
      "입력 파라미터로 DataFrame을 받으며, 이 DataFrame은 포맷팅할 텍스트 문서를 포함한 'text' 컬럼을 가져야 합니다.\n",
      "모든 텍스트 문서는 특정 구분자(\"--- --- \\n --- ---\")를 사용하여 연결되어 단일 문자열로 반환됩니다.\n",
      "함수는 연결된 텍스트 문서를 포함하는 단일 문자열을 반환합니다.\n",
      "def fmt_txt(df: pd.DataFrame) -> str:\n",
      "\"\"\"\n",
      "DataFrame에 있는 텍스트 문서를 단일 문자열로 포맷합니다.\n",
      "매개변수:\n",
      "- df: 'text' 열에 포맷할 텍스트 문서가 포함된 DataFrame.\n",
      "반환값:\n",
      "- 모든 텍스트 문서가 특정 구분자로 결합된 단일 문자열.\n",
      "\"\"\"\n",
      "unique_txt = df[\"text\"].tolist()  # 'text' 열의 모든 텍스트를 리스트로 변환\n",
      "return \"--- --- \\n --- --- \".join(\n",
      "unique_txt\n",
      ")  # 텍스트 문서들을 특정 구분자로 결합하여 반환\n",
      "텍스트 데이터를 임베딩하고, 클러스터링하며, 각 클러스터에 대한 요약을 생성하는 과정을 수행합니다.\n",
      "주어진 텍스트 목록에 대해 임베딩을 생성하고 유사성에 기반한 클러스터링을 진행합니다. 이 과정은 df_clusters 데이터프레임을 결과로 합니다. 이 데이터프레임에는 원본 텍스트, 임베딩, 그리고 클러스터 할당 정보가 포함됩니다.\n",
      "클러스터 할당을 쉽게 처리하기 위해 데이터프레임 항목을 확장합니다. 각 행은 텍스트, 임베딩, 클러스터를 포함하는 새로운 데이터프레임으로 변환됩니다.\n",
      "확장된 데이터프레임에서 고유한 클러스터 식별자를 추출하고, 각 클러스터에 대한 텍스트를 포맷팅하여 요약을 생성합니다. 이 요약은 df_summary 데이터프레임에 저장됩니다. 이 데이터프레임은 각 클러스터의 요약, 지정된 세부 수준, 그리고 클러스터 식별자를 포함합니다.\n",
      "최종적으로, 함수는 두 개의 데이터프레임을 포함하는 튜플을 반환합니다. 첫 번째 데이터프레임은 원본 텍스트, 임베딩, 클러스터 할당 정보를 포함하며, 두 번째 데이터프레임은 각 클러스터에 대한 요약과 해당 세부 수준, 클러스터 식별자를 포함합니다.\n",
      "def embed_cluster_summarize_texts(\n",
      "texts: List[str], level: int\n",
      ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
      "\"\"\"\n",
      "텍스트 목록에 대해 임베딩, 클러스터링 및 요약을 수행합니다. 이 함수는 먼저 텍스트에 대한 임베딩을 생성하고,\n",
      "유사성을 기반으로 클러스터링을 수행한 다음, 클러스터 할당을 확장하여 처리를 용이하게 하고 각 클러스터 내의 내용을 요약합니다.\n",
      "매개변수:\n",
      "- texts: 처리할 텍스트 문서 목록입니다.\n",
      "- level: 처리의 깊이나 세부 사항을 정의할 수 있는 정수 매개변수입니다.\n",
      "반환값:\n",
      "- 두 개의 데이터프레임을 포함하는 튜플:\n",
      "1. 첫 번째 데이터프레임(`df_clusters`)은 원본 텍스트, 그들의 임베딩, 그리고 클러스터 할당을 포함합니다.\n",
      "2. 두 번째 데이터프레임(`df_summary`)은 각 클러스터에 대한 요약, 지정된 세부 수준, 그리고 클러스터 식별자를 포함합니다.\n",
      "\"\"\"\n",
      "# 텍스트를 임베딩하고 클러스터링하여 'text', 'embd', 'cluster' 열이 있는 데이터프레임을 생성합니다.\n",
      "df_clusters = embed_cluster_texts(texts)\n",
      "# 클러스터를 쉽게 조작하기 위해 데이터프레임을 확장할 준비를 합니다.\n",
      "expanded_list = []\n",
      "# 데이터프레임 항목을 문서-클러스터 쌍으로 확장하여 처리를 간단하게 합니다.\n",
      "for index, row in df_clusters.iterrows():\n",
      "for cluster in row[\"cluster\"]:\n",
      "expanded_list.append(\n",
      "{\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
      ")\n",
      "# 확장된 목록에서 새 데이터프레임을 생성합니다.\n",
      "expanded_df = pd.DataFrame(expanded_list)\n",
      "# 처리를 위해 고유한 클러스터 식별자를 검색합니다.\n",
      "all_clusters = expanded_df[\"cluster\"].unique()\n",
      "print(f\"--Generated {len(all_clusters)} clusters--\")\n",
      "# 요약\n",
      "template = \"\"\"여기 LangChain 표현 언어 문서의 하위 집합이 있습니다.\n",
      "LangChain 표현 언어는 LangChain에서 체인을 구성하는 방법을 제공합니다.\n",
      "제공된 문서의 자세한 요약을 제공하십시오.\n",
      "문서:\n",
      "{context}\n",
      "\"\"\"\n",
      "prompt = ChatPromptTemplate.from_template(template)\n",
      "chain = prompt | model | StrOutputParser()\n",
      "# 각 클러스터 내의 텍스트를 요약을 위해 포맷팅합니다.\n",
      "summaries = []\n",
      "for i in all_clusters:\n",
      "df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
      "formatted_txt = fmt_txt(df_cluster)\n",
      "summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
      "# 요약, 해당 클러스터 및 레벨을 저장할 데이터프레임을 생성합니다.\n",
      "df_summary = pd.DataFrame(\n",
      "{\n",
      "\"summaries\": summaries,\n",
      "\"level\": [level] * len(summaries),\n",
      "\"cluster\": list(all_clusters),\n",
      "}\n",
      ")\n",
      "return df_clusters, df_summary\n",
      "텍스트 데이터를 재귀적으로 임베딩, 클러스터링 및 요약하는 과정을 구현한 함수입니다.\n",
      "주어진 텍스트 리스트를 임베딩, 클러스터링 및 요약하여 각 단계별로 결과를 저장합니다.\n",
      "함수는 최대 지정된 재귀 레벨까지 실행되거나, 유일한 클러스터의 수가 1이 될 때까지 반복됩니다.\n",
      "각 재귀 단계에서는 현재 레벨의 클러스터링 결과와 요약 결과를 데이터프레임 형태로 반환하고, 이를 결과 딕셔너리에 저장합니다.\n",
      "만약 현재 레벨이 최대 재귀 레벨보다 작고, 유일한 클러스터의 수가 1보다 크다면, 현재 레벨의 요약 결과를 다음 레벨의 입력 텍스트로 사용하여 재귀적으로 함수를 호출합니다.\n",
      "최종적으로 각 레벨별 클러스터 데이터프레임과 요약 데이터프레임을 포함하는 딕셔너리를 반환합니다.\n",
      "def recursive_embed_cluster_summarize(\n",
      "texts: List[str], level: int = 1, n_levels: int = 3\n",
      ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
      "\"\"\"\n",
      "지정된 레벨까지 또는 고유 클러스터의 수가 1이 될 때까지 텍스트를 재귀적으로 임베딩, 클러스터링, 요약하여\n",
      "각 레벨에서의 결과를 저장합니다.\n",
      "매개변수:\n",
      "- texts: List[str], 처리할 텍스트들.\n",
      "- level: int, 현재 재귀 레벨 (1에서 시작).\n",
      "- n_levels: int, 재귀의 최대 깊이.\n",
      "반환값:\n",
      "- Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], 재귀 레벨을 키로 하고 해당 레벨에서의 클러스터 DataFrame과 요약 DataFrame을 포함하는 튜플을 값으로 하는 사전.\n",
      "\"\"\"\n",
      "results = {}  # 각 레벨에서의 결과를 저장할 사전\n",
      "# 현재 레벨에 대해 임베딩, 클러스터링, 요약 수행\n",
      "df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
      "# 현재 레벨의 결과 저장\n",
      "results[level] = (df_clusters, df_summary)\n",
      "# 추가 재귀가 가능하고 의미가 있는지 결정\n",
      "unique_clusters = df_summary[\"cluster\"].nunique()\n",
      "if level < n_levels and unique_clusters > 1:\n",
      "# 다음 레벨의 재귀 입력 텍스트로 요약 사용\n",
      "new_texts = df_summary[\"summaries\"].tolist()\n",
      "next_level_results = recursive_embed_cluster_summarize(\n",
      "new_texts, level + 1, n_levels\n",
      ")\n",
      "# 다음 레벨의 결과를 현재 결과 사전에 병합\n",
      "results.update(next_level_results)\n",
      "return results\n",
      "# 전체 문서의 개수\n",
      "len(docs_texts)\n",
      "33\n",
      "# 트리 구축\n",
      "leaf_texts = docs_texts  # 문서 텍스트를 리프 텍스트로 설정\n",
      "results = recursive_embed_cluster_summarize(\n",
      "leaf_texts, level=1, n_levels=3\n",
      ")  # 재귀적으로 임베딩, 클러스터링 및 요약을 수행하여 결과를 얻음\n",
      "--Generated 6 clusters--\n",
      "LangChain Expression Language (LCEL)는 LangChain에서 체인을 구성하는 선언적 방법을 제공합니다. LCEL은 프로토타입을 생산에 투입하는 것을 지원하도록 설계되었으며, 가장 간단한 \"프롬프트 + LLM\" 체인부터 수백 단계의 복잡한 체인까지 코드 변경 없이 생산에서 성공적으로 실행할 수 있습니다. LCEL을 사용하는 몇 가지 이유는 다음과 같습니다:\n",
      "- **스트리밍 지원**: LCEL로 체인을 구축하면 최상의 첫 번째 토큰까지의 시간을 얻을 수 있습니다. 이는 일부 체인에서 LLM에서 스트리밍 출력 파서로 토큰을 직접 스트리밍하고, LLM 제공자가 원시 토큰을 출력하는 속도로 파싱된 증분 출력 청크를 받을 수 있음을 의미합니다.\n",
      "- **비동기 지원**: LCEL로 구축된 모든 체인은 동기 API(예: 프로토타이핑 중인 Jupyter 노트북에서)와 비동기 API(예: LangServe 서버에서) 모두에서 호출할 수 있습니다. 이를 통해 프로토타입과 생산에서 동일한 코드를 사용하고, 뛰어난 성능을 제공하며, 동일한 서버에서 많은 동시 요청을 처리할 수 있습니다.\n",
      "- **최적화된 병렬 실행**: LCEL 체인에 병렬로 실행할 수 있는 단계가 있는 경우(예: 여러 검색기에서 문서를 가져올 때), 동기 및 비동기 인터페이스 모두에서 자동으로 수행하여 가능한 가장 작은 대기 시간을 제공합니다.\n",
      "- **재시도 및 대체**: LCEL 체인의 모든 부분에 대해 재시도 및 대체를 구성할 수 있습니다. 이는 규모에서 체인을 더욱 신뢰할 수 있게 만드는 좋은 방법입니다.\n",
      "- **중간 결과 접근**: 더 복잡한 체인의 경우 최종 출력이 생성되기 전에 중간 단계의 결과에 액세스하는 것이 매우 유용할 수 있습니다. 이를 통해 최종 사용자에게 무언가가 진행되고 있음을 알리거나 체인을 디버깅할 수 있습니다.\n",
      "- **입력 및 출력 스키마**: 입력 및 출력 스키마는 체인의 구조로부터 추론된 Pydantic 및 JSONSchema 스키마를 모든 LCEL 체인에 제공합니다. 이는 입력 및 출력의 유효성 검사에 사용될 수 있으며 LangServe의 중요한 부분입니다.\n",
      "- **LangSmith 추적 통합 및 LangServe 배포 통합**: 체인이 점점 더 복잡해짐에 따라 각 단계에서 정확히 무슨 일이 일어나고 있는지 이해하는 것이 점점 더 중요해집니다. LCEL을 사용하면 모든 단계가 LangSmith에 자동으로 기록되어 최대한의 관찰 가능성과 디버깅 가능성을 제공합니다.\n",
      "LCEL은 다양한 공통 작업을 수행하기 위한 예제 코드를 제공하는 쿡북도 포함하고 있습니다. 이 예제들은 다양한 Runnable(핵심 LCEL 인터페이스) 구성 요소를 조합하여 다양한 작업을 수행하는 방법을 보여줍니다. 예를 들어, LCEL을 사용하여 의미적 유사성에 따라 사용자 입력을 기반으로 체인 로직을 동적으로 결정하는 사용자 지정 라우팅 로직을 쉽게 추가할 수 있습니다. 이는 쿼리를 가장 관련성이 높은 프롬프트로 라우팅하는 데 특히 유용한 기술입니다.LangChain 표현 언어(LCEL) 문서는 LangChain을 사용하여 체인을 구성하는 방법에 대한 다양한 가이드와 예시를 제공합니다. 이 문서는 LangChain의 기능을 최대한 활용하고자 하는 개발자들을 위한 것입니다. 주요 내용은 다음과 같습니다:\n",
      "1. **Runnable 타입**:\n",
      "- **RunnableParallel**: 데이터를 조작하는 방법을 설명합니다.\n",
      "- **RunnablePassthrough**: 데이터를 그대로 통과시키는 방법을 설명합니다.\n",
      "- **RunnableLambda**: 사용자 정의 함수를 실행하는 방법을 설명합니다.\n",
      "- **RunnableBranch**: 입력에 기반하여 동적으로 로직을 라우팅하는 방법을 설명합니다.\n",
      "2. **런타임 인자 바인딩**: Runnable 시퀀스 내에서 특정 인자를 상수로 전달하는 방법을 설명합니다.\n",
      "3. **런타임에서 체인 내부 구성**: 런타임에서 체인의 내부 설정을 실험하거나 최종 사용자에게 노출하는 방법을 설명합니다.\n",
      "4. **`@chain` 데코레이터를 사용하여 runnable 생성**: 임의의 함수를 체인으로 변환하는 방법을 설명합니다. 이 방법은 관찰 가능성을 향상시키고, 함수 내부에서 호출되는 runnable을 중첩된 자식으로 추적할 수 있게 합니다.\n",
      "5. **Fallback 추가**: LLM(Large Language Model) 애플리케이션에서 발생할 수 있는 다양한 실패 지점에 대비하는 방법을 설명합니다.\n",
      "6. **커스텀 제너레이터 함수 스트리밍**: `yield` 키워드를 사용하는 제너레이터 함수를 LCEL 파이프라인에 사용하는 방법을 설명합니다. 이는 커스텀 출력 파서를 구현하거나 이전 단계의 출력을 수정하면서 스트리밍 기능을 유지하는 데 유용합니다.\n",
      "7. **Runnable 검사**: LCEL로 생성된 runnable을 검사하는 방법을 설명합니다.\n",
      "8. **메시지 기록(메모리) 추가**: 특정 runnable에 메시지 기록을 추가하는 방법을 설명합니다.\n",
      "이 문서는 LangChain을 사용하여 복잡한 데이터 처리 및 변환 파이프라인을 구축하고자 하는 개발자에게 유용한 정보를 제공합니다. 각 섹션은 구체적인 예시와 함께 설명되어 있어, 개발자가 LangChain의 다양한 기능을 쉽게 이해하고 적용할 수 있도록 돕습니다.이 문서는 LangChain 표현 언어(LCEL)를 사용하여 다양한 작업을 수행하는 방법에 대한 예제 코드를 제공하는 쿡북입니다. LCEL은 LangChain에서 체인을 구성하는 방법을 제공하며, 이 쿡북은 다양한 Runnable(핵심 LCEL 인터페이스) 구성 요소를 조합하여 여러 작업을 수행하는 방법을 보여줍니다. 여기에는 다음과 같은 주제들이 포함됩니다:\n",
      "1. **Prompt + LLM**: 가장 일반적이고 가치 있는 구성으로, 프롬프트와 LLM(Large Language Model)을 결합하는 방법을 설명합니다.\n",
      "2. **RAG**: 프롬프트와 LLM에 검색 단계를 추가하는 방법을 소개합니다.\n",
      "3. **Multiple Chains**: Runnable을 사용하여 여러 체인을 연결하는 방법을 설명합니다.\n",
      "4. **Querying a SQL DB**: Runnable을 사용하여 SQL 데이터베이스 체인을 복제하는 방법을 보여줍니다.\n",
      "5. **Agents**: Runnable을 에이전트로 전달하는 방법을 설명합니다.\n",
      "6. **Code Writing**: LCEL을 사용하여 Python 코드를 작성하는 예제를 제공합니다.\n",
      "7. **Routing by Semantic Similarity**: LCEL을 사용하여 의미적 유사성에 따라 라우팅을 추가하는 방법을 설명합니다.\n",
      "8. **Adding Memory**: 임의의 체인에 메모리를 추가하는 방법을 보여줍니다.\n",
      "9. **Adding Moderation**: LLM 애플리케이션 주변에 검열(또는 다른 안전장치)을 추가하는 방법을 설명합니다.\n",
      "10. **Managing Prompt Size**: 에이전트가 동적으로 도구를 호출하고, 그 도구 호출의 결과가 프롬프트에 추가되는 방식을 관리하는 방법을 보여줍니다.\n",
      "11. **Using Tools**: Runnable과 함께 도구를 쉽게 사용하는 방법을 설명합니다.\n",
      "각 섹션은 특정 작업을 수행하기 위한 코드 예제와 함께 구체적인 방법론을 제공합니다. 예를 들어, \"Adding Moderation\" 섹션에서는 OpenAI의 내용 정책을 위반하는 텍스트를 찾아내는 방법을 보여주고, \"Multiple Chains\" 섹션에서는 여러 체인을 연결하여 복잡한 질문에 답하는 방법을 설명합니다. \"Agents\" 섹션에서는 에이전트를 구성하고 실행하는 과정을, \"Code Writing\"에서는 LCEL을 사용하여 코드를 작성하고 실행하는 방법을 보여줍니다.\n",
      "이 문서는 LangChain을 사용하여 다양한 작업을 수행하고자 하는 개발자들에게 유용한 리소스입니다. 각 예제는 구체적인 작업을 수행하는 방법을 단계별로 설명하며, 이를 통해 개발자들은 LCEL의 다양한 기능을 이해하고 자신의 프로젝트에 적용할 수 있습니다.The provided documents are part of the LangChain documentation, which details the LangChain Expression Language (LCEL) and its applications. LangChain is a framework designed to facilitate the creation, manipulation, and execution of complex chains of operations, particularly in the context of language models and related tasks. The documentation covers various aspects of using LangChain, including installation, quick start guides, security considerations, and detailed explanations of LCEL's components and capabilities. Here's a summary of the key points from the documents:\n",
      "1. **Introduction to LangChain and LCEL**: LangChain offers a way to build complex chains from basic components, supporting functionalities like streaming, parallelism, and logging. LCEL (LangChain Expression Language) simplifies the process of chaining together prompts, models, and output parsers to perform tasks like generating jokes based on a given topic or conducting retrieval-augmented generation.\n",
      "2. **Basic Examples**: The documentation provides examples of basic use cases, such as chaining a prompt template with a model and an output parser to generate content based on user input. It also demonstrates more complex scenarios like retrieval-augmented generation, where additional context is retrieved and used to inform the generation process.\n",
      "3. **Inspecting Runnables**: LangChain allows users to inspect runnables (components of a chain) to understand their structure and operation better. This can include generating a graph representation of a chain or retrieving the prompts used within a chain.\n",
      "4. **Using Tools with Runnables**: LangChain supports the integration of various tools with runnables. An example provided is using DuckDuckGoSearchRun with a chain to turn user input into a search query and retrieve relevant information.\n",
      "5. **Self-querying Retrievers**: The documentation discusses self-querying retrievers, which can construct structured queries based on natural language input and apply these queries to their underlying VectorStore. This allows for sophisticated retrieval operations based on both semantic similarity and metadata filters.\n",
      "6. **Advanced Features**: LangChain documentation also touches on advanced features like adding memory to chains, managing prompt size, and routing by semantic similarity. These features enable the creation of more sophisticated and efficient chains capable of handling complex tasks.\n",
      "7. **Community and Support**: The documents encourage community engagement and feedback, providing links to community resources like Discord, Twitter, and GitHub. This suggests an active and supportive community around LangChain.\n",
      "Overall, the LangChain documentation provides a comprehensive guide to using the LangChain framework and LCEL for building and executing complex chains of operations involving language models and other components. It covers both basic and advanced use cases, offering practical examples and encouraging community involvement.The provided documents from LangChain cover a range of topics related to the LangChain Expression Language (LCEL) and its applications, including interface design, streaming, dynamic routing, parallel processing, data passing, message history management, and prompt size management. Here's a detailed summary of the key points from each document:\n",
      "### Interface\n",
      "- LangChain introduces a \"Runnable\" protocol to simplify the creation of custom chains.\n",
      "- The standard interface includes methods for streaming, invoking, and batching calls, with asynchronous versions available.\n",
      "- Input and output types vary by component, with schemas provided for inspection.\n",
      "### Streaming\n",
      "- Streaming is crucial for making applications feel responsive.\n",
      "- LangChain supports synchronous and asynchronous streaming, including intermediate steps and final output.\n",
      "- Examples demonstrate streaming with LLMs and chat models, highlighting the importance of handling input streams effectively.\n",
      "### RunnableBranch\n",
      "- RunnableBranch allows for dynamic routing based on input, enabling non-deterministic chains.\n",
      "- Two methods for routing include using a custom function (recommended) or a RunnableBranch.\n",
      "- Examples show how to classify questions and route them to corresponding prompt chains based on the classification.\n",
      "### RunnableParallel\n",
      "- RunnableParallel is used for manipulating data and executing multiple Runnables in parallel.\n",
      "- It can be used to match the output format of one Runnable to the input format of another.\n",
      "- Examples demonstrate parallel execution and the use of itemgetter for shorthand data extraction.\n",
      "### RunnablePassthrough\n",
      "- RunnablePassthrough passes inputs unchanged or with added keys, often used with RunnableParallel.\n",
      "- It allows for the assignment of data to new keys in a map.\n",
      "- An example shows its use in a retrieval chain, passing user input under a specific key.\n",
      "### Add Message History (Memory)\n",
      "- RunnableWithMessageHistory adds message history to chains, managing chat message history.\n",
      "- It supports various input and output formats, including sequences of BaseMessage and dictionaries.\n",
      "- Examples cover in-memory and persistent storage (using Redis) for message histories, demonstrating how to manage and utilize chat histories in chains.\n",
      "### Managing Prompt Size\n",
      "- Managing prompt size is crucial for preventing context window overflow in models.\n",
      "- Custom functionality can be added to LCEL chains for prompt size management.\n",
      "- An example demonstrates a multi-step question with prompt handling logic to condense prompts and ensure the model's context window is not exceeded.\n",
      "These documents collectively provide a comprehensive guide to using LangChain Expression Language for building and managing complex chains, incorporating dynamic routing, parallel processing, and efficient data handling techniques. They emphasize the flexibility and power of LCEL in creating responsive and intelligent applications.The provided documents are part of the LangChain documentation, focusing on the LangChain Expression Language (LCEL), a tool designed to facilitate the construction of complex chains from basic components for language model applications. Here's a detailed summary of the key points from each section:\n",
      "### Why Use LCEL\n",
      "- **Purpose**: LCEL simplifies building complex chains by offering a unified interface and composition primitives.\n",
      "- **Features**:\n",
      "1. **Unified Interface**: Implements the Runnable interface, allowing chains of LCEL objects to support common invocation methods (invoke, batch, stream, etc.).\n",
      "2. **Composition Primitives**: Facilitates composing chains, parallelizing components, adding fallbacks, and more.\n",
      "- **Example**: Demonstrates LCEL's utility through a basic example of creating a prompt + model chain and compares the process with and without LCEL, highlighting LCEL's efficiency and simplicity.\n",
      "### Prompt + LLM\n",
      "- **Common Composition**: Combining a PromptTemplate/ChatPromptTemplate with an LLM/ChatModel and an OutputParser is a fundamental building block in LCEL.\n",
      "- **Simplification and Flexibility**: Shows how to simplify input, attach kwargs, and use different parsers for structured outputs.\n",
      "- **Runnable Parallel**: Introduces RunnableParallel for easier invocation, demonstrating how to streamline the process of creating input dictionaries for prompts.\n",
      "### Add Fallbacks\n",
      "- **Handling Failures**: Discusses using fallbacks to gracefully handle failures at various points, especially useful for LLM API errors.\n",
      "- **Implementation**: Provides examples of implementing fallbacks, including handling specific errors and creating fallbacks for sequences.\n",
      "- **Practical Use**: Offers code snippets to illustrate how fallbacks can be applied to LLMs, showing how to switch between different models or prompts based on runtime configurations.\n",
      "### Configure Chain Internals at Runtime\n",
      "- **Dynamic Configuration**: Explains methods to experiment with or expose different configurations to end-users by adjusting chain internals at runtime.\n",
      "- **Configuration Fields and Alternatives**:\n",
      "- **Fields**: Allows configuring specific fields of a runnable, such as LLM temperature.\n",
      "- **Alternatives**: Enables listing out alternatives for any particular runnable that can be set during runtime, useful for switching between models or prompts.\n",
      "- **Examples**: Provides code examples to demonstrate configuring LLMs and prompts, including saving configured chains as their own objects for reuse.\n",
      "### Quickstart\n",
      "- **Output Parsers**: Introduces output parsers as a means to structure language model responses into more useful formats.\n",
      "- **PydanticOutputParser**: Highlights the use of PydanticOutputParser for defining desired data structures and parsing model outputs into these structures.\n",
      "- **Streaming and Invocation**: Discusses the support for various invocation methods within LCEL and the ability of some parsers to stream through partially parsed objects.\n",
      "Each section of the documentation emphasizes the flexibility, efficiency, and ease of use provided by LCEL, showcasing how it can significantly streamline the process of working with language models by offering a structured approach to building, configuring, and managing complex chains.--Generated 1 clusters--\n",
      "The provided documents offer a comprehensive overview of the LangChain Expression Language (LCEL), a powerful tool designed to facilitate the construction and management of complex operation chains, particularly in the context of language models and related tasks. Here's a consolidated summary of the key points and features highlighted across the documents:\n",
      "1. **Introduction and Purpose**: LCEL is introduced as a declarative method to construct chains within LangChain, aimed at supporting the transition from prototype to production without code changes. It's designed to handle simple to highly complex chains efficiently.\n",
      "2. **Key Features of LCEL**:\n",
      "- **Streaming and Asynchronous Support**: Enables direct streaming of tokens from LLMs to output parsers, supporting both synchronous and asynchronous API calls. This feature is crucial for reducing latency and improving responsiveness.\n",
      "- **Optimized Parallel Execution**: Automatically executes parallelizable steps in a chain to minimize latency, enhancing performance.\n",
      "- **Retry and Fallback Mechanisms**: Offers configurable retry and fallback options for all parts of a chain, increasing reliability at scale.\n",
      "- **Access to Intermediate Results**: Allows access to results from intermediate steps, useful for debugging and providing progress feedback.\n",
      "- **Input and Output Schemas**: Generates Pydantic and JSONSchema schemas from the chain's structure, facilitating input and output validation.\n",
      "- **Integration with LangSmith and LangServe**: Ensures maximum observability and debugging capabilities by automatically logging each step in LangSmith and supporting deployment through LangServe.\n",
      "3. **Runnable Types and Runtime Features**:\n",
      "- Various runnable types such as `RunnableParallel`, `RunnablePassthrough`, `RunnableLambda`, and `RunnableBranch` are introduced, each serving different purposes like data manipulation, dynamic routing, and parallel execution.\n",
      "- Features like runtime argument binding, chain internal configuration at runtime, and the use of the `@chain` decorator to enhance observability and manageability of chains are discussed.\n",
      "4. **Advanced Usage and Examples**:\n",
      "- The documents provide a plethora of examples demonstrating LCEL's versatility, including prompt + LLM chains, adding fallbacks, custom generator function streaming, and managing prompt size.\n",
      "- Specific use cases like routing by semantic similarity, adding memory, and integrating moderation are covered, showcasing LCEL's capability to handle complex logic and dynamic routing based on user input or other criteria.\n",
      "5. **Community and Support**: The documentation emphasizes community engagement and support, encouraging users to contribute feedback and participate in community resources.\n",
      "Overall, the LangChain documentation and the detailed exploration of LCEL highlight its role as a critical tool for developers looking to leverage language models and build sophisticated data processing and transformation pipelines. LCEL's design principles focus on ease of use, flexibility, and efficiency, enabling developers to construct, manage, and scale complex chains with minimal overhead and maximum reliability.\n",
      "논문에서는 collapsed tree retrieval이 최고의 성능을 보고하고 있습니다.\n",
      "이는 트리 구조를 단일 계층으로 평탄화한 다음, 모든 노드에 대해 동시에 k-최근접 이웃(kNN) 검색을 적용하는 과정을 포함합니다.\n",
      "아래에서 이 과정을 간단히 수행합니다.\n",
      "Chroma 벡터 저장소를 사용하여 텍스트 데이터의 벡터화 및 검색 가능한 저장소를 구축하는 과정을 설명합니다.\n",
      "초기에 leaf_texts에 저장된 텍스트 데이터를 all_texts 변수에 복사합니다.\n",
      "결과 데이터(results)를 순회하며 각 레벨에서 요약된 텍스트를 추출하고, 이를 all_texts에 추가합니다.\n",
      "각 레벨의 DataFrame에서 summaries 컬럼의 값을 리스트로 변환하여 추출합니다.\n",
      "추출된 요약문을 all_texts에 추가합니다.\n",
      "모든 텍스트 데이터(all_texts)를 사용하여 Chroma 벡터 저장소를 구축합니다.\n",
      "Chroma.from_texts 함수를 호출하여 텍스트 데이터를 벡터화하고, 벡터 저장소를 생성합니다.\n",
      "생성된 벡터 저장소를 검색 가능하게 만들기 위해 .as_retriever() 메소드를 사용하여 검색기(retriever)를 초기화합니다.\n",
      "이 과정을 통해, 다양한 레벨의 요약문을 포함한 텍스트 데이터를 벡터화하고, 이를 기반으로 검색 가능한 Chroma 벡터 저장소를 구축합니다.\n",
      "from langchain_community.vectorstores import FAISS\n",
      "# leaf_texts를 복사하여 all_texts를 초기화합니다.\n",
      "all_texts = leaf_texts.copy()\n",
      "# 각 레벨의 요약을 추출하여 all_texts에 추가하기 위해 결과를 순회합니다.\n",
      "for level in sorted(results.keys()):\n",
      "# 현재 레벨의 DataFrame에서 요약을 추출합니다.\n",
      "summaries = results[level][1][\"summaries\"].tolist()\n",
      "# 현재 레벨의 요약을 all_texts에 추가합니다.\n",
      "all_texts.extend(summaries)\n",
      "# 이제 all_texts를 사용하여 FAISS vectorstore를 구축합니다.\n",
      "vectorstore = FAISS.from_texts(texts=all_texts, embedding=embd)\n",
      "DB 를 로컬에 저장합니다.\n",
      "import os\n",
      "DB_INDEX = \"RAPTOR\"\n",
      "# 로컬에 FAISS DB 인덱스가 이미 존재하는지 확인하고, 그렇다면 로드하여 vectorstore와 병합한 후 저장합니다.\n",
      "if os.path.exists(DB_INDEX):\n",
      "local_index = FAISS.load_local(DB_INDEX, embd)\n",
      "local_index.merge_from(vectorstore)\n",
      "local_index.save_local(DB_INDEX)\n",
      "else:\n",
      "vectorstore.save_local(folder_path=DB_INDEX)\n",
      "# retriever 생성\n",
      "retriever = vectorstore.as_retriever()\n",
      "Retrieval Augmented Generation(RAG) 체인을 정의하고 특정 코드 예제를 요청하는 방법을 구현합니다.\n",
      "hub.pull을 사용하여 RAG 프롬프트를 불러옵니다.\n",
      "문서 포맷팅을 위한 format_docs 함수를 정의합니다. 이 함수는 문서의 페이지 내용을 연결하여 반환합니다.\n",
      "RAG 체인을 구성합니다. 이 체인은 검색기(retriever)로부터 문맥을 가져오고, format_docs 함수로 포맷팅한 후, 질문을 처리합니다.\n",
      "RunnablePassthrough()를 사용하여 질문을 그대로 전달합니다.\n",
      "체인은 프롬프트, 모델, 그리고 StrOutputParser()를 통해 최종 출력을 문자열로 파싱합니다.\n",
      "rag_chain.invoke 메소드를 사용하여 \"How to define a RAG chain? Give me a specific code example.\"라는 질문을 처리합니다.\n",
      "from langchain import hub\n",
      "from langchain_core.runnables import RunnablePassthrough\n",
      "# 프롬프트 생성\n",
      "prompt = hub.pull(\"rlm/rag-prompt\")\n",
      "# 문서 포스트 프로세싱\n",
      "def format_docs(docs):\n",
      "# 문서의 페이지 내용을 이어붙여 반환합니다.\n",
      "return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
      "# RAG 체인 정의\n",
      "rag_chain = (\n",
      "# 검색 결과를 포맷팅하고 질문을 처리합니다.\n",
      "{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
      "| prompt  # 프롬프트를 적용합니다.\n",
      "| model  # 모델을 적용합니다.\n",
      "| StrOutputParser()  # 문자열 출력 파서를 적용합니다.\n",
      ")\n",
      "LangSmith 링크 (https://smith.langchain.com/public/178afde0-8dcc-472f-8c47-233fe81cbbad/r)LangSmith 링크\n",
      "# 추상적인 질문 실행\n",
      "_ = rag_chain.invoke(\"전체 문서의 핵심 주제에 대해 설명해주세요.\")\n",
      "LangChain 표현 언어(LCEL) 문서는 LangChain을 사용하여 복잡한 데이터 처리 및 변환 파이프라인을 구축하는 방법에 대한 가이드와 예시를 제공합니다. 이 문서는 다양한 Runnable 타입, 런타임 인자 바인딩, 체인 내부 구성, `@chain` 데코레이터 사용, Fallback 추가, 커스텀 제너레이터 함수 스트리밍 등 LCEL의 핵심 기능과 사용 방법을 설명합니다. 또한, LCEL을 사용하여 프롬프트와 LLM을 결합하는 방법, 검색 단계 추가, 여러 체인 연결, SQL DB 쿼리, 코드 작성, 의미적 유사성에 따른 라우팅, 메모리 추가, 검열 추가 등 다양한 작업을 수행하는 방법을 단계별로 제공합니다.\n",
      "LangSmith 링크 (https://smith.langchain.com/public/1d72738b-f378-4676-9961-dd12fe424918/r)LangSmith 링크\n",
      "# Low Level 질문 실행\n",
      "_ = rag_chain.invoke(\"PydanticOutputParser 을 활용한 예시 코드를 작성해 주세요.\")\n",
      "from langchain.output_parsers import PydanticOutputParser\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
      "from langchain_openai import OpenAI\n",
      "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
      "class Joke(BaseModel):\n",
      "setup: str = Field(description=\"question to set up a joke\")\n",
      "punchline: str = Field(description=\"answer to resolve the joke\")\n",
      "@validator(\"setup\")\n",
      "def question_ends_with_question_mark(cls, field):\n",
      "if field[-1] != \"?\":\n",
      "raise ValueError(\"Badly formed question!\")\n",
      "return field\n",
      "parser = PydanticOutputParser(pydantic_object=Joke)\n",
      "prompt = PromptTemplate(\n",
      "template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
      "input_variables=[\"query\"],\n",
      "partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
      ")\n",
      "prompt_and_model = prompt | model\n",
      "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
      "parser.invoke(output)\n",
      "```\n",
      "LangSmith 링크 (https://smith.langchain.com/public/bd725efb-d854-4d0e-b34b-52748dddcf4b/r)LangSmith 링크\n",
      "# Low Level 질문 실행\n",
      "_ = rag_chain.invoke(\"self-querying 방법과 예시 코드를 작성해 주세요.\")\n",
      "Self-querying 방법은 자연어 쿼리를 받아 구조화된 쿼리를 작성하고, 이를 기반으로 VectorStore에 적용하여 문서의 의미적 유사성 비교 및 사용자 쿼리에서 추출한 필터를 메타데이터에 적용하여 실행하는 방식입니다. 예를 들어, Chroma vector store를 사용하여 영화 요약 문서가 포함된 작은 데모 세트를 생성하고, 이를 통해 자체 쿼리 검색기를 인스턴스화할 수 있습니다. 다음은 자체 쿼리 검색기를 사용하는 예시 코드입니다:\n",
      "```python\n",
      "%pip install --upgrade --quiet  lark chromadb\n",
      "from langchain_community.vectorstores import Chroma\n",
      "from langchain_core.documents import Document\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "docs = [\n",
      "Document(\n",
      "page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
      "metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
      "),\n",
      "# Additional documents...\n",
      "]\n",
      "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n",
      "from langchain.chains.query_constructor.base import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "from langchain_openai import ChatOpenAI\n",
      "metadata_field_info = [\n",
      "AttributeInfo(\n",
      "name=\"genre\",\n",
      "description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
      "type=\"string\",\n",
      "),\n",
      "# Additional metadata fields...\n",
      "]\n",
      "document_content_description = \"Brief summary of a movie\"\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "retriever = SelfQueryRetriever.from_llm(\n",
      "llm,\n",
      "vectorstore,\n",
      "document_content_description,\n",
      "metadata_field_info,\n",
      ")\n",
      "# example usage\n",
      "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in retrieved_docs[:5] :\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    # 파일을 로드합니다.\n",
    "    TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\pt_context.txt\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    # 로더를 사용하여 문서를 로드하고 docs 리스트에 추가합니다.\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 50,\n",
    "    separators=['\\n\\n', '\\n', '.', '']\n",
    ")\n",
    "\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
