{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_chapter_meta(chapter_num: str, chapter_title: str) -> dict:\n",
    "    \"\"\"\n",
    "    chapter_numì´ '01' â†’ level 1, parent ì—†ìŒ\n",
    "    chapter_numì´ '01-02' â†’ level 2, parent '01'\n",
    "    \"\"\"\n",
    "    if \"-\" in chapter_num:\n",
    "        parent = chapter_num.split(\"-\")[0]\n",
    "        level = 2\n",
    "    else:\n",
    "        parent = None\n",
    "        level = 1\n",
    "    return {\n",
    "        \"chapter\": chapter_num,\n",
    "        \"title\": chapter_title,\n",
    "        \"level\": level,\n",
    "        \"parent\": parent\n",
    "    }\n",
    "\n",
    "def split_by_chapter(text: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Wikidocs í˜•ì‹ í…ìŠ¤íŠ¸ë¥¼ ì±•í„°ë³„ë¡œ ë‚˜ëˆ„ê³  ê³„ì¸µ ì •ë³´ë¥¼ í¬í•¨í•œ Documentë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    pattern = r\"(?=^---\\s+(\\d{2}(?:-\\d{2})?)\\.\\s+(.*?)\\s+---)\"\n",
    "    matches = list(re.finditer(pattern, text, flags=re.MULTILINE))\n",
    "\n",
    "    documents = []\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "        chapter_num = match.group(1)\n",
    "        chapter_title = match.group(2).strip()\n",
    "        chapter_text = text[start:end].strip()\n",
    "\n",
    "        metadata = extract_chapter_meta(chapter_num, chapter_title)\n",
    "\n",
    "        doc = Document(page_content=chapter_text, metadata=metadata)\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def load_and_split_wikidocs(path: str) -> List[Document]:\n",
    "    text = load_txt(path)\n",
    "    return split_by_chapter(text)\n",
    "\n",
    "def filter_chapters(\n",
    "    documents: List[Document],\n",
    "    level: Optional[int] = None,\n",
    "    parent: Optional[str] = None,\n",
    "    contains_title: Optional[str] = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    ì±•í„° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¡°ê±´ì— ë§ëŠ” ì±•í„°ë§Œ í•„í„°ë§\n",
    "    - level: 1 (ëŒ€ì±•í„°), 2 (ì†Œì±•í„°)\n",
    "    - parent: '01' ë“± ìƒìœ„ ì±•í„° ë²ˆí˜¸\n",
    "    - contains_title: ì œëª© í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for doc in documents:\n",
    "        meta = doc.metadata\n",
    "        if level and meta[\"level\"] != level:\n",
    "            continue\n",
    "        if parent and meta[\"parent\"] != parent:\n",
    "            continue\n",
    "        if contains_title and contains_title not in meta[\"title\"]:\n",
    "            continue\n",
    "        filtered.append(doc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: 165\n",
      "ğŸ”¹ ì†Œì±•í„° ìˆ˜: 0\n",
      "ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: 0\n",
      "ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "first_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt\")\n",
    "\n",
    "# ì†Œì±•í„°ë§Œ ì¶”ì¶œ (level 2)\n",
    "subchapters = filter_chapters(first_docs, level=2)\n",
    "\n",
    "# ëŒ€ì±•í„° 02ì˜ ëª¨ë“  ì†Œì±•í„° ì¶”ì¶œ\n",
    "chap02_children = filter_chapters(first_docs, parent=\"02\")\n",
    "\n",
    "# \"ì„¤ì¹˜\"ë¼ëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì±•í„°ë§Œ ì¶”ì¶œ\n",
    "install_sections = filter_chapters(first_docs, contains_title=\"ì„¤ì¹˜\")\n",
    "\n",
    "print(f\"ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: {len(first_docs)}\")\n",
    "print(f\"ğŸ”¹ ì†Œì±•í„° ìˆ˜: {len(subchapters)}\")\n",
    "print(f\"ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: {len(chap02_children)}\")\n",
    "print(f\"ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: 27\n",
      "ğŸ”¹ ì†Œì±•í„° ìˆ˜: 0\n",
      "ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: 0\n",
      "ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "second_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_02.txt\")\n",
    "\n",
    "# ì†Œì±•í„°ë§Œ ì¶”ì¶œ (level 2)\n",
    "subchapters = filter_chapters(second_docs, level=2)\n",
    "\n",
    "# ëŒ€ì±•í„° 02ì˜ ëª¨ë“  ì†Œì±•í„° ì¶”ì¶œ\n",
    "chap02_children = filter_chapters(second_docs, parent=\"02\")\n",
    "\n",
    "# \"ì„¤ì¹˜\"ë¼ëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì±•í„°ë§Œ ì¶”ì¶œ\n",
    "install_sections = filter_chapters(second_docs, contains_title=\"ì„¤ì¹˜\")\n",
    "\n",
    "print(f\"ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: {len(second_docs)}\")\n",
    "print(f\"ğŸ”¹ ì†Œì±•í„° ìˆ˜: {len(subchapters)}\")\n",
    "print(f\"ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: {len(chap02_children)}\")\n",
    "print(f\"ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: 33\n",
      "ğŸ”¹ ì†Œì±•í„° ìˆ˜: 4\n",
      "ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: 0\n",
      "ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "third_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_03.txt\")\n",
    "\n",
    "# ì†Œì±•í„°ë§Œ ì¶”ì¶œ (level 2)\n",
    "subchapters = filter_chapters(third_docs, level=2)\n",
    "\n",
    "# ëŒ€ì±•í„° 02ì˜ ëª¨ë“  ì†Œì±•í„° ì¶”ì¶œ\n",
    "chap02_children = filter_chapters(third_docs, parent=\"02\")\n",
    "\n",
    "# \"ì„¤ì¹˜\"ë¼ëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì±•í„°ë§Œ ì¶”ì¶œ\n",
    "install_sections = filter_chapters(third_docs, contains_title=\"ì„¤ì¹˜\")\n",
    "\n",
    "print(f\"ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: {len(third_docs)}\")\n",
    "print(f\"ğŸ”¹ ì†Œì±•í„° ìˆ˜: {len(subchapters)}\")\n",
    "print(f\"ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: {len(chap02_children)}\")\n",
    "print(f\"ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "27\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(first_docs))\n",
    "print(len(second_docs))\n",
    "print(len(third_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: 4746\n",
      "ë‘ ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: 1983\n",
      "ì„¸ ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: 2964\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” (í•˜ë‚˜ì˜ ìŠ¤í”Œë¦¬í„°ë§Œ ì‚¬ìš©)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # ì²­í¬ í¬ê¸°\n",
    "    chunk_overlap=50,  # ì²­í¬ ê°„ ì¤‘ë³µ\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # ë¶„í•  ê¸°ì¤€\n",
    ")\n",
    "\n",
    "first_splits = []\n",
    "second_splits = []\n",
    "third_splits = []\n",
    "\n",
    "for doc in first_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    first_splits.extend(splits)\n",
    "    # print(first_splits)\n",
    "\n",
    "for doc in second_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    second_splits.extend(splits)\n",
    "    # print(second_splits)\n",
    "\n",
    "for doc in third_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    third_splits.extend(splits)\n",
    "    # print(third_splits)\n",
    "\n",
    "print(f\"ì²« ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(first_splits)}\")\n",
    "print(f\"ë‘ ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(second_splits)}\")\n",
    "print(f\"ì„¸ ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(third_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4640\\1310670779.py:30: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  first_db.persist()\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embeddings = OpenAIEmbeddings()  # OpenAI ì„ë² ë”© ëª¨ë¸ ì§€ì •\n",
    "\n",
    "# DB ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ì‚­ì œ\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\")\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\")\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë¬¸ì„œ DB ìƒì„±\n",
    "first_db = Chroma.from_texts(\n",
    "    texts=first_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\"\n",
    ")\n",
    "first_db.persist()\n",
    "\n",
    "# # ë‘ ë²ˆì§¸ ë¬¸ì„œ DB ìƒì„±\n",
    "# second_db = Chroma.from_texts(\n",
    "#     texts=second_splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\"\n",
    "# )\n",
    "# second_db.persist()\n",
    "\n",
    "# # ì„¸ ë²ˆì§¸ ë¬¸ì„œ DB ìƒì„±\n",
    "# third_db = Chroma.from_texts(\n",
    "#     texts=third_splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\"\n",
    "# )\n",
    "# third_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# ì›í•˜ëŠ” Pandas DataFrameì„ ì •ì˜í•©ë‹ˆë‹¤.\\ndf = pd.read_csv(\"./data/titanic.csv\")\\ndf.head()'),\n",
       " Document(metadata={}, page_content='PassengerId\\nSurvived\\nPclass\\nName\\nSex\\nAge\\nSibSp\\nParch\\nTicket\\nFare\\nCabin\\nEmbarked\\n1\\n0\\n3\\nBraund, Mr. Owen Harris\\nmale\\n22\\n1\\n0\\nA/5 21171\\n7.25\\nS\\n2\\n1\\n1\\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\\nfemale\\n38\\n1\\n0\\nPC 17599\\n71.2833\\nC85\\nC\\n3\\n1\\n3\\nHeikkinen, Miss. Laina\\nfemale\\nDataFrameLoader\\nPandasëŠ” Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„° ë¶„ì„ ë° ì¡°ì‘ ë„êµ¬ì…ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë°ì´í„° ê³¼í•™, ë¨¸ì‹ ëŸ¬ë‹, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ë°ì´í„° ì‘ì—…ì— ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\\nimport pandas as pd\\n# CSV íŒŒì¼ ì½ê¸°\\ndf = pd.read_csv(\"./data/titanic.csv\")\\nì²« 5ê°œ í–‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤.'),\n",
       " Document(metadata={}, page_content=\"ì—°ê´€í‚¤ì›Œë“œ: ë”¥ëŸ¬ë‹, ìì—°ì–´ ì²˜ë¦¬, ì‹œí€€ìŠ¤ ëª¨ë¸ë§\\níŒë‹¤ìŠ¤ (Pandas)\\nMetadata: {'source': './data/appendix-keywords.txt', 'id': 10, 'relevance_score': 0.9997084}\"),\n",
       " Document(metadata={}, page_content='# !pip install -qU langchain-teddynote\\nfrom langchain_teddynote import logging\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"CH15-Agent-Toolkits\")\\nLangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\\n[í”„ë¡œì íŠ¸ëª…]\\nCH15-Agent-Toolkits\\nimport pandas as pd\\ndf = pd.read_csv(\"./data/titanic.csv\")  # CSV íŒŒì¼ì„ ì½ìŠµë‹ˆë‹¤.\\n# df = pd.read_excel(\"./data/titanic.xlsx\") # ì—‘ì…€ íŒŒì¼ë„ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\ndf.head()\\n[ì´ë¯¸ì§€: ]\\nfrom langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\\nfrom langchain.agents.agent_types import AgentType'),\n",
       " Document(metadata={}, page_content=\"[ì´ë¯¸ì§€: ]\\n[ë„êµ¬ í˜¸ì¶œ]\\nTool: python_repl_ast\\nquery: import pandas as pd\\nimport matplotlib.pyplot as plt\\n# ë‚¨ìì™€ ì—¬ì ìŠ¹ê°ì˜ ìƒì¡´ìœ¨ ê³„ì‚°\\nsurvival_rate = df.groupby('Sex')['Survived'].mean()\\n# barplot ì‹œê°í™”\\nsurvival_rate.plot(kind='bar', color=['blue', 'pink'])\\nplt.title('Survival Rate by Gender')\\nplt.xlabel('Gender')\\nplt.ylabel('Survival Rate')\\nplt.xticks(rotation=0)\\nplt.show()\\nLog:\"),\n",
       " Document(metadata={}, page_content='# !pip install -qU langchain-teddynote\\nfrom langchain_teddynote import logging\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"CH16-Evaluations\")\\nLangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\\n[í”„ë¡œì íŠ¸ëª…]\\nCH16-Evaluations\\nì €ì¥í•œ CSV íŒŒì¼ë¡œë¶€í„° ë¡œë“œ\\ndata/ragas_synthetic_dataset.csv íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\\nimport pandas as pd\\ndf = pd.read_csv(\"data/ragas_synthetic_dataset.csv\")\\ndf.head()\\n[ì´ë¯¸ì§€: ]\\nfrom datasets import Dataset\\ntest_dataset = Dataset.from_pandas(df)\\ntest_dataset'),\n",
       " Document(metadata={}, page_content='ì•„ë˜ ì½”ë“œëŠ” ì—…ë¡œë“œí•œ HuggingFace Dataset ì„ í™œìš©í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\\n(ì°¸ê³ ) ì•„ë˜ì˜ ì£¼ì„ì„ í’€ê³  ì‹¤í–‰í•˜ì—¬ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì—…ë°ì´íŠ¸ í›„ ì§„í–‰í•´ ì£¼ì„¸ìš”.\\n# !pip install -qU datasets\\nimport pandas as pd\\nfrom datasets import load_dataset, Dataset\\nimport os\\n# huggingface Datasetì—ì„œ repo_idë¡œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\\ndataset = load_dataset(\\n\"teddylee777/rag-synthetic-dataset\",  # ë°ì´í„°ì…‹ ì´ë¦„\\ntoken=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # private ë°ì´í„°ì¸ ê²½ìš° í•„ìš”í•©ë‹ˆë‹¤.\\n)\\n# ë°ì´í„°ì…‹ì—ì„œ split ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ\\nhuggingface_df = dataset[\"korean_v1\"].to_pandas()\\nhuggingface_df.head()\\n[ì´ë¯¸ì§€: ]'),\n",
       " Document(metadata={}, page_content='----------------------------------------------------------------------------------------------------\\nDocument 6:\\níŒë‹¤ìŠ¤ (Pandas)\\nì •ì˜: íŒë‹¤ìŠ¤ëŠ” íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ìœ„í•œ ë°ì´í„° ë¶„ì„ ë° ì¡°ì‘ ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ë¶„ì„ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\\nì˜ˆì‹œ: íŒë‹¤ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ CSV íŒŒì¼ì„ ì½ê³ , ë°ì´í„°ë¥¼ ì •ì œí•˜ë©°, ë‹¤ì–‘í•œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì—°ê´€í‚¤ì›Œë“œ: ë°ì´í„° ë¶„ì„, íŒŒì´ì¬, ë°ì´í„° ì²˜ë¦¬\\nGPT (Generative Pretrained Transformer)\\nì •ì˜: GPTëŠ” ëŒ€ê·œëª¨ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ìƒì„±ì  ì–¸ì–´ ëª¨ë¸ë¡œ, ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ì—…ì— í™œìš©ë©ë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì˜ˆì‹œ: ì‚¬ìš©ìê°€ ì œê³µí•œ ì§ˆë¬¸ì— ëŒ€í•´ ìì„¸í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì±—ë´‡ì€ GPT ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'),\n",
       " Document(metadata={}, page_content='```\\n22.19937\\n```PandasDataFrameOutputParser\\nPandas DataFrameì€ Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° êµ¬ì¡°ë¡œ, ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•´ í”íˆ ì‚¬ìš©ë©ë‹ˆë‹¤. êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ í¬ê´„ì ì¸ ë„êµ¬ ì„¸íŠ¸ë¥¼ ì œê³µí•˜ì—¬, ë°ì´í„° ì •ì œ, ë³€í™˜ ë° ë¶„ì„ê³¼ ê°™ì€ ì‘ì—…ì— ë‹¤ì–‘í•˜ê²Œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì´ ì¶œë ¥ íŒŒì„œëŠ” ì‚¬ìš©ìê°€ ì„ì˜ì˜ Pandas DataFrameì„ ì§€ì •í•˜ê³  í•´ë‹¹ DataFrameì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ í˜•ì‹í™”ëœ ì‚¬ì „ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” LLMì„ ìš”ì²­í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nTrue\\n# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com\\n# !pip install langchain-teddynote\\nfrom langchain_teddynote import logging'),\n",
       " Document(metadata={}, page_content='Document 7:\\níŒë‹¤ìŠ¤ (Pandas)\\nì •ì˜: íŒë‹¤ìŠ¤ëŠ” íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ìœ„í•œ ë°ì´í„° ë¶„ì„ ë° ì¡°ì‘ ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ë¶„ì„ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\\nì˜ˆì‹œ: íŒë‹¤ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ CSV íŒŒì¼ì„ ì½ê³ , ë°ì´í„°ë¥¼ ì •ì œí•˜ë©°, ë‹¤ì–‘í•œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì—°ê´€í‚¤ì›Œë“œ: ë°ì´í„° ë¶„ì„, íŒŒì´ì¬, ë°ì´í„° ì²˜ë¦¬\\nGPT (Generative Pretrained Transformer)\\nì •ì˜: GPTëŠ” ëŒ€ê·œëª¨ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ìƒì„±ì  ì–¸ì–´ ëª¨ë¸ë¡œ, ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ì—…ì— í™œìš©ë©ë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì˜ˆì‹œ: ì‚¬ìš©ìê°€ ì œê³µí•œ ì§ˆë¬¸ì— ëŒ€í•´ ìì„¸í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì±—ë´‡ì€ GPT ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì—°ê´€í‚¤ì›Œë“œ: ìì—°ì–´ ì²˜ë¦¬, í…ìŠ¤íŠ¸ ìƒì„±, ë”¥ëŸ¬ë‹\\nInstructGPT')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10}).invoke('import pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from typing import List\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def load_txt(path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     í…ìŠ¤íŠ¸ íŒŒì¼ì„ UTF-8ë¡œ ë¡œë“œ\n",
    "#     \"\"\"\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return f.read()\n",
    "\n",
    "# def extract_chapter_meta(chapter_num: str, chapter_title: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     ê°œì„ ëœ ë²„ì „: ì œëª© ë‚´ [ì¹´í…Œê³ ë¦¬] â†’ parent\n",
    "#     \"\"\"\n",
    "#     # [ì¹´í…Œê³ ë¦¬]ê°€ ì¡´ì¬í•˜ë©´ parentë¡œ ì¶”ì •\n",
    "#     category_match = re.match(r\"\\[(.*?)\\]\", chapter_title)\n",
    "#     if category_match:\n",
    "#         parent = category_match.group(1)\n",
    "#     else:\n",
    "#         parent = None\n",
    "\n",
    "#     return {\n",
    "#         \"chapter\": chapter_num,\n",
    "#         \"title\": chapter_title,\n",
    "#         \"level\": 1,  # ì‹¤ì œ íŒŒì¼ì—ëŠ” ëŒ€ì±•í„°ë§Œ ì¡´ì¬\n",
    "#         \"parent\": parent\n",
    "#     }\n",
    "\n",
    "# def protect_code_blocks(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     ```ë¡œ ê°ì‹¸ì§„ ì½”ë“œ ë¸”ë¡ì„ <CODE_BLOCK>ìœ¼ë¡œ ê°ì‹¸ ë³´ì¡´\n",
    "#     \"\"\"\n",
    "#     code_pattern = re.compile(r\"```.*?\\n.*?```\", re.DOTALL)\n",
    "#     protected = []\n",
    "#     last_end = 0\n",
    "\n",
    "#     for match in code_pattern.finditer(text):\n",
    "#         start, end = match.span()\n",
    "#         protected.append(text[last_end:start])\n",
    "#         code = match.group()\n",
    "#         protected.append(f\"\\n<CODE_BLOCK>\\n{code}\\n</CODE_BLOCK>\\n\")\n",
    "#         last_end = end\n",
    "\n",
    "#     protected.append(text[last_end:])\n",
    "#     return \"\".join(protected)\n",
    "\n",
    "# def split_by_chapter_with_code(text: str) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     ì±•í„° í—¤ë”(--- 01. ì œëª© ---) ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ë˜, ê° ì±•í„° ë‚´ ì½”ë“œë¸”ë¡ ë³´ì¡´\n",
    "#     \"\"\"\n",
    "#     pattern = r\"(?=^---\\s+(\\d{2}(?:-\\d{2})?)\\.\\s+(.*?)\\s+---)\"\n",
    "#     matches = list(re.finditer(pattern, text, flags=re.MULTILINE))\n",
    "\n",
    "#     documents = []\n",
    "#     for i, match in enumerate(matches):\n",
    "#         start = match.start()\n",
    "#         end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "#         chapter_num = match.group(1)\n",
    "#         chapter_title = match.group(2).strip()\n",
    "#         chapter_text = text[start:end].strip()\n",
    "\n",
    "#         protected_text = protect_code_blocks(chapter_text)\n",
    "#         metadata = extract_chapter_meta(chapter_num, chapter_title)\n",
    "\n",
    "#         documents.append(Document(page_content=protected_text, metadata=metadata))\n",
    "\n",
    "#     return documents\n",
    "\n",
    "# def process_wikidocs_files(paths: List[str]) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     ì—¬ëŸ¬ Wikidocs í˜•ì‹ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "#     \"\"\"\n",
    "#     all_docs = []\n",
    "#     for path in paths:\n",
    "#         text = load_txt(path)\n",
    "#         docs = split_by_chapter_with_code(text)\n",
    "#         all_docs.extend(docs)\n",
    "#     return all_docs\n",
    "\n",
    "# def extract_and_replace_code_blocks(text: str):\n",
    "#     \"\"\"\n",
    "#     <CODE_BLOCK>...</CODE_BLOCK> êµ¬ê°„ì„ [[CODE:0]], [[CODE:1]], ...ë¡œ ì¹˜í™˜\n",
    "#     \"\"\"\n",
    "#     code_blocks = []\n",
    "#     pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "\n",
    "#     def replacer(match):\n",
    "#         code_blocks.append(match.group())\n",
    "#         return f\"[[CODE:{len(code_blocks) - 1}]]\"\n",
    "\n",
    "#     modified_text = pattern.sub(replacer, text)\n",
    "#     return modified_text, code_blocks\n",
    "\n",
    "# def split_protected_chunks(docs: List[Document], chunk_size=1000, chunk_overlap=200) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     ì½”ë“œ ë¸”ë¡ì´ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë³´í˜¸í•œ ìƒíƒœë¡œ chunk ë¶„í• \n",
    "#     \"\"\"\n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap\n",
    "#     )\n",
    "\n",
    "#     chunked_docs = []\n",
    "#     for doc in docs:\n",
    "#         # ì½”ë“œ ë¸”ë¡ì„ ì„ì‹œ í† í°ìœ¼ë¡œ ì¹˜í™˜\n",
    "#         mod_text, code_blocks = extract_and_replace_code_blocks(doc.page_content)\n",
    "#         temp_doc = Document(page_content=mod_text, metadata=doc.metadata)\n",
    "\n",
    "#         # ë¶„í• \n",
    "#         split_docs = splitter.split_documents([temp_doc])\n",
    "\n",
    "#         # ì½”ë“œ ë¸”ë¡ ë³µì›\n",
    "#         for d in split_docs:\n",
    "#             restored_text = d.page_content\n",
    "#             for i, block in enumerate(code_blocks):\n",
    "#                 restored_text = restored_text.replace(f\"[[CODE:{i}]]\", block)\n",
    "#             d.page_content = restored_text\n",
    "#             chunked_docs.append(d)\n",
    "\n",
    "#     return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def protect_code_blocks(text: str) -> str:\n",
    "    code_pattern = re.compile(r\"```.*?\\n.*?```\", re.DOTALL)\n",
    "    protected = []\n",
    "    last_end = 0\n",
    "    for match in code_pattern.finditer(text):\n",
    "        start, end = match.span()\n",
    "        protected.append(text[last_end:start])\n",
    "        code = match.group()\n",
    "        protected.append(f\"\\n<CODE_BLOCK>\\n{code}\\n</CODE_BLOCK>\\n\")\n",
    "        last_end = end\n",
    "    protected.append(text[last_end:])\n",
    "    return \"\".join(protected)\n",
    "\n",
    "def split_by_structure_with_code(text: str) -> List[Document]:\n",
    "    lines = text.splitlines()\n",
    "    documents = []\n",
    "    buffer = []\n",
    "\n",
    "    current_chapter_num = None\n",
    "    current_chapter_title = None\n",
    "    current_section_num = None\n",
    "    current_section_title = None\n",
    "\n",
    "    chapter_pattern = re.compile(r\"^---\\s+CH(\\d+)\\s+(.*?)\\s+---$\")\n",
    "    section_pattern = re.compile(r\"^---\\s+(\\d{2})\\.\\s+(.*?)\\s+---$\")\n",
    "    title_only_pattern = re.compile(r\"^(---|===).*?(---|===)$\")\n",
    "\n",
    "    for line in lines:\n",
    "        chapter_match = chapter_pattern.match(line)\n",
    "        section_match = section_pattern.match(line)\n",
    "\n",
    "        if chapter_match:\n",
    "            # flush ì´ì „ buffer\n",
    "            if buffer:\n",
    "                chunk = \"\\n\".join(buffer).strip()\n",
    "                # ì œëª©ë§Œ ìˆëŠ” ë¬¸ì„œì¸ì§€ í™•ì¸\n",
    "                if not is_title_only_document(chunk):\n",
    "                    protected = protect_code_blocks(chunk)\n",
    "                    documents.append(Document(\n",
    "                        page_content=protected,\n",
    "                        metadata={\n",
    "                            \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                            \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "                        }\n",
    "                    ))\n",
    "                buffer = []\n",
    "\n",
    "            current_chapter_num = chapter_match.group(1)\n",
    "            current_chapter_title = chapter_match.group(2).strip()\n",
    "            current_section_num, current_section_title = None, None\n",
    "\n",
    "        elif section_match:\n",
    "            if buffer:\n",
    "                chunk = \"\\n\".join(buffer).strip()\n",
    "                # ì œëª©ë§Œ ìˆëŠ” ë¬¸ì„œì¸ì§€ í™•ì¸\n",
    "                if not is_title_only_document(chunk):\n",
    "                    protected = protect_code_blocks(chunk)\n",
    "                    documents.append(Document(\n",
    "                        page_content=protected,\n",
    "                        metadata={\n",
    "                            \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                            \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "                        }\n",
    "                    ))\n",
    "                buffer = []\n",
    "\n",
    "            current_section_num = section_match.group(1)\n",
    "            current_section_title = section_match.group(2).strip()\n",
    "\n",
    "        buffer.append(line)\n",
    "\n",
    "    if buffer:\n",
    "        chunk = \"\\n\".join(buffer).strip()\n",
    "        # ì œëª©ë§Œ ìˆëŠ” ë¬¸ì„œì¸ì§€ í™•ì¸\n",
    "        if not is_title_only_document(chunk):\n",
    "            protected = protect_code_blocks(chunk)\n",
    "            documents.append(Document(\n",
    "                page_content=protected,\n",
    "                metadata={\n",
    "                    \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                    \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "                }\n",
    "            ))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def is_title_only_document(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œê°€ ì œëª©ë§Œ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ê³µë°±ê³¼ ì¤„ë°”ê¿ˆì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
    "    cleaned_text = text.strip()\n",
    "    \n",
    "    # ì œëª© íŒ¨í„´ (---ë¡œ ì‹œì‘í•˜ê³  ---ë¡œ ëë‚˜ê±°ë‚˜, ===ë¡œ ì‹œì‘í•˜ê³  ===ë¡œ ëë‚˜ëŠ” íŒ¨í„´)\n",
    "    title_pattern = re.compile(r\"^(---|===).*?(---|===)$\", re.DOTALL)\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ê°€ ì œëª© íŒ¨í„´ë§Œ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸\n",
    "    if title_pattern.match(cleaned_text):\n",
    "        # ì œëª© íŒ¨í„´ì„ ì œê±°í•œ í›„ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "        content_without_title = re.sub(r\"(---|===).*?(---|===)\", \"\", cleaned_text, flags=re.DOTALL).strip()\n",
    "        return not content_without_title\n",
    "    \n",
    "    return False\n",
    "\n",
    "def process_wikidocs_files(paths: List[str]) -> List[Document]:\n",
    "    all_docs = []\n",
    "    for path in paths:\n",
    "        text = load_txt(path)\n",
    "        docs = split_by_structure_with_code(text)\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    "def extract_and_replace_code_blocks(text: str):\n",
    "    code_blocks = []\n",
    "    pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "    def replacer(match):\n",
    "        code_blocks.append(match.group())\n",
    "        return f\"[[CODE:{len(code_blocks) - 1}]]\"\n",
    "    modified_text = pattern.sub(replacer, text)\n",
    "    return modified_text, code_blocks\n",
    "\n",
    "def split_protected_chunks(docs: List[Document], chunk_size=2000, chunk_overlap=50) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunked_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        mod_text, code_blocks = extract_and_replace_code_blocks(doc.page_content)\n",
    "        temp_doc = Document(page_content=mod_text, metadata=doc.metadata)\n",
    "        split_docs = splitter.split_documents([temp_doc])\n",
    "\n",
    "        for d in split_docs:\n",
    "            # ì‹¤ì œë¡œ ì‚¬ìš©ëœ [[CODE:X]]ë§Œ ì°¾ì•„ì„œ ë³µì›\n",
    "            used_codes = re.findall(r\"\\[\\[CODE:(\\d+)\\]\\]\", d.page_content)\n",
    "            for code_index in set(used_codes):\n",
    "                code_index = int(code_index)\n",
    "                if code_index < len(code_blocks):\n",
    "                    d.page_content = d.page_content.replace(f\"[[CODE:{code_index}]]\", code_blocks[code_index])\n",
    "            chunked_docs.append(d)\n",
    "\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ì›ë³¸ ì±•í„° ìˆ˜: 184\n",
      "ì´ ë¶„í• ëœ ë¬¸ì„œ ìˆ˜ (ì„ë² ë”©ìš©): 1241\n",
      "--- ìƒ˜í”Œ ---\n",
      "--- CH01 LangChain ì‹œì‘í•˜ê¸° ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Wikidocs í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì±•í„° ë‹¨ìœ„ë¡œ ë¡œë”© (ì„¤ëª… + ì½”ë“œ ë³´ì¡´)\n",
    "docs = process_wikidocs_files([\n",
    "    \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt\",\n",
    "    \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_02.txt\",\n",
    "    \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_03.txt\"\n",
    "])\n",
    "\n",
    "# 2. ì„ë² ë”© ì „ìš© chunk ë‹¨ìœ„ë¡œ ë¶„í•  (ì½”ë“œë¸”ë¡ ë³´í˜¸ë¨)\n",
    "chunked_docs = split_protected_chunks(docs)\n",
    "\n",
    "# 3. í™•ì¸\n",
    "print(f\"ì´ ì›ë³¸ ì±•í„° ìˆ˜: {len(docs)}\")\n",
    "print(f\"ì´ ë¶„í• ëœ ë¬¸ì„œ ìˆ˜ (ì„ë² ë”©ìš©): {len(chunked_docs)}\")\n",
    "print(\"--- ìƒ˜í”Œ ---\")\n",
    "print(chunked_docs[1].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶”ì¶œ\n",
    "text_contents = [doc.page_content for doc in chunked_docs]\n",
    "\n",
    "test_db = Chroma.from_texts(\n",
    "    texts=text_contents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\test_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='--- 04. FlashRank Reranker ---'),\n",
       " Document(metadata={}, page_content='--- 04. FlashRank Reranker ---'),\n",
       " Document(metadata={}, page_content='--- 04. LLM ì²´ì¸ ë¼ìš°íŒ…(RunnableLambda, RunnableBranch) ---'),\n",
       " Document(metadata={}, page_content='--- 04. LLM ì²´ì¸ ë¼ìš°íŒ…(RunnableLambda, RunnableBranch) ---'),\n",
       " Document(metadata={}, page_content='<CODE_BLOCK>\\n```\\nPassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked\\n```\\n</CODE_BLOCK>\\nPassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked\\nì»¬ëŸ¼ì— ëŒ€í•œ ê°’ì„ ì¡°íšŒí•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n# ì—´ ì‘ì—… ì˜ˆì‹œì…ë‹ˆë‹¤.\\ndf_query = \"Age column ì„ ì¡°íšŒí•´ ì£¼ì„¸ìš”.\"\\n# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì„¤ì •í•©ë‹ˆë‹¤.\\nprompt = PromptTemplate(\\ntemplate=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",\\ninput_variables=[\"query\"],  # ì…ë ¥ ë³€ìˆ˜ ì„¤ì •\\npartial_variables={\\n\"format_instructions\": parser.get_format_instructions()\\n},  # ë¶€ë¶„ ë³€ìˆ˜ ì„¤ì •\\n)\\n# ì²´ì¸ ìƒì„±\\nchain = prompt | model | parser\\n# ì²´ì¸ ì‹¤í–‰\\nparser_output = chain.invoke({\"query\": df_query})\\n# ì¶œë ¥\\nformat_parser_output(parser_output)\\n{\\'Age\\': {0: 22.0,\\n1: 38.0,\\n2: 26.0,\\n3: 35.0,\\n4: 35.0,\\n5: nan,\\n6: 54.0,\\n7: 2.0,\\n8: 27.0,\\n9: 14.0,\\n10: 4.0,\\n11: 58.0,\\n12: 20.0,\\n13: 39.0,\\n14: 14.0,\\n15: 55.0,\\n16: 2.0,\\n17: nan,\\n18: 31.0,\\n19: nan}}\\nì²« ë²ˆì§¸ í–‰ì„ ê²€ìƒ‰í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\\n# í–‰ ì¡°íšŒ ì˜ˆì‹œì…ë‹ˆë‹¤.\\ndf_query = \"Retrieve the first row.\"\\n# ì²´ì¸ ì‹¤í–‰\\nparser_output = chain.invoke({\"query\": df_query})\\n# ê²°ê³¼ ì¶œë ¥\\nformat_parser_output(parser_output)\\n{\\'0\\': {\\'Age\\': 22.0,\\n\\'Cabin\\': nan,\\n\\'Embarked\\': \\'S\\',\\n\\'Fare\\': 7.25,\\n\\'Name\\': \\'Braund, \\'\\n\\'Mr. \\'\\n\\'Owen \\'\\n\\'Harris\\',\\n\\'Parch\\': 0,\\n\\'PassengerId\\': 1,\\n\\'Pclass\\': 3,\\n\\'Sex\\': \\'male\\',\\n\\'SibSp\\': 1,\\n\\'Survived\\': 0,\\n\\'Ticket\\': \\'A/5 \\'\\n\\'21171\\'}}\\níŠ¹ì • ì—´ì—ì„œ ì¼ë¶€ í–‰ì˜ í‰ê· ì„ ê²€ìƒ‰í•˜ëŠ” ì‘ì—… ì˜ˆì œì…ë‹ˆë‹¤.\\n# row 0 ~ 4ì˜ í‰ê·  ë‚˜ì´ë¥¼ êµ¬í•©ë‹ˆë‹¤.\\ndf[\"Age\"].head().mean()\\n31.2\\n# ì„ì˜ì˜ Pandas DataFrame ì‘ì—… ì˜ˆì‹œ, í–‰ì˜ ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.\\ndf_query = \"Retrieve the average of the Ages from row 0 to 4.\"\\n# ì²´ì¸ ì‹¤í–‰\\nparser_output = chain.invoke({\"query\": df_query})\\n# ê²°ê³¼ ì¶œë ¥\\nprint(parser_output)\\n{\\'mean\\': 31.2}\\në‹¤ìŒì€ ìš”ê¸ˆ(Fare) ì— ëŒ€í•œ í‰ê·  ê°€ê²©ì„ ì‚°ì •í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\\n# ì˜ëª» í˜•ì‹í™”ëœ ì¿¼ë¦¬ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.\\ndf_query = \"Calculate average `Fare` rate.\"\\n# ì²´ì¸ ì‹¤í–‰\\nparser_output = chain.invoke({\"query\": df_query})\\n# ê²°ê³¼ ì¶œë ¥\\nprint(parser_output)\\n{\\'mean\\': 22.19937}\\n# ê²°ê³¼ ê²€ì¦\\ndf[\"Fare\"].mean()\\n22.19937\\n\\n==================================================')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}).invoke('ë­ì²´ì¸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: None\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 01. ì„¤ì¹˜ ì˜ìƒë³´ê³  ë”°ë¼í•˜ê¸°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 02. OpenAI API í‚¤ ë°œê¸‰ ë° í…ŒìŠ¤íŠ¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 03. LangSmith ì¶”ì  ì„¤ì •\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 04. OpenAI API ì‚¬ìš©(GPT-4o ë©€í‹°ëª¨ë‹¬)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 05. LangChain Expression Language(LCEL)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 06. LCEL ì¸í„°í˜ì´ìŠ¤\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 07. Runnable\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: 01. í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: 02. í“¨ìƒ· í”„ë¡¬í”„íŠ¸(FewShotPromptTemplate)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: 03. LangChain Hub\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: 04. ê°œì¸í™”ëœ í”„ë¡¬í”„íŠ¸(Hubì— ì—…ë¡œë“œ)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 01. Pydantic ì¶œë ¥ íŒŒì„œ(PydanticOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 02. ì½¤ë§ˆ êµ¬ë¶„ì ì¶œë ¥ íŒŒì„œ(CommaSeparatedListOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 03. êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì„œ(StructuredOuputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 04. JSON ì¶œë ¥ íŒŒì„œ(JsonOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 05. ë°ì´í„°í”„ë ˆì„ ì¶œë ¥ íŒŒì„œ(PandasDataFrameOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 06. ë‚ ì§œ í˜•ì‹ ì¶œë ¥ íŒŒì„œ(DatetimeOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 07. ì—´ê±°í˜• ì¶œë ¥ íŒŒì„œ(EnumOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 08. ì¶œë ¥ ìˆ˜ì • íŒŒì„œ(OutputFixingParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 01. ë‹¤ì–‘í•œ LLM ëª¨ë¸ í™œìš©\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 02. ìºì‹±(Cache)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 03. ëª¨ë¸ ì§ë ¬í™”(Serialization) - ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 04. í† í° ì‚¬ìš©ëŸ‰ í™•ì¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 05. êµ¬ê¸€ ìƒì„± AI(Google Generative AI)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 06. í—ˆê¹…í˜ì´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸(HuggingFace Endpoints)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 07. í—ˆê¹…í˜ì´ìŠ¤ ë¡œì»¬(HuggingFace Local)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 08. í—ˆê¹…í˜ì´ìŠ¤ íŒŒì´í”„ë¼ì¸(HuggingFace Pipeline)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 09. ì˜¬ë¼ë§ˆ(Ollama)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 10. GPT4ALL\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 11. ë¹„ë””ì˜¤(Video) ì§ˆì˜ ì‘ë‹µ LLM (Gemini)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 01. ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬(ConversationBufferMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 02. ëŒ€í™” ë²„í¼ ìœˆë„ìš° ë©”ëª¨ë¦¬(ConversationBufferWindowMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 03. ëŒ€í™” í† í° ë²„í¼ ë©”ëª¨ë¦¬(ConversationTokenBufferMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 04. ëŒ€í™” ì—”í‹°í‹° ë©”ëª¨ë¦¬(ConversationEntityMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 05. ëŒ€í™” ì§€ì‹ê·¸ë˜í”„ ë©”ëª¨ë¦¬(ConversationKGMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 06. ëŒ€í™” ìš”ì•½ ë©”ëª¨ë¦¬(ConversationSummaryMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 07. ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ ë©”ëª¨ë¦¬(VectorStoreRetrieverMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 08. LCEL Chain ì— ë©”ëª¨ë¦¬ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 09. SQLite ì— ëŒ€í™”ë‚´ìš© ì €ì¥\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 10. RunnableWithMessageHistoryì— ChatMessageHistoryì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 01. ë„íë¨¼íŠ¸(Document) ì˜ êµ¬ì¡°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 02. PDF\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 03. í•œê¸€(HWP)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 04. CSV\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 05. Excel\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 06. Word\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 07. PowerPoint\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 08. ì›¹ ë¬¸ì„œ(WebBaseLoader)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 09. í…ìŠ¤íŠ¸(TextLoader)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 10. JSON\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 11. Arxiv\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 12. UpstageLayoutAnalysisLoader\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 13. LlamaParser\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 01. ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• (CharacterTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 02. ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• (RecursiveCharacterTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 03. í† í° í…ìŠ¤íŠ¸ ë¶„í• (TokenTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 04. ì‹œë©˜í‹± ì²­ì»¤(SemanticChunker)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 05. ì½”ë“œ ë¶„í• (Python, Markdown, JAVA, C++, C#, GO, JS, Latex ë“±)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 06. ë§ˆí¬ë‹¤ìš´ í—¤ë” í…ìŠ¤íŠ¸ ë¶„í• (MarkdownHeaderTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 07. HTML í—¤ë” í…ìŠ¤íŠ¸ ë¶„í• (HTMLHeaderTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 08. ì¬ê·€ì  JSON ë¶„í• (RecursiveJsonSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 01. OpenAIEmbeddings\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 02. ìºì‹œ ì„ë² ë”©(CacheBackedEmbeddings)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 03. í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”©(HuggingFace Embeddings)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 04. UpstageEmbeddings\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 05. OllamaEmbeddings\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 06. GPT4ALL ì„ë² ë”©\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 07. Llama CPP ì„ë² ë”©\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)\n",
      "  section_info: 01. Chroma\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)\n",
      "  section_info: 02. FAISS\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)\n",
      "  section_info: 03. Pinecone\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 01. ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ ê²€ìƒ‰ê¸°(VectorStore-backed Retriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 02. ë¬¸ë§¥ ì••ì¶• ê²€ìƒ‰ê¸°(ContextualCompressionRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 03. ì•™ìƒë¸” ê²€ìƒ‰ê¸°(EnsembleRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 04. ê¸´ ë¬¸ë§¥ ì¬ì •ë ¬(LongContextReorder)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 05. ìƒìœ„ ë¬¸ì„œ ê²€ìƒ‰ê¸°(ParentDocumentRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 06. ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°(MultiQueryRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 07. ë‹¤ì¤‘ ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ê¸°(MultiVectorRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 08. ì…€í”„ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°(SelfQueryRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 09. ì‹œê°„ ê°€ì¤‘ ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ê¸°(TimeWeightedVectorStoreRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 10. í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸°(Kiwi, Kkma, Okt) + BM25 ê²€ìƒ‰ê¸°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 11. Convex Combination(CC) ì ìš©ëœ ì•™ìƒë¸” ê²€ìƒ‰ê¸°(EnsembleRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: 01. Cross Encoder Reranker\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: 02. Cohere Reranker\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: 03. Jina Reranker\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: 04. FlashRank Reranker\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 01. PDF ë¬¸ì„œ ê¸°ë°˜ QA(Question-Answer)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 02. ë„¤ì´ë²„ ë‰´ìŠ¤ê¸°ì‚¬ QA(Question-Answer)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 03. RAG ì˜ ê¸°ëŠ¥ë³„ ë‹¤ì–‘í•œ ëª¨ë“ˆ í™œìš©ê¸°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 04. RAPTOR: ê¸´ ë¬¸ë§¥ ìš”ì•½(Long Context Summary)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 05. ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” RAG ì²´ì¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 01. RunnablePassthrough\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 02. Runnable êµ¬ì¡°(ê·¸ë˜í”„) ê²€í† \n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 03. RunnableLambda\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 04. LLM ì²´ì¸ ë¼ìš°íŒ…(RunnableLambda, RunnableBranch)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 05. RunnableParallel\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 06. ë™ì  ì†ì„± ì§€ì •(configurable_fields, configurable_alternatives)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 07. @chain ë°ì½”ë ˆì´í„°ë¡œ Runnable êµ¬ì„±\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 08. RunnableWithMessageHistory\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 09. ì‚¬ìš©ì ì •ì˜ ì œë„¤ë ˆì´í„°(generator)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 10. Runtime Arguments ë°”ì¸ë”©\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 11. í´ë°±(fallback) ëª¨ë¸ ì§€ì •\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH14 ì²´ì¸(Chains)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH14 ì²´ì¸(Chains)\n",
      "  section_info: 01. ë¬¸ì„œ ìš”ì•½\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH14 ì²´ì¸(Chains)\n",
      "  section_info: 02. SQL\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH14 ì²´ì¸(Chains)\n",
      "  section_info: 03. êµ¬ì¡°í™”ëœ ì¶œë ¥ ì²´ì¸(with_structered_output)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 01. í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±(RAGAS)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 02. RAGAS ë¥¼ í™œìš©í•œ í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 03. ìƒì„±í•œ í‰ê°€ìš© ë°ì´í„°ì…‹ ì—…ë¡œë“œ(HuggingFace Dataset)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 04. LangSmith ë°ì´í„°ì…‹ ìƒì„±\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 05. LLM-as-Judge\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 06. ì„ë² ë”© ê¸°ë°˜ í‰ê°€(embedding_distance)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 07. ì‚¬ìš©ì ì •ì˜(Custom) LLM í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 08. Rouge, BLEU, METEOR, SemScore ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 09. ì‹¤í—˜(Experiment) í‰ê°€ ë¹„êµ\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 10. ìš”ì•½(Summary) ë°©ì‹ì˜ í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 11. Groundedness(í• ë£¨ì‹œë„¤ì´ì…˜) í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 12. ì‹¤í—˜ ë¹„êµ(Pairwise Evaluation)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 13. ë°˜ë³µ í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 14. ì˜¨ë¼ì¸ í‰ê°€ë¥¼ í™œìš©í•œ í‰ê°€ ìë™í™”\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 01. ë„êµ¬(Tools)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 02. ë„êµ¬ ë°”ì¸ë”©(Binding Tools)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 03. ì—ì´ì „íŠ¸(Agent)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 04. Claude, Gemini, Ollama, Together.ai ë¥¼ í™œìš©í•œ Agent\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 05. Iteration ê¸°ëŠ¥ê³¼ ì‚¬ëŒ ê°œì…(Human-in-the-loop)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 06. Agentic RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 07. CSVExcel ë°ì´í„° ë¶„ì„ Agent\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 08. Toolkits í™œìš© Agent\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 09. RAG + Image Generator Agent(ë³´ê³ ì„œ ì‘ì„±)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 10. ë„êµ¬ë¥¼ í™œìš©í•œ í† ë¡  ì—ì´ì „íŠ¸(Two Agent Debates with Tools)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. í•µì‹¬ ê¸°ëŠ¥\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. LangGraph ì— ìì£¼ ë“±ì¥í•˜ëŠ” Python ë¬¸ë²•ì´í•´\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. LangGraphë¥¼ í™œìš©í•œ ì±—ë´‡ êµ¬ì¶•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. LangGraphë¥¼ í™œìš©í•œ Agent êµ¬ì¶•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. Agent ì— ë©”ëª¨ë¦¬(memory) ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. ë…¸ë“œì˜ ë‹¨ê³„ë³„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. Human-in-the-loop(ì‚¬ëŒì˜ ê°œì…)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. ì¤‘ê°„ë‹¨ê³„ ê°œì…  ë˜ëŒë¦¼ì„ í†µí•œ ìƒíƒœ ìˆ˜ì •ê³¼ Replay\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 08. ì‚¬ëŒ(Human)ì—ê²Œ ë¬¼ì–´ë³´ëŠ” ë…¸ë“œ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 09. ë©”ì‹œì§€ ì‚­ì œ(RemoveMessage)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 10. ToolNode ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 11. ë³‘ë ¬ ë…¸ë“œ ì‹¤í–‰ì„ ìœ„í•œ ë¶„ê¸° ìƒì„± ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 12. ëŒ€í™” ê¸°ë¡ ìš”ì•½ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 13. ì„œë¸Œê·¸ë˜í”„ ì¶”ê°€ ë° ì‚¬ìš© ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 14. ì„œë¸Œê·¸ë˜í”„ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë³€í™˜í•˜ëŠ” ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 15. LangGraph ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì˜ ëª¨ë“  ê²ƒ\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. êµ¬ì¡° ì„¤ê³„\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. ê¸°ë³¸ ê·¸ë˜í”„ ìƒì„±\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. Naive RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. ê´€ë ¨ì„± ì²´ì»¤(Relevance Checker) ëª¨ë“ˆ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. ì›¹ ê²€ìƒ‰ ëª¨ë“ˆ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. ì¿¼ë¦¬ ì¬ì‘ì„± ëª¨ë“ˆ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. Agentic RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. Adaptive RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. Use Cases\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. ì—ì´ì „íŠ¸ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜ (ê³ ê° ì‘ëŒ€ ì‹œë‚˜ë¦¬ì˜¤)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì—ì´ì „íŠ¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. CRAG(Corrective RAG)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. Self-RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. ê³„íš í›„ ì‹¤í–‰(Plan-and-Execute)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… ë„¤íŠ¸ì›Œí¬(Multi-Agent Collaboration Network)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. ë©€í‹° ì—ì´ì „íŠ¸ ê°ë…ì(Multi-Agent Supervisor)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 08. ê³„ì¸µì  ë©€í‹° ì—ì´ì „íŠ¸ íŒ€(Hierarchical Multi-Agent Teams)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 09. SQL ë°ì´í„°ë² ì´ìŠ¤ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì—ì´ì „íŠ¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 10. STORM ê°œë…ì„ ë„ì…í•œ ì—°êµ¬ë¥¼ ìœ„í•œ ë©€í‹° ì—ì´ì „íŠ¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH18 ê¸°íƒ€ ì •ë³´\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH18 ê¸°íƒ€ ì •ë³´\n",
      "  section_info: 01. StreamEvent íƒ€ì…ë³„ ì •ë¦¬\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(f\"ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        if key == 'level':\n",
    "            print(f\"  {'  ' * (int(value) - 1)}â””â”€ {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# def extract_code_blocks_from_documents(docs: List[Document]) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     ê° Document ê°ì²´ ë‚´ì˜ <CODE_BLOCK>...</CODE_BLOCK> êµ¬ê°„ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "#     \"\"\"\n",
    "#     code_blocks = []\n",
    "#     pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "\n",
    "#     for doc in docs:\n",
    "#         matches = pattern.findall(doc.page_content)\n",
    "#         code_blocks.extend(matches)\n",
    "\n",
    "#     return code_blocks\n",
    "\n",
    "# chunked_docs = split_protected_chunks(docs)\n",
    "\n",
    "# code_blocks = extract_code_blocks_from_documents(chunked_docs)\n",
    "\n",
    "# # ì¶œë ¥ í™•ì¸\n",
    "# for i, code in enumerate(code_blocks[100:150]):\n",
    "#     print(f\"ğŸ”¹ Code Block {i + 1}:\\n{code}\\n{'-'*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ë‹µë³€:\n",
      " LangGraphëŠ” LangChainì˜ êµ¬ì„± ìš”ì†Œ ì¤‘ í•˜ë‚˜ë¡œ, ì£¼ë¡œ ì±—ë´‡ì´ë‚˜ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. LangGraphëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ê³¼ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ëŒ€í™” íë¦„ì„ ê´€ë¦¬í•˜ê³ , ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°œë°œìëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ê°€ì§„ ì±—ë´‡ì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìœ¼ë©°, LangChainì˜ ë‹¤ë¥¸ ë„êµ¬ë“¤ê³¼ í†µí•©í•˜ì—¬ ë”ìš± ê°•ë ¥í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LangGraphëŠ” íŠ¹íˆ ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ê³ , ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ê³  ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 1. ë¡œì»¬ LLM (gemma:3.12b)\n",
    "# llm = Ollama(model=\"gemma3:12b\")\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# 4. ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ì˜ (ì‚¬ìš© í•„ë“œë§Œ)\n",
    "metadata_field_info = [\n",
    "    {\"name\": \"chapter_info\", \"type\": \"string\"},\n",
    "    {\"name\": \"section_info\", \"type\": \"string\"},\n",
    "]\n",
    "\n",
    "# 5. ParentDocumentRetriever êµ¬ì„±\n",
    "retriever = test_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# 6. QA Chain\n",
    "prompt = PromptTemplate.from_template(\n",
    "    '''\n",
    "        ë‹¹ì‹ ì€ ë­ì²´ì¸ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "        ë­ì²´ì¸ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´, \"ë­ì²´ì¸ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "        ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "        question : \n",
    "        {question}\n",
    "\n",
    "        context :\n",
    "        {context}\n",
    "\n",
    "        answer : \n",
    "    '''\n",
    ")\n",
    "\n",
    "# LCEL ë°©ì‹ìœ¼ë¡œ ì²´ì¸ êµ¬ì„±\n",
    "qa_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 7. ì§ˆë¬¸ ì‹¤í–‰\n",
    "question = \"LangGraphì— ëŒ€í•´ ì„¤ëª…í•´\"\n",
    "result = qa_chain.invoke(question)\n",
    "\n",
    "# 8. ì¶œë ¥\n",
    "print(\"ğŸ§  ë‹µë³€:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParentDocumentRetriever êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv(r'C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    # íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\wikidocs_01.txt\"),\n",
    "    TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\wikidocs_02.txt\"),\n",
    "    TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\wikidocs_03.txt\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    # ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  docs ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ìì‹ ë¶„í• ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# # ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "# # embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# # DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# vectorstore = Chroma(\n",
    "#     collection_name=\"test\", \n",
    "#     embedding_function=embeddings, \n",
    "#     # persist_directory = r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\test\"\n",
    "# )\n",
    "\n",
    "# store = InMemoryStore()\n",
    "\n",
    "# # Retriever ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# retriever = ParentDocumentRetriever(\n",
    "#     vectorstore=vectorstore,\n",
    "#     docstore=store,\n",
    "#     child_splitter=child_splitter,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ë¬¸ì„œë¥¼ ê²€ìƒ‰ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤. docsëŠ” ë¬¸ì„œ ëª©ë¡ì´ê³ , idsëŠ” ë¬¸ì„œì˜ ê³ ìœ  ì‹ë³„ì ëª©ë¡ì…ë‹ˆë‹¤.\n",
    "# retriever.add_documents(docs, ids=None, add_to_docstore=True)\n",
    "\n",
    "# # ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "# sub_docs = vectorstore.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1184519c-d2c9-462f-ba18-df1113dd490f', metadata={'doc_id': 'de864918-a334-4bae-bb3b-ce948fde3499', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='==================================================\\n\\n\\n--- 03. LangChain Hub ---'),\n",
       " Document(id='ee3ed590-ed81-4822-b0b1-18136e76cced', metadata={'doc_id': 'de864918-a334-4bae-bb3b-ce948fde3499', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='Overall, the LangChain documentation provides a comprehensive guide to using the LangChain framework and LCEL for building and executing complex chains of operations involving language models and other components. It covers both basic and advanced use cases, offering practical examples and encouraging community involvement.The provided documents from LangChain cover a range of topics related to the LangChain Expression Language (LCEL) and its applications, including interface design, streaming,'),\n",
       " Document(id='868c3b2e-de19-42b8-845a-9d9f05cab10c', metadata={'doc_id': 'de864918-a334-4bae-bb3b-ce948fde3499', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='1. **Introduction to LangChain and LCEL**: LangChain offers a way to build complex chains from basic components, supporting functionalities like streaming, parallelism, and logging. LCEL (LangChain Expression Language) simplifies the process of chaining together prompts, models, and output parsers to perform tasks like generating jokes based on a given topic or conducting retrieval-augmented generation.'),\n",
       " Document(id='0b297c9c-faad-4fd6-bce7-0d43ac85c358', metadata={'doc_id': 'de864918-a334-4bae-bb3b-ce948fde3499', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='-\\\\nêµ¬ê²½í•˜ëŸ¬ ê°€ê¸°!\\\\nâ‘¡ LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼\\\\në°”ë¡œê°€ê¸° ğŸ‘€\\\\nâ‘¢ ë­ì²´ì¸ ë…¸íŠ¸ ë¬´ë£Œ ì „ìì±…(wikidocs)\\\\në°”ë¡œê°€ê¸° ğŸ™Œ\\\\nâ‘£ RAG ë¹„ë²•ë…¸íŠ¸ LangChain ê°•ì˜ì˜¤í”ˆ\\\\në°”ë¡œê°€ê¸° ğŸ™Œ\\\\nâ‘¤ ì„œìš¸ëŒ€ PyTorch ë”¥ëŸ¬ë‹ ê°•ì˜\\\\në°”ë¡œê°€ê¸° ğŸ™Œ\\\\nLangGraph - Multi-Agent Collaboration(ë‹¤ì¤‘ í˜‘ì—… ì—ì´ì „íŠ¸) ë¡œ ë³µì¡í•œ í…ŒìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” LLM ì–´í”Œë¦¬ì¼€ì´ì…˜ ì œì‘\\\\n2024ë…„ 01ì›” 29ì¼\\\\n26 ë¶„ ì†Œìš”Retrieval...\\\\n[LangChain] ì—ì´ì „íŠ¸(Agent)ì™€ ë„êµ¬(tools)ë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ\\\\n2024ë…„ 02ì›” 09ì¼\\\\n41 ë¶„ ì†Œìš”\\\\nì´ ê¸€ì—ì„œëŠ” LangChain ì˜ Agent í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ê²€ìƒ‰ê³¼ ë°ì´í„° ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. LangSmith ë¥¼ ì‚¬ìš©í•˜ì—¬ Agentì˜ ì¶”ë¡  ë‹¨ê³„ë¥¼ ì¶”ì í•©ë‹ˆë‹¤. Agentê°€ í™œìš©í•  ê²€ìƒ‰ ë„êµ¬(Tavily Search), PDF ê¸°ë°˜ ê²€ìƒ‰')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      "\n",
      "--- 03. LangChain Hub ---\n"
     ]
    }
   ],
   "source": [
    "# print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œì˜ ê¸¸ì´: 1800480\n",
      "\n",
      "=====================\n",
      "\n",
      "ëŠ” ëª¨ë“ˆì‹ìœ¼ë¡œ ì„¤ê³„ë˜ì–´, ì‚¬ìš©í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ì´ëŠ” ê°œë°œìê°€ LangChain í”„ë ˆì„ì›Œí¬ë¥¼ ììœ ë¡­ê²Œ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
      "ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´ì¸ ğŸš€\n",
      "ê³ ìˆ˜ì¤€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì»´í¬ë„ŒíŠ¸ì˜ ë‚´ì¥ ì¡°í•©ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "ì´ëŸ¬í•œ ì²´ì¸ì€ ê°œë°œ ê³¼ì •ì„ ê°„ì†Œí™”í•˜ê³  ì†ë„ë¥¼ ë†’ì—¬ì¤ë‹ˆë‹¤.\n",
      "ì£¼ìš” ëª¨ë“ˆ ğŸ“Œ\n",
      "ëª¨ë¸ I/O ğŸ“ƒ\n",
      "í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ìµœì í™” ë° LLMê³¼ì˜ ì¼ë°˜ì ì¸ ì¸í„°í˜ì´ìŠ¤ì™€ ì‘ì—…ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "ê²€ìƒ‰ ğŸ“š\n",
      "'ë°ì´í„° ê°•í™” ìƒì„±'ì— ì´ˆì ì„ ë§ì¶˜ ì´ ëª¨ë“ˆì€ ìƒì„± ë‹¨ê³„ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì‘ì—…ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
      "ì—ì´ì „íŠ¸ ğŸ¤–\n",
      "ì–¸ì–´ ëª¨ë¸ì´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í• ì§€ ê²°ì •í•˜ê³ , í•´ë‹¹ ì¡°ì¹˜ë¥¼ ì‹¤í–‰í•˜ë©°, ê´€ì°°í•˜ê³ , í•„ìš”í•œ ê²½ìš° ë°˜ë³µí•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "LangChainì„ í™œìš©í•˜ë©´, ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê°œë°œì„ ë³´ë‹¤ ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìœ¼ë©°, í•„ìš”ì— ë§ê²Œ ê¸°ëŠ¥ì„ ë§ì¶¤ ì„¤ì •í•˜ê³ , ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ í†µí•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "# retrieved_docs = retriever.invoke(\"LangChain\")\n",
    "\n",
    "# # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì˜ ê¸¸ì´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "# print(\n",
    "#     f\"ë¬¸ì„œì˜ ê¸¸ì´: {len(retrieved_docs[0].page_content)}\",\n",
    "#     end=\"\\n\\n=====================\\n\\n\",\n",
    "# )\n",
    "\n",
    "# # ë¬¸ì„œì˜ ì¼ë¶€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "# print(retrieved_docs[0].page_content[2000:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶€ëª¨ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì…ë‹ˆë‹¤.\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 250, separators=['==================================================', '---.*?---', '===.*?==='])\n",
    "# ìì‹ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì…ë‹ˆë‹¤.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 100)\n",
    "# ìì‹ ì²­í¬ë¥¼ ì¸ë±ì‹±í•˜ëŠ” ë° ì‚¬ìš©í•  ë²¡í„° ì €ì¥ì†Œì…ë‹ˆë‹¤.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_knowledge\", \n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory = r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\split_knowledge\"\n",
    ")\n",
    "# ë¶€ëª¨ ë¬¸ì„œì˜ ì €ì¥ ê³„ì¸µì…ë‹ˆë‹¤.\n",
    "parent_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    # ë²¡í„° ì €ì¥ì†Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    vectorstore=vectorstore,\n",
    "    # ë¬¸ì„œ ì €ì¥ì†Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    docstore=parent_store,\n",
    "    # í•˜ìœ„ ë¬¸ì„œ ë¶„í• ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    child_splitter=child_splitter,\n",
    "    # ìƒìœ„ ë¬¸ì„œ ë¶„í• ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      "\n",
      "--- 03. LangChain Hub ---\n"
     ]
    }
   ],
   "source": [
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ===\n",
      "\n",
      "\n",
      "--- ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ---\n"
     ]
    }
   ],
   "source": [
    "# # ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "# sub_docs = vectorstore.similarity_search(\"ë”¥ëŸ¬ë‹\")\n",
    "\n",
    "# # sub_docs ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œì˜ page_content ì†ì„±ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "# print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ===\n",
      "\n",
      "\n",
      "--- ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ---\n",
      "\n",
      "25ë…„ 1ì›” ê¸°ì¤€: ëˆ„ì  ì¡°íšŒìˆ˜: 1,700ë§Œ ë² ìŠ¤íŠ¸ì…€ëŸ¬\n",
      "ë§ì€ ë¶„ë“¤ì˜ í”¼ë“œë°±ìœ¼ë¡œ ìˆ˜ ë…„ê°„ ë³´ì™„ëœ í˜„ì—… ì—°êµ¬ì›ë“¤ì´ ì‘ì„±í•œ ë”¥ ëŸ¬ë‹ ìì—°ì–´ ì²˜ë¦¬ êµì¬ ì…ë¬¸ì„œì…ë‹ˆë‹¤.\n",
      "Q) ì…ë¬¸ìë„ ê³µë¶€ ê°€ëŠ¥í•œê°€ìš”?\n",
      "A) ì´ ì±…ì€ ì• ì´ˆ AIë¥¼ ì•„ì˜ˆ ì²˜ìŒ ê³µë¶€í•˜ëŠ” ì…ë¬¸ìê°€ íƒ€ê²Ÿì…ë‹ˆë‹¤. íŒŒì´ì¬ì„ ì–´ëŠ ì •ë„ í•  ì¤„ ì•„ì‹ ë‹¤ë©´ AI ê³µë¶€ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ìœ ë£Œ E-book/ì „ì²´ PDF íŒŒì¼ (https://wikidocs.net/buy/ebook/2155)\n",
      "A) ê±°ì˜ 90% ì´ìƒì˜ ë‚´ìš©ì„ í˜„ì¬ ë¬´ë£Œë¡œ ê³µê°œí–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‹ˆ ë¬´ë£Œë¡œ í¸í•˜ê²Œ ì…ë¬¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
      "ë‹¨, íŒŒì¸ íŠœë‹ ë“± ì¼ë¶€ì‹¬í™” ë‚´ìš©ì€ ìœ ë£Œ E-book/ì „ì²´ PDF íŒŒì¼ì—ì„œë§Œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ì˜¨ë¼ì¸ ê°•ì˜ (https://bit.ly/4fWkdRa)ì˜¨ë¼ì¸ ê°•ì˜ëŠ” ì—†ë‚˜ìš”?\n",
      "A) LLM íŒŒì¸ íŠœë‹ì„ ë‹¤ë£¨ëŠ” ì…ë¬¸ìš© ì˜¨ë¼ì¸ ê°•ì˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ğŸ‰í• ì¸ ì¿ í° ì½”ë“œ ì…ë ¥: 'íŒŒì¸íŠœë‹' (í• ì¸ìœ¨ : 20%)\n",
      "ì˜¤í”„ë¼ì¸ ê°•ì˜ (https://learningspoons.com/course/detail/llm-master/)ì˜¤í”„ë¼ì¸ ê°•ì˜ëŠ” ì—†ë‚˜ìš”?\n",
      "A) ê°•ë‚¨ì—ì„œ ë§¤ì£¼ í˜„ì¥ì—ì„œ ì§ˆë‹µì„ ë°›ìœ¼ë©° ì§„í–‰ë˜ëŠ” LLM íŒŒì¸ íŠœë‹ì„ ë‹¤ë£¨ëŠ” ì…ë¬¸ìš© ì˜¤í”„ë¼ì¸ ê°•ì˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "ì¡´ì¬ (https://wikidocs.net/book/2788)íŒŒì´í† ì¹˜ ë²„ì „ì€ ì—†ë‚˜ìš”?\n",
      "A) ì¡´ì¬í•©ë‹ˆë‹¤. ëª©ì°¨ëŠ” ê±°ì˜ ë™ì¼í•˜ë¯€ë¡œ ì„ í˜¸í•˜ëŠ” ì±…ìœ¼ë¡œ êµ¬ë§¤í•˜ì„¸ìš”.\n",
      "Q) ì…ë¬¸ìë“¤ì´ ì‰½ê²Œ ì§ˆë¬¸í•˜ê³  ì†Œí†µí•  ìˆ˜ ìˆëŠ” ê³µê°„ì€ ì—†ë‚˜ìš”?\n",
      "A) ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ìë“¤ì„ ìœ„í•œ ì˜¤í”ˆ ì¹´í†¡ë°©: https://open.kakao.com/o/gciNJmPg\n",
      "ëŒ“ê¸€ ë˜ëŠ” í”¼ë“œë°±(ì§ˆë¬¸/ì§€ì ) ë˜ëŠ” ì´ë©”ì¼ í™˜ì˜í•©ë‹ˆë‹¤.\n",
      "ê° ë‚´ìš©ì— ëŒ€í•œ í˜ì´ì§€ë§ˆë‹¤ ëŒ“ê¸€ ë²„íŠ¼ ì˜†ì„ ë³´ë©´ í”¼ë“œë°± ë²„íŠ¼ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "ìœ„í‚¤ë…ìŠ¤ íšŒì›ê°€ì…ì´ ë²ˆê±°ë¡œìš°ì‹œë‹¤ë©´ í”¼ë“œë°± ë²„íŠ¼ìœ¼ë¡œ ì˜ê²¬ì£¼ì…”ë„ ëŒ“ê¸€ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n",
      "ê°ì‚¬í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "retrieved_docs = retriever.invoke(\"ë”¥ëŸ¬ë‹\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in retrieved_docs[:5] :\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ ì§ˆë¬¸: ì´ í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: ì´ í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” í¬ê²Œ ë°ì´í„° ìˆ˜ì§‘, ëª¨ë¸ í•™ìŠµ ë° í‰ê°€, ì„±ëŠ¥ ê°œì„ , ê·¸ë¦¬ê³  ì„œë¹„ìŠ¤ êµ¬í˜„ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œëŠ” ì•„ë™ ê·¸ë¦¼ ë°ì´í„°ë¥¼ YOLO ëª¨ë¸ë¡œ ì •ë¦¬í•˜ê³ , ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” YOLO11n ëª¨ë¸ì„ í†µí•´ ê·¸ë¦¼ ìš”ì†Œë¥¼ íƒì§€í•©ë‹ˆë‹¤. ì´í›„ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ì¡°ì •í•˜ê³  í•™ìŠµ ì†ë„ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í•™ìŠµëœ ëª¨ë¸ì„ ì„œë¹„ìŠ¤ì— ì ìš©í•˜ì—¬ HTP ê²€ì‚¬ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ê³  í•´ì„í•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ ì§ˆë¬¸: í”„ë¡œì íŠ¸ë¥¼ ì¢€ ë” ê³ ë„í™”í•œë‹¤ë©´ ì–´ë–¤ ë°©ì•ˆì„ ìƒê°í•´ë³´ì…¨ë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: í”„ë¡œì íŠ¸ë¥¼ ê³ ë„í™”í•˜ê¸° ìœ„í•´, ë‹¤ìŒê³¼ ê°™ì€ ë°©ì•ˆì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²«ì§¸, ë‹¤ì–‘í•œ ì•„ë™ ë¯¸ìˆ  ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ ìˆ˜ì§‘í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘˜ì§¸, ì‹¬ë¦¬ ì§„ë‹¨ì˜ ì •í™•ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë‹¤ì¤‘ ëª¨ë‹¬ í•™ìŠµì„ ë„ì…í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í†µí•© ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…‹ì§¸, ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•œ ì§€ì†ì ì¸ ëª¨ë¸ ì—…ë°ì´íŠ¸ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ ì§ˆë¬¸: í”„ë¡œì íŠ¸ì˜ í–¥í›„ ë°œì „ ë°©í–¥ìœ¼ë¡œ Langchainì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ”ê²Œ ì¢‹ì•„ë³´ì´ëŠ”ë° ê±°ê¸°ê¹Œì§€ ê³ ë ¤í•´ë³´ì…¨ì„ê¹Œìš”?\n",
      "ğŸ’¬ ë‹µë³€: ë„¤, Langchainì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ” ë°©í–¥ì€ ë§¤ìš° ìœ ë§í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°ì´í„° ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±ì˜ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê³ , ë” ë‚˜ì€ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•˜ê¸° ìœ„í•´ Langchainì˜ ë„ì…ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# # from langchain.llms import Ollama  # ë¡œì»¬ LLM ì“¸ ê²½ìš° ì‚¬ìš©\n",
    "\n",
    "# # LLM ì´ˆê¸°í™”\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "# # llm = Ollama(model=\"gemma3:12b\")  # ë¡œì»¬ ëª¨ë¸ ì“¸ ê²½ìš°\n",
    "\n",
    "# # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# with open(\"C:/Users/user/Documents/GitHub/Presentation-Agent/data/txt/DeePrint.txt\", encoding=\"utf-8\") as f:\n",
    "#     pt_context = f.read()\n",
    "\n",
    "# # í…œí”Œë¦¿ ì •ì˜\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", f\"ë‹¹ì‹ ì€ ë°œí‘œìë£Œì— ëŒ€í•œ ë‚´ìš©ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ ê·¸ì— ëŒ€í•œ ë‹µì„ í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.\\në‹¤ìŒì€ ë°œí‘œìë£Œì— ëŒ€í•œ ë°°ê²½ ì •ë³´ì…ë‹ˆë‹¤:\\n\\n{pt_context}\\n ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ 100í† í° ì´ë‚´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\"),\n",
    "#     (\"human\", \"ì§ˆë¬¸: {question}\\n\\në¬¸ì„œ:\\n{documents}\")\n",
    "# ])\n",
    "\n",
    "# # LCEL ì²´ì¸ êµ¬ì„±\n",
    "# rag_chain = (\n",
    "#     {\n",
    "#         \"question\": RunnablePassthrough(),\n",
    "#         \"documents\": retriever\n",
    "#     }\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# # ì§ˆë¬¸ ëª©ë¡\n",
    "# questions = [\n",
    "#     \"ì´ í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\",\n",
    "#     \"í”„ë¡œì íŠ¸ë¥¼ ì¢€ ë” ê³ ë„í™”í•œë‹¤ë©´ ì–´ë–¤ ë°©ì•ˆì„ ìƒê°í•´ë³´ì…¨ë‚˜ìš”?\",\n",
    "#     \"í”„ë¡œì íŠ¸ì˜ í–¥í›„ ë°œì „ ë°©í–¥ìœ¼ë¡œ Langchainì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ”ê²Œ ì¢‹ì•„ë³´ì´ëŠ”ë° ê±°ê¸°ê¹Œì§€ ê³ ë ¤í•´ë³´ì…¨ì„ê¹Œìš”?\"\n",
    "# ]\n",
    "\n",
    "# # ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±\n",
    "# for q in questions:\n",
    "#     try:\n",
    "#         response = rag_chain.invoke(q)\n",
    "#         print(f\"\\nâ“ ì§ˆë¬¸: {q}\\nğŸ’¬ ë‹µë³€: {response}\\n\" + \"-\"*50)\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\\n\" + \"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ ì§ˆë¬¸: ì´ í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì‚¬ìš©ìê°€ ë°œí‘œ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ë©´, AIê°€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë°œí‘œ ëŒ€ë³¸ì„ ìƒì„±í•˜ê³ , ìŒì„±ìœ¼ë¡œ ë°œí‘œê¹Œì§€ ì§„í–‰í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. VectorDBì— ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³ , LLMì´ ì´ë¥¼ ë¶„ì„í•˜ì—¬ ë°œí‘œ ë‚´ìš©ì„ êµ¬ì„±í•©ë‹ˆë‹¤. FastAPIë¥¼ í†µí•´ API ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³ , Streamlitì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ ì§ˆë¬¸: í”„ë¡œì íŠ¸ë¥¼ ì¢€ ë” ê³ ë„í™”í•œë‹¤ë©´ ì–´ë–¤ ë°©ì•ˆì„ ìƒê°í•´ë³´ì…¨ë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: í”„ë¡œì íŠ¸ë¥¼ ê³ ë„í™”í•˜ê¸° ìœ„í•œ ë°©ì•ˆìœ¼ë¡œëŠ” ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš© ê¸°ëŠ¥ ì¶”ê°€, ì‚¬ìš©ì ë§ì¶¤ ë°œí‘œ ìŠ¤íƒ€ì¼ ì ìš©, ë””ì§€í„¸ ì•„ë°”íƒ€ í™œìš©, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ë„ë©”ì¸ìœ¼ë¡œì˜ í™•ì¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°œí‘œ ê²½í—˜ì„ ë”ìš± ìì—°ìŠ¤ëŸ½ê³  ê°œì¸í™”ëœ ë°©ì‹ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ ì§ˆë¬¸: LangChain ê¸°ë°˜ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ” ê²ƒë„ ê³ ë ¤í–ˆë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: LangChain ê¸°ë°˜ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ” ë°©ì•ˆì€ ê³ ë ¤í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ í”„ë¡œì íŠ¸ì—ì„œëŠ” RAGì™€ ì—ì´ì „íŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„í•˜ì—¬ ìš´ìš©í•˜ê³  ìˆìœ¼ë©°, LangChainì˜ ì‚¬ìš©ì€ í•„ìˆ˜ê°€ ì•„ë‹™ë‹ˆë‹¤. ëŒ€ì‹ , LLMì˜ ë™ì‘ ì›ë¦¬ì™€ RAGì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ ì§ˆë¬¸: ì œê°€ ì²˜ìŒìœ¼ë¡œ ë¬¼ì–´ë³¸ ì§ˆë¬¸ì´ ë­ì˜€ë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: ì´ì „ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìëŠ” í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ì™€ ê³ ë„í™” ë°©ì•ˆì— ëŒ€í•´ ì§ˆë¬¸í•˜ì˜€ê³ , AIëŠ” ì‹œìŠ¤í…œ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ê³  ê³ ë„í™” ë°©ì•ˆìœ¼ë¡œ ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©, ë§ì¶¤ ë°œí‘œ ìŠ¤íƒ€ì¼, ë””ì§€í„¸ ì•„ë°”íƒ€ í™œìš© ë“±ì„ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, LangChain ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ê³ ë„í™”ì— ëŒ€í•œ ì§ˆë¬¸ì— AIëŠ” í˜„ì¬ í”„ë¡œì íŠ¸ì—ì„œ LangChain ì‚¬ìš©ì´ í•„ìˆ˜ê°€ ì•„ë‹ˆë©°, LLMê³¼ RAGì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•˜ë‹¤ê³  ë‹µë³€í–ˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# âœ… LLM ì„¤ì • (ChatOpenAI ë˜ëŠ” Ollama ì‚¬ìš© ê°€ëŠ¥)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "# from langchain.llms import Ollama\n",
    "# llm = Ollama(model=\"gemma3:12b\")\n",
    "\n",
    "# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ\n",
    "with open(\"C:/Users/user/Documents/GitHub/Presentation-Agent/data/txt/pt_context.txt\", encoding=\"utf-8\") as f:\n",
    "    pt_context = f.read()\n",
    "\n",
    "# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"ë‹¹ì‹ ì€ ë°œí‘œìë£Œì— ëŒ€í•œ ë‚´ìš©ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ ê·¸ì— ëŒ€í•œ ë‹µì„ í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.\\n\"\n",
    "               f\"ë‹¤ìŒì€ ë°œí‘œìë£Œì— ëŒ€í•œ ë°°ê²½ ì •ë³´ì…ë‹ˆë‹¤:\\n\\n{pt_context}\\n\\n\"\n",
    "               f\"ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ 100í† í° ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\\n\"\n",
    "               f\"ì‚¬ìš©ìê°€ ì²˜ìŒ ë¬¼ì–´ë´ì„œ ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ì—†ì–´ë„ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ í•˜ì„¸ìš”. \\n\"\n",
    "               f\"ì‚¬ìš©ìê°€ ì´ì „ ëŒ€í™” ë‚´ìš©ì— ëŒ€í•´ ë¬¼ì–´ë³´ë©´, ëŒ€í™” ê¸°ë¡ì„ í™•ì¸í•˜ì—¬ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.\"), \n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"human\", \"ë¬¸ì„œ:\\n{documents}\"),\n",
    "    (\"human\", \"ì´ì „ ëŒ€í™” ë‚´ìš©:\\n{chat_history}\")\n",
    "])\n",
    "\n",
    "# âœ… ëŒ€í™” ì´ë ¥ ì €ì¥ìš© í•¨ìˆ˜ ë° ì €ì¥ì†Œ\n",
    "chat_histories = {}\n",
    "\n",
    "def get_message_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = ChatMessageHistory()\n",
    "    return chat_histories[session_id]\n",
    "\n",
    "# âœ… ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def format_chat_history(chat_history):\n",
    "    formatted_history = []\n",
    "    for message in chat_history:\n",
    "        if hasattr(message, 'content') and hasattr(message, 'type'):\n",
    "            formatted_history.append(f\"{message.type}: {message.content}\")\n",
    "    return \"\\n\".join(formatted_history)\n",
    "\n",
    "# âœ… RAG LCEL ì²´ì¸ êµ¬ì„±\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"documents\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"chat_history\": lambda x: format_chat_history(x.get(\"chat_history\", []))\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# âœ… RAG + ë©”ëª¨ë¦¬ ì—°ê²°ëœ ì²´ì¸ êµ¬ì„±\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history=get_message_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# âœ… ì§ˆì˜ í…ŒìŠ¤íŠ¸\n",
    "questions = [\n",
    "    \"ì´ í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\",\n",
    "    \"í”„ë¡œì íŠ¸ë¥¼ ì¢€ ë” ê³ ë„í™”í•œë‹¤ë©´ ì–´ë–¤ ë°©ì•ˆì„ ìƒê°í•´ë³´ì…¨ë‚˜ìš”?\",\n",
    "    \"LangChain ê¸°ë°˜ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ” ê²ƒë„ ê³ ë ¤í–ˆë‚˜ìš”?\",\n",
    "    \"ì œê°€ ì²˜ìŒìœ¼ë¡œ ë¬¼ì–´ë³¸ ì§ˆë¬¸ì´ ë­ì˜€ë‚˜ìš”?\"\n",
    "]\n",
    "\n",
    "session_id = \"user-session-1\"\n",
    "for i, q in enumerate(questions):\n",
    "    try:\n",
    "        response = chat_chain.invoke(\n",
    "            {\"question\": q},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        print(f\"\\nâ“ ì§ˆë¬¸: {q}\\nğŸ’¬ ë‹µë³€: {response}\\n\" + \"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ ì§ˆë¬¸: ì´ í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì‚¬ìš©ìê°€ ë°œí‘œ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ë©´, AIê°€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë°œí‘œ ëŒ€ë³¸ì„ ìƒì„±í•˜ê³  ìŒì„±ìœ¼ë¡œ ë°œí‘œê¹Œì§€ ì§„í–‰í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ë¬¸ì„œëŠ” VectorDBì— ì €ì¥ë˜ê³ , LLMì´ ì´ë¥¼ ë¶„ì„í•˜ì—¬ ë°œí‘œ ë‚´ìš©ì„ êµ¬ì„±í•©ë‹ˆë‹¤. FastAPIë¥¼ í†µí•´ API ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³ , Streamlitì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤(UI)ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ ì§ˆë¬¸: í”„ë¡œì íŠ¸ë¥¼ ì¢€ ë” ê³ ë„í™”í•œë‹¤ë©´ ì–´ë–¤ ë°©ì•ˆì„ ìƒê°í•´ë³´ì…¨ë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: í”„ë¡œì íŠ¸ë¥¼ ê³ ë„í™”í•˜ê¸° ìœ„í•œ ë°©ì•ˆìœ¼ë¡œëŠ” ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš© ê¸°ëŠ¥ ì¶”ê°€, ì¸ê°„ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ëŠ” ìŒì„± í•©ì„±, ì‚¬ìš©ìë³„ ë§ì¶¤ ë°œí‘œ ìŠ¤íƒ€ì¼ ì ìš©, ë””ì§€í„¸ ì•„ë°”íƒ€ë¥¼ í™œìš©í•œ ë°œí‘œ ê¸°ëŠ¥, ê·¸ë¦¬ê³  ë„ë©”ì¸ í™•ì¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê°œì„ ì„ í†µí•´ ë”ìš± ìì—°ìŠ¤ëŸ¬ìš´ ë°œí‘œ ê²½í—˜ì„ ì œê³µí•˜ê³  ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•  ê³„íšì…ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ ì§ˆë¬¸: LangChain ê¸°ë°˜ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ” ê²ƒë„ ê³ ë ¤í–ˆë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: ë„¤, LangChain ê¸°ë°˜ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ” ê²ƒë„ ê³ ë ¤í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¬¸ì„œ ê²€ìƒ‰ ë° ë¶„ì„ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ì‚¬ìš©ìì—ê²Œ ë”ìš± ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ë°œí‘œ ëŒ€ë³¸ì„ ì œê³µí•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. RAG ì‹œìŠ¤í…œì˜ ê°œì„ ì€ ë°œí‘œì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•  ê²ƒì…ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ ì§ˆë¬¸: ì œê°€ ì´ ëŒ€í™”ì—ì„œ ì²˜ìŒìœ¼ë¡œ ë¬¼ì–´ë³¸ ì§ˆë¬¸ì´ ë­ì˜€ë‚˜ìš”?\n",
      "ğŸ’¬ ë‹µë³€: ì´ì „ ëŒ€í™” ë‚´ìš©ì—ì„œ ì‚¬ìš©ìê°€ ì²˜ìŒìœ¼ë¡œ ë¬¼ì–´ë³¸ ì§ˆë¬¸ì€ \"ì´ í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\"ì…ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv(r'C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\.env')\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "# loaders = [\n",
    "#     # íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "#     TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\wikidocs_01.txt\"),\n",
    "#     TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\wikidocs_02.txt\"),\n",
    "#     TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\wikidocs_03.txt\"),\n",
    "# ]\n",
    "\n",
    "# docs = []\n",
    "# for loader in loaders:\n",
    "#     # ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  docs ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "#     docs.extend(loader.load())\n",
    "\n",
    "# ë¶€ëª¨ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì…ë‹ˆë‹¤.\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 250, separators=['==================================================', '---.*?---', '===.*?==='])\n",
    "# ìì‹ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì…ë‹ˆë‹¤.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 100)\n",
    "\n",
    "# ìì‹ ì²­í¬ë¥¼ ì¸ë±ì‹±í•˜ëŠ” ë° ì‚¬ìš©í•  ë²¡í„° ì €ì¥ì†Œì…ë‹ˆë‹¤.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_knowledge\", \n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory = r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\split_knowledge\"\n",
    ")\n",
    "\n",
    "# retriever.add_documents(docs)\n",
    "\n",
    "# ë¶€ëª¨ ë¬¸ì„œì˜ ì €ì¥ ê³„ì¸µì…ë‹ˆë‹¤.\n",
    "parent_store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    # ë²¡í„° ì €ì¥ì†Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    vectorstore=vectorstore,\n",
    "    # ë¬¸ì„œ ì €ì¥ì†Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    docstore=parent_store,\n",
    "    # í•˜ìœ„ ë¬¸ì„œ ë¶„í• ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    child_splitter=child_splitter,\n",
    "    # ìƒìœ„ ë¬¸ì„œ ë¶„í• ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# âœ… LLM ì„¤ì • (ChatOpenAI ë˜ëŠ” Ollama ì‚¬ìš© ê°€ëŠ¥)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "# from langchain.llms import Ollama\n",
    "# llm = Ollama(model=\"gemma3:12b\")\n",
    "\n",
    "# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ\n",
    "with open(\"C:/Users/user/Documents/GitHub/Presentation-Agent/data/txt/pt_context.txt\", encoding=\"utf-8\") as f:\n",
    "    pt_context = f.read()\n",
    "\n",
    "# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"ë‹¹ì‹ ì€ ë°œí‘œìë£Œì— ëŒ€í•œ ë‚´ìš©ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ ê·¸ì— ëŒ€í•œ ë‹µì„ í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.\\n\"\n",
    "               f\"ë‹¤ìŒì€ ë°œí‘œìë£Œì— ëŒ€í•œ ë°°ê²½ ì •ë³´ì…ë‹ˆë‹¤:\\n\\n{pt_context}\\n\\n\"\n",
    "               f\"ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ 100í† í° ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\\n\"\n",
    "               f\"ì‚¬ìš©ìê°€ ì²˜ìŒ ë¬¼ì–´ë´ì„œ ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ì—†ì–´ë„ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ í•˜ì„¸ìš”. \\n\"\n",
    "               f\"ì‚¬ìš©ìê°€ ì´ì „ ëŒ€í™” ë‚´ìš©ì— ëŒ€í•´ ë¬¼ì–´ë³´ë©´, ëŒ€í™” ê¸°ë¡ì„ í™•ì¸í•˜ì—¬ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.\"), \n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"human\", \"ë¬¸ì„œ:\\n{documents}\"),\n",
    "    (\"human\", \"ì´ì „ ëŒ€í™” ë‚´ìš©:\\n{chat_history}\")\n",
    "])\n",
    "\n",
    "# âœ… ëŒ€í™” ì´ë ¥ ì €ì¥ìš© í•¨ìˆ˜ ë° ì €ì¥ì†Œ\n",
    "chat_histories = {}\n",
    "\n",
    "def get_message_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = ChatMessageHistory()\n",
    "    return chat_histories[session_id]\n",
    "\n",
    "# âœ… ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def format_chat_history(chat_history):\n",
    "    formatted_history = []\n",
    "    for message in chat_history:\n",
    "        if hasattr(message, 'content') and hasattr(message, 'type'):\n",
    "            formatted_history.append(f\"{message.type}: {message.content}\")\n",
    "    return \"\\n\".join(formatted_history)\n",
    "\n",
    "# âœ… RAG LCEL ì²´ì¸ êµ¬ì„±\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"documents\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"chat_history\": lambda x: format_chat_history(x.get(\"chat_history\", []))\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# âœ… RAG + ë©”ëª¨ë¦¬ ì—°ê²°ëœ ì²´ì¸ êµ¬ì„±\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history=get_message_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# âœ… ì§ˆì˜ í…ŒìŠ¤íŠ¸\n",
    "questions = [\n",
    "    \"ì´ í”„ë¡œì íŠ¸ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\",\n",
    "    \"í”„ë¡œì íŠ¸ë¥¼ ì¢€ ë” ê³ ë„í™”í•œë‹¤ë©´ ì–´ë–¤ ë°©ì•ˆì„ ìƒê°í•´ë³´ì…¨ë‚˜ìš”?\",\n",
    "    \"LangChain ê¸°ë°˜ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ ê³ ë„í™”í•˜ëŠ” ê²ƒë„ ê³ ë ¤í–ˆë‚˜ìš”?\",\n",
    "    \"ì œê°€ ì´ ëŒ€í™”ì—ì„œ ì²˜ìŒìœ¼ë¡œ ë¬¼ì–´ë³¸ ì§ˆë¬¸ì´ ë­ì˜€ë‚˜ìš”?\"\n",
    "]\n",
    "\n",
    "session_id = \"user-session-1\"\n",
    "for i, q in enumerate(questions):\n",
    "    try:\n",
    "        response = chat_chain.invoke(\n",
    "            {\"question\": q},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        print(f\"\\nâ“ ì§ˆë¬¸: {q}\\nğŸ’¬ ë‹µë³€: {response}\\n\" + \"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.llms import Ollama\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "# ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´, \"ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "# ë¬¸ì„œ:\n",
    "# {context}\n",
    "\n",
    "# ì§ˆë¬¸:\n",
    "# {input}\n",
    "# \"\"\")\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, max_tokens=1000)\n",
    "# # llm = Ollama(model=\"gemma3:12b\", temperature = 0)\n",
    "\n",
    "# # document_variable_nameì„ ì œê±°í•´ì•¼ ì˜¤ë¥˜ ì—†ìŒ\n",
    "# document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# response = chain.invoke({\"input\": \"ì´ í”„ë¡œì íŠ¸ë¥¼ ë“£ê³  ë‚œ í›„ì— ìƒê¸¸ ìˆ˜ ìˆëŠ” ì˜ˆìƒ ì§ˆë¬¸ì„ 10ê°œ ë§Œë“¤ê³  ê·¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë§Œë“¤ì–´\"})\n",
    "# print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
