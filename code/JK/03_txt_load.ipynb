{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "def load_docling_chapters(path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"위키독스 텍스트 파일을 로드하고 챕터별로 분할\"\"\"\n",
    "    # 파일을 직접 읽기\n",
    "    try:\n",
    "        with open(path, 'r', encoding='cp949') as f:\n",
    "            document = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # cp949로 안되면 euc-kr 시도\n",
    "        try : \n",
    "            with open(path, 'r', encoding='euc-kr') as f:\n",
    "                document = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                document = f.read()            \n",
    "    \n",
    "    # 챕터 정보 추출\n",
    "    chapters = []\n",
    "    current_chapter = None\n",
    "    current_content = []\n",
    "    \n",
    "    # 문서의 각 줄을 순회\n",
    "    for line in document.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # 챕터 제목 확인 (숫자. 제목 형식)\n",
    "        if re.match(r'^\\d+\\.\\s+', line):\n",
    "            # 이전 챕터가 있다면 저장\n",
    "            if current_chapter:\n",
    "                chapters.append({\n",
    "                    'title': current_chapter,\n",
    "                    'content': '\\n'.join(current_content)\n",
    "                })\n",
    "            \n",
    "            # 새 챕터 시작\n",
    "            current_chapter = line\n",
    "            current_content = []\n",
    "        else:\n",
    "            # 현재 챕터의 내용 추가\n",
    "            current_content.append(line)\n",
    "    \n",
    "    # 마지막 챕터 저장\n",
    "    if current_chapter:\n",
    "        chapters.append({\n",
    "            'title': current_chapter,\n",
    "            'content': '\\n'.join(current_content)\n",
    "        })\n",
    "    \n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정\n",
    "file_path = \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_book_2155.txt\"\n",
    "\n",
    "# 챕터 로드\n",
    "first_docs = load_docling_chapters(file_path)\n",
    "\n",
    "# # 결과 출력\n",
    "# print(f\"총 {len(first_docs)}개의 챕터가 발견되었습니다.\")\n",
    "# for i, chapter in enumerate(first_docs, 1):\n",
    "#     print(f\"\\n챕터 {i}:\")\n",
    "#     print(f\"제목: {chapter['title']}\")\n",
    "#     print(f\"내용 길이: {len(chapter['content'])}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # 챕터 내용 확인\n",
    "# print(\"\\n첫 번째 챕터 내용 미리보기:\")\n",
    "# if first_docs:\n",
    "#     first_chapter = first_docs[0]\n",
    "#     print(f\"제목: {first_chapter['title']}\")\n",
    "#     print(\"내용:\")\n",
    "#     print(first_chapter['content'][:500] + \"...\")  # 처음 500자만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정\n",
    "file_path = \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_book_2788.txt\"\n",
    "\n",
    "# 챕터 로드\n",
    "second_docs = load_docling_chapters(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정\n",
    "file_path = \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_book_14314.txt\"\n",
    "\n",
    "# 챕터 로드\n",
    "third_docs = load_docling_chapters(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538\n",
      "417\n",
      "776\n"
     ]
    }
   ],
   "source": [
    "print(len(first_docs))\n",
    "print(len(second_docs))\n",
    "print(len(third_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['링크 : https://www.anaconda.com/distribution/\\n[이미지: ]\\n위 사이트 링크로 이동하여 사이트 하단으로 이동하면 (저자가 이 책을 작성할 당시 기준) 좌측에 파이썬 3.7 버전과 우측에 파이썬 2.7 버전의 아나콘다 설치 파일이 있습니다. 여기서는 파이썬 3.7 버전 64 비트를 설치합니다.\\n[이미지: ]\\n설치 파일을 실행한 후에 다른 윈도우 프로그램을 설치하는 것과 마찬가지로 Next > 를 누르면서 설치를 완료하면 됩니다. 아나콘다를 설치하면 머신 러닝을 위한 기본적인 파이썬 패키지들은 자동으로 설치가 됩니다. 다만 텐서플로우, 케라스, 젠심, 코엔엘파이와 같은 패키지들은 별도 설치가 필요한데 이에 대해서는 뒤에서 추가적으로 설명하겠습니다.\\n아나콘다를 다 설치했다면 아나콘다 프롬프트를 오픈하겠습니다.\\n[이미지: ]\\n아나콘다 프롬프트를 열었다면 아나콘다 프롬프트에 아래의 커맨드를 입력하여 아나콘다 파이썬 패키지를 전부 최신 버전으로 업데이트 합니다.', '[이미지: ]\\n아나콘다 프롬프트를 열었다면 아나콘다 프롬프트에 아래의 커맨드를 입력하여 아나콘다 파이썬 패키지를 전부 최신 버전으로 업데이트 합니다.\\n> conda update -n base conda\\n> conda update --all\\n이 책이 작성되었을 당시에는 파이썬 3.7 버전이 최신 버전이었지만, 독자분들이 파이썬을 설치하기 위해서 아나콘다 페이지에 접속하였을 때는 3.7보다 더욱 최신 버전으로 업데이트가 되었을 수 있습니다. 이 경우 무작정 파이썬 최신 버전을 설치하는 것은 좋은 방법이 아닙니다. 일반적으로 아래의 링크에서 파이썬 버전과 호환되는 텐서플로우 버전에 대한 안내가 나와있으니 반드시 설치 전 확인이 필요합니다.\\n링크 : https://www.tensorflow.org/install/pip?hl=ko', \"링크 : https://www.tensorflow.org/install/pip?hl=ko\\n예를 들어 위 페이지에서 'Python 3.9 지원에는 Tensorflow 2.5 이상이 필요합니다.' 라고 기재되어져 있다면, 파이썬 3.9를 설치하였을 때는 반드시 Tensorflow는 2.5 이상을 설치해야만 합니다.\"]\n",
      "[\"텐서플로우는 기본적으로 64비트 플랫폼만을 지원하므로 32비트 환경에서는 딥 러닝 실습 환경을 구축하기에는 많은 애로 사항이 있습니다. 또는 개인의 컴퓨터 사양이나 다른 이유로 아나콘다나 여러 패키지 설치가 어려운 경우도 있을 것입니다. 이런 경우에는 인터넷만 된다면 바로 파이썬을 실습할 수 있는 구글의 코랩(Colab)이 있습니다. 구글의 Colab은 뒤에서 설명하게 될 '주피터 노트북'과 매우 유사한 실습 환경을 제공합니다.\\nColab 주소 :  https://colab.research.google.com/\\n구글의 Colab에 접속하는 방법은 위의 URL을 통해서 접속하거나, 구글(http://www.google.co.kr/)에서 Colab이라고 검색해서 접속할 수 있습니다.\\n1) 파이썬 실습하기\\nColab 사용 시에는 구글 계정이 필요하므로 구글 아이디가 없으신 분들은 먼저 회원가입 후 로그인부터 해주세요.\\n[이미지: ]\", \"1) 파이썬 실습하기\\nColab 사용 시에는 구글 계정이 필요하므로 구글 아이디가 없으신 분들은 먼저 회원가입 후 로그인부터 해주세요.\\n[이미지: ]\\n로그인 후 좌측 상단에서 파일 > 새 노트 를 클릭합니다. 조금만 기다리면 파이썬을 실습할 수 있는 실습 환경 창이 뜨게 됩니다.\\n[이미지: ]\\n이때 위 그림과 같이 Colab에서 코드를 작성하는 부분의 단위를 '셀'이라고 합니다. 그림에서 보이는 좌측 상단의 '+ 코드' 버튼을 클릭하여 새로운 셀을 추가할 수 있으며, 셀에서 코드를 작성하고 Shift + Enter키를 눌러서 코드를 실행할 수 있습니다.\\n[이미지: ]\\n셀에 3 + 5라는 코드를 작성하고, Shift + Enter를 누르면 8이라는 결과가 나오게 됩니다. 좌측에 [1]은 해당 코드가 몇 번째로 실행되었는지를 의미합니다. 셀을 추가해보면서 다른 파이썬 코드도 추가적으로 작성해보세요.\\n2) 무료로 GPU 사용하기\", '2) 무료로 GPU 사용하기\\n딥 러닝에서는 CPU보다는 GPU를 사용합니다. Colab에서 실습할 때의 장점은 GPU를 무료로 사용할 수 있다는 점입니다. GPU가 장착된 컴퓨터가 없는 딥 러닝 입문자들은 향후 이 책의 실습을 진행할 때 Colab에서 GPU를 사용하면서 딥 러닝을 공부하는 것을 강하게 권장드립니다. GPU를 사용하지 않고 실습을 진행하면 딥 러닝 모델을 학습하는 시간이 지나치게 소요될 수 있습니다.\\n[이미지: ]\\nColab에서 GPU를 사용하는 방법은 새 노트에 진입했을 때 상단에서 런타임 > 런타임 유형 변경을 클릭합니다.\\n[이미지: ]\\n노트 설정의 하드웨어 가속기 > GPU 를 선택 후 저장을 누릅니다. 이후 실습을 진행합니다.\\n3) 파일 업로드\\n[이미지: ]', '[이미지: ]\\n노트 설정의 하드웨어 가속기 > GPU 를 선택 후 저장을 누릅니다. 이후 실습을 진행합니다.\\n3) 파일 업로드\\n[이미지: ]\\n구글의 Colab에서 데이터를 업로드하여 해당 데이터로 실습을 하고자 한다면, 좌측 상단에서 폴더 모양의 버튼을 클릭합니다. 그 후 윗 방향의 화살표(↑)가 그려진 버튼을 클릭하여 파일을 업로드 할 수 있습니다. 위 그림에서 숫자 1번 버튼과 숫자 2번 버튼이 각각 이에 해당합니다. 예를 들어 test.txt 파일을 업로드한다고 해봅시다.\\n[이미지: ]\\n업로드 후에는 파일 목록에 test.txt 파일이 보입니다.\\n==================================================\\n--- 01-02 필요 프레임워크와 라이브러리 ---\\n```\\n> jupyter notebook', '--- 01-02 필요 프레임워크와 라이브러리 ---\\n```\\n> jupyter notebook\\n```아나콘다를 설치했다면 기본적으로 Numpy, Pandas, Jupyter notebook, scikit-learn, matplotlib, seaborn, nltk 등이 이미 설치되어져 있습니다. 그래서 아나콘다에 포함되어있지 않은 tensorflow, keras, gensim과 같은 패키지만 별도로 pip를 통해 설치합니다.\\n하지만 컴퓨터에 아나콘다를 설치하지 않고 단순히 파이썬만 설치된 상태라면 위에서 언급한 모든 패키지를 pip로 설치해야 합니다. 여기서는 윈도우 환경을 기준으로 설명합니다.']\n",
      "[\"텐서플로우는 구글이 2015년에 공개한 머신 러닝 오픈소스 라이브러리입니다. 머신 러닝과 딥 러닝을 직관적이고 손쉽게 할 수 있도록 설계되었습니다. 뒤의 딥 러닝 실습을 위해서 텐서플로우를 설치해야 합니다.\\n아나콘다 프롬프트(Anaconda Prompt) 또는 명령 프롬프트를 통해서 설치할 수 있습니다. 아나콘다 프롬프트를 열었다면 아나콘다 프롬프트에 아래의 커맨드를 입력하여 텐서플로우를 설치합니다.\\n> pip install tensorflow\\n이제 ipython 쉘을 실행하여 텐서플로우가 정상 설치되었는지 확인하는 의미에서 텐서플로우를 임포트하고 버전을 확인합니다.\\n> ipython\\n...\\nIn [1]: import tensorflow as tf\\nIn [2]: tf.__version__\\nOut[2]: '2.0.0'\\n텐서플로우 2.0이 설치되었습니다. 이해를 돕기 위해 제 컴퓨터 화면의 스크린샷을 아래에 첨부하였습니다.\\n[이미지: ]\", \"Out[2]: '2.0.0'\\n텐서플로우 2.0이 설치되었습니다. 이해를 돕기 위해 제 컴퓨터 화면의 스크린샷을 아래에 첨부하였습니다.\\n[이미지: ]\\n쉘을 나올때는 exit라는 커맨드로 나올 수 있습니다. 다른 패키지들도 동일한 방식으로 설치 및 정상적으로 설치가 되었는지 버전을 확인하면 됩니다. 앞으로 저자가 각 패키지의 버전들을 기재하는 이유는 저자가 해당 버전으로 실습했으므로 참고하라는 의미에서 공개하는 것이지, 독자가 더 높은 버전임에도 저자가 공개한 버전들과 동일해야 한다는 의미는 아닙니다.\\n텐서플로우는 주로 tf라는 명칭으로 임포트하는 것이 관례입니다.\"]\n",
      "['케라스(Keras)는 딥 러닝 프레임워크인 텐서플로우에 대한 추상화 된 API를 제공합니다. 케라스는 백엔드로 텐서플로우를 사용하며, 좀 더 쉽게 딥 러닝을 사용할 수 있게 해줍니다. 쉽게 말해, 텐서플로우 코드를 훨씬 간단하게 작성할 수 있습니다.\\n> pip install keras\\n케라스를 설치 후에 사용할 수도 있지만, 텐서플로우에서 케라스를 사용할 수도 있습니다. 영어 커뮤니티에서는 순수 케라스를 keras라고 표기한다면, 텐서플로우에서 케라스 API를 사용하는 경우는 tf.keras라고 표기합니다. 이 두 가지는 실제로 문법도 많은 면에서 같아서 keras 코드를 tf.keras로 변경하는 건 아주 쉽고, keras를 학습하였다면 tf.keras도 금방 익숙하게 사용할 수 있습니다. 케라스 개발자인 프랑소와 숄레(François Chollet)는 앞으로는 keras보다는 tf.keras를 사용할 것을 권장합니다. 이 책에서도 주로 tf.keras를 사용합니다.', \"> ipython\\n...\\nIn [1]: import keras\\nIn [2]: keras.__version__\\nOut[2]: '2.3.1'\"]\n",
      "[\"젠심(Gensim)은 머신 러닝을 사용하여 토픽 모델링과 자연어 처리 등을 수행할 수 있게 해주는 오픈 소스 라이브러리입니다. 이 책에서도 젠심을 사용하여 Word2Vec 등 다양한 모델들을 학습해볼 것입니다.\\n> pip install gensim\\n> ipython\\n...\\nIn [1]: import gensim\\nIn [2]: gensim.__version__\\nOut[2]: '3.8.1'\"]\n",
      "[\"사이킷런(Scikit-learn)은 파이썬 머신러닝 라이브러리입니다. 사이킷런을 통해 나이브 베이즈 분류, 서포트 벡터 머신 등 다양한 머신 러닝 모듈을 불러올 수 있습니다. 또한, 사이킷런에는 머신러닝을 연습하기 위한 아이리스 데이터, 당뇨병 데이터 등 자체 데이터 또한 제공하고 있습니다. 사이킷런은 위 패키지들과 달리 아나콘다로 자동 설치되지만 아나콘다를 설치하지 않았다면 아래의 커맨드로 Scikit-learn을 별도 설치할 수 있습니다.\\n> pip install scikit-learn\\n> ipython\\n...\\nIn [1]: import sklearn\\nIn [2]: sklearn.__version__\\nOut[2]: '0.21.3'\"]\n",
      "['주피터 노트북은 웹에서 코드를 작성하고 실행할 수 있는 오픈소스 웹 어플리케이션입니다. 이 책의 모든 코드들은 기본적으로 본인의 컴퓨터에 설치된 주피터 노트북 또는 주피터 노트북과 실습 환경이 유사한 구글의 코랩(Colab)을 사용한다고 가정합니다. 주피터 노트북도 아나콘다를 설치하면 자동으로 설치되어져 있습니다. 아나콘다를 설치하지 않았다면 아래의 커맨드로 Jupyter notebook을 별도 설치할 수 있습니다.\\n> pip install jupyter\\n설치가 완료되었으면 프롬프트에서 다음 명령어를 통해 주피터 노트북을 실행할 수 있습니다.\\n> jupyter notebook\\n해당 명령어를 치면 웹 브라우저가 자동으로 열리면서 주피터 노트북이 실행됩니다. 만약 실행되지 않는다면, 직접 실행시켜야 합니다. 웹 브라우저를 열고 프롬프트에서 나오고 있는 주소인 \"localhost:8888\"에 접속합니다.\\n[이미지: ]\\n1) 새로운 노트 실행\\n[이미지: ]', '[이미지: ]\\n1) 새로운 노트 실행\\n[이미지: ]\\n주피터 노트북에서는 노트를 생성해서 해당 노트에 코드를 작성할 수 있습니다. 화면 우측의 New 버튼을 누르고, Python3을 눌러서 새로운 노트를 실행해봅시다.\\n2) 셀에 코드 작성해보기\\n[이미지: ]\\n노트가 실행되면 In [ ]이라는 문자가 적힌 텍스트 상자가 나옵니다. 주피터 노트북에서는 해당 텍스트 상자의 단위를 셀(cell)이라고 부릅니다. 해당 셀에 코드를 입력하고 [Cell] → [Run Cells]를 클릭하면 실행됩니다. 만약, 마우스로 일일히 실행하는 것이 번거롭게 느껴진다면 키보드의 Shift + Enter를 통해서 현재 셀 실행 후 다음 셀로 이동합니다. 이는 구글의 Colab에서 설명했던 실행 방식과 동일한 방식입니다.\\n==================================================\\n--- 01-03 자연어 처리를 위한 NLTK와 KoNLPy 설치하기 ---\\n```', '==================================================\\n--- 01-03 자연어 처리를 위한 NLTK와 KoNLPy 설치하기 ---\\n```\\n> pip install JPype1-0.6.3-cp36-cp36m-win_amd64.whl\\n```다음 챕터인 텍스트 전처리(Text preprocessing) 챕터에서는 전처리를 위한 이론에 대해서 학습하고, 그 이론을 바탕으로 실습을 진행하게 됩니다. 이번 챕터에서는 실습에 필요한 기본적인 자연어 패키지들을 소개합니다.']\n",
      "[\"엔엘티케이(NLTK)는 자연어 처리를 위한 파이썬 패키지입니다. 아나콘다를 설치하였다면 NLTK는 기본적으로 설치가 되어져 있습니다. 아나콘다를 설치하지 않았다면 아래의 커맨드로 NLTK를 별도 설치할 수 있습니다.\\n> pip install nltk\\n> ipython\\n...\\nIn [1]: import nltk\\nIn [2]: nltk.__version__\\nOut[2]: '3.4.5'\\nNLTK의 기능을 제대로 사용하기 위해서는 NLTK Data라는 여러 데이터를 추가적으로 설치해야 합니다. 이를 위해서는 파이썬 코드 내에서 import nltk 이후에 nltk.download()라는 코드를 수행하여 설치합니다.\\nIn [3]: nltk.download()\", 'In [3]: nltk.download()\\n해당 코드를 실행 후에 NLTK 실습에 필요한 각종 패키지와 코퍼스를 다운로드할 수 있습니다. 이를 통칭하여 NLTK Data라고 하겠습니다. 만약, NLTK 실습을 수행하던 도중에 에러가 발생한다면 아래의 2번과 3번 가이드를 참고하시기 바랍니다.']\n",
      "[\"NLTK는 각 실습마다 필요한 NLTK Data가 있습니다. 만약 해당 실습에 필요한 NLTK Data가 설치되지 않은 경우에는 코드 실행 시에 아래와 같은 경고문이 나타납니다.\\nLookupError:\\n**********************************************************************\\nResource treebank not found.\\nPlease use the NLTK Downloader to obtain the resource:\\n>>> import nltk\\n>>> nltk.download('treebank')\\n**********************************************************************\\n위의 경우에는 NLTK Data 중에서 'treebank' 라는 리소스가 설치되지 않았을 경우입니다. 이 경우 위의 안내처럼 주피터 노트북 또는 iPython 쉘 안에서 동일하게 코드를 수행하면 됩니다.\", \"In [1]: import nltk\\nIn [2]: nltk.download('treebank')\"]\n",
      "[\"[이미지: ]\\n링크 : https://github.com/nltk/nltk_data\\n설치 시 에러가 발생한다면, 수동 설치를 진행하여 정해진 경로에 위치시켜야 합니다. 수동 설치를 진행할 수 있는 경로는 nltk_data의 깃허브 주소와 nltk_data 공식 사이트 두 곳이 있습니다. 우선 nltk_data의 깃허브 사이트의 링크는 위와 같습니다.\\n위 링크의 packages 디렉토리에서 필요한 nltk_data 파일들을 모두 다운로드 할 수 있습니다.\\n[이미지: ]\\n예를 들어서 토큰화 작업을 위해 'punkt' 파일이 필요하다면, nltk_data/packages/tokenizer 경로에서 punkt.zip 파일을 다운로드 하면 됩니다. 이렇게 필요한 파일들을 다운로드 한 후, 각 O/S 별 정해진 경로에 위치시킵니다. 각 O/S 별 정해진 경로는 다음과 같습니다.\\n윈도우 : C:/nltk_data또는 D:/nltk_data\", \"윈도우 : C:/nltk_data또는 D:/nltk_data\\nUNIX : /usr/local/share/nltk_data/ 또는 /usr/share/nltk_data\\n가장 간단한 방법으로는 packages 디렉토리 전체를 다운로드 받아 경로에 위치시키는 방법도 있습니다.\\n[이미지: ]\\n링크 : http://www.nltk.org/nltk_data/\\n수동 설치를 진행할 수 있는 사이트로 nltk_data 공식 사이트도 있습니다.\\n토큰화 작업이 필요한 상황이라고 다시 가정합시다. 그렇다면 위 링크로 이동해 CTRL + F를 눌러 'tokenizer'를 검색하고, 검색에 나오는 106. Punkt Tokenizer Models [ download | source ] 해당 줄을 찾은 뒤, download 버튼을 누르면 됩니다. 그러면 punkt.zip 파일을 다운로드 할 수 있습니다. 다운로드 후 위치시켜야 하는 알맞은 경로는 위에서 언급한 경로와 동일합니다.\"]\n",
      "[\"코엔엘파이(KoNLPy)는 한국어 자연어 처리를 위한 형태소 분석기 패키지입니다. 프롬프트에서 아래 커맨드로 설치합니다.\\n> pip install konlpy\\n> ipython\\n...\\nIn [1]: import konlpy\\nIn [2]: konlpy.__version__\\nOut[2]: '0.5.1'\"]\n",
      "['윈도우에서 KoNLPy를 설치하거나 실행 시 JDK 관련 오류나 JPype 오류에 부딪히는 경우가 있습니다. 이는 KoNLPy가 JAVA로 구성되어 있기 때문인데 오류 해결을 위해서는 JDK 1.7 이상의 버전과 JPype가 설치되어 있어야 합니다.\\n1) JDK 설치\\n우선 JDK를 1.7 버전 이상으로 설치해야 합니다.\\n설치 주소 : https://www.oracle.com/technetwork/java/javase/downloads/index.html\\n설치한 후에는 JDK가 설치된 경로를 찾아야 합니다. 예를 들어 저자의 경우에는 jdk가 아래의 경로에 설치되어 있습니다.\\n경로 : C:\\\\Program Files\\\\Java\\\\jdk-11.0.1\\n11.0.1과 같이 버전에 대한 숫자는 어떤 버전을 설치했느냐에 따라 다를 수 있습니다.\\n2) JDK 환경 변수\\n설치 경로를 찾았다면 해당 경로를 복사합니다. 해당 경로를 윈도우 환경 변수에 추가해야하기 때문입니다.\\n윈도우 10기준)', '2) JDK 환경 변수\\n설치 경로를 찾았다면 해당 경로를 복사합니다. 해당 경로를 윈도우 환경 변수에 추가해야하기 때문입니다.\\n윈도우 10기준)\\n제어판 > 시스템 및 보안 > 시스템 > 고급 시스템 설정 > 고급 > 환경 변수\\n새로 만들기(N)...를 누르고 JAVA_HOME이라는 환경 변수를 만듭니다. 환경 변수의 값은 앞서 찾았던 jdk 설치 경로입니다.\\n[이미지: ]\\n이제 KoNLPy를 사용하기 위한 JDK 설정을 마쳤습니다.\\n3) JPype 설치\\n이제 JAVA와 Python을 연결해주는 역할을 하는 JPype를 설치해야 합니다.\\n설치 주소 : https://github.com/jpype-project/jpype/releases', '설치 주소 : https://github.com/jpype-project/jpype/releases\\n해당 링크에서 Assets 라고 기재된 곳에서 적절한 버전을 설치해야 하는데 cp27은 파이썬 2.7, cp36은 파이썬 3.6을 의미합니다. 저자가 책을 집필할 당시에는 파이썬 3.6을 사용하고 있었으므로 cp36이라고 적힌 JPype를 설치했습니다.\\n또 사용하는 윈도우 O/S가 32비트인지, 64비트인지에 따라서 설치 JPype가 다른데, 윈도우 32비트를 사용하고 있다면 win32를, 윈도우 64비트를 사용하고 있다면 win_amd64를 설치해야 합니다. 예를 들어 파이썬 3.6, 윈도우 64비트를 사용 중이라면 JPype1-0.6.3-cp36-cp36m-win_amd64.whl를 다운로드합니다.\\n프롬프트에서 해당 파일의 경로로 이동하여 아래 커맨드를 통해 설치합니다.\\n> pip install JPype1-0.6.3-cp36-cp36m-win_amd64.whl', \"프롬프트에서 해당 파일의 경로로 이동하여 아래 커맨드를 통해 설치합니다.\\n> pip install JPype1-0.6.3-cp36-cp36m-win_amd64.whl\\n이제 JPype의 설치가 완료되었다면, KoNLPy를 사용할 준비가 되었습니다.\\n참고 : KoNLPy 수행시 자바 오류는, 파이썬 bit와 자바 bit가 다른 경우도 발생 하는 것 같습니다.\\n가장 확실한 해결 방법은 설치된 자바를 전부 지우고 최신 버전(JRE, JDK)으로 새로 깔면 대부분 해결 됩니다.\\n==================================================\\n--- 01-04 판다스(Pandas) and 넘파이(Numpy) and 맷플롭립(Matplotlib) ---\\n```\\nplt.title('students')\\nplt.plot([1,2,3,4],[2,4,8,6])\\nplt.plot([1.5,2.5,3.5,4.5],[3,5,8,10]) # 라인 새로 추가\", \"plt.plot([1,2,3,4],[2,4,8,6])\\nplt.plot([1.5,2.5,3.5,4.5],[3,5,8,10]) # 라인 새로 추가\\nplt.xlabel('hours')\\nplt.ylabel('score')\\nplt.legend(['A student', 'B student']) # 범례 삽입\\nplt.show()\\n```데이터 분석을 위한 필수 패키지 삼대장이 있습니다. 바로 Pandas와 Numpy 그리고 Matplotlib입니다. 세 개의 패키지 모두 아나콘다를 설치했다면 추가 설치 없이 사용할 수 있습니다. 이 세 개의 패키지를 간단히 실습해봅시다.\"]\n",
      "[\"판다스(Pandas)는 파이썬 데이터 처리를 위한 라이브러리입니다. 파이썬을 이용한 데이터 분석과 같은 작업에서 필수 라이브러리로 알려져있습니다. 참고 할 수 있는 Pandas 링크는 다음과 같습니다.\\n링크 : http://pandas.pydata.org/pandas-docs/stable/\\n아나콘다를 설치하지 않았다면 아래의 커맨드로 Pandas를 별도 설치할 수 있습니다.\\npip install pandas\\n> ipython\\n...\\nIn [1]: import pandas as pd\\nIn [2]: pd.__version__\\nOut[2]: '0.25.1'\\nPandas의 경우 pd라는 명칭으로 임포트하는 것이 관례입니다.\\nimport pandas as pd\\nPandas는 총 세 가지의 데이터 구조를 사용합니다.\\n시리즈(Series)\\n데이터프레임(DataFrame)\\n패널(Panel)\\n이 중 데이터프레임이 가장 많이 사용되며 여기서는 시리즈와 데이터프레임에 대해서 다룹니다.\", '시리즈(Series)\\n데이터프레임(DataFrame)\\n패널(Panel)\\n이 중 데이터프레임이 가장 많이 사용되며 여기서는 시리즈와 데이터프레임에 대해서 다룹니다.\\n1) 시리즈(Series)\\n시리즈 클래스는 1차원 배열의 값(values)에 각 값에 대응되는 인덱스(index)를 부여할 수 있는 구조를 갖고 있습니다.\\nsr = pd.Series([17000, 18000, 1000, 5000],\\nindex=[\"피자\", \"치킨\", \"콜라\", \"맥주\"])\\nprint(\\'시리즈 출력 :\\')\\nprint(\\'-\\'*15)\\nprint(sr)\\n시리즈 출력 :\\n---------------\\n피자    17000\\n치킨    18000\\n콜라     1000\\n맥주     5000\\ndtype: int64\\n값(values)과 인덱스(index)를 출력합니다.\\nprint(\\'시리즈의 값 : {}\\'.format(sr.values))\\nprint(\\'시리즈의 인덱스 : {}\\'.format(sr.index))', \"print('시리즈의 값 : {}'.format(sr.values))\\nprint('시리즈의 인덱스 : {}'.format(sr.index))\\n시리즈의 값 : [17000 18000  1000  5000]\\n시리즈의 인덱스 : Index(['피자', '치킨', '콜라', '맥주'], dtype='object')\\n2) 데이터프레임(DataFrame)\\n데이터프레임은 2차원 리스트를 매개변수로 전달합니다. 2차원이므로 행방향 인덱스(index)와 열방향 인덱스(column)가 존재합니다. 다시 말해 행과 열을 가지는 자료구조입니다. 시리즈가 인덱스(index)와 값(values)으로 구성된다면, 데이터프레임은 열(columns)까지 추가되어 열(columns), 인덱스(index), 값(values)으로 구성됩니다. 이 세 개의 구성 요소로부터 데이터프레임을 생성해봅시다.\\nvalues = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\", \"values = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\nindex = ['one', 'two', 'three']\\ncolumns = ['A', 'B', 'C']\\ndf = pd.DataFrame(values, index=index, columns=columns)\\nprint('데이터프레임 출력 :')\\nprint('-'*18)\\nprint(df)\\n데이터프레임 출력 :\\n------------------\\nA  B  C\\none    1  2  3\\ntwo    4  5  6\\nthree  7  8  9\\n생성된 데이터프레임으로부터 인덱스(index), 값(values), 열(columns)을 각각 출력해보겠습니다.\\nprint('데이터프레임의 인덱스 : {}'.format(df.index))\\nprint('데이터프레임의 열이름: {}'.format(df.columns))\\nprint('데이터프레임의 값 :')\\nprint('-'*18)\\nprint(df.values)\", \"print('데이터프레임의 열이름: {}'.format(df.columns))\\nprint('데이터프레임의 값 :')\\nprint('-'*18)\\nprint(df.values)\\n데이터프레임의 인덱스 : Index(['one', 'two', 'three'], dtype='object')\\n데이터프레임의 열이름: Index(['A', 'B', 'C'], dtype='object')\\n데이터프레임의 값 :\\n------------------\\n[[1 2 3]\\n[4 5 6]\\n[7 8 9]]\\n3) 데이터프레임의 생성\\n데이터프레임은 리스트(List), 시리즈(Series), 딕셔너리(dict), Numpy의 ndarrays, 또 다른 데이터프레임으로부터 생성할 수 있습니다. 여기서는 리스트와 딕셔너리를 사용하여 데이터프레임을 생성해보겠습니다. 우선 이중 리스트로 생성하는 경우입니다.\\n# 리스트로 생성하기\\ndata = [\\n['1000', 'Steve', 90.72],\", \"# 리스트로 생성하기\\ndata = [\\n['1000', 'Steve', 90.72],\\n['1001', 'James', 78.09],\\n['1002', 'Doyeon', 98.43],\\n['1003', 'Jane', 64.19],\\n['1004', 'Pilwoong', 81.30],\\n['1005', 'Tony', 99.14],\\n]\\ndf = pd.DataFrame(data)\\nprint(df)\\n0         1      2\\n0  1000     Steve  90.72\\n1  1001     James  78.09\\n2  1002    Doyeon  98.43\\n3  1003      Jane  64.19\\n4  1004  Pilwoong  81.30\\n5  1005      Tony  99.14\\n생성된 데이터프레임에 열(columns)을 지정해줄 수 있습니다. 열이름을 지정하고 출력해봅시다.\\ndf = pd.DataFrame(data, columns=['학번', '이름', '점수'])\", \"df = pd.DataFrame(data, columns=['학번', '이름', '점수'])\\nprint(df)\\n학번        이름     점수\\n0  1000     Steve  90.72\\n1  1001     James  78.09\\n2  1002    Doyeon  98.43\\n3  1003      Jane  64.19\\n4  1004  Pilwoong  81.30\\n5  1005      Tony  99.14\\n파이썬 자료구조 중 하나인 딕셔너리(dictionary)를 통해 데이터프레임을 생성해보겠습니다.\\n# 딕셔너리로 생성하기\\ndata = {\\n'학번' : ['1000', '1001', '1002', '1003', '1004', '1005'],\\n'이름' : [ 'Steve', 'James', 'Doyeon', 'Jane', 'Pilwoong', 'Tony'],\\n'점수': [90.72, 78.09, 98.43, 64.19, 81.30, 99.14]\\n}\", \"'점수': [90.72, 78.09, 98.43, 64.19, 81.30, 99.14]\\n}\\ndf = pd.DataFrame(data)\\nprint(df)\\n학번        이름     점수\\n0  1000     Steve  90.72\\n1  1001     James  78.09\\n2  1002    Doyeon  98.43\\n3  1003      Jane  64.19\\n4  1004  Pilwoong  81.30\\n5  1005      Tony  99.14\\n4) 데이터프레임 조회하기\\n아래의 명령어는 데이터프레임에서 원하는 구간만 확인하기 위한 명령어로서 유용하게 사용됩니다.\\ndf.head(n) - 앞 부분을 n개만 보기\\ndf.tail(n) - 뒷 부분을 n개만 보기\\ndf['열이름'] - 해당되는 열을 확인\\n위에서 사용한 데이터프레임을 그대로 사용하여 실습해봅시다.\\n# 앞 부분을 3개만 보기\\nprint(df.head(3))\\n학번      이름     점수\", \"위에서 사용한 데이터프레임을 그대로 사용하여 실습해봅시다.\\n# 앞 부분을 3개만 보기\\nprint(df.head(3))\\n학번      이름     점수\\n0  1000   Steve  90.72\\n1  1001   James  78.09\\n2  1002  Doyeon  98.43\\n# 뒷 부분을 3개만 보기\\nprint(df.tail(3))\\n학번        이름     점수\\n3  1003      Jane  64.19\\n4  1004  Pilwoong  81.30\\n5  1005      Tony  99.14\\n# '학번'에 해당되는 열을 보기\\nprint(df['학번'])\\n0    1000\\n1    1001\\n2    1002\\n3    1003\\n4    1004\\n5    1005\\nName: 학번, dtype: object\\n5) 외부 데이터 읽기\\nPandas는 CSV, 텍스트, Excel, SQL, HTML, JSON 등 다양한 데이터 파일을 읽고 데이터 프레임을 생성할 수 있습니다.\", \"5) 외부 데이터 읽기\\nPandas는 CSV, 텍스트, Excel, SQL, HTML, JSON 등 다양한 데이터 파일을 읽고 데이터 프레임을 생성할 수 있습니다.\\n예를 들어 csv 파일을 읽을 때는 pandas.read_csv()를 통해 읽을 수 있습니다.\\n다음과 같은 example.csv 파일이 있다고 합시다.\\n[이미지: ]\\ndf = pd.read_csv('example.csv')\\nprint(df)\\nstudent id      name  score\\n0        1000     Steve  90.72\\n1        1001     James  78.09\\n2        1002    Doyeon  98.43\\n3        1003      Jane  64.19\\n4        1004  Pilwoong  81.30\\n5        1005      Tony  99.14\\n이 경우 인덱스가 자동으로 부여됩니다. 인덱스를 출력해보겠습니다.\\nprint(df.index)\", '5        1005      Tony  99.14\\n이 경우 인덱스가 자동으로 부여됩니다. 인덱스를 출력해보겠습니다.\\nprint(df.index)\\nRangeIndex(start=0, stop=6, step=1)']\n",
      "[\"넘파이(Numpy)는 수치 데이터를 다루는 파이썬 패키지입니다. Numpy의 핵심이라고 불리는 다차원 행렬 자료구조인 ndarray를 통해 벡터 및 행렬을 사용하는 선형 대수 계산에서 주로 사용됩니다. Numpy는 편의성뿐만 아니라, 속도면에서도 순수 파이썬에 비해 압도적으로 빠르다는 장점이 있습니다.\\n아나콘다를 설치하지 않았다면 아래의 커맨드로 Numpy를 별도 설치할 수 있습니다.\\npip install numpy\\n> ipython\\n...\\nIn [1]: import numpy as np\\nIn [2]: np.__version__\\nOut[2]: '1.16.5'\\nNumpy의 경우  np라는 명칭으로 임포트하는 것이 관례입니다.\\nimport numpy as np\\n1) np.array()\\nNumpy의 핵심은 ndarray입니다. np.array()는 리스트, 튜플, 배열로 부터 ndarray를 생성합니다. 파이썬 자료구조 중 하나인 리스트를 가지고 1차원 배열을 생성해보겠습니다.\", \"# 1차원 배열\\nvec = np.array([1, 2, 3, 4, 5])\\nprint(vec)\\n[1 2 3 4 5]\\n2차원 배열을 만들어보겠습니다. 주의할 점은 array() 안에 하나의 리스트만 들어가므로 리스트의 리스트를 넣어야 합니다.\\n# 2차원 배열\\nmat = np.array([[10, 20, 30], [ 60, 70, 80]])\\nprint(mat)\\n[[10 20 30]\\n[60 70 80]]\\n두 배열의 타입을 확인해봅시다.\\nprint('vec의 타입 :',type(vec))\\nprint('mat의 타입 :',type(mat))\\nvec의 타입 : <class 'numpy.ndarray'>\\nmat의 타입 : <class 'numpy.ndarray'>\", \"print('mat의 타입 :',type(mat))\\nvec의 타입 : <class 'numpy.ndarray'>\\nmat의 타입 : <class 'numpy.ndarray'>\\n동일하게 타입이 numpy.ndarray라고 나오게 됩니다. Numpy 배열에는 축의 개수(ndim)와 크기(shape)라는 개념이 존재하는데, 배열의 크기를 정확히 숙지하는 것은 딥 러닝에서 매우 중요합니다. 축의 개수와 크기가 어떤 의미를 가지는지에 대해서는 머신 러닝 챕터에서 벡터와 행렬 연산을 설명할 때 언급하겠습니다.\\nprint('vec의 축의 개수 :',vec.ndim) # 축의 개수 출력\\nprint('vec의 크기(shape) :',vec.shape) # 크기 출력\\nvec의 축의 개수 : 1\\nvec의 크기(shape) : (5,)\\nprint('mat의 축의 개수 :',mat.ndim) # 축의 개수 출력\\nprint('mat의 크기(shape) :',mat.shape) # 크기 출력\", \"print('mat의 축의 개수 :',mat.ndim) # 축의 개수 출력\\nprint('mat의 크기(shape) :',mat.shape) # 크기 출력\\nmat의 축의 개수 : 2\\nmat의 크기(shape) : (2, 3)\\n2) ndarray의 초기화\\n위에서는 리스트를 가지고 ndarray를 생성했지만 ndarray를 만드는 다양한 다른 방법이 존재합니다. 이 외에도 다양한 방법이 존재하므로 필요에 따라서 다양한 배열을 생성할 수 있습니다.\\nnp.zeros()는 배열의 모든 원소에 0을 삽입합니다.\\n# 모든 값이 0인 2x3 배열 생성.\\nzero_mat = np.zeros((2,3))\\nprint(zero_mat)\\n[[0. 0. 0.]\\n[0. 0. 0.]]\\nnp.ones()는 배열의 모든 원소에 1을 삽입합니다.\\n# 모든 값이 1인 2x3 배열 생성.\\none_mat = np.ones((2,3))\\nprint(one_mat)\\n[[1. 1. 1.]\\n[1. 1. 1.]]\", '# 모든 값이 1인 2x3 배열 생성.\\none_mat = np.ones((2,3))\\nprint(one_mat)\\n[[1. 1. 1.]\\n[1. 1. 1.]]\\nnp.full()은 배열에 사용자가 지정한 값을 삽입합니다.\\n# 모든 값이 특정 상수인 배열 생성. 이 경우 7.\\nsame_value_mat = np.full((2,2), 7)\\nprint(same_value_mat)\\n[[7 7]\\n[7 7]]\\nnp.eye()는 대각선으로는 1이고 나머지는 0인 2차원 배열을 생성합니다.\\n# 대각선 값이 1이고 나머지 값이 0인 2차원 배열을 생성.\\neye_mat = np.eye(3)\\nprint(eye_mat)\\n[[1. 0. 0.]\\n[0. 1. 0.]]\\n[0. 0. 1.]]\\nnp.random.random()은 임의의 값을 가지는 배열을 생성합니다.\\n# 임의의 값으로 채워진 배열 생성\\nrandom_mat = np.random.random((2,2)) # 임의의 값으로 채워진 배열 생성', '# 임의의 값으로 채워진 배열 생성\\nrandom_mat = np.random.random((2,2)) # 임의의 값으로 채워진 배열 생성\\nprint(random_mat)\\n[[0.3111881  0.72996102]\\n[0.65667734 0.40758328]]\\n이 외에도 Numpy에는 배열을 만드는 다양한 방법이 존재하므로 필요한 방법을 사용하여 배열을 생성할 수 있습니다.\\n3) np.arange()\\nnp.arange(n)은 0부터 n-1까지의 값을 가지는 배열을 생성합니다.\\n# 0부터 9까지\\nrange_vec = np.arange(10)\\nprint(range_vec)\\n[0 1 2 3 4 5 6 7 8 9]\\nnp.arange(i, j, k)는 i부터 j-1까지 k씩 증가하는 배열을 생성합니다.\\n# 1부터 9까지 +2씩 적용되는 범위\\nn = 2\\nrange_n_step_vec = np.arange(1, 10, n)\\nprint(range_n_step_vec)\\n[1 3 5 7 9]', 'n = 2\\nrange_n_step_vec = np.arange(1, 10, n)\\nprint(range_n_step_vec)\\n[1 3 5 7 9]\\n4) np.reshape()\\nnp.reshape()은 내부 데이터는 변경하지 않으면서 배열의 구조를 바꿉니다. 0부터 29까지의 숫자를 생성하는 arange(30)을 수행한 후, 원소의 개수가 30개이므로 5행 6열의 행렬로 변경해봅시다.\\nreshape_mat = np.array(np.arange(30)).reshape((5,6))\\nprint(reshape_mat)\\n[[ 0  1  2  3  4  5]\\n[ 6  7  8  9 10 11]\\n[12 13 14 15 16 17]\\n[18 19 20 21 22 23]\\n[24 25 26 27 28 29]]\\n5) Numpy 슬라이싱', '[ 6  7  8  9 10 11]\\n[12 13 14 15 16 17]\\n[18 19 20 21 22 23]\\n[24 25 26 27 28 29]]\\n5) Numpy 슬라이싱\\nndarray를 통해 만든 다차원 배열은 파이썬의 자료구조인 리스트처럼 슬라이싱(slicing) 기능을 지원합니다. 슬라이싱 기능을 사용하여 특정 행이나 열들의 원소들을 접근할 수 있습니다.\\nmat = np.array([[1, 2, 3], [4, 5, 6]])\\nprint(mat)\\n[[1 2 3]\\n[4 5 6]]\\n# 첫번째 행 출력\\nslicing_mat = mat[0, :]\\nprint(slicing_mat)\\n[1 2 3]\\n# 두번째 열 출력\\nslicing_mat = mat[:, 1]\\nprint(slicing_mat)\\n[2 5]\\n6) Numpy 정수 인덱싱(integer indexing)', '# 두번째 열 출력\\nslicing_mat = mat[:, 1]\\nprint(slicing_mat)\\n[2 5]\\n6) Numpy 정수 인덱싱(integer indexing)\\n슬라이싱을 사용하면 배열로부터 부분 배열을 추출할 수 있지만, 연속적이지 않은 원소로 배열을 만들 경우에는 슬라이싱으로는 만들 수 없습니다. 예를 들어서 2행 2열의 원소와 5행 5열의 원소를 뽑아서 하나의 배열로 만들고자 하는 경우가 그렇습니다. 이런 경우에는 인덱싱을 사용하여 배열을 구성할 수 있습니다. 인덱싱은 원하는 위치의 원소들을 뽑을 수 있습니다.\\nmat = np.array([[1, 2], [4, 5], [7, 8]])\\nprint(mat)\\n[[1 2]\\n[4 5]\\n[7 8]]\\n특정 위치의 원소만을 가져와봅시다.\\n# 1행 0열의 원소\\n# => 0부터 카운트하므로 두번째 행 첫번째 열의 원소.\\nprint(mat[1, 0])\\n4\\n특정 위치의 원소 두 개를 가져와 새로운 배열을 만들어봅시다.', '# 1행 0열의 원소\\n# => 0부터 카운트하므로 두번째 행 첫번째 열의 원소.\\nprint(mat[1, 0])\\n4\\n특정 위치의 원소 두 개를 가져와 새로운 배열을 만들어봅시다.\\n# mat[[2행, 1행],[0열, 1열]]\\n# 각 행과 열의 쌍을 매칭하면 2행 0열, 1행 1열의 두 개의 원소.\\nindexing_mat = mat[[2, 1],[0, 1]]\\nprint(indexing_mat)\\n[7 5]\\n7) Numpy 연산\\nNumpy를 사용하면 배열간 연산을 손쉽게 수행할 수 있습니다. 덧셈, 뺄셈, 곱셈, 나눗셈을 위해서는 연산자 +, -, *, /를 사용할 수 있으며 또는 np.add(), np.subtract(), np.multiply(), np.divide()를 사용할 수도 있습니다.\\nx = np.array([1,2,3])\\ny = np.array([4,5,6])\\n# result = np.add(x, y)와 동일.\\nresult = x + y\\nprint(result)\\n[5 7 9]', 'y = np.array([4,5,6])\\n# result = np.add(x, y)와 동일.\\nresult = x + y\\nprint(result)\\n[5 7 9]\\n# result = np.subtract(x, y)와 동일.\\nresult = x - y\\nprint(result)\\n[-3 -3 -3]\\n# result = np.multiply(result, x)와 동일.\\nresult = result * x\\nprint(result)\\n[-3 -6 -9]\\n# result = np.divide(result, x)와 동일.\\nresult = result / x\\nprint(result)\\n[-3. -3. -3.]\\n위에서 *를 통해 수행한 것은 요소별 곱입니다. Numpy에서 벡터와 행렬곱 또는 행렬곱을 위해서는 dot()을 사용해야 합니다.\\nmat1 = np.array([[1,2],[3,4]])\\nmat2 = np.array([[5,6],[7,8]])\\nmat3 = np.dot(mat1, mat2)', 'mat1 = np.array([[1,2],[3,4]])\\nmat2 = np.array([[5,6],[7,8]])\\nmat3 = np.dot(mat1, mat2)\\nprint(mat3)\\n[[19 22]\\n[43 50]]']\n",
      "[\"맷플롯립(Matplotlib)은 데이터를 차트(chart)나 플롯(plot)으로 시각화하는 패키지입니다. 데이터 분석에서 Matplotlib은 데이터 분석 이전에 데이터 이해를 위한 시각화나, 데이터 분석 후에 결과를 시각화하기 위해서 사용됩니다. 아나콘다를 설치하지 않았다면 아래의 커맨드로 Matplotlib를 별도 설치할 수 있습니다.\\npip install matplotlib\\n> ipython\\n...\\nIn [1]: import matplotlib as mpl\\nIn [2]: mpl.__version__\\nOut[2]: '2.2.3'\\nMatplotlib을 다 설치하였다면 Matplotlib의 주요 모듈인 pyplot를 관례상 plt라는 명칭으로 임포트해봅시다.\\nimport matplotlib.pyplot as plt\\n1) 라인 플롯 그리기\", \"import matplotlib.pyplot as plt\\n1) 라인 플롯 그리기\\nplot()은 라인 플롯을 그리는 기능을 수행합니다. plot()에 x축과 y축의 값을 기재하고 그림을 표시하는 show()를 통해서 시각화해봅시다. 그래프에는 title('제목')을 사용하여 제목을 지정할 수 있습니다. 여기서는 그래프에 'test'라는 제목을 넣어봅시다. 주피터 노트북에서는 show()를 사용하지 않더라도 그래프가 자동으로 렌더링 되므로 그래프가 시각화가 되지만 다른 개발 환경에서 사용할 때를 가정하여 show()를 코드에 삽입하였습니다.\\nplt.title('test')\\nplt.plot([1,2,3,4],[2,4,8,6])\\nplt.show()\\n[이미지: ]\\n2) 축 레이블 삽입하기\\nx축과 y축 각각에 축이름을 삽입하고 싶다면 xlabel('넣고 싶은 축이름')과 ylabel('넣고 싶은 축이름')을 사용합니다. 위의 그래프에 hours와 score라는 축이름을 각각 추가해봅시다.\", \"plt.title('test')\\nplt.plot([1,2,3,4],[2,4,8,6])\\nplt.xlabel('hours')\\nplt.ylabel('score')\\nplt.show()\\n[이미지: ]\\n3) 라인 추가와 범례 삽입하기\\n다수의 plot()을 하나의 그래프에 나타낼 수 있습니다. 여러개의 라인 플롯을 동시에 사용할 경우에는 각 선이 어떤 데이터를 나타내는지를 보여주기 위해 범례(legend)를 사용할 수 있습니다.\\nplt.title('students')\\nplt.plot([1,2,3,4],[2,4,8,6])\\nplt.plot([1.5,2.5,3.5,4.5],[3,5,8,10]) # 라인 새로 추가\\nplt.xlabel('hours')\\nplt.ylabel('score')\\nplt.legend(['A student', 'B student']) # 범례 삽입\\nplt.show()\\n[이미지: ]\", \"plt.ylabel('score')\\nplt.legend(['A student', 'B student']) # 범례 삽입\\nplt.show()\\n[이미지: ]\\n좀 더 다양한 형태의 그래프를 그리는 실습은 딥 러닝 챕터의 인공 신경망 훑어보기 실습에서 확인할 수 있습니다.\\n==================================================\\n--- 01-05 머신 러닝 워크플로우(Machine Learning Workflow) ---\\n이번 챕터에서는 데이터 사이언스(Data Science) 또는 머신 러닝(Machine Learning) 과정에서 거치는 전반적인 과정에 대해서 알아보겠습니다. 이 책의 제목은 딥 러닝(Deep Learning)을 이용한 자연어 처리이지만, 딥 러닝 또한 머신 러닝의 한 갈래로 딥 러닝 워크플로우 또한 머신 러닝 워크플로우로 간주 할 수 있습니다.\"]\n",
      "['데이터를 수집하고 머신 러닝을 하는 과정을 크게 6가지로 나누면, 아래의 그림과 같습니다.\\n[이미지: ]\\n1) 수집(Acquisition)\\n머신 러닝을 하기 위해서는 기계에 학습시켜야 할 데이터가 필요합니다. 자연어 처리의 경우, 자연어 데이터를 말뭉치 또는 코퍼스(corpus)라고 부르는데 코퍼스의 의미를 풀이하면, 조사나 연구 목적에 의해서 특정 도메인으로부터 수집된 텍스트 집합을 말합니다. 텍스트 데이터의 파일 형식은 txt 파일, csv 파일, xml 파일 등 다양하며 그 출처도 음성 데이터, 웹 수집기를 통해 수집된 데이터, 영화 리뷰 등 다양합니다.\\n2) 점검 및 탐색(Inspection and exploration)\\n데이터가 수집되었다면, 이제 데이터를 점검하고 탐색하는 단계입니다. 여기서는 데이터의 구조, 노이즈 데이터, 머신 러닝 적용을 위해서 데이터를 어떻게 정제해야하는지 등을 파악해야 합니다.', '이 단계를 탐색적 데이터 분석(Exploratory Data Analysis, EDA) 단계라고도 하는데 이는 독립 변수, 종속 변수, 변수 유형, 변수의 데이터 타입 등을 점검하며 데이터의 특징과 내재하는 구조적 관계를 알아내는 과정을 의미합니다. 이 과정에서 시각화와 간단한 통계 테스트를 진행하기도 합니다.\\n3) 전처리 및 정제(Preprocessing and Cleaning)\\n데이터에 대한 파악이 끝났다면, 머신 러닝 워크플로우에서 가장 까다로운 작업 중 하나인 데이터 전처리 과정에 들어갑니다. 이 단계는 많은 단계를 포함하고 있는데, 가령 자연어 처리라면 토큰화, 정제, 정규화, 불용어 제거 등의 단계를 포함합니다. 빠르고 정확한 데이터 전처리를 하기 위해서는 사용하고 있는 툴(이 책에서는 파이썬)에 대한 다양한 라이브러리에 대한 지식이 필요합니다. 정말 까다로운 전처리의 경우에는 전처리 과정에서 머신 러닝이 사용되기도 합니다.', '4) 모델링 및 훈련(Modeling and Training)\\n데이터 전처리가 끝났다면, 머신 러닝에 대한 코드를 작성하는 단계인 모델링 단계에 들어갑니다. 적절한 머신 러닝 알고리즘을 선택하여 모델링이 끝났다면, 전처리가 완료 된 데이터를 머신 러닝 알고리즘을 통해 기계에게 학습(training)시킵니다. 이를 훈련이라고도 하는데, 이 두 용어를 혼용해서 사용합니다. 기계가 데이터에 대한 학습을 마치고나서 훈련이 제대로 되었다면 그 후에 기계는 우리가 원하는 태스크(task)인 기계 번역, 음성 인식, 텍스트 분류 등의 자연어 처리 작업을 수행할 수 있게 됩니다.', '여기서 주의해야 할 점은 대부분의 경우에서 모든 데이터를 기계에게 학습시켜서는 안 된다는 점입니다. 뒤의 실습에서 보게되겠지만 데이터 중 일부는 테스트용으로 남겨두고 훈련용 데이터만 훈련에 사용해야 합니다. 그래야만 기계가 학습을 하고나서, 테스트용 데이터를 통해서 현재 성능이 얼마나 되는지를 측정할 수 있으며 과적합(overfitting) 상황을 막을 수 있습니다. 사실 최선은 훈련용, 테스트용으로 두 가지만 나누는 것보다는 훈련용, 검증용, 테스트용. 데이터를 이렇게 세 가지로 나누고 훈련용 데이터만 훈련에 사용하는 것입니다.\\n[이미지: ]', '[이미지: ]\\n검증용과 테스트용의 차이는 무엇일까요? 수능 시험에 비유하자면 훈련용은 학습지, 검증용은 모의고사, 테스트용은 수능 시험이라고 볼 수 있습니다. 학습지를 풀고 수능 시험을 볼 수도 있겠지만, 모의 고사를 풀며 부족한 부분이 무엇인지 검증하고 보완하는 단계를 하나 더 놓는 방법도 있겠지요. 사실 현업의 경우라면 검증용 데이터는 거의 필수적입니다.\\n검증용 데이터는 현재 모델의 성능. 즉, 기계가 훈련용 데이터로 얼마나 제대로 학습이 되었는지를 판단하는 용으로 사용되며 검증용 데이터를 사용하여 모델의 성능을 개선하는데 사용됩니다. 테스트용 데이터는 모델의 최종 성능을 평가하는 데이터로 모델의 성능을 개선하는 일에 사용되는 것이 아니라, 모델의 성능을 수치화하여 평가하기 위해 사용됩니다. 쉽게 말해 시험에 비유하면 채점하는 단계입니다.', '이 책에서는 실습 상황에 따라서 훈련용, 검증용, 테스트용 세 가지를 모두 사용하거나 때로는 훈련용, 테스트용 두 가지만 사용하기도 합니다. 하지만 현업에서 최선은 검증용 데이터 또한 사용하는 것임을 기억해둡시다.\\n5) 평가(Evaluation)\\n미리 언급하였는데, 기계가 다 학습이 되었다면 테스트용 데이터로 성능을 평가하게 됩니다. 평가 방법은 기계가 예측한 데이터가 테스트용 데이터의 실제 정답과 얼마나 가까운지를 측정합니다.\\n6) 배포(Deployment)\\n평가 단계에서 기계가 성공적으로 훈련이 된 것으로 판단된다면 완성된 모델이 배포되는 단계가 됩니다.  다만, 여기서 완성된 모델에 대한 전체적인 피드백으로 인해 모델을 업데이트 해야하는 상황이 온다면 수집 단계로 돌아갈 수 있습니다.\\n==================================================\\n--- 02. 텍스트 전처리(Text preprocessing) ---', \"==================================================\\n--- 02. 텍스트 전처리(Text preprocessing) ---\\n마지막 편집일시 : 2021년 12월 20일 2:37 오후\\n==================================================\\n--- 02-01 토큰화(Tokenization) ---\\n```\\n꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\\n꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\", \"꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\\n```자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태라면, 해당 데이터를 사용하고자하는 용도에 맞게 토큰화(tokenization) & 정제(cleaning) & 정규화(normalization)하는 일을 하게 됩니다. 이번에는 그 중에서도 토큰화에 대해서 학습합니다.\\n주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 합니다. 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의합니다. 여기서는 토큰화에 대한 발생할 수 있는 여러가지 상황에 대해서 언급하여 토큰화에 대한 개념을 이해합니다. 이어서 NLTK, KoNLPY를 통해 실습을 진행하며 토큰화를 수행합니다.\"]\n",
      "['토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화(word tokenization)라고 합니다. 다만, 여기서 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 합니다.\\n예를 들어보겠습니다. 아래의 입력으로부터 구두점(punctuation)과 같은 문자는 제외시키는 간단한 단어 토큰화 작업을 해봅시다. 구두점이란 마침표(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호를 말합니다.\\n입력: Time is an illusion. Lunchtime double so!\\n이러한 입력으로부터 구두점을 제외시킨 토큰화 작업의 결과는 다음과 같습니다.\\n출력 : \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\"', '출력 : \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\"\\n이 예제에서 토큰화 작업은 굉장히 간단합니다. 구두점을 지운 뒤에 띄어쓰기(whitespace)를 기준으로 잘라냈습니다. 하지만 이 예제는 토큰화의 가장 기초적인 예제를 보여준 것에 불과합니다.\\n보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning) 작업을 수행하는 것만으로 해결되지 않습니다. 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생하기도 합니다. 심지어 띄어쓰기 단위로 자르면 사실상 단어 토큰이 구분되는 영어와 달리, 한국어는 띄어쓰기만으로는 단어 토큰을 구분하기 어렵습니다. 그 이유는 뒤에서 언급하겠습니다.']\n",
      "[\"토큰화를 하다보면, 예상하지 못한 경우가 있어서 토큰화의 기준을 생각해봐야 하는 경우가 발생합니다. 물론, 이러한 선택은 해당 데이터를 가지고 어떤 용도로 사용할 것인지에 따라서 그 용도에 영향이 없는 기준으로 정하면 됩니다. 예를 들어 영어권 언어에서 아포스트로피를(')가 들어가있는 단어는 어떻게 토큰으로 분류해야 하는지에 대한 선택의 문제를 보여드리겠습니다.\\n다음과 같은 문장이 있다고 해봅시다.\\nDon't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\\n아포스트로피가 들어간 상황에서 Don't와 Jone's는 어떻게 토큰화할 수 있을까요? 다양한 선택지가 있습니다.\\nDon't\\nDon t\\nDont\\nDo n't\\nJone's\\nJone s\\nJone\\nJones\", \"Don't\\nDon t\\nDont\\nDo n't\\nJone's\\nJone s\\nJone\\nJones\\n이 중 사용자가 원하는 결과가 나오도록 토큰화 도구를 직접 설계할 수도 있겠지만, 기존에 공개된 도구들을 사용하였을 때의 결과가 사용자의 목적과 일치한다면 해당 도구를 사용할 수도 있을 것입니다. NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공합니다. 그 중 word_tokenize와 WordPunctTokenizer를 사용해서 아포스트로피를 어떻게 처리하는지 확인해보겠습니다.\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.tokenize import WordPunctTokenizer\\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\\n우선 word_tokenize를 사용해봅시다.\", 'from tensorflow.keras.preprocessing.text import text_to_word_sequence\\n우선 word_tokenize를 사용해봅시다.\\nprint(\\'단어 토큰화1 :\\',word_tokenize(\"Don\\'t be fooled by the dark sounding name, Mr. Jone\\'s Orphanage is as cheery as cheery goes for a pastry shop.\"))\\n단어 토큰화1 : [\\'Do\\', \"n\\'t\", \\'be\\', \\'fooled\\', \\'by\\', \\'the\\', \\'dark\\', \\'sounding\\', \\'name\\', \\',\\', \\'Mr.\\', \\'Jone\\', \"\\'s\", \\'Orphanage\\', \\'is\\', \\'as\\', \\'cheery\\', \\'as\\', \\'cheery\\', \\'goes\\', \\'for\\', \\'a\\', \\'pastry\\', \\'shop\\', \\'.\\']', 'word_tokenize는 Don\\'t를 Do와 n\\'t로 분리하였으며, 반면 Jone\\'s는 Jone과 \\'s로 분리한 것을 확인할 수 있습니다.\\n그렇다면, wordPunctTokenizer는 아포스트로피가 들어간 코퍼스를 어떻게 처리할까요?\\nprint(\\'단어 토큰화2 :\\',WordPunctTokenizer().tokenize(\"Don\\'t be fooled by the dark sounding name, Mr. Jone\\'s Orphanage is as cheery as cheery goes for a pastry shop.\"))\\n[\\'Don\\', \"\\'\", \\'t\\', \\'be\\', \\'fooled\\', \\'by\\', \\'the\\', \\'dark\\', \\'sounding\\', \\'name\\', \\',\\', \\'Mr\\', \\'.\\', \\'Jone\\', \"\\'\", \\'s\\', \\'Orphanage\\', \\'is\\', \\'as\\', \\'cheery\\', \\'as\\', \\'cheery\\', \\'goes\\', \\'for\\', \\'a\\', \\'pastry\\', \\'shop\\', \\'.\\']', 'WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기때문에, 앞서 확인했던 word_tokenize와는 달리 Don\\'t를 Don과 \\'와 t로 분리하였으며, 이와 마찬가지로 Jone\\'s를 Jone과 \\'와 s로 분리한 것을 확인할 수 있습니다. 케라스 또한 토큰화 도구로서 text_to_word_sequence를 지원합니다.\\nprint(\\'단어 토큰화3 :\\',text_to_word_sequence(\"Don\\'t be fooled by the dark sounding name, Mr. Jone\\'s Orphanage is as cheery as cheery goes for a pastry shop.\"))', '단어 토큰화3 : [\"don\\'t\", \\'be\\', \\'fooled\\', \\'by\\', \\'the\\', \\'dark\\', \\'sounding\\', \\'name\\', \\'mr\\', \"jone\\'s\", \\'orphanage\\', \\'is\\', \\'as\\', \\'cheery\\', \\'as\\', \\'cheery\\', \\'goes\\', \\'for\\', \\'a\\', \\'pastry\\', \\'shop\\']\\n케라스의 text_to_word_sequence는 기본적으로 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거합니다. 하지만 don\\'t나 jone\\'s와 같은 경우 아포스트로피는 보존하는 것을 볼 수 있습니다.']\n",
      "['토큰화 작업을 단순하게 코퍼스에서 구두점을 제외하고 공백 기준으로 잘라내는 작업이라고 간주할 수는 없습니다. 이러한 일은 보다 섬세한 알고리즘이 필요한데 그 이유를 정리해봅니다.\\n1) 구두점이나 특수 문자를 단순 제외해서는 안 된다.\\n갖고있는 코퍼스에서 단어들을 걸러낼 때, 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않습니다. 코퍼스에 대한 정제 작업을 진행하다보면, 구두점조차도 하나의 토큰으로 분류하기도 합니다. 가장 기본적인 예를 들어보자면, 마침표(.)와 같은 경우는 문장의 경계를 알 수 있는데 도움이 되므로 단어를 뽑아낼 때, 마침표(.)를 제외하지 않을 수 있습니다.', '또 다른 예로 단어 자체에 구두점을 갖고 있는 경우도 있는데, m.p.h나 Ph.D나 AT&T 같은 경우가 있습니다. 또 특수 문자의 달러나 슬래시(/)로 예를 들어보면, $45.55와 같은 가격을 의미 하기도 하고, 01/02/06은 날짜를 의미하기도 합니다. 보통 이런 경우 45.55를 하나로 취급하고 45와 55로 따로 분류하고 싶지는 않을 수 있습니다.\\n숫자 사이에 컴마(,)가 들어가는 경우도 있습니다. 보통 수치를 표현할 때는 123,456,789와 같이 세 자리 단위로 컴마가 있습니다.\\n2) 줄임말과 단어 내에 띄어쓰기가 있는 경우.', \"2) 줄임말과 단어 내에 띄어쓰기가 있는 경우.\\n토큰화 작업에서 종종 영어권 언어의 아포스트로피(')는 압축된 단어를 다시 펼치는 역할을 하기도 합니다. 예를 들어 what're는 what are의 줄임말이며, we're는 we are의 줄임말입니다. 위의 예에서 re를 접어(clitic)이라고 합니다. 즉, 단어가 줄임말로 쓰일 때 생기는 형태를 말합니다. 가령 I am을 줄인 I'm이 있을 때, m을 접어라고 합니다.\\nNew York이라는 단어나 rock 'n' roll이라는 단어를 봅시다. 이 단어들은 하나의 단어이지만 중간에 띄어쓰기가 존재합니다. 사용 용도에 따라서, 하나의 단어 사이에 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야하는 경우도 있을 수 있으므로, 토큰화 작업은 저러한 단어를 하나로 인식할 수 있는 능력도 가져야합니다.\\n3) 표준 토큰화 예제\", '3) 표준 토큰화 예제\\n이해를 돕기 위해 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treebank Tokenization의 규칙에 대해서 소개하고, 토큰화의 결과를 확인해보겠습니다.\\n규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\\n규칙 2. doesn\\'t와 같이 아포스트로피로 \\'접어\\'가 함께하는 단어는 분리해준다.\\n해당 표준에 아래의 문장을 입력으로 넣어봅니다.\\n\"Starting a home-based restaurant may be an ideal. it doesn\\'t have a food chain or restaurant of their own.\"\\nfrom nltk.tokenize import TreebankWordTokenizer\\ntokenizer = TreebankWordTokenizer()', 'from nltk.tokenize import TreebankWordTokenizer\\ntokenizer = TreebankWordTokenizer()\\ntext = \"Starting a home-based restaurant may be an ideal. it doesn\\'t have a food chain or restaurant of their own.\"\\nprint(\\'트리뱅크 워드토크나이저 :\\',tokenizer.tokenize(text))\\n트리뱅크 워드토크나이저 : [\\'Starting\\', \\'a\\', \\'home-based\\', \\'restaurant\\', \\'may\\', \\'be\\', \\'an\\', \\'ideal.\\', \\'it\\', \\'does\\', \"n\\'t\", \\'have\\', \\'a\\', \\'food\\', \\'chain\\', \\'or\\', \\'restaurant\\', \\'of\\', \\'their\\', \\'own\\', \\'.\\']', \"결과를 보면, 각각 규칙1과 규칙2에 따라서 home-based는 하나의 토큰으로 취급하고 있으며, dosen't의 경우 does와 n't는 분리되었음을 볼 수 있습니다.\"]\n",
      "['이번에는 토큰의 단위가 문장(sentence)일 경우를 논의해보겠습니다. 이 작업은 갖고있는 코퍼스 내에서 문장 단위로 구분하는 작업으로 때로는 문장 분류(sentence segmentation)라고도 부릅니다. 보통 갖고있는 코퍼스가 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어 있지 않아서 이를 사용하고자 하는 용도에 맞게 문장 토큰화가 필요할 수 있습니다.\\n어떻게 주어진 코퍼스로부터 문장 단위로 분류할 수 있을까요? 직관적으로 생각해봤을 때는 ?나 마침표(.)나 ! 기준으로 문장을 잘라내면 되지 않을까라고 생각할 수 있지만, 꼭 그렇지만은 않습니다. !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할을 하지만 마침표는 그렇지 않기 때문입니다. 마침표는 문장의 끝이 아니더라도 등장할 수 있습니다.\\nEX1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.', \"EX1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.\\nEX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\\n예를 들어 위의 예제에 마침표를 기준으로 문장 토큰화를 적용해본다면 어떨까요? 첫번째 예제에서는 보내줘.에서 그리고 두번째 예제에서는 year.에서 처음으로 문장이 끝난 것으로 인식하는 것이 제대로 문장의 끝을 예측했다고 볼 수 있습니다. 하지만 단순히 마침표(.)로 문장을 구분짓는다고 가정하면, 문장의 끝이 나오기 전에 이미 마침표가 여러번 등장하여 예상한 결과가 나오지 않게 됩니다.\", '사용하는 코퍼스가 어떤 국적의 언어인지, 또는 해당 코퍼스 내에서 특수문자들이 어떻게 사용되고 있는지에 따라서 직접 규칙들을 정의해볼 수 있겠습니다. 100% 정확도를 얻는 일은 쉬운 일이 아닌데, 갖고있는 코퍼스 데이터에 오타나, 문장의 구성이 엉망이라면 정해놓은 규칙이 소용이 없을 수 있기 때문입니다.\\nNLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원하고 있습니다. NLTK를 통해 문장 토큰화를 실습해보겠습니다.\\nfrom nltk.tokenize import sent_tokenize', 'from nltk.tokenize import sent_tokenize\\ntext = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\\nprint(\\'문장 토큰화1 :\\',sent_tokenize(text))', \"print('문장 토큰화1 :',sent_tokenize(text))\\n문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\\n위 코드는 text에 저장된 여러 개의 문장들로부터 문장을 구분하는 코드입니다. 출력 결과를 보면 성공적으로 모든 문장을 구분해내었음을 볼 수 있습니다. 그렇다면 이번에는 문장 중간에 마침표가 다수 등장하는 경우에 대해서도 실습해보겠습니다.\", 'text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\\nprint(\\'문장 토큰화2 :\\',sent_tokenize(text))\\n문장 토큰화2 : [\\'I am actively looking for Ph.D. students.\\', \\'and you are a Ph.D student.\\']\\nNLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않았기 때문에, Ph.D.를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있습니다. 한국어에 대한 문장 토큰화 도구 또한 존재합니다. 한국어의 경우에는 박상길님이 개발한 KSS(Korean Sentence Splitter)를 추천합니다. 다음과 같이 KSS를 설치합니다.\\npip install kss\\nKSS를 통해서 문장 토큰화를 진행해보겠습니다.\\nimport kss', \"pip install kss\\nKSS를 통해서 문장 토큰화를 진행해보겠습니다.\\nimport kss\\ntext = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\\nprint('한국어 문장 토큰화 :',kss.split_sentences(text))\\n한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\\n출력 결과는 정상적으로 모든 문장이 분리된 결과를 보여줍니다.\"]\n",
      "[\"영어는 New York과 같은 합성어나 he's 와 같이 줄임말에 대한 예외처리만 한다면, 띄어쓰기(whitespace)를 기준으로 하는 띄어쓰기 토큰화를 수행해도 단어 토큰화가 잘 작동합니다. 거의 대부분의 경우에서 단어 단위로 띄어쓰기가 이루어지기 때문에 띄어쓰기 토큰화와 단어 토큰화가 거의 같기 때문입니다.\\n하지만 한국어는 영어와는 달리 띄어쓰기만으로는 토큰화를 하기에 부족합니다. 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 어절 토큰화는 한국어 NLP에서 지양되고 있습니다. 어절 토큰화와 단어 토큰화는 같지 않기 때문입니다. 그 근본적인 이유는 한국어가 영어와는 다른 형태를 가지는 언어인 교착어라는 점에서 기인합니다. 교착어란 조사, 어미 등을 붙여서 말을 만드는 언어를 말합니다.\\n1) 교착어의 특성\", \"1) 교착어의 특성\\n예를 들어봅시다. 영어와는 달리 한국어에는 조사라는 것이 존재합니다. 예를 들어 한국어에 그(he/him)라는 주어나 목적어가 들어간 문장이 있다고 합시다. 이 경우, 그라는 단어 하나에도 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게됩니다. 자연어 처리를 하다보면 같은 단어임에도 서로 다른 조사가 붙어서 다른 단어로 인식이 되면 자연어 처리가 힘들고 번거로워지는 경우가 많습니다. 대부분의 한국어 NLP에서 조사는 분리해줄 필요가 있습니다.\\n띄어쓰기 단위가 영어처럼 독립적인 단어라면 띄어쓰기 단위로 토큰화를 하면 되겠지만 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 한다는 의미입니다.\", \"한국어 토큰화에서는 형태소(morpheme) 란 개념을 반드시 이해해야 합니다. 형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위를 말합니다. 이 형태소에는 두 가지 형태소가 있는데 자립 형태소와 의존 형태소입니다.\\n자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.\\n의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간을 말한다.\\n예를 들어 다음과 같은 문장이 있다고 합시다.\\n문장 : 에디가 책을 읽었다\\n이 문장을 띄어쓰기 단위 토큰화를 수행한다면 다음과 같은 결과를 얻습니다.\\n['에디가', '책을', '읽었다']\\n하지만 이를 형태소 단위로 분해하면 다음과 같습니다.\\n자립 형태소 : 에디, 책\\n의존 형태소 : -가, -을, 읽-, -었, -다\", \"['에디가', '책을', '읽었다']\\n하지만 이를 형태소 단위로 분해하면 다음과 같습니다.\\n자립 형태소 : 에디, 책\\n의존 형태소 : -가, -을, 읽-, -었, -다\\n'에디'라는 사람 이름과 '책'이라는 명사를 얻어낼 수 있습니다. 이를 통해 유추할 수 있는 것은 한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 어절 토큰화가 아니라 형태소 토큰화를 수행해야한다는 겁니다.\\n2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.\\n사용하는 한국어 코퍼스가 뉴스 기사와 같이 띄어쓰기를 철저하게 지키려고 노력하는 글이라면 좋겠지만, 많은 경우에 띄어쓰기가 틀렸거나 지켜지지 않는 코퍼스가 많습니다.\", '사용하는 한국어 코퍼스가 뉴스 기사와 같이 띄어쓰기를 철저하게 지키려고 노력하는 글이라면 좋겠지만, 많은 경우에 띄어쓰기가 틀렸거나 지켜지지 않는 코퍼스가 많습니다.\\n한국어는 영어권 언어와 비교하여 띄어쓰기가 어렵고 잘 지켜지지 않는 경향이 있습니다. 그 이유는 여러 견해가 있으나, 가장 기본적인 견해는 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어라는 점입니다. 띄어쓰기가 없던 한국어에 띄어쓰기가 보편화된 것도 근대(1933년, 한글맞춤법통일안)의 일입니다. 띄어쓰기를 전혀 하지 않은 한국어와 영어 두 가지 경우를 봅시다.\\nEX1) 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\\nEX2) Tobeornottobethatisthequestion', 'EX1) 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\\nEX2) Tobeornottobethatisthequestion\\n영어의 경우에는 띄어쓰기를 하지 않으면 손쉽게 알아보기 어려운 문장들이 생깁니다. 이는 한국어(모아쓰기 방식)와 영어(풀어쓰기 방식)라는 언어적 특성의 차이에 기인합니다. 이 책에서는 모아쓰기와 풀어쓰기에 대한 설명은 하지 않겠습니다. 다만, 결론적으로 한국어는 수많은 코퍼스에서 띄어쓰기가 무시되는 경우가 많아 자연어 처리가 어려워졌다는 것입니다.']\n",
      "[\"단어는 표기는 같지만 품사에 따라서 단어의 의미가 달라지기도 합니다. 예를 들어서 영어 단어 'fly'는 동사로는 '날다'라는 의미를 갖지만, 명사로는 '파리'라는 의미를 갖고있습니다. 한국어도 마찬가지입니다. '못'이라는 단어는 명사로서는 망치를 사용해서 목재 따위를 고정하는 물건을 의미합니다. 하지만 부사로서의 '못'은 '먹는다', '달린다'와 같은 동작 동사를 할 수 없다는 의미로 쓰입니다. 결국 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 보는 것이 주요 지표가 될 수도 있습니다. 그에 따라 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓기도 하는데, 이 작업을 품사 태깅(part-of-speech tagging)이라고 합니다. NLTK와 KoNLPy를 통해 품사 태깅 실습을 진행합니다.\"]\n",
      "['NLTK에서는 Penn Treebank POS Tags라는 기준을 사용하여 품사를 태깅합니다.\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.tag import pos_tag\\ntext = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\\ntokenized_sentence = word_tokenize(text)\\nprint(\\'단어 토큰화 :\\',tokenized_sentence)\\nprint(\\'품사 태깅 :\\',pos_tag(tokenized_sentence))\\n단어 토큰화 : [\\'I\\', \\'am\\', \\'actively\\', \\'looking\\', \\'for\\', \\'Ph.D.\\', \\'students\\', \\'.\\', \\'and\\', \\'you\\', \\'are\\', \\'a\\', \\'Ph.D.\\', \\'student\\', \\'.\\']', \"품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\\n영어 문장에 대해서 토큰화를 수행한 결과를 입력으로 품사 태깅을 수행하였습니다. Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다.\", '한국어 자연어 처리를 위해서는 KoNLPy(코엔엘파이)라는 파이썬 패키지를 사용할 수 있습니다. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있습니다.\\n한국어 NLP에서 형태소 분석기를 사용하여 단어 토큰화. 더 정확히는 형태소 토큰화(morpheme tokenization)를 수행해보겠습니다. 여기서는 Okt와 꼬꼬마 두 개의 형태소 분석기를 사용하여 토큰화를 수행하겠습니다.\\nfrom konlpy.tag import Okt\\nfrom konlpy.tag import Kkma\\nokt = Okt()\\nkkma = Kkma()\\nprint(\\'OKT 형태소 분석 :\\',okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\\nprint(\\'OKT 품사 태깅 :\\',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))', 'print(\\'OKT 품사 태깅 :\\',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\\nprint(\\'OKT 명사 추출 :\\',okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\\nOKT 형태소 분석 : [\\'열심히\\', \\'코딩\\', \\'한\\', \\'당신\\', \\',\\', \\'연휴\\', \\'에는\\', \\'여행\\', \\'을\\', \\'가봐요\\']\\nOKT 품사 태깅 : [(\\'열심히\\', \\'Adverb\\'), (\\'코딩\\', \\'Noun\\'), (\\'한\\', \\'Josa\\'), (\\'당신\\', \\'Noun\\'), (\\',\\', \\'Punctuation\\'), (\\'연휴\\', \\'Noun\\'), (\\'에는\\', \\'Josa\\'), (\\'여행\\', \\'Noun\\'), (\\'을\\', \\'Josa\\'), (\\'가봐요\\', \\'Verb\\')]\\nOKT 명사 추출 : [\\'코딩\\', \\'당신\\', \\'연휴\\', \\'여행\\']\\n위의 예제는 Okt 형태소 분석기로 토큰화를 시도해본 예제입니다. 각각의 메소드는 아래와 같은 기능을 갖고 있습니다.\\n1) morphs : 형태소 추출', '위의 예제는 Okt 형태소 분석기로 토큰화를 시도해본 예제입니다. 각각의 메소드는 아래와 같은 기능을 갖고 있습니다.\\n1) morphs : 형태소 추출\\n2) pos : 품사 태깅(Part-of-speech tagging)\\n3) nouns : 명사 추출\\n앞서 언급한 코엔엘파이의 형태소 분석기들은 공통적으로 이 메소드들을 제공하고 있습니다. 위 예제에서 형태소 추출과 품사 태깅 메소드의 결과를 보면 조사를 기본적으로 분리하고 있음을 확인할 수 있습니다. 한국어 NLP에서 전처리에 형태소 분석기를 사용하는 것은 굉장히 유용합니다. 이번에는 꼬꼬마 형태소 분석기를 사용하여 같은 문장에 대해서 토큰화를 진행해봅시다.\\nprint(\\'꼬꼬마 형태소 분석 :\\',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\\nprint(\\'꼬꼬마 품사 태깅 :\\',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))', 'print(\\'꼬꼬마 품사 태깅 :\\',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\\nprint(\\'꼬꼬마 명사 추출 :\\',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\\n꼬꼬마 형태소 분석 : [\\'열심히\\', \\'코딩\\', \\'하\\', \\'ㄴ\\', \\'당신\\', \\',\\', \\'연휴\\', \\'에\\', \\'는\\', \\'여행\\', \\'을\\', \\'가보\\', \\'아요\\']\\n꼬꼬마 품사 태깅 : [(\\'열심히\\', \\'MAG\\'), (\\'코딩\\', \\'NNG\\'), (\\'하\\', \\'XSV\\'), (\\'ㄴ\\', \\'ETD\\'), (\\'당신\\', \\'NP\\'), (\\',\\', \\'SP\\'), (\\'연휴\\', \\'NNG\\'), (\\'에\\', \\'JKM\\'), (\\'는\\', \\'JX\\'), (\\'여행\\', \\'NNG\\'), (\\'을\\', \\'JKO\\'), (\\'가보\\', \\'VV\\'), (\\'아요\\', \\'EFN\\')]\\n꼬꼬마 명사 추출 : [\\'코딩\\', \\'당신\\', \\'연휴\\', \\'여행\\']', \"꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\\n앞서 사용한 Okt 형태소 분석기와 결과가 다른 것을 볼 수 있습니다. 각 형태소 분석기는 성능과 결과가 다르게 나오기 때문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 됩니다. 예를 들어서 속도를 중시한다면 메캅을 사용할 수 있습니다.\\n==================================================\\n--- 02-02 정제(Cleaning) and 정규화(Normalization) ---\\n```\\nwas wondering anyone out there could enlighten this car.\", '```\\nwas wondering anyone out there could enlighten this car.\\n```코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)하는 일이 항상 함께합니다. 정제 및 정규화의 목적은 각각 다음과 같습니다.\\n정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\\n정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.', '정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\\n정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\\n정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기위해 지속적으로 이루어지기도 합니다. 사실 완벽한 정제 작업은 어려운 편이라서, 대부분의 경우 이 정도면 됐다.라는 일종의 합의점을 찾기도 합니다.']\n",
      "['필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화 규칙의 예로서 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하\\n는 방법을 사용할 수 있습니다.\\n가령, USA와 US는 같은 의미를 가지므로 하나의 단어로 정규화해볼 수 있습니다. uh-huh와 uhhuh는 형태는 다르지만 여전히 같은 의미를 갖고 있습니다. 이러한 정규화를 거치게 되면, US를 찾아도 USA도 함께 찾을 수 있을 것입니다. 뒤에서 표기가 다른 단어들을 통합하는 방법인 어간 추출(stemming)과 표제어 추출(lemmatizaiton)에 대해서 더 자세히 알아봅니다.']\n",
      "['영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법입니다. 영어권 언어에서 대문자는 문장의 맨 앞 등과 같은 특정 상황에서만 쓰이고, 대부분의 글은 소문자로 작성되기 때문에 대, 소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환작업으로 이루어지게 됩니다.', '소문자 변환이 왜 유용한지 예를 들어보도록 하겠습니다. 가령, Automobile이라는 단어가 문장의 첫 단어였기때문에 A가 대문자였다고 생각해봅시다. 여기에 소문자 변환을 사용하면, automobile을 찾는 질의(query)의 결과로서 Automobile도 찾을 수 있게 됩니다. 검색 엔진에서 사용자가 페라리 차량에 관심이 있어서 페라리를 검색해본다고 합시다. 엄밀히 말해서 사실 사용자가 검색을 통해 찾고자하는 결과는 a Ferrari car라고 봐야합니다. 하지만 검색 엔진은 소문자 변환을 적용했을 것이기 때문에 ferrari만 쳐도 원하는 결과를 얻을 수 있을 것입니다.\\n물론 대문자와 소문자를 무작정 통합해서는 안 됩니다. 대문자와 소문자가 구분되어야 하는 경우도 있습니다. 가령 미국을 뜻하는 단어 US와 우리를 뜻하는 us는 구분되어야 합니다. 또 회사 이름(General Motors)나, 사람 이름(Bush) 등은 대문자로 유지되는 것이 옳습니다.', '모든 토큰을 소문자로 만드는 것이 문제를 가져온다면, 또 다른 대안은 일부만 소문자로 변환시키는 방법도 있습니다. 가령, 이런 규칙은 어떨까요? 문장의 맨 앞에서 나오는 단어의 대문자만 소문자로 바꾸고, 다른 단어들은 전부 대문자인 상태로 놔두는 것입니다.\\n이러한 작업은 더 많은 변수를 사용해서 소문자 변환을 언제 사용할지 결정하는 머신 러닝 시퀀스 모델로 더 정확하게 진행시킬 수 있습니다. 하지만 만약 올바른 대문자 단어를 얻고 싶은 상황에서 훈련에 사용하는 코퍼스가 사용자들이 단어의 대문자, 소문자의 올바른 사용 방법과 상관없이 소문자를 사용하는 사람들로부터 나온 데이터라면 이러한 방법 또한 그다지 도움이 되지 않을 수 있습니다. 결국에는 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 합니다.']\n",
      "['정제 작업에서 제거해야하는 노이즈 데이터(noise data)는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)을 의미하기도 하지만, 분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 하기도 합니다.\\n불필요 단어들을 제거하는 방법으로는 불용어 제거와 등장 빈도가 적은 단어, 길이가 짧은 단어들을 제거하는 방법이 있습니다. 불용어 제거는 불용어 챕터에서 더욱 자세히 다루기로 하고, 여기서는 등장 빈도가 적은 단어와 길이가 짧은 단어를 제거하는 경우에 대해서 간략히 설명하겠습니다.\\n(1) 등장 빈도가 적은 단어', '(1) 등장 빈도가 적은 단어\\n때로는 텍스트 데이터에서 너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어들이 존재합니다. 예를 들어 입력된 메일이 정상 메일인지 스팸 메일인지를 분류하는 스팸 메일 분류기를 설계한다고 가정해보겠습니다. 총 100,000개의 메일을 가지고 정상 메일에서는 어떤 단어들이 주로 등장하고, 스팸 메일에서는 어떤 단어들이 주로 등장하는지를 가지고 설계하고자 합니다. 그런데 이때 100,000개의 메일 데이터에서 총 합 5번 밖에 등장하지 않은 단어가 있다면 이 단어는 직관적으로 분류에 거의 도움이 되지 않을 것임을 알 수 있습니다.\\n(2) 길이가 짧은 단어', '(2) 길이가 짧은 단어\\n영어권 언어에서는 길이가 짧은 단어를 삭제하는 것만으로도 어느정도 자연어 처리에서 크게 의미가 없는 단어들을 제거하는 효과를 볼 수 있다고 알려져 있습니다. 즉, 영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당됩니다. 사실 길이가 짧은 단어를 제거하는 2차 이유는 길이를 조건으로 텍스트를 삭제하면서 단어가 아닌 구두점들까지도 한꺼번에 제거하기 위함도 있습니다. 하지만 한국어에서는 길이가 짧은 단어라고 삭제하는 이런 방법이 크게 유효하지 않을 수 있는데 그 이유에 대해서 정리해보도록 하겠습니다.\\n단정적으로 말할 수는 없지만, 영어 단어의 평균 길이는 6~7 정도이며, 한국어 단어의 평균 길이는 2~3 정도로 추정되고 있습니다. 두 나라의 단어 평균 길이가 몇 인지에 대해서는 확실히 말하기 어렵지만 그럼에도 확실한 사실은 영어 단어의 길이가 한국어 단어의 길이보다는 평균적으로 길다는 점입니다.', \"이는 영어 단어와 한국어 단어에서 각 한 글자가 가진 의미의 크기가 다르다는 점에서 기인합니다. 한국어 단어는 한자어가 많고, 한 글자만으로도 이미 의미를 가진 경우가 많습니다. 예를 들어 '학교'라는 한국어 단어를 생각해보면, 배울 학(學)과 학교 교(校)로 글자 하나, 하나가 이미 함축적인 의미를 갖고있어 두 글자만으로 학교라는 단어를 표현합니다. 하지만 영어의 경우에는 학교라는 글자를 표현하기 위해서는 s, c, h, o, o, l이라는 총 6개의 글자가 필요합니다. 다른 예로는 전설 속 동물인 용(龍)을 표현하기 위해서는 한국어로는 한 글자면 충분하지만, 영어에서는 d, r, a, g, o, n이라는 총 6개의 글자가 필요합니다.\", '이러한 특성으로 인해 영어는 길이가 2~3 이하인 단어를 제거하는 것만으로도 크게 의미를 갖지 못하는 단어를 줄이는 효과를 갖고있습니다. 예를 들어 갖고 있는 텍스트 데이터에서 길이가 1인 단어를 제거하는 코드를 수행하면 대부분의 자연어 처리에서 의미를 갖지 못하는 단어인 관사 \\'a\\'와 주어로 쓰이는 \\'I\\'가 제거됩니다. 마찬가지로 길이가 2인 단어를 제거한다고 하면 it, at, to, on, in, by 등과 같은 대부분 불용어에 해당되는 단어들이 제거됩니다.  필요에 따라서는 길이가 3인 단어도 제거할 수 있지만, 이 경우 fox, dog, car 등 길이가 3인 명사들이 제거 되기시작하므로 사용하고자 하는 데이터에서 해당 방법을 사용해도 되는지에 대한 고민이 필요합니다.\\nimport re\\ntext = \"I was wondering if anyone out there could enlighten me on this car.\"', 'import re\\ntext = \"I was wondering if anyone out there could enlighten me on this car.\"\\n# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\\nshortword = re.compile(r\\'\\\\W*\\\\b\\\\w{1,2}\\\\b\\')\\nprint(shortword.sub(\\'\\', text))\\nwas wondering anyone out there could enlighten this car.']\n",
      "['얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해서 이를 제거할 수 있는 경우가 많습니다. 가령, HTML 문서로부터 가져온 코퍼스라면 문서 여기저기에 HTML 태그가 있습니다. 뉴스 기사를 크롤링 했다면, 기사마다 게재 시간이 적혀져 있을 수 있습니다. 정규 표현식은 이러한 코퍼스 내에 계속해서 등장하는 글자들을 규칙에 기반하여 한 번에 제거하는 방식으로서 매우 유용합니다.\\n이 책에서도 전처리를 위해 정규 표현식을 앞으로 종종 사용하게 될 겁니다. 예를 들어 위에서 길이가 짧은 단어를 제거할 때도, 정규 표현식이 유용하게 사용되었습니다. 정규 표현식에 대한 자세한 내용은 뒤에서 좀 더 상세하게 다룹니다.\\n==================================================\\n--- 02-03 어간 추출(Stemming) and 표제어 추출(Lemmatization) ---\\n```\\n잡/어간 + 다/어미', '--- 02-03 어간 추출(Stemming) and 표제어 추출(Lemmatization) ---\\n```\\n잡/어간 + 다/어미\\n```정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법인 표제어 추출(lemmatization)과 어간 추출(stemming)의 개념에 대해서 알아봅니다. 또한 이 둘의 결과가 어떻게 다른지 이해합니다.\\n이 두 작업이 갖고 있는 의미는 눈으로 봤을 때는 서로 다른 단어들이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것입니다. 이러한 방법들은 단어의 빈도수를 기반으로 문제를 풀고자 하는 뒤에서 학습하게 될 BoW(Bag of Words) 표현을 사용하는 자연어 처리 문제에서 주로 사용됩니다. 자연어 처리에서 전처리, 더 정확히는 정규화의 지향점은 언제나 갖고 있는 코퍼스로부터 복잡성을 줄이는 일입니다.']\n",
      "[\"표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미를 갖습니다. 표제어 추출은 단어들로부터 표제어를 찾아가는 과정입니다. 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단합니다. 예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있습니다. 이때, 이 단어들의 표제어는 be라고 합니다.\\n표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것입니다. 형태소란 '의미를 가진 가장 작은 단위'를 뜻합니다. 그리고 형태학(morphology)이란 형태소로부터 단어들을 만들어가는 학문을 뜻합니다. 형태소의 종류로 어간(stem)과 접사(affix)가 존재합니다.\\n1) 어간(stem)\\n: 단어의 의미를 담고 있는 단어의 핵심 부분.\\n2) 접사(affix)\\n: 단어에 추가적인 의미를 주는 부분.\", '1) 어간(stem)\\n: 단어의 의미를 담고 있는 단어의 핵심 부분.\\n2) 접사(affix)\\n: 단어에 추가적인 의미를 주는 부분.\\n형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 말합니다. 가령, cats라는 단어에 대해 형태학적 파싱을 수행한다면, 형태학적 파싱은 결과로 cat(어간)와 -s(접사)를 분리합니다. 꼭 두 가지로 분리되지 않는 경우도 있습니다. 단어 fox는 형태학적 파싱을 한다고 하더라도 더 이상 분리할 수 없습니다. fox는 독립적인 형태소이기 때문입니다. 이와 유사하게 cat 또한 더 이상 분리되지 않습니다.\\nNLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원합니다. 이를 통해 표제어 추출을 실습해보겠습니다.\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()', \"from nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\nwords = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\\nprint('표제어 추출 전 :',words)\\nprint('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])\\n표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\", \"표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\\n표제어 추출은 뒤에서 언급할 어간 추출과는 달리 단어의 형태가 적절히 보존되는 양상을 보이는 특징이 있습니다. 하지만 그럼에도 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있습니다. 이는 표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문입니다.\\nWordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있습니다. 즉, dies와 watched, has가 문장에서 동사로 쓰였다는 것을 알려준다면 표제어 추출기는 품사의 정보를 보존하면서 정확한 Lemma를 출력하게 됩니다.\\nlemmatizer.lemmatize('dies', 'v')\\n'die'\", \"lemmatizer.lemmatize('dies', 'v')\\n'die'\\nlemmatizer.lemmatize('watched', 'v')\\n'watch'\\nlemmatizer.lemmatize('has', 'v')\\n'have'\\n표제어 추출은 문맥을 고려하며 수행했을 때의 결과는 해당 단어의 품사 정보를 보존합니다. 하지만 어간 추출을 수행한 결과는 품사 정보가 보존되지 않습니다. 더 정확히는 어간 추출을 한 결과는 사전에 존재하지 않는 단어일 경우가 많습니다.\"]\n",
      "[\"어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다. 예제를 보면 쉽게 이해할 수 있습니다. 어간 추출 알고리즘 중 하나인 포터 알고리즘(Porter Algorithm)에 아래의 문자열을 입력으로 넣는다고 해봅시다.\\nThis was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\", 'from nltk.stem import PorterStemmer\\nfrom nltk.tokenize import word_tokenize\\nstemmer = PorterStemmer()\\nsentence = \"This was not the map we found in Billy Bones\\'s chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\\ntokenized_sentence = word_tokenize(sentence)\\nprint(\\'어간 추출 전 :\\', tokenized_sentence)\\nprint(\\'어간 추출 후 :\\',[stemmer.stem(word) for word in tokenized_sentence])', 'print(\\'어간 추출 후 :\\',[stemmer.stem(word) for word in tokenized_sentence])\\n어간 추출 전 : [\\'This\\', \\'was\\', \\'not\\', \\'the\\', \\'map\\', \\'we\\', \\'found\\', \\'in\\', \\'Billy\\', \\'Bones\\', \"\\'s\", \\'chest\\', \\',\\', \\'but\\', \\'an\\', \\'accurate\\', \\'copy\\', \\',\\', \\'complete\\', \\'in\\', \\'all\\', \\'things\\', \\'--\\', \\'names\\', \\'and\\', \\'heights\\', \\'and\\', \\'soundings\\', \\'--\\', \\'with\\', \\'the\\', \\'single\\', \\'exception\\', \\'of\\', \\'the\\', \\'red\\', \\'crosses\\', \\'and\\', \\'the\\', \\'written\\', \\'notes\\', \\'.\\']', '어간 추출 후 : [\\'thi\\', \\'wa\\', \\'not\\', \\'the\\', \\'map\\', \\'we\\', \\'found\\', \\'in\\', \\'billi\\', \\'bone\\', \"\\'s\", \\'chest\\', \\',\\', \\'but\\', \\'an\\', \\'accur\\', \\'copi\\', \\',\\', \\'complet\\', \\'in\\', \\'all\\', \\'thing\\', \\'--\\', \\'name\\', \\'and\\', \\'height\\', \\'and\\', \\'sound\\', \\'--\\', \\'with\\', \\'the\\', \\'singl\\', \\'except\\', \\'of\\', \\'the\\', \\'red\\', \\'cross\\', \\'and\\', \\'the\\', \\'written\\', \\'note\\', \\'.\\']\\n규칙 기반의 접근을 하고 있으므로 어간 추출 후의 결과에는 사전에 없는 단어들도 포함되어 있습니다. 가령, 포터 알고리즘의 어간 추출은 이러한 규칙들을 가집니다.\\nALIZE → AL\\nANCE → 제거\\nICAL → IC\\n위의 규칙에 따르면 좌측의 단어는 우측의 단어와 같은 결과를 얻게됩니다.', \"ALIZE → AL\\nANCE → 제거\\nICAL → IC\\n위의 규칙에 따르면 좌측의 단어는 우측의 단어와 같은 결과를 얻게됩니다.\\nformalize → formal\\nallowance → allow\\nelectricical → electric\\n실제 코드를 통해 확인해봅시다.\\nwords = ['formalize', 'allowance', 'electricical']\\nprint('어간 추출 전 :',words)\\nprint('어간 추출 후 :',[stemmer.stem(word) for word in words])\\n어간 추출 전 : ['formalize', 'allowance', 'electricical']\\n어간 추출 후 : ['formal', 'allow', 'electric']\\n※ Porter 알고리즘의 상세 규칙은 마틴 포터의 홈페이지에서 확인할 수 있다.\", \"어간 추출 후 : ['formal', 'allow', 'electric']\\n※ Porter 알고리즘의 상세 규칙은 마틴 포터의 홈페이지에서 확인할 수 있다.\\n어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택입니다. NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원합니다. 이번에는 포터 알고리즘과 랭커스터 스태머 알고리즘으로 각각 어간 추출을 진행했을 때, 이 둘의 결과를 비교해보겠습니다.\\nfrom nltk.stem import PorterStemmer\\nfrom nltk.stem import LancasterStemmer\\nporter_stemmer = PorterStemmer()\\nlancaster_stemmer = LancasterStemmer()\", \"porter_stemmer = PorterStemmer()\\nlancaster_stemmer = LancasterStemmer()\\nwords = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\\nprint('어간 추출 전 :', words)\\nprint('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\\nprint('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])\\n어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\", \"포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\\n랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\\n동일한 단어들의 나열에 대해서 두 스태머는 전혀 다른 결과를 보여줍니다. 두 스태머 알고리즘은 서로 다른 알고리즘을 사용하기 때문입니다. 그렇기 때문에 이미 알려진 알고리즘을 사용할 때는, 사용하고자 하는 코퍼스에 스태머를 적용해보고 어떤 스태머가 해당 코퍼스에 적합한지를 판단한 후에 사용하여야 합니다.\", '이런 규칙에 기반한 알고리즘은 종종 제대로 된 일반화를 수행하지 못 할 수 있습니다. 어간 추출을 하고나서 일반화가 지나치게 되거나, 또는 덜 되거나 하는 경우입니다. 예를 들어 포터 알고리즘에서 organization을 어간 추출했을 때의 결과를 봅시다.\\norganization → organ\\norganization과 organ은 완전히 다른 단어 임에도 organization에 대해서 어간 추출을 했더니 organ이라는 단어가 나왔습니다. organ에 대해서 어간 추출을 한다고 하더라도 결과는 역시 organ이 되기 때문에, 두 단어에 대해서 어간 추출을 한다면 동일한 어간을 갖게 됩니다. 이는 의미가 동일한 경우에만 같은 단어를 얻기를 원하는 정규화의 목적에는 맞지 않습니다. 마지막으로 동일한 단어에 대해서 표제어 추출과 어간 추출을 각각 수행했을 때, 결과에서 어떤 차이가 있는지 간단한 예를 보겠습니다.\\nStemming\\nam → am\\nthe going → the go', 'Stemming\\nam → am\\nthe going → the go\\nhaving → hav\\nLemmatization\\nam → be\\nthe going → the going\\nhaving → have']\n",
      "[\"한국어의 어간에 대해서 간략히 설명합니다. 한국어는 아래의 표와 같이 5언 9품사의 구조를 가지고 있습니다.\\n언\\n품사\\n체언\\n명사, 대명사, 수사\\n수식언\\n관형사, 부사\\n관계언\\n조사\\n독립언\\n감탄사\\n용언\\n동사, 형용사\\n이 중 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성됩니다. 앞으로 용언이라고 언급하는 부분은 전부 동사와 형용사를 포함하여 언급하는 개념입니다.\\n(1) 활용(conjugation)\\n활용(conjugation)은 한국어에서만 가지는 특징이 아니라, 인도유럽어(indo-european language)에서도 주로 볼 수 있는 언어적 특징 중 하나를 말하는 통칭적인 개념입니다. 다만, 여기서는 한국어에 한정하여 설명합니다.\\n활용이란 용언의 어간(stem)이 어미(ending)를 가지는 일을 말합니다.\", '활용이란 용언의 어간(stem)이 어미(ending)를 가지는 일을 말합니다.\\n어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라).\\n어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행\\n활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다면 규칙 활용, 어간이나 어미의 모습이 변하는 불규칙 활용으로 나뉩니다.\\n(2) 규칙 활용\\n규칙 활용은 어간이 어미를 취할 때, 어간의 모습이 일정합니다. 아래의 예제는 어간과 어미가 합쳐질 때, 어간의 형태가 바뀌지 않음을 보여줍니다.\\n잡/어간 + 다/어미\\n이 경우에는 어간이 어미가 붙기전의 모습과 어미가 붙은 후의 모습이 같으므로, 규칙 기반으로 어미를 단순히 분리해주면 어간 추출이 됩니다.\\n(3) 불규칙 활용', '이 경우에는 어간이 어미가 붙기전의 모습과 어미가 붙은 후의 모습이 같으므로, 규칙 기반으로 어미를 단순히 분리해주면 어간 추출이 됩니다.\\n(3) 불규칙 활용\\n불규칙 활용은 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우를 말합니다.\\n예를 들어 ‘듣-, 돕-, 곱-, 잇-, 오르-, 노랗-’ 등이 ‘듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’와 같이 어간의 형식이 달라지는 일이 있거나 ‘오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러’와 같이 일반적인 어미가 아닌 특수한 어미를 취하는 경우 불규칙활용을 하는 예에 속합니다.\\n이 경우에는 어간이 어미가 붙는 과정에서 어간의 모습이 바뀌었으므로 단순한 분리만으로 어간 추출이 되지 않고 좀 더 복잡한 규칙을 필요로 합니다. 아래의 링크는 다양한 불규칙 활용의 예를 보여줍니다.\\n링크 : https://namu.wiki/w/한국어/불규칙%20활용', \"링크 : https://namu.wiki/w/한국어/불규칙%20활용\\n==================================================\\n--- 02-04 불용어(Stopword) ---\\n```\\n불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\\n불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\", \"불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\\n```갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. 여기서 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말합니다. 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있습니다. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다.\", '물론 불용어는 개발자가 직접 정의할 수도 있습니다. 이번에는 영어 문장에서 NLTK가 정의한 영어 불용어를 제거하는 실습을 하고, 한국어 문장에서 직접 정의한 불용어를 제거해보겠습니다.\\nNLTK 실습에서는 1챕터에서 언급했듯이 NLTK Data가 필요합니다. 만약, 데이터가 없다는 에러가 발생 시에는 nltk.download(필요한 데이터)라는 커맨드를 통해 다운로드 할 수 있습니다.\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nfrom konlpy.tag import Okt']\n",
      "['stop_words_list = stopwords.words(\\'english\\')\\nprint(\\'불용어 개수 :\\', len(stop_words_list))\\nprint(\\'불용어 10개 출력 :\\',stop_words_list[:10])\\n불용어 개수 : 179\\n불용어 10개 출력 : [\\'i\\', \\'me\\', \\'my\\', \\'myself\\', \\'we\\', \\'our\\', \\'ours\\', \\'ourselves\\', \\'you\\', \"you\\'re\"]\\nstopwords.words(\"english\")는 NLTK가 정의한 영어 불용어 리스트를 리턴합니다. 출력 결과가 100개 이상이기 때문에 여기서는 간단히 10개만 확인해보았습니다. \\'i\\', \\'me\\', \\'my\\'와 같은 단어들을 NLTK에서 불용어로 정의하고 있음을 확인할 수 있습니다.']\n",
      "['example = \"Family is not an important thing. It\\'s everything.\"\\nstop_words = set(stopwords.words(\\'english\\'))\\nword_tokens = word_tokenize(example)\\nresult = []\\nfor word in word_tokens:\\nif word not in stop_words:\\nresult.append(word)\\nprint(\\'불용어 제거 전 :\\',word_tokens)\\nprint(\\'불용어 제거 후 :\\',result)\\n불용어 제거 전 : [\\'Family\\', \\'is\\', \\'not\\', \\'an\\', \\'important\\', \\'thing\\', \\'.\\', \\'It\\', \"\\'s\", \\'everything\\', \\'.\\']\\n불용어 제거 후 : [\\'Family\\', \\'important\\', \\'thing\\', \\'.\\', \\'It\\', \"\\'s\", \\'everything\\', \\'.\\']', '불용어 제거 후 : [\\'Family\\', \\'important\\', \\'thing\\', \\'.\\', \\'It\\', \"\\'s\", \\'everything\\', \\'.\\']\\n위 코드는 \"Family is not an important thing. It\\'s everything.\"라는 임의의 문장을 정의하고, NLTK의 word_tokenize를 통해서 단어 토큰화를 수행합니다. 그리고 단어 토큰화 결과로부터 NLTK가 정의하고 있는 불용어를 제외한 결과를 출력하고 있습니다. 결과적으로 \\'is\\', \\'not\\', \\'an\\'과 같은 단어들이 문장에서 제거되었음을 볼 수 있습니다.']\n",
      "['한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후에 조사, 접속사 등을 제거하는 방법이 있습니다. 하지만 불용어를 제거하려고 하다보면 조사나 접속사와 같은 단어들뿐만 아니라 명사, 형용사와 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 생기기도 합니다. 결국에는 사용자가 직접 불용어 사전을 만들게 되는 경우가 많습니다. 이번에는 직접 불용어를 정의해보고, 주어진 문장으로부터 사용자가 정의한 불용어 사전으로부터 불용어를 제거해보겠습니다. 아래의 불용어는 임의 선정한 것으로 실제 의미있는 선정 기준이 아닙니다.\\nokt = Okt()\\nexample = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\\nstop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\\nstop_words = set(stop_words.split(\\' \\'))', 'stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\\nstop_words = set(stop_words.split(\\' \\'))\\nword_tokens = okt.morphs(example)\\nresult = [word for word in word_tokens if not word in stop_words]\\nprint(\\'불용어 제거 전 :\\',word_tokens)\\nprint(\\'불용어 제거 후 :\\',result)\\n불용어 제거 전 : [\\'고기\\', \\'를\\', \\'아무렇게나\\', \\'구\\', \\'우려\\', \\'고\\', \\'하면\\', \\'안\\', \\'돼\\', \\'.\\', \\'고기\\', \\'라고\\', \\'다\\', \\'같은\\', \\'게\\', \\'아니거든\\', \\'.\\', \\'예컨대\\', \\'삼겹살\\', \\'을\\', \\'구울\\', \\'때\\', \\'는\\', \\'중요한\\', \\'게\\', \\'있지\\', \\'.\\']', \"불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\\n아래의 링크는 보편적으로 선택할 수 있는 한국어 불용어 리스트를 보여줍니다. 하지만 여전히 절대적인 기준은 아닙니다.\\n링크 : https://www.ranks.nl/stopwords/korean\\n불용어가 많은 경우에는 코드 내에서 직접 정의하지 않고 txt 파일이나 csv 파일로 정리해놓고 이를 불러와서 사용하기도 합니다.\\n==================================================\\n--- 02-05 정규 표현식(Regular Expression) ---\\n```\", '==================================================\\n--- 02-05 정규 표현식(Regular Expression) ---\\n```\\n[\\'Don\\', \\'t\\', \\'be\\', \\'fooled\\', \\'by\\', \\'the\\', \\'dark\\', \\'sounding\\', \\'name\\', \\'Mr\\', \\'Jone\\', \\'s\\', \\'Orphanage\\', \\'is\\', \\'as\\', \\'cheery\\', \\'as\\', \\'cheery\\', \\'goes\\', \\'for\\', \\'a\\', \\'pastry\\', \\'shop\\']\\n[\"Don\\'t\", \\'be\\', \\'fooled\\', \\'by\\', \\'the\\', \\'dark\\', \\'sounding\\', \\'name,\\', \\'Mr.\\', \"Jone\\'s\", \\'Orphanage\\', \\'is\\', \\'as\\', \\'cheery\\', \\'as\\', \\'cheery\\', \\'goes\\', \\'for\\', \\'a\\', \\'pastry\\', \\'shop\\']', '```텍스트 전처리에서 정규 표현식은 아주 유용한 도구입니다. 이번에는 파이썬에서 지원하고 있는 정규 표현식 모듈 re의 사용 방법과 NLTK를 통한 정규 표현식을 이용한 토큰화에 대해서 알아봅니다.']\n",
      "['파이썬에서는 정규 표현식 모듈 re을 지원하므로, 이를 이용하면 특정 규칙이 있는 텍스트 데이터를 빠르게 정제할 수 있습니다. 정규 표현식을 위해 사용되는 특수 문자와 모듈 함수에 대해서 표를 통해 정리해보겠습니다. 표만으로는 이해하기 어렵습니다. 표 아래의 실습과 표를 병행하여 이해하시는 것이 좋습니다.\\n1) 정규 표현식 문법\\n정규 표현식을 위해 사용되는 문법 중 특수 문자들은 아래와 같습니다.\\n특수 문자\\n설명\\n.\\n한 개의 임의의 문자를 나타냅니다. (줄바꿈 문자인 \\\\n는 제외)\\n?\\n앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있습니다. (문자가 0개 또는 1개)\\n*\\n앞의 문자가 무한개로 존재할 수도 있고, 존재하지 않을 수도 있습니다. (문자가 0개 이상)\\n+\\n앞의 문자가 최소 한 개 이상 존재합니다. (문자가 1개 이상)\\n^\\n뒤의 문자열로 문자열이 시작됩니다.\\n$\\n앞의 문자열로 문자열이 끝납니다.\\n{숫자}\\n숫자만큼 반복합니다.\\n{숫자1, 숫자2}', '^\\n뒤의 문자열로 문자열이 시작됩니다.\\n$\\n앞의 문자열로 문자열이 끝납니다.\\n{숫자}\\n숫자만큼 반복합니다.\\n{숫자1, 숫자2}\\n숫자1 이상 숫자2 이하만큼 반복합니다. ?, *, +를 이것으로 대체할 수 있습니다.\\n{숫자,}\\n숫자 이상만큼 반복합니다.\\n[ ]\\n대괄호 안의 문자들 중 한 개의 문자와 매치합니다. [amk]라고 한다면 a 또는 m 또는 k 중 하나라도 존재하면 매치를 의미합니다.  [a-z]와 같이 범위를 지정할 수도 있습니다. [a-zA-Z]는 알파벳 전체를 의미하는 범위이며, 문자열에 알파벳이 존재하면 매치를 의미합니다.\\n[^문자]\\n해당 문자를 제외한 문자를 매치합니다.\\nl\\nAlB와 같이 쓰이며 A 또는 B의 의미를 가집니다.\\n정규 표현식 문법에는 역 슬래쉬(\\\\)를 이용하여 자주 쓰이는 문자 규칙들이 있습니다.\\n문자 규칙\\n설명\\n\\\\\\\\\\\\\\n역 슬래쉬 문자 자체를 의미합니다\\n\\\\\\\\d\\n모든 숫자를 의미합니다. [0-9]와 의미가 동일합니다.\\n\\\\\\\\D', '문자 규칙\\n설명\\n\\\\\\\\\\\\\\n역 슬래쉬 문자 자체를 의미합니다\\n\\\\\\\\d\\n모든 숫자를 의미합니다. [0-9]와 의미가 동일합니다.\\n\\\\\\\\D\\n숫자를 제외한 모든 문자를 의미합니다. [^0-9]와 의미가 동일합니다.\\n\\\\\\\\s\\n공백을 의미합니다. [ \\\\t\\\\n\\\\r\\\\f\\\\v]와 의미가 동일합니다.\\n\\\\\\\\S\\n공백을 제외한 문자를 의미합니다. [^ \\\\t\\\\n\\\\r\\\\f\\\\v]와 의미가 동일합니다.\\n\\\\\\\\w\\n문자 또는 숫자를 의미합니다. [a-zA-Z0-9]와 의미가 동일합니다.\\n\\\\\\\\W\\n문자 또는 숫자가 아닌 문자를 의미합니다. [^a-zA-Z0-9]와 의미가 동일합니다.\\n2) 정규표현식 모듈 함수\\n정규표현식 모듈에서 지원하는 함수는 이와 같습니다.\\n모듈 함수\\n설명\\nre.compile()\\n정규표현식을 컴파일하는 함수입니다. 다시 말해, 파이썬에게 전해주는 역할을 합니다. 찾고자 하는 패턴이 빈번한 경우에는 미리 컴파일해놓고 사용하면 속도와 편의성면에서 유리합니다.\\nre.search()', 're.search()\\n문자열 전체에 대해서 정규표현식과 매치되는지를 검색합니다.\\nre.match()\\n문자열의 처음이 정규표현식과 매치되는지를 검색합니다.\\nre.split()\\n정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴합니다.\\nre.findall()\\n문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴합니다. 만약, 매치되는 문자열이 없다면 빈 리스트가 리턴됩니다.\\nre.finditer()\\n문자열에서 정규 표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체를 리턴합니다.\\nre.sub()\\n문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체합니다.', 're.sub()\\n문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체합니다.\\n앞으로 진행될 실습에서는 re.compile()에 정규 표현식을 컴파일하고, re.search()를 통해서 해당 정규 표현식이 입력 텍스트와 매치되는지를 확인하면서 각 정규 표현식에 대해서 이해해보겠습니다. re.search()는 매치된다면 Match Object를 리턴하고 매치되지 않으면 아무런 값도 출력되지 않습니다.']\n",
      "['앞서 표로 봤던 정규 표현식을 예시를 통해 이해해보겠습니다.\\nimport re\\n1) . 기호\\n.은 한 개의 임의의 문자를 나타냅니다. 예를 들어서 정규 표현식이 a.c라고 합시다. a와 c 사이에는 어떤 1개의 문자라도 올 수 있습니다. akc, azc, avc, a5c, a!c와 같은 형태는 모두 a.c의 정규 표현식과 매치됩니다.\\nr = re.compile(\"a.c\")\\nr.search(\"kkk\") # 아무런 결과도 출력되지 않는다.\\nr.search(\"abc\")\\n<_sre.SRE_Match object; span=(0, 3), match=\\'abc\\'>\\n.은 어떤 문자로도 인식될 수 있기 때문에 abc라는 문자열은 a.c라는 정규 표현식 패턴으로 매치됩니다.\\n2) ?기호', '.은 어떤 문자로도 인식될 수 있기 때문에 abc라는 문자열은 a.c라는 정규 표현식 패턴으로 매치됩니다.\\n2) ?기호\\n?는 ?앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있는 경우를 나타냅니다. 예를 들어서 정규 표현식이 ab?c라고 합시다. 이 경우 이 정규 표현식에서의 b는 있다고 취급할 수도 있고, 없다고 취급할 수도 있습니다. 즉, abc와 ac 모두 매치할 수 있습니다.\\nr = re.compile(\"ab?c\")\\nr.search(\"abbc\") # 아무런 결과도 출력되지 않는다.\\nr.search(\"abc\")\\n<_sre.SRE_Match object; span=(0, 3), match=\\'abc\\'>\\nb가 있는 것으로 판단하여 abc를 매치했습니다.\\nr.search(\"ac\")\\n<_sre.SRE_Match object; span=(0, 2), match=\\'ac\\'>\\nb가 없는 것으로 판단하여 ac를 매치했습니다.\\n3) *기호', 'r.search(\"ac\")\\n<_sre.SRE_Match object; span=(0, 2), match=\\'ac\\'>\\nb가 없는 것으로 판단하여 ac를 매치했습니다.\\n3) *기호\\n*은 바로 앞의 문자가 0개 이상일 경우를 나타냅니다. 앞의 문자는 존재하지 않을 수도 있으며, 또는 여러 개일 수도 있습니다. 정규 표현식이 ab*c라면 ac, abc, abbc, abbbc 등과 매치할 수 있으며 b의 개수는 무수히 많을 수 있습니다.\\nr = re.compile(\"ab*c\")\\nr.search(\"a\") # 아무런 결과도 출력되지 않는다.\\nr.search(\"ac\")\\n<_sre.SRE_Match object; span=(0, 2), match=\\'ac\\'>\\nr.search(\"abc\")\\n<_sre.SRE_Match object; span=(0, 3), match=\\'abc\\'>\\nr.search(\"abbbbc\")\\n<_sre.SRE_Match object; span=(0, 6), match=\\'abbbbc\\'>', 'r.search(\"abbbbc\")\\n<_sre.SRE_Match object; span=(0, 6), match=\\'abbbbc\\'>\\n4) +기호\\n+는 *와 유사합니다. 다른 점은 앞의 문자가 최소 1개 이상이어야 합니다. 정규 표현식이 ab+c라고 한다면 ac는 매치되지 않습니다. 하지만 abc, abbc, abbbc 등과 매치할 수 있으며 b의 개수는 무수히 많을 수 있습니다.\\nr = re.compile(\"ab+c\")\\nr.search(\"ac\") # 아무런 결과도 출력되지 않는다.\\nr.search(\"abc\")\\n<_sre.SRE_Match object; span=(0, 3), match=\\'abc\\'>\\nr.search(\"abbbbc\")\\n<_sre.SRE_Match object; span=(0, 6), match=\\'abbbbc\\'>\\n5) ^기호\\n^는 시작되는 문자열을 지정합니다. 정규표현식이 ^ab라면 문자열 ab로 시작되는 경우 매치합니다.\\nr = re.compile(\"^ab\")', '5) ^기호\\n^는 시작되는 문자열을 지정합니다. 정규표현식이 ^ab라면 문자열 ab로 시작되는 경우 매치합니다.\\nr = re.compile(\"^ab\")\\n# 아무런 결과도 출력되지 않는다.\\nr.search(\"bbc\")\\nr.search(\"zab\")\\nr.search(\"abz\")\\n<re.Match object; span=(0, 2), match=\\'ab\\'>\\n6) {숫자} 기호\\n문자에 해당 기호를 붙이면, 해당 문자를 숫자만큼 반복한 것을 나타냅니다. 예를 들어서 정규 표현식이 ab{2}c라면 a와 c 사이에 b가 존재하면서 b가 2개인 문자열에 대해서 매치합니다.\\nr = re.compile(\"ab{2}c\")\\n# 아무런 결과도 출력되지 않는다.\\nr.search(\"ac\")\\nr.search(\"abc\")\\nr.search(\"abbbbbc\")\\nr.search(\"abbc\")\\n<_sre.SRE_Match object; span=(0, 4), match=\\'abbc\\'>\\n7) {숫자1, 숫자2} 기호', 'r.search(\"abbc\")\\n<_sre.SRE_Match object; span=(0, 4), match=\\'abbc\\'>\\n7) {숫자1, 숫자2} 기호\\n문자에 해당 기호를 붙이면, 해당 문자를 숫자1 이상 숫자2 이하만큼 반복합니다. 예를 들어서 정규 표현식이 ab{2,8}c라면 a와 c 사이에 b가 존재하면서 b는 2개 이상 8개 이하인 문자열에 대해서 매치합니다.\\nr = re.compile(\"ab{2,8}c\")\\n# 아무런 결과도 출력되지 않는다.\\nr.search(\"ac\")\\nr.search(\"abc\")\\nr.search(\"abbbbbbbbbc\")\\nr.search(\"abbc\")\\n<_sre.SRE_Match object; span=(0, 4), match=\\'abbc\\'>\\nr.search(\"abbbbbbbbc\")\\n<_sre.SRE_Match object; span=(0, 10), match=\\'abbbbbbbbc\\'>\\n8) {숫자,} 기호', 'r.search(\"abbbbbbbbc\")\\n<_sre.SRE_Match object; span=(0, 10), match=\\'abbbbbbbbc\\'>\\n8) {숫자,} 기호\\n문자에 해당 기호를 붙이면 해당 문자를 숫자 이상 만큼 반복합니다. 예를 들어서 정규 표현식이 a{2,}bc라면 뒤에 bc가 붙으면서 a의 개수가 2개 이상인 경우인 문자열과 매치합니다. 또한 만약 {0,}을 쓴다면 *와 동일한 의미가 되며, {1,}을 쓴다면 +와 동일한 의미가 됩니다.\\nr = re.compile(\"a{2,}bc\")\\n# 아무런 결과도 출력되지 않는다.\\nr.search(\"bc\")\\nr.search(\"aa\")\\nr.search(\"aabc\")\\n<_sre.SRE_Match object; span=(0, 4), match=\\'aabc\\'>\\nr.search(\"aaaaaaaabc\")\\n<_sre.SRE_Match object; span=(0, 10), match=\\'aaaaaaaabc\\'>\\n9) [ ] 기호', 'r.search(\"aaaaaaaabc\")\\n<_sre.SRE_Match object; span=(0, 10), match=\\'aaaaaaaabc\\'>\\n9) [ ] 기호\\n[ ]안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치라는 의미를 가집니다. 예를 들어서 정규 표현식이 [abc]라면, a 또는 b또는 c가 들어가있는 문자열과 매치됩니다. 범위를 지정하는 것도 가능합니다. [a-zA-Z]는 알파벳 전부를 의미하며, [0-9]는 숫자 전부를 의미합니다.\\nr = re.compile(\"[abc]\") # [abc]는 [a-c]와 같다.\\nr.search(\"zzz\") # 아무런 결과도 출력되지 않는다.\\nr.search(\"a\")\\n<_sre.SRE_Match object; span=(0, 1), match=\\'a\\'>\\nr.search(\"aaaaaaa\")\\n<_sre.SRE_Match object; span=(0, 1), match=\\'a\\'>\\nr.search(\"baac\")', 'r.search(\"aaaaaaa\")\\n<_sre.SRE_Match object; span=(0, 1), match=\\'a\\'>\\nr.search(\"baac\")\\n<_sre.SRE_Match object; span=(0, 1), match=\\'b\\'>\\n이번에는 알파벳 소문자에 대해서 범위 지정하여 정규 표현식을 만들어보고 문자열과 매치해보겠습니다.\\nr = re.compile(\"[a-z]\")\\n# 아무런 결과도 출력되지 않는다.\\nr.search(\"AAA\")\\nr.search(\"111\")\\nr.search(\"aBC\")\\n<_sre.SRE_Match object; span=(0, 1), match=\\'a\\'>\\n10) [^문자] 기호\\n[^문자]는 ^기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치하는 역할을 합니다. 예를 들어서 [^abc]라는 정규 표현식이 있다면, a 또는 b 또는 c가 들어간 문자열을 제외한 모든 문자열을 매치합니다.\\nr = re.compile(\"[^abc]\")', 'r = re.compile(\"[^abc]\")\\n# 아무런 결과도 출력되지 않는다.\\nr.search(\"a\")\\nr.search(\"ab\")\\nr.search(\"b\")\\nr.search(\"d\")\\n<_sre.SRE_Match object; span=(0, 1), match=\\'d\\'>\\nr.search(\"1\")\\n<_sre.SRE_Match object; span=(0, 1), match=\\'1\\'>']\n",
      "['앞서 re.compile()과 re.search()를 사용해보았습니다. 다른 정규 표현식 모듈 함수에 대해서 실습을 진행해보겠습니다.\\n(1) re.match() 와 re.search()의 차이\\nsearch()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, match()는 문자열의 첫 부분부터 정규 표현식과 매치하는지를 확인합니다. 문자열 중간에 찾을 패턴이 있더라도 match 함수는 문자열의 시작에서 패턴이 일치하지 않으면 찾지 않습니다.\\nr = re.compile(\"ab.\")\\nr.match(\"kkkabc\") # 아무런 결과도 출력되지 않는다.\\nr.search(\"kkkabc\")\\n<_sre.SRE_Match object; span=(3, 6), match=\\'abc\\'>\\nr.match(\"abckkk\")\\n<_sre.SRE_Match object; span=(0, 3), match=\\'abc\\'>', 'r.match(\"abckkk\")\\n<_sre.SRE_Match object; span=(0, 3), match=\\'abc\\'>\\n위 경우 정규 표현식이 ab. 이기때문에, ab 다음에는 어떤 한 글자가 존재할 수 있다는 패턴을 의미합니다. search 모듈 함수에 kkkabc라는 문자열을 넣어 매치되는지 확인한다면 abc라는 문자열에서 매치되어 Match object를 리턴합니다. 하지만 match 모듈 함수의 경우 앞 부분이 ab.와 매치되지 않기때문에, 아무런 결과도 출력되지 않습니다. 하지만 반대로 abckkk로 매치를 시도해보면, 시작 부분에서 패턴과 매치되었기 때문에 정상적으로 Match object를 리턴합니다.\\n(2) re.split()\\nsplit() 함수는 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴합니다. 토큰화에 유용하게 쓰일 수 있습니다. 공백을 기준으로 문자열 분리를 수행하고 결과로서 리스트를 리턴해봅시다.\\n# 공백 기준 분리', '# 공백 기준 분리\\ntext = \"사과 딸기 수박 메론 바나나\"\\nre.split(\" \", text)\\n[\\'사과\\', \\'딸기\\', \\'수박\\', \\'메론\\', \\'바나나\\']\\n이와 유사하게 줄바꿈이나 다른 정규 표현식을 기준으로 텍스트를 분리할 수도 있습니다.\\n# 줄바꿈 기준 분리\\ntext = \"\"\"사과\\n딸기\\n수박\\n메론\\n바나나\"\"\"\\nre.split(\"\\\\n\", text)\\n[\\'사과\\', \\'딸기\\', \\'수박\\', \\'메론\\', \\'바나나\\']\\n# \\'+\\'를 기준으로 분리\\ntext = \"사과+딸기+수박+메론+바나나\"\\nre.split(\"\\\\+\", text)\\n[\\'사과\\', \\'딸기\\', \\'수박\\', \\'메론\\', \\'바나나\\']\\n(3) re.findall()\\nfindall() 함수는 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴합니다. 단, 매치되는 문자열이 없다면 빈 리스트를 리턴합니다. 임의의 텍스트에 정규 표현식으로 숫자를 의미하는 규칙으로 findall()을 수행하면 전체 텍스트로부터 숫자만 찾아내서 리스트로 리턴합니다.', 'text = \"\"\"이름 : 김철수\\n전화번호 : 010 - 1234 - 1234\\n나이 : 30\\n성별 : 남\"\"\"\\nre.findall(\"\\\\d+\", text)\\n[\\'010\\', \\'1234\\', \\'1234\\', \\'30\\']\\n하지만 만약 입력 텍스트에 숫자가 없다면 빈 리스트를 리턴하게 됩니다.\\nre.findall(\"\\\\d+\", \"문자열입니다.\")\\n[]\\n(4) re.sub()\\nsub() 함수는 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체할 수 있습니다. 아래와 같은 정제 작업에 많이 사용되는데, 영어 문장에 각주 등과 같은 이유로 특수 문자가 섞여있는 경우에 특수 문자를 제거하고 싶다면 알파벳 외의 문자는 공백으로 처리하는 등의 용도로 쓸 수 있습니다.', 'text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\\npreprocessed_text = re.sub(\\'[^a-zA-Z]\\', \\' \\', text)\\nprint(preprocessed_text)', \"preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)\\nprint(preprocessed_text)\\n'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '\"]\n",
      "['다음과 같은 임의의 텍스트가 있다고 해봅시다. 테이블 형식의 데이터를 텍스트에 저장하였습니다.\\ntext = \"\"\"100 John    PROF\\n101 James   STUD\\n102 Mac   STUD\"\"\"\\n\\\\s+는 공백을 찾아내는 정규표현식입니다. 뒤에 붙는 +는 최소 1개 이상의 패턴을 찾아낸다는 의미입니다. s는 공백을 의미하기 때문에 최소 1개 이상의 공백인 패턴을 찾아냅니다. split은 주어진 정규표현식을 기준으로 분리하므로 결과는 아래와 같습니다.\\nre.split(\\'\\\\s+\\', text)\\n[\\'100\\', \\'John\\', \\'PROF\\', \\'101\\', \\'James\\', \\'STUD\\', \\'102\\', \\'Mac\\', \\'STUD\\']\\n공백을 기준으로 값이 구분되었습니다. 해당 입력으로부터 숫자만을 뽑아온다고 해봅시다. 여기서 \\\\d는 숫자에 해당되는 정규표현식입니다. +를 붙이면 최소 1개 이상의 숫자에 해당하는 값을 의미합니다. findall()은 해당 표현식에 일치하는 값을 찾아냅니다.', \"re.findall('\\\\d+',text)\\n['100', '101', '102]\\n이번에는 텍스트로부터 대문자인 행의 값만 가져와봅시다. 이 경우 정규 표현식에 대문자를 기준으로 매치시키면 됩니다. 하지만 정규 표현식에 대문자라는 기준만을 넣을 경우에는 문자열을 가져오는 것이 아니라 모든 대문자 각각을 갖고오게 됩니다.\\nre.findall('[A-Z]',text)\\n['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']\\n대문자가 연속적으로 네 번 등장하는 경우라는 조건을 추가해봅시다.\\nre.findall('[A-Z]{4}',text)\\n['PROF', 'STUD', 'STUD']\\n대문자로 구성된 문자열들을 가져옵니다. 이름의 경우에는 대문자와 소문자가 섞여있는 상황입니다. 이름에 대한 행의 값을 갖고오고 싶다면 처음에 대문자가 등장한 후에 소문자가 여러번 등장하는 경우에 매치하게 합니다.\", \"re.findall('[A-Z][a-z]+',text)\\n['John', 'James', 'Mac']\"]\n",
      "['NLTK에서는 정규 표현식을 사용해서 단어 토큰화를 수행하는 RegexpTokenizer를 지원합니다. RegexpTokenizer()에서 괄호 안에 하나의 토큰으로 규정하기를 원하는 정규 표현식을 넣어서 토큰화를 수행합니다. tokenizer1에 사용한 \\\\w+는 문자 또는 숫자가 1개 이상인 경우를 의미합니다.\\ntokenizer2에서는 공백을 기준으로 토큰화하도록 했습니다. gaps=true는 해당 정규 표현식을 토큰으로 나누기 위한 기준으로 사용한다는 의미입니다. 만약 gaps=True라는 부분을 기재하지 않는다면, 토큰화의 결과는 공백들만 나오게 됩니다. tokenizer2의 결과는 위의 tokenizer1의 결과와는 달리 아포스트로피나 온점을 제외하지 않고 토큰화가 수행된 것을 확인할 수 있습니다.\\nfrom nltk.tokenize import RegexpTokenizer', 'from nltk.tokenize import RegexpTokenizer\\ntext = \"Don\\'t be fooled by the dark sounding name, Mr. Jone\\'s Orphanage is as cheery as cheery goes for a pastry shop\"\\ntokenizer1 = RegexpTokenizer(\"[\\\\w]+\")\\ntokenizer2 = RegexpTokenizer(\"\\\\s+\", gaps=True)\\nprint(tokenizer1.tokenize(text))\\nprint(tokenizer2.tokenize(text))\\n[\\'Don\\', \\'t\\', \\'be\\', \\'fooled\\', \\'by\\', \\'the\\', \\'dark\\', \\'sounding\\', \\'name\\', \\'Mr\\', \\'Jone\\', \\'s\\', \\'Orphanage\\', \\'is\\', \\'as\\', \\'cheery\\', \\'as\\', \\'cheery\\', \\'goes\\', \\'for\\', \\'a\\', \\'pastry\\', \\'shop\\']', '[\"Don\\'t\", \\'be\\', \\'fooled\\', \\'by\\', \\'the\\', \\'dark\\', \\'sounding\\', \\'name,\\', \\'Mr.\\', \"Jone\\'s\", \\'Orphanage\\', \\'is\\', \\'as\\', \\'cheery\\', \\'as\\', \\'cheery\\', \\'goes\\', \\'for\\', \\'a\\', \\'pastry\\', \\'shop\\']\\n==================================================\\n--- 02-06 정수 인코딩(Integer Encoding) ---\\n```\\n[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]', '```컴퓨터는 텍스트보다는 숫자를 더 잘 처리 할 수 있습니다. 이를 위해 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다. 그리고 그러한 기법들을 본격적으로 적용시키기 위한 첫 단계로 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업이 필요할 때가 있습니다.\\n예를 들어 갖고 있는 텍스트에 단어가 5,000개가 있다면, 5,000개의 단어들 각각에 1번부터 5,000번까지 단어와 맵핑되는 고유한 정수. 다른 표현으로는 인덱스를 부여합니다. 가령, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 숫자가 부여됩니다. 인덱스를 부여하는 방법은 여러 가지가 있을 수 있는데 랜덤으로 부여하기도 하지만, 보통은 단어 등장 빈도수를 기준으로 정렬한 뒤에 부여합니다.']\n",
      "['왜 이러한 작업이 필요한 지에 대해서는 뒤에서 원-핫 인코딩 실습이나, 워드 임베딩 챕터 등에서 알아보기로 하고 여기서는 어떤 과정으로 단어에 정수 인덱스를 부여하는지에 대해서만 정리하겠습니다.\\n단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법이 있습니다. 이해를 돕기위해 단어의 빈도수가 적당하게 분포되도록 의도적으로 만든 텍스트 데이터를 가지고 실습해보겠습니다.\\n1) dictionary 사용하기\\nfrom nltk.tokenize import sent_tokenize\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords', 'from nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\nraw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\\n우선 여러 문장이 함께 있는 텍스트 데이터로부터 문장 토큰화를 수행해보겠습니다.\\n# 문장 토큰화', \"우선 여러 문장이 함께 있는 텍스트 데이터로부터 문장 토큰화를 수행해보겠습니다.\\n# 문장 토큰화\\nsentences = sent_tokenize(raw_text)\\nprint(sentences)\\n['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\", \"기존의 텍스트 데이터가 문장 단위로 토큰화 된 것을 확인할 수 있습니다. 이제 정제 작업과 정규화 작업을 병행하며, 단어 토큰화를 수행합니다. 여기서는 단어들을 소문자화하여 단어의 개수를 통일시키고, 불용어와 단어 길이가 2이하인 경우에 대해서 단어를 일부 제외시켜주었습니다. 텍스트를 수치화하는 단계라는 것은 본격적으로 자연어 처리 작업에 들어간다는 의미이므로, 단어가 텍스트일 때만 할 수 있는 최대한의 전처리를 끝내놓아야 합니다.\\nvocab = {}\\npreprocessed_sentences = []\\nstop_words = set(stopwords.words('english'))\\nfor sentence in sentences:\\n# 단어 토큰화\\ntokenized_sentence = word_tokenize(sentence)\\nresult = []\\nfor word in tokenized_sentence:\", '# 단어 토큰화\\ntokenized_sentence = word_tokenize(sentence)\\nresult = []\\nfor word in tokenized_sentence:\\nword = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\\nif word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\\nif len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\\nresult.append(word)\\nif word not in vocab:\\nvocab[word] = 0\\nvocab[word] += 1\\npreprocessed_sentences.append(result)\\nprint(preprocessed_sentences)', \"vocab[word] += 1\\npreprocessed_sentences.append(result)\\nprint(preprocessed_sentences)\\n[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\\n현재 vocab에는 각 단어에 대한 빈도수가 기록되어져 있습니다. vocab을 출력해보겠습니다.\", '현재 vocab에는 각 단어에 대한 빈도수가 기록되어져 있습니다. vocab을 출력해보겠습니다.\\nprint(\\'단어 집합 :\\',vocab)\\n단어 집합 : {\\'barber\\': 8, \\'person\\': 3, \\'good\\': 1, \\'huge\\': 5, \\'knew\\': 1, \\'secret\\': 6, \\'kept\\': 4, \\'word\\': 2, \\'keeping\\': 2, \\'driving\\': 1, \\'crazy\\': 1, \\'went\\': 1, \\'mountain\\': 1}\\n파이썬의 딕셔너리 구조로 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴합니다.\\n# \\'barber\\'라는 단어의 빈도수 출력\\nprint(vocab[\"barber\"])\\n8\\n이제 빈도수가 높은 순서대로 정렬해보겠습니다.\\nvocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)', \"이제 빈도수가 높은 순서대로 정렬해보겠습니다.\\nvocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\\nprint(vocab_sorted)\\n[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\\n높은 빈도수를 가진 단어일수록 낮은 정수를 부여합니다. 정수는 1부터 부여합니다.\\nword_to_index = {}\\ni = 0\\nfor (word, frequency) in vocab_sorted :\\nif frequency > 1 : # 빈도수가 작은 단어는 제외.\\ni = i + 1\\nword_to_index[word] = i\", \"if frequency > 1 : # 빈도수가 작은 단어는 제외.\\ni = i + 1\\nword_to_index[word] = i\\nprint(word_to_index)\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\\n1의 인덱스를 가진 단어가 가장 빈도수가 높은 단어가 됩니다. 그리고 이러한 작업을 수행하는 동시에 각 단어의 빈도수를 알 경우에만 할 수 있는 전처리인 빈도수가 적은 단어를 제외시키는 작업을 수행했습니다. 등장 빈도가 낮은 단어는 자연어 처리에서 의미를 가지지 않을 가능성이 높기 때문입니다. 여기서는 빈도수가 1인 단어들은 전부 제외시켰습니다.\", \"자연어 처리를 하다보면, 텍스트 데이터에 있는 단어를 모두 사용하기 보다는 빈도수가 가장 높은 n개의 단어만 사용하고 싶은 경우가 많습니다. 위 단어들은 빈도수가 높은 순으로 낮은 정수가 부여되어져 있으므로 빈도수 상위 n개의 단어만 사용하고 싶다고하면 vocab에서 정수값이 1부터 n까지인 단어들만 사용하면 됩니다. 여기서는 상위 5개 단어만 사용한다고 가정하겠습니다.\\nvocab_size = 5\\n# 인덱스가 5 초과인 단어 제거\\nwords_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\\n# 해당 단어에 대한 인덱스 정보를 삭제\\nfor w in words_frequency:\\ndel word_to_index[w]\\nprint(word_to_index)\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\", \"print(word_to_index)\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\\nword_to_index에는 빈도수가 높은 상위 5개의 단어만 저장되었습니다. word_to_index를 사용하여 단어 토큰화가 된 상태로 저장된 sentences에 있는 각 단어를 정수로 바꾸는 작업을 하겠습니다.\\n예를 들어 sentences에서 첫번째 문장은 ['barber', 'person']이었는데, 이 문장에 대해서는 [1, 5]로 인코딩합니다. 그런데 두번째 문장인 ['barber', 'good', 'person']에는 더 이상 word_to_index에는 존재하지 않는 단어인 'good'이라는 단어가 있습니다.\", \"이처럼 단어 집합에 존재하지 않는 단어들이 생기는 상황을 Out-Of-Vocabulary(단어 집합에 없는 단어) 문제라고 합니다. 약자로 'OOV 문제'라고도 합니다. word_to_index에 'OOV'란 단어를 새롭게 추가하고, 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩하겠습니다.\\nword_to_index['OOV'] = len(word_to_index) + 1\\nprint(word_to_index)\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\\n이제 word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩하겠습니다.\\nencoded_sentences = []\\nfor sentence in preprocessed_sentences:\\nencoded_sentence = []\\nfor word in sentence:\\ntry:\", \"for sentence in preprocessed_sentences:\\nencoded_sentence = []\\nfor word in sentence:\\ntry:\\n# 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\\nencoded_sentence.append(word_to_index[word])\\nexcept KeyError:\\n# 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\\nencoded_sentence.append(word_to_index['OOV'])\\nencoded_sentences.append(encoded_sentence)\\nprint(encoded_sentences)\\n[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\", '지금까지 파이썬의 dictionary 자료형으로 정수 인코딩을 진행해보았습니다. 그런데 이보다는 좀 더 쉽게 하기 위해서 Counter, FreqDist, enumerate를 사용하거나, 케라스 토크나이저를 사용하는 것을 권장합니다.\\n2) Counter 사용하기\\nfrom collections import Counter\\nprint(preprocessed_sentences)', \"2) Counter 사용하기\\nfrom collections import Counter\\nprint(preprocessed_sentences)\\n[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\", '현재 sentences는 단어 토큰화가 된 결과가 저장되어져 있습니다. 단어 집합(vocabulary)을 만들기 위해서  sentences에서 문장의 경계인 [, ]를 제거하고 단어들을 하나의 리스트로 만들겠습니다.\\n# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\\nall_words_list = sum(preprocessed_sentences, [])\\nprint(all_words_list)', \"all_words_list = sum(preprocessed_sentences, [])\\nprint(all_words_list)\\n['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\\n이를 파이썬의 Counter()의 입력으로 사용하면 중복을 제거하고 단어의 빈도수를 기록합니다.\\n# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\", '이를 파이썬의 Counter()의 입력으로 사용하면 중복을 제거하고 단어의 빈도수를 기록합니다.\\n# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\\nvocab = Counter(all_words_list)\\nprint(vocab)\\nCounter({\\'barber\\': 8, \\'secret\\': 6, \\'huge\\': 5, \\'kept\\': 4, \\'person\\': 3, \\'word\\': 2, \\'keeping\\': 2, \\'good\\': 1, \\'knew\\': 1, \\'driving\\': 1, \\'crazy\\': 1, \\'went\\': 1, \\'mountain\\': 1})\\n단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴합니다.\\nprint(vocab[\"barber\"]) # \\'barber\\'라는 단어의 빈도수 출력\\n8', 'print(vocab[\"barber\"]) # \\'barber\\'라는 단어의 빈도수 출력\\n8\\nbarber란 단어가 총 8번 등장하였습니다. most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴합니다. 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼만 얻을 수 있습니다. 등장 빈도수 상위 5개의 단어만 단어 집합으로 저장해봅시다.\\nvocab_size = 5\\nvocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\\nvocab\\n[(\\'barber\\', 8), (\\'secret\\', 6), (\\'huge\\', 5), (\\'kept\\', 4), (\\'person\\', 3)]\\n이제 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여합니다.\\nword_to_index = {}\\ni = 0\\nfor (word, frequency) in vocab :\\ni = i + 1\\nword_to_index[word] = i', \"word_to_index = {}\\ni = 0\\nfor (word, frequency) in vocab :\\ni = i + 1\\nword_to_index[word] = i\\nprint(word_to_index)\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\\n3) NLTK의 FreqDist 사용하기\\nNLTK에서는 빈도수 계산 도구인 FreqDist()를 지원합니다. 위에서 사용한 Counter()랑 같은 방법으로 사용할 수 있습니다.\\nfrom nltk import FreqDist\\nimport numpy as np\\n# np.hstack으로 문장 구분을 제거\\nvocab = FreqDist(np.hstack(preprocessed_sentences))\\n단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴합니다.\", '단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴합니다.\\nprint(vocab[\"barber\"]) # \\'barber\\'라는 단어의 빈도수 출력\\n8\\nbarber란 단어가 총 8번 등장하였습니다. most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴합니다. 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼만 얻을 수 있습니다. 등장 빈도수 상위 5개의 단어만 단어 집합으로 저장해봅시다.\\nvocab_size = 5\\nvocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\\nprint(vocab)\\n[(\\'barber\\', 8), (\\'secret\\', 6), (\\'huge\\', 5), (\\'kept\\', 4), (\\'person\\', 3)]', \"print(vocab)\\n[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\\n앞서 Counter()를 사용했을 때와 결과가 같습니다. 이전 실습들과 마찬가지로 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여합니다. 그런데 이번에는 enumerate()를 사용하여 좀 더 짧은 코드로 인덱스를 부여하겠습니다.\\nword_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\\nprint(word_to_index)\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\\n위와 같이 인덱스를 부여할 때는 enumerate()를 사용하는 것이 편리합니다. enumerate()에 대해서 간단히 소개해보겠습니다.\\n4) enumerate 이해하기\", '위와 같이 인덱스를 부여할 때는 enumerate()를 사용하는 것이 편리합니다. enumerate()에 대해서 간단히 소개해보겠습니다.\\n4) enumerate 이해하기\\nenumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴한다는 특징이 있습니다. 간단한 예제를 통해 enumerate()를 이해해봅시다.\\ntest_input = [\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\']\\nfor index, value in enumerate(test_input): # 입력의 순서대로 0부터 인덱스를 부여함.\\nprint(\"value : {}, index: {}\".format(value, index))\\nvalue : a, index: 0\\nvalue : b, index: 1\\nvalue : c, index: 2\\nvalue : d, index: 3\\nvalue : e, index: 4', 'value : a, index: 0\\nvalue : b, index: 1\\nvalue : c, index: 2\\nvalue : d, index: 3\\nvalue : e, index: 4\\n위의 출력 결과는 리스트의 모든 토큰에 대해서 인덱스가 순차적으로 증가되며 부여된 것을 보여줍니다.']\n",
      "['케라스(Keras)는 기본적인 전처리를 위한 도구들을 제공합니다. 때로는 정수 인코딩을 위해서 케라스의 전처리 도구인 토크나이저를 사용하기도 하는데, 사용 방법과 그 특징에 대해서 이해해보겠습니다.\\nfrom tensorflow.keras.preprocessing.text import Tokenizer', \"from tensorflow.keras.preprocessing.text import Tokenizer\\npreprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\\n단어 토큰화까지 수행된 앞서 사용한 텍스트 데이터와 동일한 데이터를 사용합니다.\", '단어 토큰화까지 수행된 앞서 사용한 텍스트 데이터와 동일한 데이터를 사용합니다.\\ntokenizer = Tokenizer()\\n# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\\ntokenizer.fit_on_texts(preprocessed_sentences)\\nfit_on_texts는 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여하는데, 정확히 앞서 설명한 정수 인코딩 작업이 이루어진다고 보면됩니다. 각 단어에 인덱스가 어떻게 부여되었는지를 보려면, word_index를 사용합니다.\\nprint(tokenizer.word_index)', \"print(tokenizer.word_index)\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\\n각 단어의 빈도수가 높은 순서대로 인덱스가 부여된 것을 확인할 수 있습니다. 각 단어가 카운트를 수행하였을 때 몇 개였는지를 보고자 한다면 word_counts를 사용합니다.\\nprint(tokenizer.word_counts)\", \"print(tokenizer.word_counts)\\nOrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\\ntexts_to_sequences()는 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환합니다.\\nprint(tokenizer.texts_to_sequences(preprocessed_sentences))\\n[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\", '앞서 빈도수가 가장 높은 단어 n개만을 사용하기 위해서 most_common()을 사용했었습니다. 케라스 토크나이저에서는 tokenizer = Tokenizer(num_words=숫자)와 같은 방법으로 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정할 수 있습니다. 여기서는 1번 단어부터 5번 단어까지만 사용하겠습니다. 상위 5개 단어를 사용한다고 토크나이저를 재정의 해보겠습니다.\\nvocab_size = 5\\ntokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\\ntokenizer.fit_on_texts(preprocessed_sentences)', 'tokenizer.fit_on_texts(preprocessed_sentences)\\nnum_words에서 +1을 더해서 값을 넣어주는 이유는 num_words는 숫자를 0부터 카운트합니다. 만약 5를 넣으면 0 ~ 4번 단어 보존을 의미하게 되므로 뒤의 실습에서 1번 단어부터 4번 단어만 남게됩니다. 그렇기 때문에 1 ~ 5번 단어까지 사용하고 싶다면 num_words에 숫자 5를 넣어주는 것이 아니라 5+1인 값을 넣어주어야 합니다.\\n실질적으로 숫자 0에 지정된 단어가 존재하지 않는데도 케라스 토크나이저가 숫자 0까지 단어 집합의 크기로 산정하는 이유는 자연어 처리에서 패딩(padding)이라는 작업 때문입니다. 이에 대해서는 뒤에 다루게 되므로 여기서는 케라스 토크나이저를 사용할 때는 숫자 0도 단어 집합의 크기로 고려해야한다고만 이해합시다.\\n다시 word_index를 확인해보겠습니다.\\nprint(tokenizer.word_index)', \"다시 word_index를 확인해보겠습니다.\\nprint(tokenizer.word_index)\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\\n상위 5개의 단어만 사용하겠다고 선언하였는데 여전히 13개의 단어가 모두 출력됩니다. word_counts를 확인해보겠습니다.\\nprint(tokenizer.word_counts)\", \"상위 5개의 단어만 사용하겠다고 선언하였는데 여전히 13개의 단어가 모두 출력됩니다. word_counts를 확인해보겠습니다.\\nprint(tokenizer.word_counts)\\nOrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\\nword_counts에서도 마찬가지로 13개의 단어가 모두 출력됩니다. 사실 실제 적용은 texts_to_sequences를 사용할 때 적용이 됩니다.\\nprint(tokenizer.texts_to_sequences(preprocessed_sentences))\", 'print(tokenizer.texts_to_sequences(preprocessed_sentences))\\n[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\\n코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환하는데, 상위 5개의 단어만을 사용하겠다고 지정하였으므로 1번 단어부터 5번 단어까지만 보존되고 나머지 단어들은 제거된 것을 볼 수 있습니다. 경험상 굳이 필요하다고 생각하지는 않지만, 만약 word_index와 word_counts에서도 지정된 num_words만큼의 단어만 남기고 싶다면 아래의 코드도 방법입니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(preprocessed_sentences)\\nvocab_size = 5', 'tokenizer = Tokenizer()\\ntokenizer.fit_on_texts(preprocessed_sentences)\\nvocab_size = 5\\nwords_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1]\\n# 인덱스가 5 초과인 단어 제거\\nfor word in words_frequency:\\ndel tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\\ndel tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\\nprint(tokenizer.word_index)\\nprint(tokenizer.word_counts)\\nprint(tokenizer.texts_to_sequences(preprocessed_sentences))', \"print(tokenizer.word_counts)\\nprint(tokenizer.texts_to_sequences(preprocessed_sentences))\\n{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\\nOrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\\n[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\\n케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거한다는 특징이 있습니다. 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용합니다.\", \"# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\\nvocab_size = 5\\ntokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\\ntokenizer.fit_on_texts(preprocessed_sentences)\\n만약 oov_token을 사용하기로 했다면 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 합니다.\\nprint('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))\\n단어 OOV의 인덱스 : 1\\n이제 코퍼스에 대해서 정수 인코딩을 진행합니다.\\nprint(tokenizer.texts_to_sequences(preprocessed_sentences))\", \"이제 코퍼스에 대해서 정수 인코딩을 진행합니다.\\nprint(tokenizer.texts_to_sequences(preprocessed_sentences))\\n[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\\n빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며, 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 'OOV'의 인덱스인 1로 인코딩되었습니다.\\n==================================================\\n--- 02-07 패딩(Padding) ---\\n```\\narray([[ 1,  5, 14, 14, 14, 14, 14],\\n[ 1,  8,  5, 14, 14, 14, 14],\\n[ 1,  3,  5, 14, 14, 14, 14],\", 'array([[ 1,  5, 14, 14, 14, 14, 14],\\n[ 1,  8,  5, 14, 14, 14, 14],\\n[ 1,  3,  5, 14, 14, 14, 14],\\n[ 9,  2, 14, 14, 14, 14, 14],\\n[ 2,  4,  3,  2, 14, 14, 14],\\n[ 3,  2, 14, 14, 14, 14, 14],\\n[ 1,  4,  6, 14, 14, 14, 14],\\n[ 1,  4,  6, 14, 14, 14, 14],\\n[ 1,  4,  2, 14, 14, 14, 14],\\n[ 7,  7,  3,  2, 10,  1, 11],\\n[ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)', '[ 7,  7,  3,  2, 10,  1, 11],\\n[ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)\\n```자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있습니다. 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 다시 말해 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있습니다. 실습을 통해 이해해봅시다.']\n",
      "['import numpy as np\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\n정수 인코딩 챕터에서 수행했던 실습을 그대로 반복해보겠습니다. 아래와 같이 텍스트 데이터가 있습니다.', \"정수 인코딩 챕터에서 수행했던 실습을 그대로 반복해보겠습니다. 아래와 같이 텍스트 데이터가 있습니다.\\npreprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\\n단어 집합을 만들고, 정수 인코딩을 수행합니다.\\ntokenizer = Tokenizer()\", \"단어 집합을 만들고, 정수 인코딩을 수행합니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(preprocessed_sentences)\\nencoded = tokenizer.texts_to_sequences(preprocessed_sentences)\\nprint(encoded)\\n모든 단어가 고유한 정수로 변환되었습니다.\\n[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\\n모두 동일한 길이로 맞춰주기 위해서 이 중에서 가장 길이가 긴 문장의 길이를 계산해보겠습니다.\\nmax_len = max(len(item) for item in encoded)\\nprint('최대 길이 :',max_len)\\n최대 길이 : 7\", \"max_len = max(len(item) for item in encoded)\\nprint('최대 길이 :',max_len)\\n최대 길이 : 7\\n가장 길이가 긴 문장의 길이는 7입니다. 모든 문장의 길이를 7로 맞춰주겠습니다. 이때 가상의 단어 'PAD'를 사용합니다. 'PAD'라는 단어가 있다고 가정하고, 이 단어는 0번 단어라고 정의합니다. 길이가 7보다 짧은 문장에는 숫자 0을 채워서 길이 7로 맞춰줍니다.\\nfor sentence in encoded:\\nwhile len(sentence) < max_len:\\nsentence.append(0)\\npadded_np = np.array(encoded)\\npadded_np\\narray([[ 1,  5,  0,  0,  0,  0,  0],\\n[ 1,  8,  5,  0,  0,  0,  0],\\n[ 1,  3,  5,  0,  0,  0,  0],\\n[ 9,  2,  0,  0,  0,  0,  0],\", '[ 1,  8,  5,  0,  0,  0,  0],\\n[ 1,  3,  5,  0,  0,  0,  0],\\n[ 9,  2,  0,  0,  0,  0,  0],\\n[ 2,  4,  3,  2,  0,  0,  0],\\n[ 3,  2,  0,  0,  0,  0,  0],\\n[ 1,  4,  6,  0,  0,  0,  0],\\n[ 1,  4,  6,  0,  0,  0,  0],\\n[ 1,  4,  2,  0,  0,  0,  0],\\n[ 7,  7,  3,  2, 10,  1, 11],\\n[ 1, 12,  3, 13,  0,  0,  0]])', '[ 1,  4,  2,  0,  0,  0,  0],\\n[ 7,  7,  3,  2, 10,  1, 11],\\n[ 1, 12,  3, 13,  0,  0,  0]])\\n길이가 7보다 짧은 문장에는 전부 숫자 0이 뒤로 붙어서 모든 문장의 길이가 전부 7이된 것을 알 수 있습니다. 기계는 이들을 하나의 행렬로 보고, 병렬 처리를 할 수 있습니다. 또한, 0번 단어는 사실 아무런 의미도 없는 단어이기 때문에 자연어 처리하는 과정에서 기계는 0번 단어를 무시하게 될 것입니다. 이와 같이 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것을 패딩(padding)이라고 합니다. 숫자 0을 사용하고 있다면 제로 패딩(zero padding)이라고 합니다.']\n",
      "['케라스에서는 위와 같은 패딩을 위해 pad_sequences()를 제공하고 있습니다.\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nencoded 값이 위에서 이미 패딩 후의 결과로 저장되었기 때문에 패딩 이전의 값으로 다시 되돌리겠습니다.\\nencoded = tokenizer.texts_to_sequences(preprocessed_sentences)\\nprint(encoded)\\n[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\\n케라스의 pad_sequences를 사용하여 패딩을 해봅시다.\\npadded = pad_sequences(encoded)\\npadded\\narray([[ 0,  0,  0,  0,  0,  1,  5],', 'padded = pad_sequences(encoded)\\npadded\\narray([[ 0,  0,  0,  0,  0,  1,  5],\\n[ 0,  0,  0,  0,  1,  8,  5],\\n[ 0,  0,  0,  0,  1,  3,  5],\\n[ 0,  0,  0,  0,  0,  9,  2],\\n[ 0,  0,  0,  2,  4,  3,  2],\\n[ 0,  0,  0,  0,  0,  3,  2],\\n[ 0,  0,  0,  0,  1,  4,  6],\\n[ 0,  0,  0,  0,  1,  4,  6],\\n[ 0,  0,  0,  0,  1,  4,  2],\\n[ 7,  7,  3,  2, 10,  1, 11],\\n[ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)', \"[ 7,  7,  3,  2, 10,  1, 11],\\n[ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)\\nNumpy로 패딩을 진행하였을 때와는 패딩 결과가 다른데 그 이유는 pad_sequences는 기본적으로 문서의 뒤에 0을 채우는 것이 아니라 앞에 0으로 채우기 때문입니다. 뒤에 0을 채우고 싶다면 인자로 padding='post'를 주면됩니다.\\npadded = pad_sequences(encoded, padding='post')\\npadded\\narray([[ 1,  5,  0,  0,  0,  0,  0],\\n[ 1,  8,  5,  0,  0,  0,  0],\\n[ 1,  3,  5,  0,  0,  0,  0],\\n[ 9,  2,  0,  0,  0,  0,  0],\\n[ 2,  4,  3,  2,  0,  0,  0],\\n[ 3,  2,  0,  0,  0,  0,  0],\\n[ 1,  4,  6,  0,  0,  0,  0],\", '[ 2,  4,  3,  2,  0,  0,  0],\\n[ 3,  2,  0,  0,  0,  0,  0],\\n[ 1,  4,  6,  0,  0,  0,  0],\\n[ 1,  4,  6,  0,  0,  0,  0],\\n[ 1,  4,  2,  0,  0,  0,  0],\\n[ 7,  7,  3,  2, 10,  1, 11],\\n[ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)\\nNumpy를 이용하여 패딩을 했을 때와 결과가 동일합니다. 실제로 결과가 동일한지 두 결과를 비교합니다.\\n(padded == padded_np).all()\\nTrue', \"Numpy를 이용하여 패딩을 했을 때와 결과가 동일합니다. 실제로 결과가 동일한지 두 결과를 비교합니다.\\n(padded == padded_np).all()\\nTrue\\nTrue값이 리턴됩니다. 두 결과가 동일하다는 의미입니다. 지금까지는 가장 긴 길이를 가진 문서의 길이를 기준으로 패딩을 한다고 가정하였지만, 실제로는 꼭 가장 긴 문서의 길이를 기준으로 해야하는 것은 아닙니다. 가령, 모든 문서의 평균 길이가 20인데 문서 1개의 길이가 5,000이라고 해서 굳이 모든 문서의 길이를 5,000으로 패딩할 필요는 없을 수 있습니다. 이와 같은 경우에는 길이에 제한을 두고 패딩할 수 있습니다. maxlen의 인자로 정수를 주면, 해당 정수로 모든 문서의 길이를 동일하게 합니다.\\npadded = pad_sequences(encoded, padding='post', maxlen=5)\\npadded\\narray([[ 1,  5,  0,  0,  0],\\n[ 1,  8,  5,  0,  0],\", 'padded\\narray([[ 1,  5,  0,  0,  0],\\n[ 1,  8,  5,  0,  0],\\n[ 1,  3,  5,  0,  0],\\n[ 9,  2,  0,  0,  0],\\n[ 2,  4,  3,  2,  0],\\n[ 3,  2,  0,  0,  0],\\n[ 1,  4,  6,  0,  0],\\n[ 1,  4,  6,  0,  0],\\n[ 1,  4,  2,  0,  0],\\n[ 3,  2, 10,  1, 11],\\n[ 1, 12,  3, 13,  0]], dtype=int32)', \"[ 1,  4,  2,  0,  0],\\n[ 3,  2, 10,  1, 11],\\n[ 1, 12,  3, 13,  0]], dtype=int32)\\n길이가 5보다 짧은 문서들은 0으로 패딩되고, 기존에 5보다 길었다면 데이터가 손실됩니다. 가령, 뒤에서 두번째 문장은 본래 [ 7,  7,  3,  2, 10,  1, 11]였으나 현재는 [ 3,  2, 10,  1, 11]로 변경된 것을 볼 수 있습니다. 만약, 데이터가 손실될 경우에 앞의 단어가 아니라 뒤의 단어가 삭제되도록 하고싶다면 truncating이라는 인자를 사용합니다. truncating='post'를 사용할 경우 뒤의 단어가 삭제됩니다.\\npadded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\\npadded\\narray([[ 1,  5,  0,  0,  0],\\n[ 1,  8,  5,  0,  0],\\n[ 1,  3,  5,  0,  0],\", 'padded\\narray([[ 1,  5,  0,  0,  0],\\n[ 1,  8,  5,  0,  0],\\n[ 1,  3,  5,  0,  0],\\n[ 9,  2,  0,  0,  0],\\n[ 2,  4,  3,  2,  0],\\n[ 3,  2,  0,  0,  0],\\n[ 1,  4,  6,  0,  0],\\n[ 1,  4,  6,  0,  0],\\n[ 1,  4,  2,  0,  0],\\n[ 7,  7,  3,  2, 10],\\n[ 1, 12,  3, 13,  0]], dtype=int32)\\n숫자 0으로 패딩하는 것은 널리 퍼진 관례이긴 하지만, 반드시 지켜야하는 규칙은 아닙니다. 만약, 숫자 0이 아니라 다른 숫자를 패딩을 위한 숫자로 사용하고 싶다면 이 또한 가능합니다. 현재 사용된 정수들과 겹치지 않도록, 단어 집합의 크기에 +1을 한 숫자로 사용해봅시다.\\nlast_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용', \"last_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용\\nprint(last_value)\\n14\\n현재 단어가 총 13개이고, 1번부터 13번까지 정수가 사용되었으므로 단어 집합의 크기에 +1을 하면 마지막 숫자인 13보다 1이 큰 14를 얻습니다. pad_sequences의 인자로 value를 사용하면 0이 아닌 다른 숫자로 패딩이 가능합니다.\\npadded = pad_sequences(encoded, padding='post', value=last_value)\\npadded\\narray([[ 1,  5, 14, 14, 14, 14, 14],\\n[ 1,  8,  5, 14, 14, 14, 14],\\n[ 1,  3,  5, 14, 14, 14, 14],\\n[ 9,  2, 14, 14, 14, 14, 14],\\n[ 2,  4,  3,  2, 14, 14, 14],\\n[ 3,  2, 14, 14, 14, 14, 14],\", '[ 9,  2, 14, 14, 14, 14, 14],\\n[ 2,  4,  3,  2, 14, 14, 14],\\n[ 3,  2, 14, 14, 14, 14, 14],\\n[ 1,  4,  6, 14, 14, 14, 14],\\n[ 1,  4,  6, 14, 14, 14, 14],\\n[ 1,  4,  2, 14, 14, 14, 14],\\n[ 7,  7,  3,  2, 10,  1, 11],\\n[ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)\\n==================================================\\n--- 02-08 원-핫 인코딩(One-Hot Encoding) ---\\n```\\n[[0. 0. 1. 0. 0. 0. 0. 0.] # 인덱스 2의 원-핫 벡터\\n[0. 0. 0. 0. 0. 1. 0. 0.] # 인덱스 5의 원-핫 벡터\\n[0. 1. 0. 0. 0. 0. 0. 0.] # 인덱스 1의 원-핫 벡터', '[0. 0. 0. 0. 0. 1. 0. 0.] # 인덱스 5의 원-핫 벡터\\n[0. 1. 0. 0. 0. 0. 0. 0.] # 인덱스 1의 원-핫 벡터\\n[0. 0. 0. 0. 0. 0. 1. 0.] # 인덱스 6의 원-핫 벡터\\n[0. 0. 0. 1. 0. 0. 0. 0.] # 인덱스 3의 원-핫 벡터\\n[0. 0. 0. 0. 0. 0. 0. 1.]] # 인덱스 7의 원-핫 벡터\\n```컴퓨터 또는 기계는 문자보다는 숫자를 더 잘 처리 할 수 있습니다. 이를 위해 자연어 처리에서는 문자를 숫자로 바꾸는 여러가지 기법들이 있습니다. 원-핫 인코딩(One-Hot Encoding)은 그 많은 기법 중에서 단어를 표현하는 가장 기본적인 표현 방법이며, 머신 러닝, 딥 러닝을 하기 위해서는 반드시 배워야 하는 표현 방법입니다.', '원-핫 인코딩에 대해서 배우기에 앞서 단어 집합(vocabulary) 에 대해서 정의해보겠습니다. 단어 집합은 앞으로 자연어 처리에서 계속 나오는 개념이기 때문에 여기서 이해하고 가야합니다. 단어 집합은 서로 다른 단어들의 집합입니다. 여기서 혼동이 없도록 서로 다른 단어라는 정의에 대해서 좀 더 주목할 필요가 있습니다. 단어 집합(vocabulary)에서는 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주합니다. 이 책에서는 앞으로 단어 집합에 있는 단어들을 가지고, 문자를 숫자. 더 구체적으로는 벡터로 바꾸는 원-핫 인코딩을 포함한 여러 방법에 대해서 배우게 됩니다.', '원-핫 인코딩을 위해서 먼저 해야할 일은 단어 집합을 만드는 일입니다. 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 이를 단어 집합이라고 합니다. 그리고 이 단어 집합에 고유한 정수를 부여하는 정수 인코딩을 진행합니다. 텍스트에 단어가 총 5,000개가 존재한다면, 단어 집합의 크기는 5,000입니다. 5,000개의 단어가 있는 이 단어 집합의 단어들마다 1번부터 5,000번까지 인덱스를 부여한다고 해보겠습니다. 가령, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 부여할 수 있습니다.\\n이제 각 단어에 고유한 정수 인덱스를 부여하였다고 합시다. 이 숫자로 바뀐 단어들을 벡터로 다루고 싶다면 어떻게 하면 될까요?']\n",
      "['원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식입니다. 이렇게 표현된 벡터를 원-핫 벡터(One-Hot vector)라고 합니다.\\n원-핫 인코딩을 두 가지 과정으로 정리해보겠습니다. 첫째, 정수 인코딩을 수행합니다. 다시 말해 각 단어에 고유한 정수를 부여합니다. 둘째, 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여합니다. 한국어 문장을 예제로 원-핫 벡터를 만들어보겠습니다.\\n문장 : 나는 자연어 처리를 배운다\\nOkt 형태소 분석기를 통해서 문장에 대해서 토큰화를 수행합니다.\\nfrom konlpy.tag import Okt\\nokt = Okt()\\ntokens = okt.morphs(\"나는 자연어 처리를 배운다\")\\nprint(tokens)', 'from konlpy.tag import Okt\\nokt = Okt()\\ntokens = okt.morphs(\"나는 자연어 처리를 배운다\")\\nprint(tokens)\\n[\\'나\\', \\'는\\', \\'자연어\\', \\'처리\\', \\'를\\', \\'배운다\\']\\n각 토큰에 대해서 고유한 정수를 부여합니다. 지금은 문장이 짧기 때문에 각 단어의 빈도수를 고려하지 않지만, 빈도수 순으로 단어를 정렬하여 정수를 부여하는 경우가 많습니다.\\nword_to_index = {word : index for index, word in enumerate(tokens)}\\nprint(\\'단어 집합 :\\',word_to_index)\\n단어 집합 : {\\'나\\': 0, \\'는\\': 1, \\'자연어\\': 2, \\'처리\\': 3, \\'를\\': 4, \\'배운다\\': 5}\\n토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수를 만들었습니다.\\ndef one_hot_encoding(word, word_to_index):', '토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수를 만들었습니다.\\ndef one_hot_encoding(word, word_to_index):\\none_hot_vector = [0]*(len(word_to_index))\\nindex = word_to_index[word]\\none_hot_vector[index] = 1\\nreturn one_hot_vector\\n\\'자연어\\'라는 단어의 원-핫 벡터를 얻어봅시다.\\none_hot_encoding(\"자연어\", word_to_index)\\n[0, 0, 1, 0, 0, 0]\\n\\'자연어\\'는 정수 2이므로 원-핫 벡터는 인덱스 2의 값이 1이며, 나머지 값은 0인 벡터가 나옵니다.']\n",
      "['위에서는 원-핫 인코딩을 이해하기 위해 파이썬으로 직접 코드를 작성하였지만, 케라스는 원-핫 인코딩을 수행하는 유용한 도구 to_categorical()를 지원합니다. 이번에는 케라스만으로 정수 인코딩과 원-핫 인코딩을 순차적으로 진행해보도록 하겠습니다.\\ntext = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\\n위와 같은 문장이 있다고 했을 때, 케라스 토크나이저를 이용한 정수 인코딩은 다음과 같습니다.\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.utils import to_categorical\\ntext = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([text])\\nprint(\\'단어 집합 :\\',tokenizer.word_index)', 'tokenizer = Tokenizer()\\ntokenizer.fit_on_texts([text])\\nprint(\\'단어 집합 :\\',tokenizer.word_index)\\n단어 집합 : {\\'갈래\\': 1, \\'점심\\': 2, \\'햄버거\\': 3, \\'나랑\\': 4, \\'먹으러\\': 5, \\'메뉴는\\': 6, \\'최고야\\': 7}\\n위와 같이 생성된 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면, texts_to_sequences()를 통해서 이를 정수 시퀀스로 변환가능합니다. 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 sub_text를 만들어 확인해보겠습니다.\\nsub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\\nencoded = tokenizer.texts_to_sequences([sub_text])[0]\\nprint(encoded)\\n[2, 5, 1, 6, 3, 7]', 'encoded = tokenizer.texts_to_sequences([sub_text])[0]\\nprint(encoded)\\n[2, 5, 1, 6, 3, 7]\\n지금까지 진행한 것은 이미 이전에 정수 인코딩 실습을 하며 배운 내용입니다. 이제 해당 결과를 가지고, 원-핫 인코딩을 진행해보겠습니다. 케라스는 정수 인코딩 된 결과로부터 원-핫 인코딩을 수행하는 to_categorical()를 지원합니다.\\none_hot = to_categorical(encoded)\\nprint(one_hot)\\n[[0. 0. 1. 0. 0. 0. 0. 0.] # 인덱스 2의 원-핫 벡터\\n[0. 0. 0. 0. 0. 1. 0. 0.] # 인덱스 5의 원-핫 벡터\\n[0. 1. 0. 0. 0. 0. 0. 0.] # 인덱스 1의 원-핫 벡터\\n[0. 0. 0. 0. 0. 0. 1. 0.] # 인덱스 6의 원-핫 벡터\\n[0. 0. 0. 1. 0. 0. 0. 0.] # 인덱스 3의 원-핫 벡터', '[0. 0. 0. 0. 0. 0. 1. 0.] # 인덱스 6의 원-핫 벡터\\n[0. 0. 0. 1. 0. 0. 0. 0.] # 인덱스 3의 원-핫 벡터\\n[0. 0. 0. 0. 0. 0. 0. 1.]] # 인덱스 7의 원-핫 벡터\\n위의 결과는 \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"라는 문장이 [2, 5, 1, 6, 3, 7]로 정수 인코딩이 되고나서, 각각의 인코딩 된 결과를 인덱스로 원-핫 인코딩이 수행된 모습을 보여줍니다.']\n",
      "['이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있습니다. 다른 표현으로는 벡터의 차원이 늘어난다고 표현합니다. 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 됩니다. 가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 됩니다. 다시 말해 모든 단어 각각은 하나의 값만 1을 가지고, 999개의 값은 0의 값을 가지는 벡터가 되는데 이는 저장 공간 측면에서는 매우 비효율적인 표현 방법입니다.', '또한 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점이 있습니다. 예를 들어서 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서 원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]이라는 원-핫 벡터를 부여받았다고 합시다. 이때 원-핫 벡터로는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없습니다. 좀 더 극단적으로는 강아지, 개, 냉장고라는 단어가 있을 때 강아지라는 단어가 개와 냉장고라는 단어 중 어떤 단어와 더 유사한지도 알 수 없습니다.', \"단어 간 유사성을 알 수 없다는 단점은 검색 시스템 등에서는 문제가 될 소지가 있습니다. 가령, 여행을 가려고 웹 검색창에 '삿포로 숙소'라는 단어를 검색한다고 합시다. 제대로 된 검색 시스템이라면, '삿포로 숙소'라는 검색어에 대해서 '삿포로 게스트 하우스', '삿포로 료칸', '삿포로 호텔'과 같은 유사 단어에 대한 결과도 함께 보여줄 수 있어야 합니다. 하지만 단어간 유사성을 계산할 수 없다면, '게스트 하우스'와 '료칸'과 '호텔'이라는 연관 검색어를 보여줄 수 없습니다.\", '이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있습니다. 첫째는 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있으며, 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다. 그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법이 존재합니다.\\n여기서 언급한 방법들 중 대부분은 워드 임베딩 챕터에서 다루게 됩니다.\\n==================================================\\n--- 02-09 데이터의 분리(Splitting Data) ---\\n```\\nX 테스트 데이터 :\\n[[18 19]\\n[20 21]\\n[22 23]]\\ny 테스트 데이터 :\\n[9, 10, 11]', '```\\nX 테스트 데이터 :\\n[[18 19]\\n[20 21]\\n[22 23]]\\ny 테스트 데이터 :\\n[9, 10, 11]\\n```머신 러닝 모델을 학습시키고 평가하기 위해서는 데이터를 적절하게 분리하는 작업이 필요합니다. 이 책에서는 대부분의 경우에서 지도 학습(Supervised Learning)을 다루는데, 이번에는 지도 학습을 위한 데이터 분리 작업에 대해서 배웁니다.\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split']\n",
      "[\"지도 학습의 훈련 데이터는 문제지를 연상케 합니다. 지도 학습의 훈련 데이터는 정답이 무엇인지 맞춰 하는 '문제'에 해당되는 데이터와 레이블이라고 부르는 '정답'이 적혀있는 데이터로 구성되어 있습니다. 쉽게 비유하면, 기계는 정답이 적혀져 있는 문제지를 문제와 정답을 함께 보면서 열심히 공부하고, 향후에 정답이 없는 문제에 대해서도 정답을 잘 예측해야 합니다.\\n예를 들어 스팸 메일 분류기를 위한 데이터 같은 경우에는 메일의 본문과 해당 메일이 정상 메일인지, 스팸 메일인지 적혀있는 레이블로 구성되어져 있습니다. 예를 들어 아래와 같은 형식의 데이터가 약 20,000개 있다고 가정해보겠습니다. 이 데이터는 두 개의 열로 구성되는데, 바로 메일의 본문에 해당되는 첫번째 열과 해당 메일이 정상 메일인지 스팸 메일인지가 적혀있는 정답에 해당되는 두번째 열입니다. 그리고 이러한 데이터 배열이 총 20,000개의 행을 가집니다.\\n텍스트(메일의 내용)\\n레이블(스팸 여부)\", '텍스트(메일의 내용)\\n레이블(스팸 여부)\\n당신에게 드리는 마지막 혜택! ...\\n스팸 메일\\n내일 뵐 수 있을지 확인 부탁...\\n정상 메일\\n...\\n...\\n(광고) 멋있어질 수 있는...\\n스팸 메일\\n기계를 지도하는 선생님의 입장이 되어보겠습니다. 기계를 훈련시키기 위해서 데이터를 총 4개로 나눕니다. 우선 메일의 내용이 담긴 첫번째 열을 X에 저장합니다. 그리고 메일이 스팸인지 정상인지 정답이 적혀있는 두번째 열을 y에 저장합니다. 이제 문제지에 해당되는 20,000개의 X와 정답지에 해당되는 20,000개의 y가 생겼습니다.', '그리고 이제 이 X와 y에 대해서 일부 데이터를 또 다시 분리합니다. 이는 문제지를 다 공부하고나서 실력을 평가하기 위해서 시험(test)용으로 일부로 일부 문제와 해당 문제의 정답지를 분리해놓는 것입니다. 여기서는 2,000개를 분리한다고 가정하겠습니다. 이때 분리 시에는 여전히 X와 y의 맵핑 관계를 유지해야 합니다. 어떤 X(문제)에 대한 어떤 y(정답)인지 바로 찾을 수 있어야 합니다. 이렇게 되면 학습용에 해당되는 18,000개의 X, y의 쌍과 시험용에 해당되는 2000개의 X, y의 쌍이 생깁니다 이 책에서는 이 유형의 데이터들에게 일반적으로 다음과 같은 변수명을 부여합니다.\\n<훈련 데이터>\\nX_train : 문제지 데이터\\ny_train : 문제지에 대한 정답 데이터.\\n<테스트 데이터>\\nX_test : 시험지 데이터.\\ny_test : 시험지에 대한 정답 데이터.', 'X_train : 문제지 데이터\\ny_train : 문제지에 대한 정답 데이터.\\n<테스트 데이터>\\nX_test : 시험지 데이터.\\ny_test : 시험지에 대한 정답 데이터.\\n기계는 이제부터 X_train과 y_train에 대해서 학습을 합니다. 기계는 학습 상태에서는 정답지인 y_train을 볼 수 있기 때문에 18,000개의 문제지 X_train과 y_train을 함께 보면서 어떤 메일 내용일 때 정상 메일인지 스팸 메일인지를 열심히 규칙을 도출해나가면서 정리해나갑니다. 그리고 학습을 다 한 기계에게 y_test는 보여주지 않고, X_test에 대해서 정답을 예측하게 합니다. 그리고 기계가 예측한 답과 실제 정답인 y_test를 비교하면서 기계가 정답을 얼마나 맞췄는지를 평가합니다. 이 수치가 기계의 정확도(Accuracy)가 됩니다.']\n",
      "[\"1) zip 함수를 이용하여 분리하기\\nzip()함수는 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할을 합니다. 리스트의 리스트 구성에서 zip 함수는 X와 y를 분리하는데 유용합니다. 우선 zip 함수가 어떤 역할을 하는지 확인해보도록 하겠습니다.\\nX, y = zip(['a', 1], ['b', 2], ['c', 3])\\nprint('X 데이터 :',X)\\nprint('y 데이터 :',y)\\nX 데이터 : ('a', 'b', 'c')\\ny 데이터 : (1, 2, 3)\\n각 데이터에서 첫번째로 등장한 원소들끼리 묶이고, 두번째로 등장한 원소들끼리 묶인 것을 볼 수 있습니다.\\n# 리스트의 리스트 또는 행렬 또는 뒤에서 배울 개념인 2D 텐서.\\nsequences = [['a', 1], ['b', 2], ['c', 3]]\\nX, y = zip(*sequences)\\nprint('X 데이터 :',X)\\nprint('y 데이터 :',y)\", \"X, y = zip(*sequences)\\nprint('X 데이터 :',X)\\nprint('y 데이터 :',y)\\nX 데이터 : ('a', 'b', 'c')\\ny 데이터 : (1, 2, 3)\\n각 데이터에서 첫번째로 등장한 원소들끼리 묶이고, 두번째로 등장한 원소들끼리 묶인 것을 볼 수 있습니다. 이를 각각 X데이터와 y데이터로 사용할 수 있습니다.\\n2) 데이터프레임을 이용하여 분리하기\\nvalues = [['당신에게 드리는 마지막 혜택!', 1],\\n['내일 뵐 수 있을지 확인 부탁드...', 0],\\n['도연씨. 잘 지내시죠? 오랜만입...', 0],\\n['(광고) AI로 주가를 예측할 수 있다!', 1]]\\ncolumns = ['메일 본문', '스팸 메일 유무']\\ndf = pd.DataFrame(values, columns=columns)\\ndf\\n[이미지: ]\\n데이터프레임은 열의 이름으로 각 열에 접근이 가능하므로, 이를 이용하면 손쉽게 X 데이터와 y 데이터를 분리할 수 있습니다.\", \"df\\n[이미지: ]\\n데이터프레임은 열의 이름으로 각 열에 접근이 가능하므로, 이를 이용하면 손쉽게 X 데이터와 y 데이터를 분리할 수 있습니다.\\nX = df['메일 본문']\\ny = df['스팸 메일 유무']\\nX와 y데이터를 출력해보겠습니다.\\nprint('X 데이터 :',X.to_list())\\nprint('y 데이터 :',y.to_list())\\nX 데이터 : ['당신에게 드리는 마지막 혜택!', '내일 뵐 수 있을지 확인 부탁드...', '도연씨. 잘 지내시죠? 오랜만입...', '(광고) AI로 주가를 예측할 수 있다!']\\ny 데이터 : [1, 0, 0, 1]\\n3) Numpy를 이용하여 분리하기\\n임의의 데이터를 만들어서 Numpy의 슬라이싱(slicing)을 사용하여 데이터를 분리해봅시다.\\nnp_array = np.arange(0,16).reshape((4,4))\\nprint('전체 데이터 :')\\nprint(np_array)\\n전체 데이터 :\\n[[ 0  1  2  3]\", \"np_array = np.arange(0,16).reshape((4,4))\\nprint('전체 데이터 :')\\nprint(np_array)\\n전체 데이터 :\\n[[ 0  1  2  3]\\n[ 4  5  6  7]\\n[ 8  9 10 11]\\n[12 13 14 15]]\\n마지막 열을 제외하고 X데이터에 저장합니다. 마지막 열만을 y데이터에 저장합니다.\\nX = np_array[:, :3]\\ny = np_array[:,3]\\nprint('X 데이터 :')\\nprint(X)\\nprint('y 데이터 :',y)\\nX 데이터 :\\n[[ 0  1  2]\\n[ 4  5  6]\\n[ 8  9 10]\\n[12 13 14]]\\ny 데이터 : [ 3  7 11 15]\"]\n",
      "['이번에는 이미 X와 y가 분리된 데이터에 대해서 테스트 데이터를 분리하는 과정에 대해서 알아보겠습니다.\\n1) 사이킷 런을 이용하여 분리하기\\n사이킷런은 학습용 테스트와 테스트용 데이터를 쉽게 분리할 수 있게 해주는 train_test_split()를 지원합니다.\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)\\n각 인자는 다음을 의미합니다. train_size와 test_size는 둘 중 하나만 기재해도 됩니다.\\nX : 독립 변수 데이터. (배열이나 데이터프레임)\\ny : 종속 변수 데이터. 레이블 데이터.\\ntest_size : 테스트용 데이터 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.\\ntrain_size : 학습용 데이터의 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.\\nrandom_state : 난수 시드', \"train_size : 학습용 데이터의 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.\\nrandom_state : 난수 시드\\n예를 들어보겠습니다. 임의로 X 데이터와 y 데이터를 생성했습니다.\\n# 임의로 X와 y 데이터를 생성\\nX, y = np.arange(10).reshape((5, 2)), range(5)\\nprint('X 전체 데이터 :')\\nprint(X)\\nprint('y 전체 데이터 :')\\nprint(list(y))\\nX 전체 데이터 :\\n[[0 1]\\n[2 3]\\n[4 5]\\n[6 7]\\n[8 9]]\\ny 전체 데이터 :\\n[0, 1, 2, 3, 4]\", 'print(list(y))\\nX 전체 데이터 :\\n[[0 1]\\n[2 3]\\n[4 5]\\n[6 7]\\n[8 9]]\\ny 전체 데이터 :\\n[0, 1, 2, 3, 4]\\n여기서는 7:3의 비율로 데이터를 분리합니다. train_test_split()은 기본적으로 데이터의 순서를 섞고나서 훈련 데이터와 테스트 데이터를 분리합니다. 만약, random_state의 값을 특정 숫자로 기재해준 뒤에 다음에도 동일한 숫자로 기재해주면 항상 동일한 훈련 데이터와 테스트 데이터를 얻을 수 있습니다. 하지만 값을 변경하면 다른 순서로 섞인 채 분리되므로 이전과 다른 훈련 데이터와 테스트 데이터를 얻습니다. 실습을 통해서 이해해봅시다. random_state 값을 임의로 1234로 지정했습니다.\\n# 7:3의 비율로 훈련 데이터와 테스트 데이터 분리\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)', \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\\n70%의 비율로 분리된 X의 훈련 데이터와 30%의 비율로 분리된 X의 테스트 데이터입니다.\\nprint('X 훈련 데이터 :')\\nprint(X_train)\\nprint('X 테스트 데이터 :')\\nprint(X_test)\\nX 훈련 데이터 :\\n[[2 3]\\n[4 5]\\n[6 7]]\\nX 테스트 데이터 :\\n[[8 9]\\n[0 1]]\\n70%의 비율로 분리된 y의 훈련 데이터와 30%의 비율로 분리된 y의 테스트 데이터입니다.\\nprint('y 훈련 데이터 :')\\nprint(y_train)\\nprint('y 테스트 데이터 :')\\nprint(y_test)\\ny 훈련 데이터 :\\n[1, 2, 3]\\ny 테스트 데이터 :\\n[4, 0]\", \"print(y_train)\\nprint('y 테스트 데이터 :')\\nprint(y_test)\\ny 훈련 데이터 :\\n[1, 2, 3]\\ny 테스트 데이터 :\\n[4, 0]\\n출력 결과를 보면 데이터를 어느 중간 부분에서 앞과 뒤로 자른 것이 아니라 앞에 있던 샘플이 뒤로 가기도하고, 데이터의 순서가 전반적으로 섞이면서 분리된 것을 확인할 수 있습니다. random_state의 의미를 이해하기 위해서 이번에는 random_state의 값을 임의로 다른 값인 1을 주고 다시 분리해보겠습니다. 그리고 y데이터를 출력해봅시다.\\n# random_state의 값을 변경\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\\nprint('y 훈련 데이터 :')\\nprint(y_train)\\nprint('y 테스트 데이터 :')\\nprint(y_test)\\ny 훈련 데이터 :\\n[4, 0, 3]\", \"print('y 훈련 데이터 :')\\nprint(y_train)\\nprint('y 테스트 데이터 :')\\nprint(y_test)\\ny 훈련 데이터 :\\n[4, 0, 3]\\ny 테스트 데이터 :\\n[2, 1]\\nrandom_state 값이 1234일 때와 전혀 다른 y데이터가 출력됩니다. 데이터가 다른 순서로 섞였다는 의미입니다. 이번에는 다시 random_state의 값을 1234로 주고 다시 y데이터를 출력해봅시다.\\n# random_state을 이전의 값이었던 1234로 변경\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\\nprint('y 훈련 데이터 :')\\nprint(y_train)\\nprint('y 테스트 데이터 :')\\nprint(y_test)\\ny 훈련 데이터 :\\n[1, 2, 3]\\ny 테스트 데이터 :\\n[4, 0]\", \"print(y_train)\\nprint('y 테스트 데이터 :')\\nprint(y_test)\\ny 훈련 데이터 :\\n[1, 2, 3]\\ny 테스트 데이터 :\\n[4, 0]\\n이전과 동일한 y데이터가 출력됩니다. random_state의 값을 고정해두면 실행할 때마다 항상 동일한 순서로 데이터를 섞으므로, 동일한 코드를 다음에 재현하고자 할 때 사용할 수 있습니다.\\n2) 수동으로 분리하기\\n데이터를 분리하는 방법 중 하나는 수동으로 분리하는 것입니다. 우선 임의로 X 데이터와 y 데이터를 만들어보겠습니다.\\n# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성\\nX, y = np.arange(0,24).reshape((12,2)), range(12)\\nprint('X 전체 데이터 :')\\nprint(X)\\nprint('y 전체 데이터 :')\\nprint(list(y))\\nX 전체 데이터 :\\n[[ 0  1]\\n[ 2  3]\\n[ 4  5]\\n[ 6  7]\\n[ 8  9]\\n[10 11]\\n[12 13]\", \"print(list(y))\\nX 전체 데이터 :\\n[[ 0  1]\\n[ 2  3]\\n[ 4  5]\\n[ 6  7]\\n[ 8  9]\\n[10 11]\\n[12 13]\\n[14 15]\\n[16 17]\\n[18 19]\\n[20 21]\\n[22 23]]\\ny 전체 데이터 :\\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\\n훈련 데이터의 개수와 테스트 데이터의 개수를 정해보겠습니다. num_of_train은 훈련 데이터의 개수를 의미하며, num_of_test는 테스트 데이터의 개수를 의미합니다.\\nnum_of_train = int(len(X) * 0.8) # 데이터의 전체 길이의 80%에 해당하는 길이값을 구한다.\\nnum_of_test = int(len(X) - num_of_train) # 전체 길이에서 80%에 해당하는 길이를 뺀다.\\nprint('훈련 데이터의 크기 :',num_of_train)\\nprint('테스트 데이터의 크기 :',num_of_test)\\n훈련 데이터의 크기 : 9\", \"print('훈련 데이터의 크기 :',num_of_train)\\nprint('테스트 데이터의 크기 :',num_of_test)\\n훈련 데이터의 크기 : 9\\n테스트 데이터의 크기 : 3\\n아직 훈련 데이터와 테스트 데이터를 나눈 것이 아니라 이 두 개의 개수를 몇 개로 할지 정하기만 한 상태입니다. 여기서 num_of_test를 len(X) * 0.2로 계산해서는 안 됩니다. 데이터에 누락이 발생할 수 있습니다. 예를 들어서 전체 데이터의 개수가 4,518이라고 가정했을 때 4,518의 80%의 값은 3,614.4로 소수점을 내리면 3,614가 됩니다. 또한 4,518의 20%의 값은 903.6으로 소수점을 내리면 903이 됩니다. 그리고 3,614 + 903 = 4517이므로 데이터 1개가 누락이 됩니다. 그러므로 어느 한 쪽을 먼저 계산하고 그 값만큼 제외하는 방식으로 계산해야 합니다.\\nX_test = X[num_of_train:] # 전체 데이터 중에서 20%만큼 뒤의 데이터 저장\", \"X_test = X[num_of_train:] # 전체 데이터 중에서 20%만큼 뒤의 데이터 저장\\ny_test = y[num_of_train:] # 전체 데이터 중에서 20%만큼 뒤의 데이터 저장\\nX_train = X[:num_of_train] # 전체 데이터 중에서 80%만큼 앞의 데이터 저장\\ny_train = y[:num_of_train] # 전체 데이터 중에서 80%만큼 앞의 데이터 저장\\n데이터를 나눌 때는 num_of_train와 같이 하나의 변수만 사용하면 데이터의 누락을 방지할 수 있습니다. 앞에서 구한 데이터의 개수만큼 훈련 데이터와 테스트 데이터를 분할합니다. 그리고 테스트 데이터를 출력하여 정상적으로 분리되었는지 확인합니다.\\nprint('X 테스트 데이터 :')\\nprint(X_test)\\nprint('y 테스트 데이터 :')\\nprint(list(y_test))\\nX 테스트 데이터 :\\n[[18 19]\\n[20 21]\\n[22 23]]\\ny 테스트 데이터 :\", \"print('y 테스트 데이터 :')\\nprint(list(y_test))\\nX 테스트 데이터 :\\n[[18 19]\\n[20 21]\\n[22 23]]\\ny 테스트 데이터 :\\n[9, 10, 11]\\n각 길이가 3인 것을 확인했습니다. train_test_split()과 다른 점은 데이터가 섞이지 않은 채 어느 지점에서 데이터를 앞과 뒤로 분리했다는 점입니다. 만약, 수동으로 분리하게 된다면 데이터를 분리하기 전에 수동으로 데이터를 섞는 과정이 필요할 수 있습니다. 실제로 뒤에서 이러한 실습들을 진행합니다.\\n==================================================\\n--- 02-10 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text) ---\\n```\\n['은경이', '는', '사무실', '로', '갔습니다', '.']\", \"```\\n['은경이', '는', '사무실', '로', '갔습니다', '.']\\n```유용한 한국어 전처리 패키지를 정리해봅시다. 앞서 소개한 형태소와 문장 토크나이징 도구들인 KoNLPy와 KSS(Korean Sentence Splitter)와 함께 유용하게 사용할 수 있는 패키지들입니다.\"]\n",
      "['pip install git+https://github.com/haven-jeon/PyKoSpacing.git\\n전희원님이 개발한 PyKoSpacing은 띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지입니다. PyKoSpacing은 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있습니다.\\nsent = \\'김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\\'\\n임의의 문장을 임의로 띄어쓰기가 없는 문장으로 만들었습니다.\\nnew_sent = sent.replace(\" \", \\'\\') # 띄어쓰기가 없는 문장 임의로 만들기\\nprint(new_sent)', 'new_sent = sent.replace(\" \", \\'\\') # 띄어쓰기가 없는 문장 임의로 만들기\\nprint(new_sent)\\n김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\\n이를 PyKoSpacing의 입력으로 사용하여 원 문장과 비교해봅시다.\\nfrom pykospacing import Spacing\\nspacing = Spacing()\\nkospacing_sent = spacing(new_sent)\\nprint(sent)\\nprint(kospacing_sent)\\n김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.', '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\\n정확하게 결과가 일치합니다.']\n",
      "['현재 Py-hanspell은 제대로 동작하지 않습니다. (2024/03/03)\\npip install git+https://github.com/ssut/py-hanspell.git\\nPy-Hanspell은 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지입니다.\\nfrom hanspell import spell_checker\\nsent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\\nspelled_sent = spell_checker.check(sent)\\nhanspell_sent = spelled_sent.checked\\nprint(hanspell_sent)\\n맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지\\n이 패키지는 띄어쓰기 또한 보정합니다. PyKoSpacing에 사용한 예제를 그대로 사용해봅시다.\\nspelled_sent = spell_checker.check(new_sent)\\nhanspell_sent = spelled_sent.checked', 'spelled_sent = spell_checker.check(new_sent)\\nhanspell_sent = spelled_sent.checked\\nprint(hanspell_sent)\\nprint(kospacing_sent) # 앞서 사용한 kospacing 패키지에서 얻은 결과\\n김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.\\n김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\\nPyKoSpacing과 결과가 거의 비슷하지만 조금 다릅니다.']\n",
      "['soynlp는 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저입니다. 비지도 학습으로 단어 토큰화를 한다는 특징을 갖고 있으며, 데이터에 자주 등장하는 단어들을 단어로 분석합니다. soynlp 단어 토크나이저는 내부적으로 단어 점수 표로 동작합니다. 이 점수는 응집 확률(cohesion probability)과 브랜칭 엔트로피(branching entropy)를 활용합니다.\\npip install soynlp']\n",
      "[\"soynlp를 소개하기 전에 기존의 형태소 분석기가 가진 문제는 무엇이었는지, SOYNLP가 어떤 점에서 유용한지 정리해봅시다. 기존의 형태소 분석기는 신조어나 형태소 분석기에 등록되지 않은 단어 같은 경우에는 제대로 구분하지 못하는 단점이 있었습니다.\\nfrom konlpy.tag import Okt\\ntokenizer = Okt()\\nprint(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정'))\\n['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정']\\n에이비식스는 아이돌의 이름이고, 이대휘는 에이비식스의 멤버이며, 최애돌은 최고로 애정하는 캐릭터라는 뜻이지만 위의 형태소 분석 결과에서는 전부 분리된 결과를 보여줍니다.\", \"에이비식스는 아이돌의 이름이고, 이대휘는 에이비식스의 멤버이며, 최애돌은 최고로 애정하는 캐릭터라는 뜻이지만 위의 형태소 분석 결과에서는 전부 분리된 결과를 보여줍니다.\\n그렇다면 텍스트 데이터에서 특정 문자 시퀀스가 함께 자주 등장하는 빈도가 높고, 앞 뒤로 조사 또는 완전히 다른 단어가 등장하는 것을 고려해서 해당 문자 시퀀스를 형태소라고 판단하는 단어 토크나이저라면 어떨까요?\\n예를 들어 에이비식스라는 문자열이 자주 연결되어 등장한다면 한 단어라고 판단하고, 또한 에이비식스라는 단어 앞, 뒤에 '최고', '가수', '실력'과 같은 독립된 다른 단어들이 계속해서 등장한다면 에이비식스를 한 단어로 파악하는 식이지요. 그리고 이런 아이디어를 가진 단어 토크나이저가 soynlp입니다.\"]\n",
      "['soynlp는 기본적으로 학습에 기반한 토크나이저이므로 학습에 필요한 한국어 문서를 다운로드합니다.\\nimport urllib.request\\nfrom soynlp import DoublespaceLineCorpus\\nfrom soynlp.word import WordExtractor\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")\\n훈련 데이터를 다수의 문서로 분리합니다.\\n# 훈련 데이터를 다수의 문서로 분리\\ncorpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\\nlen(corpus)\\n30091\\n총 3만 91개의 문서가 존재합니다. 상위 3개의 문서만 출력해봅시다. 지면의 한계로 중략하였습니다.\\ni = 0\\nfor document in corpus:', '30091\\n총 3만 91개의 문서가 존재합니다. 상위 3개의 문서만 출력해봅시다. 지면의 한계로 중략하였습니다.\\ni = 0\\nfor document in corpus:\\nif len(document) > 0:\\nprint(document)\\ni = i+1\\nif i == 3:\\nbreak\\n19  1990  52 1 22\\n오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 ... 중략 ... 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다', '테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다 ... 중략 ... 이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진\\n정상 출력되는 것을 확인하였습니다. soynlp는 학습 기반의 단어 토크나이저이므로 기존의 KoNLPy에서 제공하는 형태소 분석기들과는 달리 학습 과정을 거쳐야 합니다. 이는 전체 코퍼스로부터 응집 확률과 브랜칭 엔트로피 단어 점수표를 만드는 과정입니다. WordExtractor.extract()를 통해서 전체 코퍼스에 대해 단어 점수표를 계산합니다.\\nword_extractor = WordExtractor()\\nword_extractor.train(corpus)', 'word_extractor = WordExtractor()\\nword_extractor.train(corpus)\\nword_score_table = word_extractor.extract()\\ntraining was done. used memory 5.186 Gb\\nall cohesion probabilities was computed. # words = 223348\\nall branching entropies was computed # words = 361598\\nall accessor variety was computed # words = 361598\\n학습이 완료되었습니다.']\n",
      "['응집 확률은 내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도입니다. 응집 확률은 문자열을 문자 단위로 분리하여 내부 문자열을 만드는 과정에서 왼쪽부터 순서대로 문자를 추가하면서 각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산하여 누적곱을 한 값입니다. 이 값이 높을수록 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높습니다. 수식은 아래와 같습니다.\\n[이미지: ]\\n\\'반포한강공원에\\'라는 7의 길이를 가진 문자 시퀀스에 대해서 각 내부 문자열의 스코어를 구하는 과정은 아래와 같습니다.\\n[이미지: ]\\n실습을 통해 직접 응집 확률을 계산해보겠습니다. \\'반포한\\'의 응집 확률을 계산해봅시다.\\nword_score_table[\"반포한\"].cohesion_forward\\n0.08838002913645132\\n그렇다면 \\'반포한강\\'의 응집 확률은 \\'반포한\\'의 응집 확률보다 높을까요?', '0.08838002913645132\\n그렇다면 \\'반포한강\\'의 응집 확률은 \\'반포한\\'의 응집 확률보다 높을까요?\\nword_score_table[\"반포한강\"].cohesion_forward\\n0.19841268168224552\\n\\'반포한강\\'은 \\'반포한\\'보다 응집 확률이 높습니다. 그렇다면 \\'반포한강공\\'은 어떨까요?\\nword_score_table[\"반포한강공\"].cohesion_forward\\n0.2972877884078849\\n역시나 \\'반포한강\\'보다 응집 확률이 높습니다. \\'반포한강공원\\'은 어떨까요?\\nword_score_table[\"반포한강공원\"].cohesion_forward\\n0.37891487632839754\\n\\'반포한강공\\'보다 응집 확률이 높습니다. 여기다가 조사 \\'에\\'를 붙인 \\'반포한강공원에\\'는 어떨까요?\\nword_score_table[\"반포한강공원에\"].cohesion_forward\\n0.33492963377557666', 'word_score_table[\"반포한강공원에\"].cohesion_forward\\n0.33492963377557666\\n오히려 \\'반포한강공원\\'보다 응집도가 낮아집니다. 결국 결합도는 \\'반포한강공원\\'일 때가 가장 높았습니다. 응집도를 통해 판단하기에 하나의 단어로 판단하기에 가장 적합한 문자열은 \\'반포한강공원\\'이라고 볼 수 있겠습니다.']\n",
      "[\"Branching Entropy는 확률 분포의 엔트로피값을 사용합니다. 이는 주어진 문자열에서 얼마나 다음 문자가 등장할 수 있는지를 판단하는 척도입니다. 퀴즈를 하나 내보겠습니다. 제가 어떤 단어를 생각 중인데, 한 문자씩 말해드릴테니까 매번 다음 문자를 맞추는 것이 퀴즈입니다. 첫번째 문자는 '디'입니다. 다음에 등장할 문자를 맞춰보세요. 솔직히 가늠이 잘 안 가지요? '디'로 시작하는 단어가 얼마나 많은데요. 정답은 '스' 입니다.\\n이제 '디스'까지 나왔네요. '디스 '다음 문자는 뭘까요? '디스카운트'라는 단어가 있으니까 '카'일까? 아니면 '디스코드'라는 단어가 있으니까 '코'인가? 생각해보니 '디스코'가 정답일 수도 있겠네요. 그러면 '코'인가? '디스아너드'라는 게임이 있으니까 '아'? 이 단어들을 생각하신 분들은 전부 틀렸습니다. 정답은 '플'이었습니다.\", '\\'디스플\\'까지 왔습니다. 다음 문자 맞춰보세요. 이제 좀 명백해집니다. 정답은 \\'레\\'입니다. \\'디스플레\\' 다음에는 어떤 문자일까요? 정답은 \\'이\\'입니다. 제가 생각한 단어는 \\'디스플레이\\'였습니다.\\n브랜칭 엔트로피를 주어진 문자 시퀀스에서 다음 문자 예측을 위해 헷갈리는 정도로 비유해봅시다. 브랜칭 엔트로피의 값은 하나의 완성된 단어에 가까워질수록 문맥으로 인해 점점 정확히 예측할 수 있게 되면서 점점 줄어드는 양상을 보입니다.\\nword_score_table[\"디스\"].right_branching_entropy\\n1.6371694761537934\\nword_score_table[\"디스플\"].right_branching_entropy\\n-0.0\\n\\'디스\\' 다음에는 다양한 문자가 올 수 있으니까 1.63이라는 값을 가지는 반면, \\'디스플\\'이라는 문자열 다음에는 다음 문자로 \\'레\\'가 오는 것이 너무나 명백하기 때문에 0이란 값을 가집니다.', 'word_score_table[\"디스플레\"].right_branching_entropy\\n-0.0\\nword_score_table[\"디스플레이\"].right_branching_entropy\\n3.1400392861792916\\n갑자기 값이 증가합니다. 그 이유는 문자 시퀀스 \\'디스플레이\\'라는 문자 시퀀스 다음에는 조사나 다른 단어와 같은 다양한 경우가 있을 수 있기 때문입니다. 이는 하나의 단어가 끝나면 그 경계 부분부터 다시 브랜칭 엔트로피 값이 증가하게 됨을 의미합니다. 그리고 이 값으로 단어를 판단하는 것이 가능하겠죠?']\n",
      "['한국어는 띄어쓰기 단위로 나눈 어절 토큰은 주로 L 토큰 + R 토큰의 형식을 가질 때가 많습니다. 예를 들어서 \\'공원에\\'는 \\'공원 + 에\\'로 나눌 수 있겠지요. 또는 \\'공부하는\\'은 \\'공부 + 하는\\'으로 나눌 수도 있을 것입니다. L 토크나이저는 L 토큰 + R 토큰으로 나누되, 분리 기준을 점수가 가장 높은 L 토큰을 찾아내는 원리를 가지고 있습니다.\\nfrom soynlp.tokenizer import LTokenizer\\nscores = {word:score.cohesion_forward for word, score in word_score_table.items()}\\nl_tokenizer = LTokenizer(scores=scores)\\nl_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False)\\n[(\\'국제사회\\', \\'와\\'), (\\'우리\\', \\'의\\'), (\\'노력\\', \\'들로\\'), (\\'범죄\\', \\'를\\'), (\\'척결\\', \\'하자\\')]']\n",
      "['최대 점수 토크나이저는 띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저입니다. 띄어쓰기가 되어 있지 않은 문장을 넣어서 점수를 통해 토큰화 된 결과를 보겠습니다.\\nfrom soynlp.tokenizer import MaxScoreTokenizer\\nmaxscore_tokenizer = MaxScoreTokenizer(scores=scores)\\nmaxscore_tokenizer.tokenize(\"국제사회와우리의노력들로범죄를척결하자\")\\n[\\'국제사회\\', \\'와\\', \\'우리\\', \\'의\\', \\'노력\\', \\'들로\\', \\'범죄\\', \\'를\\', \\'척결\\', \\'하자\\']']\n",
      "[\"SNS나 채팅 데이터와 같은 한국어 데이터의 경우에는 ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우가 많은데 ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋ와 같은 경우를 모두 서로 다른 단어로 처리하는 것은 불필요합니다. 이에 반복되는 것은 하나로 정규화시켜줍니다.\\nfrom soynlp.normalizer import *\\nprint(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))\\nprint(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))\\nprint(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))\\nprint(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))\\n아ㅋㅋ영화존잼쓰ㅠㅠ\\n아ㅋㅋ영화존잼쓰ㅠㅠ\\n아ㅋㅋ영화존잼쓰ㅠㅠ\\n아ㅋㅋ영화존잼쓰ㅠㅠ\", \"아ㅋㅋ영화존잼쓰ㅠㅠ\\n아ㅋㅋ영화존잼쓰ㅠㅠ\\n아ㅋㅋ영화존잼쓰ㅠㅠ\\n아ㅋㅋ영화존잼쓰ㅠㅠ\\n의미없게 반복되는 것은 비단 이모티콘에 한정되지 않습니다.\\nprint(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))\\nprint(repeat_normalize('와하하하하하하핫', num_repeats=2))\\nprint(repeat_normalize('와하하하하핫', num_repeats=2))\\n와하하핫\\n와하하핫\\n와하하핫\"]\n",
      "[\"영어권 언어는 띄어쓰기만해도 단어들이 잘 분리되지만, 한국어는 그렇지 않다고 앞에서 몇 차례 언급했었습니다. 한국어 데이터를 사용하여 모델을 구현하는 것만큼 이번에는 형태소 분석기를 사용해서 단어 토큰화를 해보겠습니다. 그런데 형태소 분석기를 사용할 때, 이런 상황에 봉착한다면 어떻게 해야할까요?\\n형태소 분석 입력 : '은경이는 사무실로 갔습니다.'\\n형태소 분석 결과 : ['은', '경이', '는', '사무실', '로', '갔습니다', '.']\\n사실 위 문장에서 '은경이'는 사람 이름이므로 제대로 된 결과를 얻기 위해서는 '은', '경이'와 같이 글자가 분리되는 것이 아니라 '은경이' 또는 최소한 '은경'이라는 단어 토큰을 얻어야만 합니다. 이런 경우에는 형태소 분석기에 사용자 사전을 추가해줄 수 있습니다. '은경이'는 하나의 단어이기 때문에 분리하지말라고 형태소 분석기에 알려주는 것입니다.\", \"사용자 사전을 추가하는 방법은 형태소 분석기마다 다른데, 생각보다 복잡한 경우들이 많습니다. 이번 실습에서는 Customized Konlpy라는 사용자 사전 추가가 매우 쉬운 패키지를 사용합니다.\\npip install customized_konlpy\\ncustomized_konlpy에서 제공하는 형태소 분석기 Twitter를 사용하여 앞서 소개했던 예문을 단어 토큰화해봅시다.\\nfrom ckonlpy.tag import Twitter\\ntwitter = Twitter()\\ntwitter.morphs('은경이는 사무실로 갔습니다.')\\n['은', '경이', '는', '사무실', '로', '갔습니다', '.']\\n앞서 소개한 예시와 마찬가지로 '은경이'라는 단어가 '은', '경이'와 같이 분리됩니다. 이때, 형태소 분석기 Twitter에 add_dictionary('단어', '품사')와 같은 형식으로 사전 추가를 해줄 수 있습니다.\", \"twitter.add_dictionary('은경이', 'Noun')\\n제대로 반영되었는지 동일한 예문을 다시 형태소 분석해봅시다.\\ntwitter.morphs('은경이는 사무실로 갔습니다.')\\n['은경이', '는', '사무실', '로', '갔습니다', '.']\\n'은경이'라는 단어가 제대로 하나의 토큰으로 인식되는 것을 확인할 수 있습니다.\\n==================================================\\n--- 03. 언어 모델(Language Model) ---\\n마지막 편집일시 : 2022년 12월 15일 7:09 오전\\n==================================================\\n--- 03-01 언어 모델(Language Model)이란? ---\\n```\\nP(나는 메롱을 먹는다) < P(나는 메론을 먹는다)\", '--- 03-01 언어 모델(Language Model)이란? ---\\n```\\nP(나는 메롱을 먹는다) < P(나는 메론을 먹는다)\\n```언어 모델(Language Model, LM)은 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델입니다.\\n언어 모델을 만드는 방법은 크게는 통계를 이용한 방법과 인공 신경망을 이용한 방법으로 구분할 수 있습니다. 최근에는 통계를 이용한 방법보다는 인공 신경망을 이용한 방법이 더 좋은 성능을 보여주고 있습니다. 최근 핫한 자연어 처리의 기술인 GPT나 BERT 또한 인공 신경망 언어 모델의 개념을 사용하여 만들어졌습니다. 이번 챕터에서는 언어 모델의 개념과 언어 모델의 전통적 접근 방식인 통계적 언어 모델에 대해서 배웁니다.']\n",
      "['언어 모델은 단어 시퀀스에 확률을 할당(assign) 하는 일을 하는 모델입니다. 이를 조금 풀어서 쓰면, 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델입니다. 단어 시퀀스에 확률을 할당하게 하기 위해서 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것입니다.\\n다른 유형의 언어 모델로는 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델이 있습니다. 이는 문장의 가운데에 있는 단어를 비워놓고 양쪽의 문맥을 통해서 빈 칸의 단어인지 맞추는 고등학교 수험 시험의 빈칸 추론 문제와 비슷합니다. 이 유형의 언어 모델은 BERT 챕터에서 다루게 될 예정이고, 그때까지는 이전 단어들로부터 다음 단어를 예측하는 방식에만 집중합니다.', '언어 모델에 -ing를 붙인 언어 모델링(Language Modeling)은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 말합니다. 즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일은 언어 모델링입니다.\\n자연어 처리로 유명한 스탠포드 대학교에서는 언어 모델을 문법(grammar)이라고 비유하기도 합니다. 언어 모델이 단어들의 조합이 얼마나 적절한지, 또는 해당 문장이 얼마나 적합한지를 알려주는 일을 하는 것이 마치 문법이 하는 일 같기 때문입니다.']\n",
      "['자연어 처리에서 단어 시퀀스에 확률을 할당하는 일이 왜 필요할까요? 예를 들어보겠습니다. 여기서 대문자 P는 확률을 의미합니다.\\na. 기계 번역(Machine Translation):\\nP(나는 버스를 탔다) > P(나는 버스를 태운다)\\n: 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다.\\nb. 오타 교정(Spell Correction)\\n선생님이 교실로 부리나케\\nP(달려갔다) > P(잘려갔다)\\n: 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다.\\nc. 음성 인식(Speech Recognition)\\nP(나는 메롱을 먹는다) < P(나는 메론을 먹는다)\\n: 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단합니다.\\n언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단합니다.']\n",
      "['언어 모델은 단어 시퀀스에 확률을 할당하는 모델입니다. 그리고 단어 시퀀스에 확률을 할당하기 위해서 가장 보편적으로 사용하는 방법은 이전 단어들이 주어졌을 때, 다음 단어를 예측하도록 하는 것입니다. 이를 조건부 확률로 표현해보겠습니다.\\nA. 단어 시퀀스의 확률\\n하나의 단어를 $w$, 단어 시퀀스를 대문자 $W$라고 한다면, $n$개의 단어가 등장하는 단어 시퀀스 $W$의 확률은 다음과 같습니다.\\n$$P(W) = P(w_1, w_2, w_3, w_4, w_5, ... ,w_n)$$\\nB. 다음 단어 등장 확률\\n다음 단어 등장 확률을 식으로 표현해보겠습니다. $n$-1개의 단어가 나열된 상태에서 $n$번째 단어의 확률은 다음과 같습니다.\\n$$P(w_n | w_1, ..., w_{n-1}) $$\\n|의 기호는 조건부 확률(conditional probability)을 의미합니다.\\n예를 들어 다섯번째 단어의 확률은 아래와 같습니다.\\n$$P(w_5 | w_1, w_2, w_3, w_4) $$', '예를 들어 다섯번째 단어의 확률은 아래와 같습니다.\\n$$P(w_5 | w_1, w_2, w_3, w_4) $$\\n전체 단어 시퀀스 $W$의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 단어 시퀀스의 확률은 다음과 같습니다.\\n$$P(W) = P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \\\\prod_{i=1}^{n}P(w_{i} | w_{1}, ... , w_{i-1})$$']\n",
      "[\"비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [?]라는 문장이 있습니다. '비행기를' 다음에 어떤 단어가 오게 될지 사람은 쉽게 '놓쳤다'라고 예상할 수 있습니다. 우리 지식에 기반하여 나올 수 있는 여러 단어들을 후보에 놓고 놓쳤다는 단어가 나올 확률이 가장 높다고 판단하였기 때문입니다.\\n그렇다면 기계에게 위 문장을 주고, '비행기를' 다음에 나올 단어를 예측해보라고 한다면 과연 어떻게 최대한 정확히 예측할 수 있을까요? 기계도 비슷합니다. 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고 가장 높은 확률을 가진 단어를 선택합니다. 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 등장 확률을 추정하고 가장 높은 확률을 가진 단어를 선택합니다.\"]\n",
      "['[이미지: ]\\n검색 엔진이 입력된 단어들의 나열에 대해서 다음 단어를 예측하는 언어 모델을 사용하고 있습니다.\\n==================================================\\n--- 03-02 통계적 언어 모델(Statistical Language Model, SLM) ---\\n언어 모델의 전통적인 접근 방법인 통계적 언어 모델을 소개합니다. 통계적 언어 모델이 통계적인 접근 방법으로 어떻게 언어를 모델링 하는지 배워보겠습니다. 통계적 언어 모델(Statistical Language Model)은 줄여서 SLM이라고 합니다.']\n",
      "['조건부 확률은 두 확률 $P(A), P(B)$에 대해서 아래와 같은 관계를 갖습니다.\\n$$ p(B|A) = P(A,B)/P(A) $$\\n$$ P(A,B) = P(A)P(B|A) $$\\n더 많은 확률에 대해서 일반화해봅시다. 4개의 확률이 조건부 확률의 관계를 가질 때, 아래와 같이 표현할 수 있습니다.\\n$$ P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C) $$\\n이를 조건부 확률의 연쇄 법칙(chain rule)이라고 합니다. 이제는 4개가 아닌 $n$개에 대해서 일반화를 해봅시다.\\n$$ P(x_1, x_2, x_3 ... x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1 ... x_{n-1}) $$\\n조건부 확률에 대한 정의를 통해 문장의 확률을 구해보겠습니다.']\n",
      "[\"문장 'An adorable little boy is spreading smiles'의 확률 $P(\\\\text{An adorable little boy is spreading smiles})$를 식으로 표현해봅시다.\\n각 단어는 문맥이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어입니다. 그리고 모든 단어로부터 하나의 문장이 완성됩니다. 그렇기 때문에 문장의 확률을 구하고자 조건부 확률을 사용하겠습니다. 앞서 언급한 조건부 확률의 일반화 식을 문장의 확률 관점에서 다시 적어보면 문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성됩니다.\\n$$P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \\\\prod_{n=1}^{n}P(w_{n} | w_{1}, ... , w_{n-1})$$\\n위의 문장에 해당 식을 적용해보면 다음과 같습니다.\\n$P(\\\\text{An adorable little boy is spreading smiles}) =$\", '위의 문장에 해당 식을 적용해보면 다음과 같습니다.\\n$P(\\\\text{An adorable little boy is spreading smiles}) =$\\n$P(\\\\text{An})  ×  P(\\\\text{adorable|An})  ×  P(\\\\text{little|An adorable})  ×  P(\\\\text{boy|An adorable little})\\n×  P(\\\\text{is|An adorable little boy})$\\n$×  P(\\\\text{spreading|An adorable little boy is})  ×  P(\\\\text{smiles|An adorable little boy is spreading})$\\n문장의 확률을 구하기 위해서 각 단어에 대한 예측 확률들을 곱합니다.']\n",
      "['문장의 확률을 구하기 위해서 다음 단어에 대한 예측 확률을 모두 곱한다는 것은 알았습니다. 그렇다면 SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까요? 정답은 카운트에 기반하여 확률을 계산합니다.\\nAn adorable little boy가 나왔을 때, is가 나올 확률인 $P(\\\\text{is|An adorable little boy})$를 구해봅시다.\\n$$P\\\\text{(is|An adorable little boy}) = \\\\frac{\\\\text{count(An adorable little boy is})}{\\\\text{count(An adorable little boy })}$$\\n그 확률은 위와 같습니다. 예를 들어 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 합시다. 이 경우 $P(\\\\text{is|An adorable little boy})$는 30%입니다.']\n",
      "['언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링 합니다. 실제로 정확하게 알아볼 방법은 없겠지만 현실에서도 An adorable little boy가 나왔을 때 is가 나올 확률이라는 것이 존재합니다. 이를 실제 자연어의 확률 분포, 현실에서의 확률 분포라고 명칭합시다. 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표입니다. 그런데 카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스(corpus). 즉, 다시 말해 기계가 훈련하는 데이터는 정말 방대한 양이 필요합니다.\\n$$P\\\\text{(is|An adorable little boy}) = \\\\frac{\\\\text{count(An adorable little boy is})}{\\\\text{count(An adorable little boy })}$$', '예를 들어 위와 같이 $P\\\\text{(is|An adorable little boy})$를 구하는 경우에서 기계가 훈련한 코퍼스에 An adorable little boy is라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0이 됩니다. 또는 An adorable little boy라는 단어 시퀀스가 없었다면 분모가 0이 되어 확률은 정의되지 않습니다. 그렇다면 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것이 정확한 모델링 방법일까요? 아닙니다. 현실에선 An adorable little boy is 라는 단어 시퀀스가 존재하고 또 문법에도 적합하므로 정답일 가능성 또한 높습니다. 이와 같이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 희소 문제(sparsity problem)라고 합니다.', '위 문제를 완화하는 방법으로 바로 이어서 배우게 되는 n-gram 언어 모델이나 이 책에서 다루지는 않지만 스무딩이나 백오프와 같은 여러가지 일반화(generalization) 기법이 존재합니다. 하지만 희소 문제에 대한 근본적인 해결책은 되지 못하였습니다. 결국 이러한 한계로 인해 언어 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어가게 됩니다.\\n==================================================\\n--- 03-03 N-gram 언어 모델(N-gram Language Model) ---\\nn-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종입니다. 다만, 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용합니다. 그리고 이때 일부 단어를 몇 개 보느냐를 결정하는데 이것이 n-gram에서의 n이 가지는 의미입니다.']\n",
      "['SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점입니다. 그리고 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높습니다. 다시 말하면 카운트할 수 없을 가능성이 높습니다. 그런데 다음과 같이 참고하는 단어들을 줄이면 카운트를 할 수 있을 가능성을 높일 수 있습니다.\\n$P(\\\\text{is|An adorable little boy}) \\\\approx\\\\ P(\\\\text{is|boy})$\\n가령, An adorable little boy가 나왔을 때 is가 나올 확률을 그냥 boy가 나왔을 때 is가 나올 확률로 생각해보는 건 어떨까요? 갖고있는 코퍼스에 An adorable little boy is가 있을 가능성 보다는 boy is라는 더 짧은 단어 시퀀스가 존재할 가능성이 더 높습니다. 조금 지나친 일반화로 느껴진다면 아래와 같이 little boy가 나왔을 때 is가 나올 확률로 생각하는 것도 대안입니다.', '$P(\\\\text{is|An adorable little boy}) \\\\approx\\\\ P(\\\\text{is|little boy})$\\n즉, 앞에서는 An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 횟수와 An adorable little boy is가 나온 횟수를 카운트해야만 했지만, 이제는 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하자는 것입니다. 이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아집니다.']\n",
      "['이때 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram입니다. n-gram은 n개의 연속적인 단어 나열을 의미합니다. 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주합니다. 예를 들어서 문장 An adorable little boy is spreading smiles이 있을 때, 각 n에 대해서 n-gram을 전부 구해보면 다음과 같습니다.\\nunigrams : an, adorable, little, boy, is, spreading, smiles\\nbigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\\ntrigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles', '4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\\nn-gram을 사용할 때는 n이 1일 때는 유니그램(unigram), 2일 때는 바이그램(bigram), 3일 때는 트라이그램(trigram)이라고 명명하고 n이 4 이상일 때는 gram 앞에 그대로 숫자를 붙여서 명명합니다. 출처에 따라서는 유니그램, 바이그램, 트라이그램 또한 각각 1-gram, 2-gram, 3-gram이라고 하기도 합니다. n-gram을 이용한 언어 모델을 설계해보겠습니다.', \"n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존합니다. 예를 들어 'An adorable little boy is spreading' 다음에 나올 단어를 예측하고 싶다고 할 때, n=4라고 한 4-gram을 이용한 언어 모델을 사용한다고 합시다. 이 경우, spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려합니다.\\n[이미지: ]\\n$$P(w\\\\text{|boy is spreading}) = \\\\frac{\\\\text{count(boy is spreading}\\\\ w)}{\\\\text{count(boy is spreading)}}$$\", '만약 갖고있는 코퍼스에서 boy is spreading가 1,000번 등장했다고 합시다. 그리고 boy is spreading insults가 500번 등장했으며, boy is spreading smiles가 200번 등장했다고 합시다. 그렇게 되면 boy is spreading 다음에 insults가 등장할 확률은 50%이며, smiles가 등장할 확률은 20%입니다. 확률적 선택에 따라 우리는 insults가 더 맞다고 판단하게 됩니다.\\n$$P(\\\\text{insults|boy is spreading}) = 0.500$$\\n$$P(\\\\text{smiles|boy is spreading}) = 0.200$$']\n",
      "[\"앞서 4-gram을 통한 언어 모델의 동작 방식을 확인했습니다. 그런데 조금 의문이 남습니다. 앞서 본 4-gram 언어 모델은 주어진 문장에서 앞에 있던 단어인 '작고 사랑스러운(an adorable little)'이라는 수식어를 제거하고, 반영하지 않았습니다. 그런데 '작고 사랑스러운' 수식어까지 모두 고려하여 작고 사랑하는 소년이 하는 행동에 대해 다음 단어를 예측하는 언어 모델이었다면 과연 '작고 사랑스러운 소년이' '모욕을 퍼트렸다'라는 부정적인 내용이 '웃음 지었다'라는 긍정적인 내용 대신 선택되었을까요?\", '물론 코퍼스 데이터를 어떻게 가정하느냐의 나름이고, 전혀 말이 안 되는 문장은 아니지만 여기서 지적하고 싶은 것은 n-gram은 앞의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생긴다는 점입니다. 문장을 읽다 보면 앞 부분과 뒷부분의 문맥이 전혀 연결 안 되는 경우도 생길 수 있습니다. 결론만 말하자면, 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없습니다. 이를 토대로 n-gram 모델에 대한 한계점을 정리해보겠습니다.\\n(1) 희소 문제(Sparsity Problem)\\n문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재합니다.\\n(2) n을 선택하는 것은 trade-off 문제.', '(2) n을 선택하는 것은 trade-off 문제.\\n앞에서 몇 개의 단어를 볼지 n을 정하는 것은 trade-off가 존재합니다. 임의의 개수인 n을 1보다는 2로 선택하는 것은 거의 대부분의 경우에서 언어 모델의 성능을 높일 수 있습니다. 가령, spreading만 보는 것보다는 is spreading을 보고 다음 단어를 예측하는 것이 더 정확하기 때문입니다. 이 경우 훈련 데이터가 적절한 데이터였다면 언어 모델이 적어도 spreading 다음에 동사를 고르지 않을 것입니다.\\nn을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해집니다. 또한 n이 커질수록 모델 사이즈가 커진다는 문제점도 있습니다. 기본적으로 코퍼스의 모든 n-gram에 대해서 카운트를 해야 하기 때문입니다.', 'n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어집니다. 그렇기 때문에 적절한 n을 선택해야 합니다. 앞서 언급한 trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있습니다.\\nn이 성능에 영향을 주는 것을 확인할 수 있는 유명한 예제 하나를 보겠습니다. 스탠퍼드 대학교의 공유 자료에 따르면, 월스트리트 저널에서 3,800만 개의 단어 토큰에 대하여 n-gram 언어 모델을 학습하고, 1,500만 개의 테스트 데이터에 대해서 테스트를 했을 때 다음과 같은 성능이 나왔다고 합니다. 뒤에서 배우겠지만, 펄플렉서티(perplexity)는 수치가 낮을수록 더 좋은 성능을 나타냅니다.\\nUnigram\\nBigram\\nTrigram\\nPerplexity\\n962\\n170\\n109\\n위의 결과는 n을 1에서 2, 2에서 3으로 올릴 때마다 성능이 올라가는 것을 보여줍니다.']\n",
      "['어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 당연히 다릅니다. 가령, 마케팅 분야에서는 마케팅 단어가 빈번하게 등장할 것이고, 의료 분야에서는 의료 관련 단어가 당연히 빈번하게 등장합니다. 이 경우 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아집니다.\\n때로는 이를 언어 모델의 약점이라고 하는 경우도 있는데, 훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라지기 때문입니다.']\n",
      "['여기서는 다루지 않겠지만, N-gram Language Model의 한계점을 극복하기위해 분모, 분자에 숫자를 더해서 카운트했을 때 0이 되는 것을 방지하는 등의 여러 일반화(generalization) 방법들이 존재합니다. 하지만 그럼에도 본질적으로 n-gram 언어 모델에 대한 취약점을 완전히 해결하지는 못하였고, 이를 위한 대안으로 N-gram Language Model보다 대체적으로 성능이 우수한 인공 신경망을 이용한 언어 모델이 많이 사용되고 있습니다.\\n==================================================\\n--- 03-04 한국어에서의 언어 모델(Language Model for Korean Sentences) ---\\n```']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['```영어나 기타 언어에 비해서 한국어는 언어 모델로 다음 단어를 예측하기가 훨씬 까다롭습니다.']\n",
      "['한국어에서는 어순이 중요하지 않습니다. 그래서 이전 단어가 주어졌을때, 다음 단어가 나타날 확률을 구해야하는데 어순이 중요하지 않다는 것은 다음 단어로 어떤 단어든 등장할 수 있다는 의미입니다.\\n예를 들어보겠습니다.']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\"4개의 문장은 전부 의미가 통하는 것을 볼 수 있습니다. 심지어 '나는' 이라는 주어를 생략해도 말이 되버립니다. 이렇게 단어 순서를 뒤죽박죽으로 바꾸어놔도 한국어는 의미가 전달 되기 때문에 확률에 기반한 언어 모델이 제대로 다음 단어를 예측하기가 어렵습니다.\"]\n",
      "[\"한국어는 교착어입니다. 이는 한국어에서의 언어 모델 작동을 어렵게 만듭니다. 띄어쓰기 단위인 어절 단위로 토큰화를 할 경우에는 문장에서 발생가능한 단어의 수가 굉장히 늘어납니다. 대표적인 예로 교착어인 한국어에는 조사가 있습니다. 영어는 기본적으로 조사가 없습니다. 하지만 한국어에는 어떤 행동을 하는 동사의 주어나 목적어를 위해서 조사라는 것이 있습니다.\\n가령 '그녀'라는 단어 하나만 해도 그녀가, 그녀를, 그녀의, 그녀와, 그녀로, 그녀께서, 그녀처럼 등과 같이 다양한 경우가 존재합니다. 그렇기 때문에, 한국어에서는 토큰화를 통해 접사나 조사 등을 분리하는 것은 중요한 작업이 되기도 합니다.\"]\n",
      "['한국어는 띄어쓰기를 제대로 하지 않아도 의미가 전달되며, 띄어쓰기 규칙 또한 상대적으로 까다로운 언어이기 때문에 자연어 처리를 하는 것에 있어서 한국어 코퍼스는 띄어쓰기가 제대로 지켜지지 않는 경우가 많습니다. 토큰이 제대로 분리 되지 않는채 훈련 데이터로 사용된다면 언어 모델은 제대로 동작하지 않습니다.\\n==================================================\\n--- 03-05 펄플렉서티(Perplexity, PPL) ---', '==================================================\\n--- 03-05 펄플렉서티(Perplexity, PPL) ---\\n두 개의 모델 A, B가 있을 때 이 모델의 성능은 어떻게 비교할 수 있을까요? 두 개의 모델을 오타 교정, 기계 번역 등의 평가에 투입해볼 수 있겠습니다. 그리고 두 모델이 해당 업무의 성능을 누가 더 잘했는지를 비교하면 되겠습니다. 그런데 두 모델의 성능을 비교하고자, 일일히 모델들에 대해서 실제 작업을 시켜보고 정확도를 비교하는 작업은 공수가 너무 많이 드는 작업입니다. 만약 비교해야하는 모델이 두 개가 아니라 그 이상의 수라면 시간은 비교해야하는 모델의 수만큼 배로 늘어날 수도 있습니다.\\n이러한 평가보다는 어쩌면 조금은 부정확할 수는 있어도 테스트 데이터에 대해서 빠르게 식으로 계산되는 더 간단한 평가 방법이 있습니다. 바로 모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 펄플렉서티(perplexity)입니다.']\n",
      "[\"펄플렉서티(perplexity)는 언어 모델을 평가하기 위한 평가 지표입니다. 보통 줄여서 PPL이 라고 표현합니다. 왜 perplexity라는 용어를 사용했을까요? 영어에서 'perplexed'는 '헷갈리는'과 유사한 의미를 가집니다. 그러니까 여기서 PPL은 '헷갈리는 정도'로 이해합시다. PPL를 처음 배울때 다소 낯설게 느껴질 수 있는 점이 있다면, PPL은 수치가 높으면 좋은 성능을 의미하는 것이 아니라, '낮을수록' 언어 모델의 성능이 좋다는 것을 의미한다는 점입니다.\\nPPL은 문장의 길이로 정규화된 문장 확률의 역수입니다. 문장 $W$의 길이가 $N$이라고 하였을 때의 PPL은 다음과 같습니다.\\n$$PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\\\\frac{1}{N}}=\\\\sqrt[N]{\\\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}$$\", '문장의 확률에 체인룰(chain rule)을 적용하면 아래와 같습니다.\\n$$PPL(W)=\\\\sqrt[N]{\\\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}=\\\\sqrt[N]{\\\\frac{1}{\\\\prod_{i=1}^{N}P(w_{i}| w_{1}, w_{2}, ... , w_{i-1})}}$$\\n여기에 n-gram을 적용해볼 수도 있습니다. 예를 들어 bigram 언어 모델의 경우에는 식이 아래와 같습니다.\\n$$PPL(W)=\\\\sqrt[N]{\\\\frac{1}{\\\\prod_{i=1}^{N}P(w_{i}| w_{i-1})}}$$']\n",
      "['PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor)입니다. PPL은 이 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지를 의미합니다. 가령, 언어 모델에 어떤 테스트 데이터을 주고 측정했더니 PPL이 10이 나왔다고 해봅시다. 그렇다면 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time step)마다 평균 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있습니다. 같은 테스트 데이터에 대해서 두 언어 모델의 PPL을 각각 계산 후에 PPL의 값을 비교하면, 두 언어 모델 중 PPL이 더 낮은 언어 모델의 성능이 더 좋다고 볼 수 있습니다.\\n$$PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\\\\frac{1}{N}}=(\\\\frac{1}{10}^{N})^{-\\\\frac{1}{N}}=\\\\frac{1}{10}^{-1}=10$$', '단, 평가 방법에 있어서 주의할 점은 PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보인다는 것이지, 사람이 직접 느끼기에 좋은 언어 모델이라는 것을 반드시 의미하진 않는다는 점입니다.  또한 언어 모델의 PPL은 테스트 데이터에 의존하므로 두 개 이상의 언어 모델을 비교할 때는 정량적으로 양이 많고, 또한 도메인에 알맞은 동일한 테스트 데이터를 사용해야 신뢰도가 높다는 것입니다.']\n",
      "['페이스북 AI 연구팀은 앞서 배운 n-gram 언어 모델과 이후 배우게 될 딥 러닝을 이용한 언어 모델에 대해서 PPL로 성능 테스트를 한 표를 공개한 바 있습니다.\\n[이미지: ]\\n링크 : https://engineering.fb.com/2016/10/25/ml-applications/building-an-efficient-neural-language-model-over-a-billion-words/', '표에서 맨 위의 줄의 언어 모델이 n-gram을 이용한 언어 모델이며 PPL이 67.6으로 측정되었습니다. 5-gram을 사용하였으며, 5-gram 앞에 Interpolated Kneser-Ney라는 이름이 붙었는데 이 책에서는 별도 설명을 생략하겠다고 했던 일반화(generalization) 방법이 사용된 모델입니다. 반면, 그 아래의 모델들은 인공 신경망을 이용한 언어 모델들로 페이스북 AI 연구팀이 자신들의 언어 모델을 다른 언어 모델과 비교하고자 하는 목적으로 기록하였습니다. 아직 RNN과 LSTM 등이 무엇인지 배우지는 않았지만, 인공 신경망을 이용한 언어 모델들은 대부분 n-gram을 이용한 언어 모델보다 더 좋은 성능 평가를 받았음을 확인할 수 있습니다.\\n==================================================\\n--- 03-06 조건부 확률(Conditional Probability) ---', '==================================================\\n--- 03-06 조건부 확률(Conditional Probability) ---\\n마지막 편집일시 : 2022년 11월 14일 2:45 오후\\n==================================================\\n--- 04. 카운트 기반의 단어 표현(Count based word Representation) ---\\n마지막 편집일시 : 2021년 12월 22일 3:18 오후\\n==================================================\\n--- 04-01 다양한 단어의 표현 방법 ---\\n여기서는 카운트 기반의 단어 표현 방법 외에도 다양한 단어의 표현 방법에는 어떤 것이 있으며, 앞으로 이 책에서는 어떤 순서로 단어 표현 방법을 학습하게 될 것인지에 대해서 먼저 설명합니다.']\n",
      "['단어의 표현 방법은 크게 국소 표현(Local Representation) 방법과 분산 표현(Distributed Representation) 방법으로 나뉩니다. 국소 표현 방법은 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법이며, 분산 표현 방법은 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법입니다.', '예를 들어 puppy(강아지), cute(귀여운), lovely(사랑스러운)라는 단어가 있을 때 각 단어에 1번, 2번, 3번 등과 같은 숫자를 맵핑(mapping)하여 부여한다면 이는 국소 표현 방법에 해당됩니다. 반면, 분산 표현 방법의 예를 하나 들어보면 해당 단어를 표현하기 위해 주변 단어를 참고합니다. puppy(강아지)라는 단어 근처에는 주로 cute(귀여운), lovely(사랑스러운)이라는 단어가 자주 등장하므로, puppy라는 단어는 cute, lovely한 느낌이다로 단어를 정의합니다. 이렇게 되면 이 두 방법의 차이는 국소 표현 방법은 단어의 의미, 뉘앙스를 표현할 수 없지만, 분산 표현 방법은 단어의 뉘앙스를 표현할 수 있게 됩니다.', '또한 비슷한 의미로 국소 표현 방법(Local Representation)을 이산 표현(Discrete Representation)이라고도 하며, 분산 표현(Distributed Representation)을 연속 표현(Continuous Represnetation)이라고도 합니다.\\n추가 의견으로 구글의 연구원 토마스 미코로브(Tomas Mikolov)는 2016년에 한 발표에서 잠재 의미 분석(LSA)이나 잠재 디리클레 할당(LDA)과 같은 방법들은 단어의 의미를 표현할 수 있다는 점에서 연속 표현(Continuous Represnetation)이지만, 엄밀히 말해서 다른 접근의 방법론을 사용하고 있는 워드투벡터(Word2vec)와 같은 분산 표현(Distributed Representation)은 아닌 것으로 분류하여 연속 표현을 분산 표현을 포괄하고 있는 더 큰 개념으로 설명하기도 했습니다.']\n",
      "['이 책에서는 아래와 같은 기준으로 단어 표현을 카테고리화하여 작성되었습니다.\\n[이미지: ]\\n이번 챕터의 Bag of Words는 국소 표현에(Local Representation)에 속하며, 단어의 빈도수를 카운트(Count)하여 단어를 수치화하는 단어 표현 방법입니다. 이 챕터에서는 BoW와 그의 확장인 DTM(또는 TDM)에 대해서 학습하고, 이러한 빈도수 기반 단어 표현에 단어의 중요도에 따른 가중치를 줄 수 있는 TF-IDF에 대해서 학습합니다.\\n워드 임베딩 챕터에서는 연속 표현(Continuous Representation)에 속하면서, 예측(prediction)을 기반으로 단어의 뉘앙스를 표현하는 워드투벡터(Word2Vec)와 그의 확장인 패스트텍스트(FastText)를 학습하고, 예측과 카운트라는 두 가지 방법이 모두 사용된 글로브(GloVe)에 대해서 학습합니다.\\n==================================================', \"==================================================\\n--- 04-02 Bag of Words(BoW) ---\\n```\\nbag of words vector : [[1 1 1 1]]\\nvocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\\n```단어의 등장 순서를 고려하지 않는 빈도수 기반의 단어 표현 방법인 Bag of Words에 대해서 학습합니다.\"]\n",
      "['Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다. Bag of Words를 직역하면 단어들의 가방이라는 의미입니다. 단어들이 들어있는 가방을 상상해봅시다. 갖고있는 어떤 텍스트 문서에 있는 단어들을 가방에다가 전부 넣습니다. 그 후에는 이 가방을 흔들어 단어들을 섞습니다. 만약, 해당 문서 내에서 특정 단어가 N번 등장했다면, 이 가방에는 그 특정 단어가 N개 있게됩니다. 또한 가방을 흔들어서 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않습니다.\\nBoW를 만드는 과정을 이렇게 두 가지 과정으로 생각해보겠습니다.\\n(1) 각 단어에 고유한 정수 인덱스를 부여합니다.  # 단어 집합 생성.\\n(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.\\n한국어 예제를 통해서 BoW에 대해서 이해해보도록 하겠습니다.', \"(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.\\n한국어 예제를 통해서 BoW에 대해서 이해해보도록 하겠습니다.\\n문서1 : 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\\n문서1에 대해서 BoW를 만들어보겠습니다. 아래의 함수는 입력된 문서에 대해서 단어 집합(vocaburary)을 만들어 각 단어에 정수 인덱스를 할당하고, BoW를 만듭니다.\\nfrom konlpy.tag import Okt\\nokt = Okt()\\ndef build_bag_of_words(document):\\n# 온점 제거 및 형태소 분석\\ndocument = document.replace('.', '')\\ntokenized_document = okt.morphs(document)\\nword_to_index = {}\\nbow = []\\nfor word in tokenized_document:\\nif word not in word_to_index.keys():\", 'word_to_index = {}\\nbow = []\\nfor word in tokenized_document:\\nif word not in word_to_index.keys():\\nword_to_index[word] = len(word_to_index)\\n# BoW에 전부 기본값 1을 넣는다.\\nbow.insert(len(word_to_index) - 1, 1)\\nelse:\\n# 재등장하는 단어의 인덱스\\nindex = word_to_index.get(word)\\n# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.\\nbow[index] = bow[index] + 1\\nreturn word_to_index, bow\\n해당 함수에 문서1을 입력으로 넣어봅시다.\\ndoc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\\nvocab, bow = build_bag_of_words(doc1)\\nprint(\\'vocabulary :\\', vocab)', \"vocab, bow = build_bag_of_words(doc1)\\nprint('vocabulary :', vocab)\\nprint('bag of words vector :', bow)\\nvocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\\nbag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\", 'bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\\n문서1에 각 단어에 대해서 인덱스를 부여한 결과는 첫번째 출력 결과입니다. 문서1의 BoW는 두번째 출력 결과입니다. 두번째 출력 결과를 보면, 인덱스 4에 해당하는 물가상승률은 두 번 언급되었기 때문에 인덱스 4에 해당하는 값이 2입니다. 인덱스는 0부터 시작됨에 주의합니다. 다시 말해 물가상승률은 BoW에서 다섯번째 값입니다. 만약, 한국어에서 불용어에 해당되는 조사들 또한 제거한다면 더 정제된 BoW를 만들 수도 있습니다.']\n",
      "[\"문서2 : 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\\n위의 함수에 임의의 문서2를 입력으로 하여 결과를 확인해봅시다.\\ndoc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\\nvocab, bow = build_bag_of_words(doc2)\\nprint('vocabulary :', vocab)\\nprint('bag of words vector :', bow)\\nvocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\\nbag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\\n문서1과 문서2를 합쳐서 문서 3이라고 명명하고, BoW를 만들 수도 있습니다.\", \"문서1과 문서2를 합쳐서 문서 3이라고 명명하고, BoW를 만들 수도 있습니다.\\n문서3: 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\\ndoc3 = doc1 + ' ' + doc2\\nvocab, bow = build_bag_of_words(doc3)\\nprint('vocabulary :', vocab)\\nprint('bag of words vector :', bow)\\nvocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\", 'bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\\n문서3의 단어 집합은 문서1과 문서2의 단어들을 모두 포함하고 있는 것들을 볼 수 있습니다. BoW는 종종 여러 문서의 단어 집합을 합친 뒤에, 해당 단어 집합에 대한 각 문서의 BoW를 구하기도 합니다. 가령, 문서3에 대한 단어 집합을 기준으로 문서1, 문서2의 BoW를 만든다고 한다면 결과는 아래와 같습니다.\\n문서3 단어 집합에 대한 문서1 BoW : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\\n문서3 단어 집합에 대한 문서2 BoW : [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]', \"문서3 단어 집합에 대한 문서2 BoW : [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]\\n문서3 단어 집합에서 물가상승률이라는 단어는 인덱스가 4에 해당됩니다. 물가상승률이라는 단어는 문서1에서는 2회 등장하며, 문서2에서는 1회 등장하였기 때문에 두 BoW의 인덱스 4의 값은 각각 2와 1이 되는 것을 볼 수 있습니다.\\nBoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이므로 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰입니다. 즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰입니다. 가령, '달리기', '체력', '근력'과 같은 단어가 자주 등장하면 해당 문서를 체육 관련 문서로 분류할 수 있을 것이며, '미분', '방정식', '부등식'과 같은 단어가 자주 등장한다면 수학 관련 문서로 분류할 수 있습니다.\"]\n",
      "[\"사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원합니다. 이를 이용하면 영어에 대해서는 손쉽게 BoW를 만들 수 있습니다. CountVectorizer로 간단하고 빠르게 BoW를 만드는 실습을 진행해보도록 하겠습니다.\\nfrom sklearn.feature_extraction.text import CountVectorizer\\ncorpus = ['you know I want your love. because I love you.']\\nvector = CountVectorizer()\\n# 코퍼스로부터 각 단어의 빈도수를 기록\\nprint('bag of words vector :', vector.fit_transform(corpus).toarray())\\n# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\\nprint('vocabulary :',vector.vocabulary_)\", \"# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\\nprint('vocabulary :',vector.vocabulary_)\\nbag of words vector : [[1 1 2 1 2 1]]\\nvocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\\n예제 문장에서 you와 love는 두 번씩 언급되었으므로 각각 인덱스 2와 인덱스 4에서 2의 값을 가지며, 그 외의 값에서는 1의 값을 가지는 것을 볼 수 있습니다. 또한 알파벳 I는 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문입니다. 정제(Cleaning) 챕터에서 언급했듯이, 영어에서는 길이가 짧은 문자를 제거하는 것 또한 전처리 작업으로 고려되기도 합니다.\", \"주의할 것은 CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다는 점입니다. 이는 영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없지만 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않음을 의미합니다.\\n예를 들어, 앞서 BoW를 만드는데 사용했던 '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.' 라는 문장을 CountVectorizer를 사용하여 BoW로 만들 경우, CountVectorizer는 '물가상승률'이라는 단어를 인식하지 못 합니다. CountVectorizer는 띄어쓰기를 기준으로 분리한 뒤에 '물가상승률과'와 '물가상승률은' 으로 조사를 포함해서 하나의 단어로 판단하기 때문에 서로 다른 두 단어로 인식합니다. 그리고 '물가상승률과'와 '물가상승률은'이 각자 다른 인덱스에서 1이라는 빈도의 값을 갖게 됩니다.\"]\n",
      "['앞서 불용어는 자연어 처리에서 별로 의미를 갖지 않는 단어들이라고 언급한 바 있습니다. BoW를 사용한다는 것은 그 문서에서 각 단어가 얼마나 자주 등장했는지를 보겠다는 것입니다. 그리고 각 단어에 대한 빈도수를 수치화 하겠다는 것은 결국 텍스트 내에서 어떤 단어들이 중요한지를 보고싶다는 의미를 함축하고 있습니다. 그렇다면 BoW를 만들때 불용어를 제거하는 일은 자연어 처리의 정확도를 높이기 위해서 선택할 수 있는 전처리 기법입니다.\\n영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원하고 있습니다.\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom nltk.corpus import stopwords\\n(1) 사용자가 직접 정의한 불용어 사용', 'from nltk.corpus import stopwords\\n(1) 사용자가 직접 정의한 불용어 사용\\ntext = [\"Family is not an important thing. It\\'s everything.\"]\\nvect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\\nprint(\\'bag of words vector :\\',vect.fit_transform(text).toarray())\\nprint(\\'vocabulary :\\',vect.vocabulary_)\\nbag of words vector : [[1 1 1 1 1]]\\nvocabulary : {\\'family\\': 1, \\'important\\': 2, \\'thing\\': 4, \\'it\\': 3, \\'everything\\': 0}\\n(2) CountVectorizer에서 제공하는 자체 불용어 사용', '(2) CountVectorizer에서 제공하는 자체 불용어 사용\\ntext = [\"Family is not an important thing. It\\'s everything.\"]\\nvect = CountVectorizer(stop_words=\"english\")\\nprint(\\'bag of words vector :\\',vect.fit_transform(text).toarray())\\nprint(\\'vocabulary :\\',vect.vocabulary_)\\nbag of words vector : [[1 1 1]]\\nvocabulary : {\\'family\\': 0, \\'important\\': 1, \\'thing\\': 2}\\n(3) NLTK에서 지원하는 불용어 사용\\ntext = [\"Family is not an important thing. It\\'s everything.\"]\\nstop_words = stopwords.words(\"english\")', 'stop_words = stopwords.words(\"english\")\\nvect = CountVectorizer(stop_words=stop_words)\\nprint(\\'bag of words vector :\\',vect.fit_transform(text).toarray())\\nprint(\\'vocabulary :\\',vect.vocabulary_)\\nbag of words vector : [[1 1 1 1]]\\nvocabulary : {\\'family\\': 1, \\'important\\': 2, \\'thing\\': 3, \\'everything\\': 0}\\n==================================================\\n--- 04-03 문서 단어 행렬(Document-Term Matrix, DTM) ---', '--- 04-03 문서 단어 행렬(Document-Term Matrix, DTM) ---\\n서로 다른 문서들의 BoW들을 결합한 표현 방법인 문서 단어 행렬(Document-Term Matrix, DTM) 표현 방법을 배워보겠습니다. 이하 DTM이라고 명명합니다. 행과 열을 반대로 선택하면 TDM이라고 부르기도 합니다. 이렇게 하면 서로 다른 문서들을 비교할 수 있게 됩니다.']\n",
      "['문서 단어 행렬(Document-Term Matrix, DTM)이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것을 말합니다. 쉽게 생각하면 각 문서에 대한 BoW를 하나의 행렬로 만든 것으로 생각할 수 있으며, BoW와 다른 표현 방법이 아니라 BoW 표현을 다수의 문서에 대해서 행렬로 표현하고 부르는 용어입니다. 예를 들어서 이렇게 4개의 문서가 있다고 합시다.\\n문서1 : 먹고 싶은 사과\\n문서2 : 먹고 싶은 바나나\\n문서3 : 길고 노란 바나나 바나나\\n문서4 : 저는 과일이 좋아요\\n띄어쓰기 단위 토큰화를 수행한다고 가정하고, 문서 단어 행렬로 표현하면 다음과 같습니다.\\n과일이\\n길고\\n노란\\n먹고\\n바나나\\n사과\\n싶은\\n저는\\n좋아요\\n문서1\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n0\\n문서2\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n문서3\\n0\\n1\\n1\\n0\\n2\\n0\\n0\\n0\\n0\\n문서4\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1', '싶은\\n저는\\n좋아요\\n문서1\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n0\\n문서2\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n문서3\\n0\\n1\\n1\\n0\\n2\\n0\\n0\\n0\\n0\\n문서4\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n각 문서에서 등장한 단어의 빈도를 행렬의 값으로 표기합니다. 문서 단어 행렬은 문서들을 서로 비교할 수 있도록 수치화할 수 있다는 점에서 의의를 갖습니다. 만약 필요에 따라서는 형태소 분석기로 단어 토큰화를 수행하고, 불용어에 해당되는 조사들 또한 제거하여 더 정제된 DTM을 만들 수도 있을 것입니다.']\n",
      "['DTM은 매우 간단하고 구현하기도 쉽지만, 본질적으로 가지는 몇 가지 한계들이 있습니다.\\n1) 희소 표현(Sparse representation)\\n원-핫 벡터는 단어 집합의 크기가 벡터의 차원이 되고 대부분의 값이 0이 되는 벡터입니다. 원-핫 벡터는 공간적 낭비와 계산 리소스를 증가시킬 수 있다는 점에서 단점을 가집니다. DTM도 마찬가지입니다. DTM에서의 각 행을 문서 벡터라고 해봅시다. 각 문서 벡터의 차원은 원-핫 벡터와 마찬가지로 전체 단어 집합의 크기를 가집니다. 만약 가지고 있는 전체 코퍼스가 방대한 데이터라면 문서 벡터의 차원은 수만 이상의 차원을 가질 수도 있습니다. 또한 많은 문서 벡터가 대부분의 값이 0을 가질 수도 있습니다. 당장 위에서 예로 들었던 문서 단어 행렬의 모든 행이 0이 아닌 값보다 0의 값이 더 많은 것을 볼 수 있습니다.', '원-핫 벡터나 DTM과 같은 대부분의 값이 0인 표현을 희소 벡터(sparse vector) 또는 희소 행렬(sparse matrix)라고 부르는데, 희소 벡터는 많은 양의 저장 공간과 높은 계산 복잡도를 요구합니다. 이러한 이유로 전처리를 통해 단어 집합의 크기를 줄이는 일은 BoW 표현을 사용하는 모델에서 중요할 수 있습니다. 앞서 배운 텍스트 전처리 방법을 사용하여 구두점, 빈도수가 낮은 단어, 불용어를 제거하고, 어간이나 표제어 추출을 통해 단어를 정규화하여 단어 집합의 크기를 줄일 수 있습니다.\\n2) 단순 빈도 수 기반 접근', '2) 단순 빈도 수 기반 접근\\n여러 문서에 등장하는 모든 단어에 대해서 빈도 표기를 하는 이런 방법은 때로는 한계를 가지기도 합니다. 예를 들어 영어에 대해서 DTM을 만들었을 때, 불용어인 the는 어떤 문서이든 자주 등장할 수 밖에 없습니다. 그런데 유사한 문서인지 비교하고 싶은 문서1, 문서2, 문서3에서 동일하게 the가 빈도수가 높다고 해서 이 문서들이 유사한 문서라고 판단해서는 안 됩니다.\\n각 문서에는 중요한 단어와 불필요한 단어들이 혼재되어 있습니다. 앞서 불용어(stopwords)와 같은 단어들은 빈도수가 높더라도 자연어 처리에 있어 의미를 갖지 못하는 단어라고 언급한 바 있습니다. 그렇다면 DTM에 불용어와 중요한 단어에 대해서 가중치를 줄 수 있는 방법은 없을까요? 이러한 아이디어를 적용한 TF-IDF를 이어서 학습해봅시다. 사이킷런의 CountVectorizer를 사용하여 DTM을 만드는 실습 또한 TF-IDF를 설명하면서 진행하겠습니다.', '==================================================\\n--- 04-04 TF-IDF(Term Frequency-Inverse Document Frequency) ---\\n```\\n[[0.         0.46735098 0.         0.46735098 0.         0.46735098 0.         0.35543247 0.46735098]\\n[0.         0.         0.79596054 0.         0.         0.         0.         0.60534851 0.        ]\\n[0.57735027 0.         0.         0.         0.57735027 0.         0.57735027 0.         0.        ]]', \"{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\\n```이번에는 DTM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치에 대해서 알아보겠습니다. TF-IDF를 사용하면, 기존의 DTM을 사용하는 것보다 보다 많은 정보를 고려하여 문서들을 비교할 수 있습니다. TF-IDF가 DTM보다 항상 좋은 성능을 보장하는 것은 아니지만, 많은 경우에서 DTM보다 더 좋은 성능을 얻을 수 있습니다.\"]\n",
      "['TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다. 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.\\nTF-IDF는 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있습니다.\\nTF-IDF는 TF와 IDF를 곱한 값을 의미하는데 이를 식으로 표현해보겠습니다. 문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때 TF, DF, IDF는 각각 다음과 같이 정의할 수 있습니다.\\n(1) tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.', '(1) tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\\n생소한 글자때문에 어려워보일 수 있지만, 잘 생각해보면 TF는 이미 앞에서 구한 적이 있습니다. TF는 앞에서 배운 DTM의 예제에서 각 단어들이 가진 값들입니다. DTM이 각 문서에서의 각 단어의 등장 빈도를 나타내는 값이었기 때문입니다.\\n(2) df(t) : 특정 단어 t가 등장한 문서의 수.\\n여기서 특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는 관심가지지 않으며 오직 특정 단어 t가 등장한 문서의 수에만 관심을 가집니다. 앞서 배운 DTM에서 바나나는 문서2와 문서3에서 등장했습니다. 이 경우, 바나나의 df는 2입니다. 문서3에서 바나나가 두 번 등장했지만, 그것은 중요한 게 아닙니다. 심지어 바나나란 단어가 문서2에서 100번 등장했고, 문서3에서 200번 등장했다고 하더라도 바나나의 df는 2가 됩니다.\\n(3) idf(t) : df(t)에 반비례하는 수.', '(3) idf(t) : df(t)에 반비례하는 수.\\n$$idf(t) = log(\\\\frac{n}{1+df(t)})$$\\nIDF라는 이름을 보고 DF의 역수가 아닐까 생각했다면, IDF는 DF의 역수를 취하고 싶은 것이 맞습니다. 그런데 log와 분모에 1을 더해주는 식에 의아하실 수 있습니다. log를 사용하지 않았을 때, IDF를 DF의 역수($\\\\frac{n}{df(t)}$라는 식)로 사용한다면 총 문서의 수 n이 커질 수록, IDF의 값은 기하급수적으로 커지게 됩니다. 그렇기 때문에 log를 사용합니다.\\n왜 log가 필요한지 n=1,000,000일 때의 예를 들어봅시다. log의 밑은 10을 사용한다고 가정하였을 때 결과는 아래와 같습니다.\\n$idf(t) = log(n/df(t))$\\n$n=1,000,000$\\n단어 $t$\\n$df(t)$\\n$idf(d, t)$\\nword1\\n1\\n6\\nword2\\n100\\n4\\nword3\\n1,000\\n3\\nword4\\n10,000\\n2\\nword5\\n100,000\\n1', '단어 $t$\\n$df(t)$\\n$idf(d, t)$\\nword1\\n1\\n6\\nword2\\n100\\n4\\nword3\\n1,000\\n3\\nword4\\n10,000\\n2\\nword5\\n100,000\\n1\\nword6\\n1,000,000\\n0\\n그렇다면 log를 사용하지 않으면 idf의 값이 어떻게 커지는지 보겠습니다.\\n$idf(t) = n/df(t)$\\n$n=1,000,000$\\n단어 $t$\\n$df(t)$\\n$idf(t)$\\nword1\\n1\\n1,000,000\\nword2\\n100\\n10,000\\nword3\\n1,000\\n1,000\\nword4\\n10,000\\n100\\nword5\\n100,000\\n10\\nword6\\n1,000,000\\n1', '1\\n1,000,000\\nword2\\n100\\n10,000\\nword3\\n1,000\\n1,000\\nword4\\n10,000\\n100\\nword5\\n100,000\\n10\\nword6\\n1,000,000\\n1\\n또 다른 직관적인 설명은 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장합니다. 그런데 비교적 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 또 최소 수백 배는 더 자주 등장하는 편입니다. 이 때문에 log를 씌워주지 않으면, 희귀 단어들에 엄청난 가중치가 부여될 수 있습니다. 로그를 씌우면 이런 격차를 줄이는 효과가 있습니다. log 안의 식에서 분모에 1을 더해주는 이유는 첫번째 이유로는 특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지하기 위함입니다.', 'TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다. TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것입니다. 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.\\n과일이\\n길고\\n노란\\n먹고\\n바나나\\n사과\\n싶은\\n저는\\n좋아요\\n문서1\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n0\\n문서2\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n문서3\\n0\\n1\\n1\\n0\\n2\\n0\\n0\\n0\\n0\\n문서4\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1', '0\\n0\\n문서2\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n문서3\\n0\\n1\\n1\\n0\\n2\\n0\\n0\\n0\\n0\\n문서4\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n앞서 DTM을 설명하기위해 들었던 위의 예제를 가지고 TF-IDF에 대해 이해해보겠습니다. 우선 TF는 앞서 사용한 DTM을 그대로 사용하면, 그것이 각 문서에서의 각 단어의 TF가 됩니다. 이제 구해야할 것은 TF와 곱해야할 값인 IDF입니다. 로그는 자연 로그를 사용하도록 하겠습니다. 자연 로그는 로그의 밑을 자연 상수 e(e=2.718281...)를 사용하는 로그를 말합니다. IDF 계산을 위해 사용하는 로그의 밑은 TF-IDF를 사용하는 사용자가 임의로 정할 수 있는데, 여기서 로그는 마치 기존의 값에 곱하여 값의 크기를 조절하는 상수의 역할을 합니다. 각종 프로그래밍 언어에서 패키지로 지원하는 TF-IDF의 로그는 대부분 자연 로그를 사용합니다. 여기서도 자연 로그를 사용하겠습니다. 자연 로그는 보통 log라고 표현하지 않고, ln이라고 표현합니다.', '단어\\nIDF(역 문서 빈도)\\n과일이\\nln(4/(1+1)) = 0.693147\\n길고\\nln(4/(1+1)) = 0.693147\\n노란\\nln(4/(1+1)) = 0.693147\\n먹고\\nln(4/(2+1)) = 0.287682\\n바나나\\nln(4/(2+1)) = 0.287682\\n사과\\nln(4/(1+1)) = 0.693147\\n싶은\\nln(4/(2+1)) = 0.287682\\n저는\\nln(4/(1+1)) = 0.693147\\n좋아요\\nln(4/(1+1)) = 0.693147', \"싶은\\nln(4/(2+1)) = 0.287682\\n저는\\nln(4/(1+1)) = 0.693147\\n좋아요\\nln(4/(1+1)) = 0.693147\\n문서의 총 수는 4이기 때문에 ln 안에서 분자는 늘 4으로 동일합니다. 분모의 경우에는 각 단어가 등장한 문서의 수(DF)를 의미하는데, 예를 들어서 '먹고'의 경우에는 총 2개의 문서(문서1, 문서2)에 등장했기 때문에 2라는 값을 가집니다. 각 단어에 대해서 IDF의 값을 비교해보면 문서 1개에만 등장한 단어와 문서2개에만 등장한 단어는 값의 차이를 보입니다. IDF는 여러 문서에서 등장한 단어의 가중치를 낮추는 역할을 하기 때문입니다.\\nTF-IDF를 계산해보겠습니다. 각 단어의 TF는 DTM에서의 각 단어의 값과 같으므로, 앞서 사용한 DTM에서 단어 별로 위의 IDF값을 곱해주면 TF-IDF 값을 얻습니다.\\n과일이\\n길고\\n노란\\n먹고\\n바나나\\n사과\\n싶은\\n저는\\n좋아요\\n문서1\\n0\\n0\\n0\\n0.287682\\n0\\n0.693147\\n0.287682\\n0\", '과일이\\n길고\\n노란\\n먹고\\n바나나\\n사과\\n싶은\\n저는\\n좋아요\\n문서1\\n0\\n0\\n0\\n0.287682\\n0\\n0.693147\\n0.287682\\n0\\n0\\n문서2\\n0\\n0\\n0\\n0.287682\\n0.287682\\n0\\n0.287682\\n0\\n0\\n문서3\\n0\\n0.693147\\n0.693147\\n0\\n0.575364\\n0\\n0\\n0\\n0\\n문서4\\n0.693147\\n0\\n0\\n0\\n0\\n0\\n0\\n0.693147\\n0.693147', '0.287682\\n0\\n0\\n문서3\\n0\\n0.693147\\n0.693147\\n0\\n0.575364\\n0\\n0\\n0\\n0\\n문서4\\n0.693147\\n0\\n0\\n0\\n0\\n0\\n0\\n0.693147\\n0.693147\\n사실 예제 문서가 굉장히 간단하기 때문에 계산은 매우 쉽습니다. 문서3에서의 바나나만 TF 값이 2이므로 IDF에 2를 곱해주고, 나머진 TF 값이 1이므로 그대로 IDF 값을 가져오면 됩니다. 문서2에서의 바나나의 TF-IDF 가중치와 문서3에서의 바나나의 TF-IDF 가중치가 다른 것을 볼 수 있습니다. 수식적으로 말하면, TF가 각각 1과 2로 달랐기 때문인데 TF-IDF에서의 관점에서 보자면 TF-IDF는 특정 문서에서 자주 등장하는 단어는 그 문서 내에서 중요한 단어로 판단하기 때문입니다. 문서2에서는 바나나를 한 번 언급했지만, 문서3에서는 바나나를 두 번 언급했기 때문에 문서3에서의 바나나를 더욱 중요한 단어라고 판단하는 것입니다.']\n",
      "[\"위의 계산 과정을 파이썬으로 직접 구현해보겠습니다. 앞의 설명에서 사용한 4개의 문서를 docs에 저장합니다.\\nimport pandas as pd # 데이터프레임 사용을 위해\\nfrom math import log # IDF 계산을 위해\\ndocs = [\\n'먹고 싶은 사과',\\n'먹고 싶은 바나나',\\n'길고 노란 바나나 바나나',\\n'저는 과일이 좋아요'\\n]\\nvocab = list(set(w for doc in docs for w in doc.split()))\\nvocab.sort()\\nTF, IDF, 그리고 TF-IDF 값을 구하는 함수를 구현합니다.\\n# 총 문서의 수\\nN = len(docs)\\ndef tf(t, d):\\nreturn d.count(t)\\ndef idf(t):\\ndf = 0\\nfor doc in docs:\\ndf += t in doc\\nreturn log(N/(df+1))\\ndef tfidf(t, d):\\nreturn tf(t,d)* idf(t)\", 'df = 0\\nfor doc in docs:\\ndf += t in doc\\nreturn log(N/(df+1))\\ndef tfidf(t, d):\\nreturn tf(t,d)* idf(t)\\nTF를 구해보겠습니다. 다시 말해 DTM을 데이터프레임에 저장하여 출력해보겠습니다.\\nresult = []\\n# 각 문서에 대해서 아래 연산을 반복\\nfor i in range(N):\\nresult.append([])\\nd = docs[i]\\nfor j in range(len(vocab)):\\nt = vocab[j]\\nresult[-1].append(tf(t, d))\\ntf_ = pd.DataFrame(result, columns = vocab)\\n[이미지: ]\\n정상적으로 DTM이 출력되었습니다. 각 단어에 대한 IDF 값을 구해봅시다.\\nresult = []\\nfor j in range(len(vocab)):\\nt = vocab[j]\\nresult.append(idf(t))', 'result = []\\nfor j in range(len(vocab)):\\nt = vocab[j]\\nresult.append(idf(t))\\nidf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\\nidf_\\n[이미지: ]\\n위에서 수기로 구한 IDF 값들과 정확히 일치합니다. TF-IDF 행렬을 출력해봅시다.\\nresult = []\\nfor i in range(N):\\nresult.append([])\\nd = docs[i]\\nfor j in range(len(vocab)):\\nt = vocab[j]\\nresult[-1].append(tfidf(t,d))\\ntfidf_ = pd.DataFrame(result, columns = vocab)\\ntfidf_\\n[이미지: ]', 'tfidf_\\n[이미지: ]\\nTF-IDF의 가장 기본적인 식에 대해서 학습하고 실제로 구현하는 실습을 진행해보았습니다. 사실 실제 TF-IDF 구현을 제공하고 있는 많은 머신 러닝 패키지들은 패키지마다 식이 조금씩 상이하지만, 위에서 배운 식과는 다른 조정된 식을 사용합니다. 그 이유는 위의 기본적인 식을 바탕으로 한 구현에는 몇 가지 문제점이 존재하기 때문입니다. 만약 전체 문서의 수 $n$이 4인데, $df(t)$의 값이 3인 경우에는 어떤 일이 벌어질까요? $df(t)$에 1이 더해지면서 log항의 분자와 분모의 값이 같아지게 됩니다. 이는 $log$의 진수값이 1이 되면서 $idf(d, t)$의 값이 0이 됨을 의미합니다. 식으로 표현하면 $idf(d, t) = log(n/(df(t)+1)) = 0$입니다. IDF의 값이 0이라면 더 이상 가중치의 역할을 수행하지 못합니다. 아래에서 실습할 사이킷런의 TF-IDF 구현체 또한 위의 식에서 조정된 식을 사용하고 있습니다.']\n",
      "[\"사이킷런을 통해 DTM과 TF-IDF를 만들어보겠습니다. BoW를 설명하며 배운 CountVectorizer를 사용하면 DTM을 만들 수 있습니다.\\nfrom sklearn.feature_extraction.text import CountVectorizer\\ncorpus = [\\n'you know I want your love',\\n'I like you',\\n'what should I do ',\\n]\\nvector = CountVectorizer()\\n# 코퍼스로부터 각 단어의 빈도수를 기록\\nprint(vector.fit_transform(corpus).toarray())\\n# 각 단어와 맵핑된 인덱스 출력\\nprint(vector.vocabulary_)\\n[[0 1 0 1 0 1 0 1 1]\\n[0 0 1 0 0 0 0 1 0]\\n[1 0 0 0 1 0 1 0 0]]\", \"print(vector.vocabulary_)\\n[[0 1 0 1 0 1 0 1 1]\\n[0 0 1 0 0 0 0 1 0]\\n[1 0 0 0 1 0 1 0 0]]\\n{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\\nDTM이 완성되었습니다. DTM에서 각 단어의 인덱스가 어떻게 부여되었는지를 확인하기 위해, 인덱스를 확인해보았습니다. 첫번째 열의 경우에는 0의 인덱스를 가진 do입니다. do는 세번째 문서에만 등장했기 때문에, 세번째 행에서만 1의 값을 가집니다. 두번째 열의 경우에는 1의 인덱스를 가진 know입니다. know는 첫번째 문서에만 등장했으므로 첫번째 행에서만 1의 값을 가집니다.\", \"사이킷런은 TF-IDF를 자동 계산해주는 TfidfVectorizer를 제공합니다. 사이킷런의 TF-IDF는 위에서 배웠던 보편적인 TF-IDF 기본 식에서 조정된 식을 사용합니다. 요약하자면, IDF의 로그항의 분자에 1을 더해주며, 로그항에 1을 더해주고, TF-IDF에 L2 정규화라는 방법으로 값을 조정하는 등의 차이로 TF-IDF가 가진 의도는 여전히 그대로 갖고 있습니다.\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\ncorpus = [\\n'you know I want your love',\\n'I like you',\\n'what should I do ',\\n]\\ntfidfv = TfidfVectorizer().fit(corpus)\\nprint(tfidfv.transform(corpus).toarray())\\nprint(tfidfv.vocabulary_)\", \"print(tfidfv.transform(corpus).toarray())\\nprint(tfidfv.vocabulary_)\\n[[0.         0.46735098 0.         0.46735098 0.         0.46735098 0.         0.35543247 0.46735098]\\n[0.         0.         0.79596054 0.         0.         0.         0.         0.60534851 0.        ]\\n[0.57735027 0.         0.         0.         0.57735027 0.         0.57735027 0.         0.        ]]\\n{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\", \"{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\\nBoW, DTM, TF-IDF에 대해서 전부 학습했습니다. 문서들 간의 유사도를 구하기 위한 재료 손질하는 방법을 배운 셈입니다. 케라스로도 DTM과 TF-IDF 행렬을 만들 수 있는데, 이는 딥 러닝 챕터의 다층 퍼셉트론으로 텍스트 분류하기 실습에서 별도로 다루겠습니다. 다음 챕터에서 유사도를 구하는 방법과 이를 이용한 실습을 진행해보겠습니다.\\n사이킷런의 TF-IDF의 수식을 이해하고 싶은 분들을 위해서 위키독스 웹 사이트에 댓글로 설명해놨습니다. 궁금하신 분들은 참고하세요.\\n==================================================\\n--- 05. 벡터의 유사도(Vector Similarity) ---\\n마지막 편집일시 : 2022년 11월 13일 8:23 오후\", '--- 05. 벡터의 유사도(Vector Similarity) ---\\n마지막 편집일시 : 2022년 11월 13일 8:23 오후\\n==================================================\\n--- 05-01 코사인 유사도(Cosine Similarity) ---\\n```\\n12481                            The Dark Knight\\n150                               Batman Forever\\n1328                              Batman Returns\\n15511                 Batman: Under the Red Hood\\n585                                       Batman\\n9230          Batman Beyond: Return of the Joker', '585                                       Batman\\n9230          Batman Beyond: Return of the Joker\\n18035                           Batman: Year One\\n19792    Batman: The Dark Knight Returns, Part 1\\n3095                Batman: Mask of the Phantasm\\n10122                              Batman Begins\\nName: title, dtype: object\\n```BoW에 기반한 단어 표현 방법인 DTM, TF-IDF, 또는 뒤에서 배우게 될 Word2Vec 등과 같이 단어를 수치화할 수 있는 방법을 이해했다면 이러한 표현 방법에 대해서 코사인 유사도를 이용하여 문서의 유사도를 구하는 게 가능합니다.']\n",
      "['코사인 유사도는 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도를 의미합니다. 두 벡터의 방향이 완전히 동일한 경우에는 1의 값을 가지며, 90°의 각을 이루면 0, 180°로 반대의 방향을 가지면 -1의 값을 갖게 됩니다. 즉, 결국 코사인 유사도는 -1 이상 1 이하의 값을 가지며 값이 1에 가까울수록 유사도가 높다고 판단할 수 있습니다. 이를 직관적으로 이해하면 두 벡터가 가리키는 방향이 얼마나 유사한가를 의미합니다.\\n[이미지: ]\\n두 벡터 A, B에 대해서 코사인 유사도는 식으로 표현하면 다음과 같습니다.\\n$$similarity=cos(Θ)=\\\\frac{A⋅B}{||A||\\\\ ||B||}=\\\\frac{\\\\sum_{i=1}^{n}{A_{i}×B_{i}}}{\\\\sqrt{\\\\sum_{i=1}^{n}(A_{i})^2}×\\\\sqrt{\\\\sum_{i=1}^{n}(B_{i})^2}}$$', '문서 단어 행렬이나 TF-IDF 행렬을 통해서 문서의 유사도를 구하는 경우에는 문서 단어 행렬이나 TF-IDF 행렬이 각각의 특징 벡터 A, B가 됩니다. 예시를 통해 문서 단어 행렬에 대해서 코사인 유사도를 구해봅시다.\\n문서1 : 저는 사과 좋아요\\n문서2 : 저는 바나나 좋아요\\n문서3 : 저는 바나나 좋아요 저는 바나나 좋아요\\n뛰어쓰기 기준 토큰화를 진행했다고 가정하고, 위의 세 문서에 대해서 문서 단어 행렬을 만들면 이와 같습니다.\\n바나나\\n사과\\n저는\\n좋아요\\n문서1\\n0\\n1\\n1\\n1\\n문서2\\n1\\n0\\n1\\n1\\n문서3\\n2\\n0\\n2\\n2\\nNumpy를 사용해서 코사인 유사도를 계산하는 함수를 구현하고 각 문서 벡터 간의 코사인 유사도를 계산해보겠습니다.\\nimport numpy as np\\nfrom numpy import dot\\nfrom numpy.linalg import norm\\ndef cos_sim(A, B):\\nreturn dot(A, B)/(norm(A)*norm(B))', \"from numpy.linalg import norm\\ndef cos_sim(A, B):\\nreturn dot(A, B)/(norm(A)*norm(B))\\ndoc1 = np.array([0,1,1,1])\\ndoc2 = np.array([1,0,1,1])\\ndoc3 = np.array([2,0,2,2])\\nprint('문서 1과 문서2의 유사도 :',cos_sim(doc1, doc2))\\nprint('문서 1과 문서3의 유사도 :',cos_sim(doc1, doc3))\\nprint('문서 2와 문서3의 유사도 :',cos_sim(doc2, doc3))\\n문서 1과 문서2의 유사도 : 0.67\\n문서 1과 문서3의 유사도 : 0.67\\n문서 2과 문서3의 유사도 : 1.00\", '문서 1과 문서2의 유사도 : 0.67\\n문서 1과 문서3의 유사도 : 0.67\\n문서 2과 문서3의 유사도 : 1.00\\n눈여겨볼만한 점은 문서1과 문서2의 코사인 유사도와 문서1과 문서3의 코사인 유사도가 같다는 점과 문서2와 문서3의 코사인 유사도가 1이 나온다는 것입니다. 앞서 1은 두 벡터의 방향이 완전히 동일한 경우에 1이 나오며, 코사인 유사도 관점에서는 유사도의 값이 최대임을 의미한다고 언급한 바 있습니다.', '문서3은 문서2에서 단지 모든 단어의 빈도수가 1씩 증가했을 뿐입니다. 다시 말해 한 문서 내의 모든 단어의 빈도수가 동일하게 증가하는 경우에는 기존의 문서와 코사인 유사도의 값이 1이라는 것입니다. 이것이 시사하는 점은 무엇일까요? 예를 들어보겠습니다. 문서 A와 B가 동일한 주제의 문서. 문서 C는 다른 주제의 문서라고 해봅시다. 그리고 문서 A와 문서 C의 문서의 길이는 거의 차이가 나지 않지만, 문서 B의 경우 문서 A의 길이보다 두 배의 길이를 가진다고 가정하겠습니다. 이런 경우 유클리드 거리로 유사도를 연산하면 문서 A가 문서 B보다 문서 C와 유사도가 더 높게 나오는 상황이 발생할 수 있습니다. 이는 유사도 연산에 문서의 길이가 영향을 받았기 때문인데, 이런 경우 코사인 유사도가 해결책이 될 수 있습니다. 코사인 유사도는 유사도를 구할 때 벡터의 방향(패턴)에 초점을 두므로 코사인 유사도는 문서의 길이가 다른 상황에서 비교적 공정한 비교를 할 수 있도록 도와줍니다.']\n",
      "[\"캐글에서 사용되었던 영화 데이터셋을 가지고 영화 추천 시스템을 만들어보겠습니다. TF-IDF와 코사인 유사도만으로 영화의 줄거리에 기반해서 영화를 추천하는 추천 시스템을 만들 수 있습니다.\\n다운로드 링크 : https://www.kaggle.com/rounakbanik/the-movies-dataset\\n원본 파일은 위 링크에서 movies_metadata.csv 파일을 다운로드 받으면 됩니다. 해당 데이터는 총 24개의 열을 가진 45,466개의 샘플로 구성된 영화 정보 데이터입니다.\\nimport pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\ndata = pd.read_csv('movies_metadata.csv', low_memory=False)\\ndata.head(2)\", \"data = pd.read_csv('movies_metadata.csv', low_memory=False)\\ndata.head(2)\\n다운로드 받은 훈련 데이터에서 상위 2개의 샘플만 출력하여 데이터의 형식을 확인합니다.\\n...\\noriginal_title\\noverview\\n...\\ntitle\\nvideo\\nvote_average\\nvote_count\\n0\\n...\\nToy Story\\nLed by Woody, Andy's toys live happily in his ... 중략 ...\\n...\\nToy Story\\nFalse\\n7.7\\n5415.0\\n1\\n...\\nJumanji\\nWhen siblings Judy and Peter discover an encha ... 중략 ...\\n...\\nJumanji\\nFalse\\n6.9\\n2413.0\", '...\\nJumanji\\nWhen siblings Judy and Peter discover an encha ... 중략 ...\\n...\\nJumanji\\nFalse\\n6.9\\n2413.0\\n훈련 데이터는 총 24개의 열을 갖고있으나 책의 지면의 한계로 일부 생략합니다. 여기서 코사인 유사도에 사용할 데이터는 영화 제목에 해당하는 title 열과 줄거리에 해당하는 overview 열입니다. 좋아하는 영화를 입력하면, 해당 영화의 줄거리와 유사한 줄거리의 영화를 찾아서 추천하는 시스템을 만들 것입니다.\\n# 상위 2만개의 샘플을 data에 저장\\ndata = data.head(20000)', \"# 상위 2만개의 샘플을 data에 저장\\ndata = data.head(20000)\\n만약 훈련 데이터의 양을 줄이고 학습을 진행하고자 한다면 위와 같이 데이터를 줄여서 재저장할 수 있습니다. 여기서는 상위 20,000개의 샘플만 사용하겠습니다. TF-IDF를 연산할 때 데이터에 Null 값이 들어있으면 에러가 발생합니다. TF-IDF의 대상이 되는 data의 overview 열에 결측값에 해당하는 Null 값이 있는지 확인합니다.\\n# overview 열에 존재하는 모든 결측값을 전부 카운트하여 출력\\nprint('overview 열의 결측값의 수:',data['overview'].isnull().sum())\\noverview 열의 결측값의 수: 135\", \"print('overview 열의 결측값의 수:',data['overview'].isnull().sum())\\noverview 열의 결측값의 수: 135\\n135개의 Null 값이 있다고 합니다. 이 경우 결측값을 가진 행을 제거하는 pandas의 dropna()나 결측값이 있던 행에 특정값으로 채워넣는 pandas의 fillna()를 사용할 수 있습니다. 괄호 안에 Null 대신 넣고자하는 값을 넣으면 되는데, 여기서는 빈 값(empty value)으로 대체하였습니다.\\n# 결측값을 빈 값으로 대체\\ndata['overview'] = data['overview'].fillna('')\\nNull 값을 빈 값으로 대체하였습니다. overview열에 대해서 TF-IDF 행렬을 구한 후 행렬의 크기를 출력해봅시다.\\ntfidf = TfidfVectorizer(stop_words='english')\\ntfidf_matrix = tfidf.fit_transform(data['overview'])\", \"tfidf = TfidfVectorizer(stop_words='english')\\ntfidf_matrix = tfidf.fit_transform(data['overview'])\\nprint('TF-IDF 행렬의 크기(shape) :',tfidf_matrix.shape)\\nTF-IDF 행렬의 크기(shape) : (20000, 47487)\\nTF-IDF 행렬의 크기는 20,000의 행을 가지고 47,847의 열을 가지는 행렬입니다. 다시 말해 20,000개의 영화를 표현하기 위해서 총 47,487개의 단어가 사용되었음을 의미합니다. 또는 47,847차원의 문서 벡터가 20,000개가 존재한다고도 표현할 수 있을 겁니다. 이제 20,000개의 문서 벡터에 대해서 상호 간의 코사인 유사도를 구합니다.\\ncosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\\nprint('코사인 유사도 연산 결과 :',cosine_sim.shape)\", \"print('코사인 유사도 연산 결과 :',cosine_sim.shape)\\n코사인 유사도 연산 결과 : (20000, 20000)\\n코사인 유사도 연산 결과로는 20,000행 20,000열의 행렬을 얻습니다. 이는 20,000개의 각 문서 벡터(영화 줄거리 벡터)와 자기 자신을 포함한 20,000개의 문서 벡터 간의 유사도가 기록된 행렬입니다. 모든 20,000개 영화의 상호 유사도가 기록되어져 있습니다. 이제 기존 데이터프레임으로부터 영화의 타이틀을 key, 영화의 인덱스를 value로 하는 딕셔너리 title_to_index를 만들어둡니다.\\ntitle_to_index = dict(zip(data['title'], data.index))\\n# 영화 제목 Father of the Bride Part II의 인덱스를 리턴\\nidx = title_to_index['Father of the Bride Part II']\\nprint(idx)\\n4\", \"idx = title_to_index['Father of the Bride Part II']\\nprint(idx)\\n4\\n선택한 영화의 제목을 입력하면 코사인 유사도를 통해 가장 overview가 유사한 10개의 영화를 찾아내는 함수를 만듭니다.\\ndef get_recommendations(title, cosine_sim=cosine_sim):\\n# 선택한 영화의 타이틀로부터 해당 영화의 인덱스를 받아온다.\\nidx = title_to_index[title]\\n# 해당 영화와 모든 영화와의 유사도를 가져온다.\\nsim_scores = list(enumerate(cosine_sim[idx]))\\n# 유사도에 따라 영화들을 정렬한다.\\nsim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\\n# 가장 유사한 10개의 영화를 받아온다.\\nsim_scores = sim_scores[1:11]\\n# 가장 유사한 10개의 영화의 인덱스를 얻는다.\", \"# 가장 유사한 10개의 영화를 받아온다.\\nsim_scores = sim_scores[1:11]\\n# 가장 유사한 10개의 영화의 인덱스를 얻는다.\\nmovie_indices = [idx[0] for idx in sim_scores]\\n# 가장 유사한 10개의 영화의 제목을 리턴한다.\\nreturn data['title'].iloc[movie_indices]\\n영화 다크 나이트 라이즈와 overview가 유사한 영화들을 찾아보겠습니다.\\nget_recommendations('The Dark Knight Rises')\\n12481                            The Dark Knight\\n150                               Batman Forever\\n1328                              Batman Returns\\n15511                 Batman: Under the Red Hood\", '1328                              Batman Returns\\n15511                 Batman: Under the Red Hood\\n585                                       Batman\\n9230          Batman Beyond: Return of the Joker\\n18035                           Batman: Year One\\n19792    Batman: The Dark Knight Returns, Part 1\\n3095                Batman: Mask of the Phantasm\\n10122                              Batman Begins\\nName: title, dtype: object\\n가장 유사한 영화가 출력되는데, 영화 다크 나이트가 첫번째고, 그 외에도 전부 배트맨 영화를 찾아낸 것을 확인할 수 있습니다.', 'Name: title, dtype: object\\n가장 유사한 영화가 출력되는데, 영화 다크 나이트가 첫번째고, 그 외에도 전부 배트맨 영화를 찾아낸 것을 확인할 수 있습니다.\\n==================================================\\n--- 05-02 여러가지 유사도 기법 ---\\n```\\n자카드 유사도 : 0.16666666666666666\\n```문서의 유사도를 구하기 위한 방법으로는 코사인 유사도 외에도 여러가지 방법들이 있습니다. 여기서는 문서의 유사도를 구할 수 있는 다른 방법들을 학습합니다.']\n",
      "['유클리드 거리(euclidean distance)는 문서의 유사도를 구할 때 자카드 유사도나 코사인 유사도만큼, 유용한 방법은 아닙니다. 하지만 여러 가지 방법을 이해하고, 시도해보는 것 자체만으로 다른 개념들을 이해할 때 도움이 되므로 의미가 있습니다.\\n다차원 공간에서 두개의 점 $p$와 $q$가 각각 $p=(p_{1}, p_{2}, p_{3}, ... , p_{n})$과 $q=(q_{1}, q_{2}, q_{3}, ..., q_{n})$의 좌표를 가질 때 두 점 사이의 거리를 계산하는 유클리드 거리 공식은 다음과 같습니다.\\n$$\\\\sqrt{(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}+\\\\ ...\\\\ +(q_{n}-p_{n})^{2}}=\\\\sqrt{\\\\sum_{i=1}^{n}(q_{i}-p_{i})^{2}}$$', '다차원 공간이라고 가정하면, 처음 보는 입장에서는 식이 너무 복잡해보입니다. 좀 더 쉽게 이해하기위해서 2차원 공간이라고 가정하고 두 점 사이의 거리를 좌표 평면 상에서 시각화해보겠습니다.\\n[이미지: ]\\n2차원 좌표 평면 상에서 두 점 $p$와 $q$사이의 직선 거리를 구하는 문제입니다. 위의 경우에는 직각 삼각형으로 표현이 가능하므로, 중학교 수학 과정인 피타고라스의 정리를 통해 $p$와 $q$ 사이의 거리를 계산할 수 있습니다. 즉, 2차원 좌표 평면에서 두 점 사이의 유클리드 거리 공식은 피타고라스의 정리를 통해 두 점 사이의 거리를 구하는 것과 동일합니다.\\n다시 원점으로 돌아가서 여러 문서에 대해서 유사도를 구하고자 유클리드 거리 공식을 사용한다는 것은, 앞서 본 2차원을 단어의 총 개수만큼의 차원으로 확장하는 것과 같습니다. 예를 들어 아래와 같은 DTM이 있다고 합시다.\\n바나나\\n사과\\n저는\\n좋아요\\n문서1\\n2\\n3\\n0\\n1\\n문서2\\n1\\n2\\n3\\n1\\n문서3\\n2\\n1\\n2\\n2', '바나나\\n사과\\n저는\\n좋아요\\n문서1\\n2\\n3\\n0\\n1\\n문서2\\n1\\n2\\n3\\n1\\n문서3\\n2\\n1\\n2\\n2\\n단어의 개수가 4개이므로, 이는 4차원 공간에 문서1, 문서2, 문서3을 배치하는 것과 같습니다. 이때 다음과 같은 문서Q에 대해서 문서1, 문서2, 문서3 중 가장 유사한 문서를 찾아내고자 합니다.\\n바나나\\n사과\\n저는\\n좋아요\\n문서Q\\n1\\n1\\n0\\n1\\n이때 유클리드 거리를 통해 유사도를 구하려고 한다면, 문서Q 또한 다른 문서들처럼 4차원 공간에 배치시켰다는 관점에서 4차원 공간에서의 각각의 문서들과의 유클리드 거리를 구하면 됩니다. 이를 파이썬 코드로 구현해보겠습니다.\\nimport numpy as np\\ndef dist(x,y):\\nreturn np.sqrt(np.sum((x-y)**2))\\ndoc1 = np.array((2,3,0,1))\\ndoc2 = np.array((1,2,3,1))\\ndoc3 = np.array((2,1,2,2))\\ndocQ = np.array((1,1,0,1))', \"doc2 = np.array((1,2,3,1))\\ndoc3 = np.array((2,1,2,2))\\ndocQ = np.array((1,1,0,1))\\nprint('문서1과 문서Q의 거리 :',dist(doc1,docQ))\\nprint('문서2과 문서Q의 거리 :',dist(doc2,docQ))\\nprint('문서3과 문서Q의 거리 :',dist(doc3,docQ))\\n문서1과 문서Q의 거리 : 2.23606797749979\\n문서2과 문서Q의 거리 : 3.1622776601683795\\n문서3과 문서Q의 거리 : 2.449489742783178\\n유클리드 거리의 값이 가장 작다는 것은 문서 간 거리가 가장 가깝다는 것을 의미합니다. 즉, 문서1이 문서Q와 가장 유사하다고 볼 수 있습니다.\"]\n",
      "['A와 B 두개의 집합이 있다고 합시다. 이때 교집합은 두 개의 집합에서 공통으로 가지고 있는 원소들의 집합을 말합니다. 즉, 합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있다는 것이 자카드 유사도(jaccard similarity)의 아이디어입니다. 자카드 유사도는 0과 1사이의 값을 가지며, 만약 두 집합이 동일하다면 1의 값을 가지고, 두 집합의 공통 원소가 없다면 0의 값을 갖습니다. 자카드 유사도를 구하는 함수를 $J$라고 하였을 때, 자카드 유사도 함수 $J$는 아래와 같습니다.\\n$$J(A,B)=\\\\frac{|A∩B|}{|A∪B|}=\\\\frac{|A∩B|}{|A|+|B|-|A∩B|}$$\\n두 개의 비교할 문서를 각각 $doc_{1}$, $doc_{2}$라고 했을 때 $doc_{1}$과 $doc_{2}$의 문서의 유사도를 구하기 위한 자카드 유사도는 이와 같습니다.', '두 개의 비교할 문서를 각각 $doc_{1}$, $doc_{2}$라고 했을 때 $doc_{1}$과 $doc_{2}$의 문서의 유사도를 구하기 위한 자카드 유사도는 이와 같습니다.\\n$$J(doc_{1},doc_{2})=\\\\frac{doc_{1}∩doc_{2}}{doc_{1}∪doc_{2}}$$\\n두 문서 $doc_{1}$, $doc_{2}$ 사이의 자카드 유사도 $J(doc_{1},doc_{2})$는 두 집합의 교집합 크기를 두 집합의 합집합 크기로 나눈 값으로 정의됩니다. 간단한 예를 통해서 이해해보겠습니다.\\ndoc1 = \"apple banana everyone like likey watch card holder\"\\ndoc2 = \"apple banana coupon passport love you\"\\n# 토큰화\\ntokenized_doc1 = doc1.split()\\ntokenized_doc2 = doc2.split()\\nprint(\\'문서1 :\\',tokenized_doc1)', \"# 토큰화\\ntokenized_doc1 = doc1.split()\\ntokenized_doc2 = doc2.split()\\nprint('문서1 :',tokenized_doc1)\\nprint('문서2 :',tokenized_doc2)\\n문서1 : ['apple', 'banana', 'everyone', 'like', 'likey', 'watch', 'card', 'holder']\\n문서2 : ['apple', 'banana', 'coupon', 'passport', 'love', 'you']\\n문서1과 문서2의 합집합을 구해보겠습니다.\\nunion = set(tokenized_doc1).union(set(tokenized_doc2))\\nprint('문서1과 문서2의 합집합 :',union)\", \"union = set(tokenized_doc1).union(set(tokenized_doc2))\\nprint('문서1과 문서2의 합집합 :',union)\\n문서1과 문서2의 합집합 : {'you', 'passport', 'watch', 'card', 'love', 'everyone', 'apple', 'likey', 'like', 'banana', 'holder', 'coupon'}\\n문서1과 문서2의 합집합의 단어의 총 개수는 12개입니다. 이제 문서1과 문서2의 교집합을 구해보겠습니다. 문서1과 문서2에서 둘 다 등장한 단어를 찾으면 됩니다.\\nintersection = set(tokenized_doc1).intersection(set(tokenized_doc2))\\nprint('문서1과 문서2의 교집합 :',intersection)\\n문서1과 문서2의 교집합 : {'apple', 'banana'}\", \"print('문서1과 문서2의 교집합 :',intersection)\\n문서1과 문서2의 교집합 : {'apple', 'banana'}\\n문서1과 문서2에서 둘 다 등장한 단어는 banana와 apple 총 2개입니다. 이제 교집합의 크기를 합집합의 크기로 나누면 자카드 유사도가 계산됩니다.\\nprint('자카드 유사도 :',len(intersection)/len(union))\\n자카드 유사도 : 0.16666666666666666\\n==================================================\\n--- 06. 머신 러닝(Machine Learning) 개요 ---\\n마지막 편집일시 : 2022년 1월 2일 2:49 오후\\n==================================================\\n--- 06-01 머신 러닝이란(What is Machine Learning?) ---\\n```\\ndef prediction(이미지 as input):\", '--- 06-01 머신 러닝이란(What is Machine Learning?) ---\\n```\\ndef prediction(이미지 as input):\\n어떻게 코딩해야하지?\\nreturn 결과\\n```딥 러닝을 포함하고 있는 개념인 머신 러닝(Machine Learning)의 개념에 대해서 학습합니다.']\n",
      "['머신 러닝이 아닌 기존의 프로그래밍 작성 방식을 통해서는 해결하기 어려운 문제 예시를 하나 들어보겠습니다.\\n예시 : 주어진 사진으로부터 고양이 사진인지 강아지 사진인지 판별하는 일.\\n위 문제는 실제 2017년에 있었던 DGIST의 딥 러닝 경진대회의 문제입니다. 사진을 보고 고양이 사진인지, 강아지 사진인지 판단하는 건 사람에게는 너무나 쉬운 일입니다. 그런데 이 문제를 풀 수 있는 프로그램을 작성하는 것은 상당히 난해한 수준입니다. 입력된 이미지로부터 강아지와 고양이를 구분할 수 있는 코드를 어떻게 작성할 수 있을까요?\\ndef prediction(이미지 as input):\\n어떻게 코딩해야하지?\\nreturn 결과', 'def prediction(이미지 as input):\\n어떻게 코딩해야하지?\\nreturn 결과\\n사진이란 건 사진을 보는 각도, 조명, 타겟의 변형(고양이의 자세)에 따라서 너무나 천차만별이라 사진으로부터 공통된 명확한 특징을 잡아내는 것이 쉽지 않습니다. 사실, 결론을 미리 말씀드리면 해당 프로그램은 숫자를 정렬하는 것과 같은 명확한 알고리즘이 애초에 존재하지 않습니다.\\n[이미지: ]\\n[이미지: ]\\n이러한 이미지 인식 분야에서 규칙을 정의하고 특징을 잡아내기 위한 많은 시도들이 있었습니다. 이미지 내의 경계선과 같은 것들을 찾아내서 알고리즘화 하려고 시도하고, 다른 사진 이미지가 들어오면 전반적인 상태를 비교하여 분류하려고 한 것입니다. 하지만 그러한 시도들은 결국 특징을 잡아내는 것에 한계가 있을 수밖에 없었습니다. 결국 요즘에 이르러서는 사진으로부터 대상을 찾아내는 일은 사람이 규칙을 정의하는 것이 아니라 머신 러닝으로 문제를 해결하고 있습니다.']\n",
      "['[이미지: ]\\n머신 러닝이 위에서 언급한 예시 문제를 해결할 수 있는 이유는 해결을 위한 접근 방식이 기존의 프로그래밍 방식과는 다르기 때문입니다. 위 이미지에서 위쪽은 기존의 프로그래밍의 접근 방식, 아래쪽은 머신 러닝의 접근 방식을 보여줍니다. 머신 러닝은 데이터가 주어지면, 기계가 스스로 데이터로부터 규칙성을 찾는 것에 집중합니다. 주어진 데이터로부터 규칙성을 찾는 과정을 우리는 훈련(training) 또는 학습(learning)이라고 합니다.', '일단 규칙성을 발견하고나면, 그 후에 들어오는 새로운 데이터에 대해서 발견한 규칙성을 기준으로 정답을 찾아내는데 이는 기존의 프로그래밍 방식으로 접근하기 어려웠던 문제의 해결책이 되기도 합니다. 이미지를 예시로 들었지만 자연어 처리도 이미지 처리 만큼이나 어려운 문제들이 많습니다. 최근에는 머신 러닝의 한 갈래인 딥 러닝이 자연어 처리에서 굉장히 뛰어난 성능을 보여주고 있습니다. 단적으로, 구글 번역기와 같은 기계 번역기가 그러한데, 이러한 번역기는 사람이 직접 규칙을 정의해서 만드는 것보다 딥 러닝으로 모델이 스스로 규칙을 찾아내도록 구현하는 것이 훨씬 더 좋은 성능을 얻을 수 있습니다.\\n==================================================\\n--- 06-02 머신 러닝 훑어보기 ---\\n머신 러닝의 특징을 이해하고, 주요 용어에 미리 친숙해져봅시다.']\n",
      "['[이미지: ]\\n머신 러닝을 위한 데이터를 준비했다면 기계를 학습하기 전 해당 데이터를 훈련용, 검증용, 테스트용 이렇게 세 가지로 분리하는 것이 일반적입니다. 훈련 데이터는 머신 러닝 모델을 학습하는 용도입니다. 테스트 데이터는 학습한 머신 러닝 모델의 성능을 평가하기 위한 용도입니다. 그렇다면 검증용 데이터의 용도는 무엇일까요?\\n검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라 모델의 성능을 조정하기 위한 용도입니다. 더 정확히는 모델이 훈련 데이터에 과적합(overfitting) 이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도입니다. 하이퍼파라미터와 매개변수라는 용어를 정리해둡시다.\\n하이퍼파라미터(초매개변수) : 모델의 성능에 영향을 주는 사람이 값을 지정하는 변수.\\n매개변수 : 가중치와 편향. 학습을 하는 동안 값이 계속해서 변하는 수.', '하이퍼파라미터(초매개변수) : 모델의 성능에 영향을 주는 사람이 값을 지정하는 변수.\\n매개변수 : 가중치와 편향. 학습을 하는 동안 값이 계속해서 변하는 수.\\n아직 이 장에서는 검증용 데이터와 하이퍼파라미터의 개념이 어떤 의미인지 와닿지 않아도 괜찮습니다. 이 개념은 앞으로 지속적으로 언급하게 될 것입니다. 이 두 변수의 가장 큰 차이는 하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수라는 점입니다. 뒤의 선형 회귀에서 배우게 되는 경사 하강법에서 학습률(learning rate)이나, 딥 러닝에서 뉴런의 수나 층의 수와 같은 것들이 대표적인 하이퍼파라미터입니다.', '반면, 가중치와 편향과 같은 매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값입니다. 훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝(tuning) 합니다. 검증용 데이터에 대해서 높은 정확도를 얻도록 하이퍼파라미터의 값을 바꿔보는 것입니다. 이렇게 튜닝하는 과정에서 모델은 검증용 데이터의 정확도를 높이는 방향으로 점차적으로 수정됩니다.', '튜닝 과정을 모두 끝내고 모델의 최종 평가를 하고자 한다면, 이제 검증용 데이터로 모델을 평가하는 것은 적합하지 않습니다. 모델은 검증용 데이터의 정확도를 높이기 위해서 수정되어져 온 모델이기 때문입니다. 모델에 대한 평가는 이제 모델이 한 번도 보지 못한 데이터인 테스트 데이터의 몫입니다. 수학능력시험을 준비하는 수험생으로 비유하자면 훈련 데이터는 실제 공부를 위한 문제지, 검증 데이터는 모의고사, 테스트 데이터는 실력을 최종적으로 평가하는 수능 시험이라고 볼 수 있습니다.']\n",
      "['전부는 아니지만 머신 러닝의 많은 문제는 분류 또는 회귀 문제에 속합니다. 이번 챕터에서는 머신 러닝 기법 중 선형 회귀(Lineare Regression)와 로지스틱 회귀(Logistic Regression)를 다루는데 선형 회귀는 대표적인 회귀 문제에 속하고, 로지스틱 회귀는 (이름은 회귀이지만) 대표적인 분류 문제에 속합니다.\\n분류는 또한 이진 분류(Binary Classification)과 다중 클래스 분류(Multi-Class Classification)로 나뉩니다. 엄밀히는 다중 레이블 분류(Multi-lable Classification)라는 또 다른 문제가 존재하지만, 이 책에서는 이진 분류와 다중 클래스 분류만을 다룹니다.\\n1) 이진 분류 문제(Binary Classification)', '1) 이진 분류 문제(Binary Classification)\\n이진 분류는 주어진 입력에 대해서 두 개의 선택지 중 하나의 답을 선택해야 하는 경우를 말합니다. 종합 시험 성적표를 보고 최종적으로 합격, 불합격인지 판단하는 문제, 메일을 보고나서 정상 메일, 스팸 메일인지를 판단하는 문제 등이 이에 속합니다.\\n2) 다중 클래스 분류(Multi-class Classification)\\n다중 클래스 분류는 주어진 입력에 대해서 세 개 이상의 선택지 중에서 답을 선택해야 하는 경우를 말합니다. 예를 들어 서점 직원이 일을 하는데 과학, 영어, IT, 학습지, 만화라는 레이블이 붙어있는 5개의 책장이 있다고 합시다. 새 책이 입고되면, 이 책은 다섯 개의 책장 중에서 분야에 맞는 적절한 책장에 책을 넣어야 합니다. 이 경우는 현실에서의 다중 클래스 분류 문제라고 할 수 있겠습니다.\\n3) 회귀 문제(Regression)', '3) 회귀 문제(Regression)\\n회귀 문제는 분류 문제처럼 둘 중 하나를 선택해야 한다거나, 책이 입고되었을 때 5개의 책장 중 하나의 책장을 골라야하는 경우처럼 정답이 몇 개의 정해진 선택지 중에서 정해져 있는 경우가 아니라 어떠한 연속적인 값의 범위 내에서 예측값이 나오는 경우를 말합니다.', '예를 들어서 역과의 거리, 인구 밀도, 방의 개수 등을 입력하면 부동산 가격을 예측하는 머신 러닝 모델이 있다고 해봅시다. 머신 러닝 모델이 부동산 가격을 7억 8,456만 3,450원으로 예측하는 경우도 있을 것이고, 8억 1257만 300원으로 예측하는 경우도 있을 수 있습니다. 특정 값의 범위 내에서는 어떤 숫자도 나올 수 있습니다. 기존의 분류 문제와 같이 분리된(비연속적인) 답이 결과가 아니라 연속된 값을 결과로 가지는 이러한 문제를 회귀 문제라고 부릅니다. 회귀 문제의 예시로 시계열 데이터(Time Series Data)를 이용한 주가 예측, 생산량 예측, 지수 예측 등이 있습니다.']\n",
      "['머신 러닝은 크게 지도 학습, 비지도 학습, 강화 학습으로 나눕니다. 강화 학습은 이 책의 범위를 벗어나므로 설명하지 않습니다. 그리고 큰 갈래로서는 자주 언급 되지는 않지만 딥 러닝 자연어 처리에서 중요한 학습 방법 중 하나인 자기지도 학습(Self-Supervised Learning, SSL)에 대해서도 언급해보겠습니다.\\n1) 지도 학습(Supervised Learning)', '1) 지도 학습(Supervised Learning)\\n지도 학습이란 레이블(Label)이라는 정답과 함께 학습하는 것을 말합니다. 자연어 처리는 대부분 지도 학습에 속합니다. 앞으로 우리가 풀게 될 자연어 처리의 많은 문제들은 레이블이 존재하는 경우가 많기 때문입니다. 이는 앞서 2챕터의 데이터의 분리를 설명하며 상세히 설명한 바 있습니다. 레이블이라는 말 외에도 $y$, 실제값 등으로 부르기도 하는데 이 책에서는 이 용어들을 상황에 따라서 바꿔서 사용합니다. 기계는 예측값과 실제값의 차이인 오차를 줄이는 방식으로 학습을 하게 되는데 예측값은 $\\\\hat{y}$과 같이 표현하기도 합니다.\\n2) 비지도 학습(Unsupervised Learning)', '2) 비지도 학습(Unsupervised Learning)\\n비지도 학습은 데이터에 별도의 레이블이 없이 학습하는 것을 말합니다. 예를 들어 텍스트 처리 분야의 토픽 모델링 알고리즘인 LSA나 LDA는 비지도 학습에 속합니다. LSA와 LDA는 온라인 웹 사이트 위키독스의 e-book( https://wikidocs.net/30707 )에서 볼 수 있습니다.\\n3) 자기지도 학습(Self-Supervised Learning, SSL)\\n레이블이 없는 데이터가 주어지면, 모델이 학습을 위해서 스스로 데이터로부터 레이블을 만들어서 학습하는 경우를 자기지도 학습이라고 합니다. 대표적인 예시로는 Word2Vec과 같은 워드 임베딩 알고리즘이나, BERT와 같은 언어 모델의 학습 방법을 들 수 있습니다. 이들이 어떻게 레이블을 만들어 학습하는지에 대한 설명은 Word2Vec과 BERT를 설명하는 페이지를 참고바랍니다.']\n",
      "['많은 머신 러닝 문제가 1개 이상의 독립 변수 $x$를 가지고 종속 변수 $y$를 예측하는 문제입니다. 머신 러닝 모델 중 특히 인공 신경망은 독립 변수, 종속 변수, 가중치, 편향 등을 행렬 연산을 통해 연산하는 경우가 많습니다. 앞으로 인공 신경망을 배우게되면 훈련 데이터를 행렬로 표현하는 경우를 많이 보게 됩니다. 독립 변수 $x$의 행렬을 X라고 하였을 때, 독립 변수의 개수가 n개이고 데이터의 개수가 m인 행렬 X는 다음과 같습니다.\\n[이미지: ]\\n이때 머신 러닝에서는 하나의 데이터. 행렬 관점에서는 하나의 행을 샘플(Sample)이라고 부릅니다. (데이터베이스에서 레코드라고 부르는 단위입니다.) 그리고 종속 변수 $y$를 예측하기 위한 각각의 독립 변수 $x$를 특성(Feature)이라고 부릅니다. 행렬 관점에서는 각 열에 해당됩니다.']\n",
      "['머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 합니다. 하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않습니다. 이를 위해서 사용하는 것이 혼동 행렬(Confusion Matrix)입니다. 예를 들어 참(True)와 거짓(False) 둘 중 하나를 예측하는 문제였다고 가정해봅시다. 아래의 혼동 행렬에서 각 열은 예측값을 나타내며, 각 행은 실제값을 나타냅니다.\\n예측 참\\n예측 거짓\\n실제 참\\nTP\\nFN\\n실제 거짓\\nFP\\nTN\\n머신 러닝에서는 다음과 같은 네 가지 케이스에 대해서 각각 TP, FP, FN, TN을 정의합니다.\\nTrue Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)\\nFalse Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)\\nFalse Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)', 'False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)\\nFalse Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)\\nTrue Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)\\n이 개념을 사용하면 정밀도(Precision)과 재현율(Recall)이 됩니다.\\n1) 정밀도(Precision)\\n정밀도란 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율입니다.\\n$$ \\\\text{정밀도} = \\\\frac{TP}{TP + FP}$$\\n2) 재현율(Recall)\\n재현율이란 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율입니다.\\n$$ \\\\text{재현율} = \\\\frac{TP}{TP + FN}$$\\nPrecision이나 Recall은\\xa0모두 실제 True인 정답을 모델이 True라고 예측한 경우. 즉, TP에 관심이 있습니다. 두 식 모두 분자가 TP임에 주목합시다.', 'Precision이나 Recall은\\xa0모두 실제 True인 정답을 모델이 True라고 예측한 경우. 즉, TP에 관심이 있습니다. 두 식 모두 분자가 TP임에 주목합시다.\\n3) 정확도(Accuracy)\\n정확도(Accuracy)는 우리가 일반적으로 실생활에서도 가장 많이 사용하는 지표입니다. 전체 예측한 데이터 중에서 정답을 맞춘 것에 대한 비율입니다. TP, FP, FN, TN을 가지고 수식을 설명하면 다음과 같습니다.\\n$$ \\\\text{정확도} = \\\\frac{TP+TN}{TP + FN+FP+TN}$$', '$$ \\\\text{정확도} = \\\\frac{TP+TN}{TP + FN+FP+TN}$$\\n그런데 Accuracy로 성능을 예측하는 것이 적절하지 않은 때가 있습니다. 비가 오는 날을 예측하는 모델을 만들었다고 했을 때, 200일 동안 총 6일만 비가 왔다고 해봅시다. 그런데 이 모델은 200일 내내 날씨가 맑았다고 예측했습니다. 이 모델은 200번 중 총 6회 틀렸습니다. 194/200=0.97이므로 정확도는 97%입니다. 하지만 정작 비가 온 날은 하나도 못 맞춘 셈입니다.\\n또 다른 예를 들어봅시다. 스팸 메일을 분류하는 스팸 메일 분류기를 만들었습니다. 메일 100개 중 스팸 메일은 5개였습니다. 스팸 메일 분류기는 모두 정상 메일이라고 탐지했습니다. 정확도는 95%입니다. 그런데 정작 스팸 메일은 하나도 못 찾아낸 셈입니다.', '이렇게 실질적으로 더 중요한 경우에 대한 데이터가 전체 데이터에서 너무 적은 비율을 차지한다면 정확도는 좋은 측정 지표가 될 수 없습니다. 이런 경우에는 F1-Score를 사용하며, 이에 대해서는 개체명 인식 챕터에서 설명하겠습니다.']\n",
      "['학생의 입장이 되어 하나의 문제지를 과하도록 많이 풀어서 문제 번호만 봐도 정답을 맞출 수 있게 되었다고 가정합시다. 그런데 너무 오랜 시간 하나의 문제지만 반복해서 푼 나머지 다른 문제를 풀거나 시험을 봤을 때 점수가 안 좋다면 그게 의미가 있을까요?\\n머신 러닝에서 과적합(Overfitting) 이란 위의 비유처럼 훈련 데이터를 과하게 학습한 경우를 말합니다. 머신 러닝 모델이 학습에 사용하는 훈련 데이터는 실제로 앞으로 기계가 풀어야 할 현실의 수많은 문제에 비하면 극히 일부에 불과한 데이터입니다. 기계가 훈련 데이터에 대해서만 과하게 학습하면 성능 측정을 위한 데이터인 테스트 데이터나 실제 서비스에서는 정확도가 좋지 않은 현상이 발생합니다.', '과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 커집니다. 아래의 그래프는 과적합 상황에서 발생할 수 있는 훈련 데이터에 대한 훈련 횟수에 따른 훈련 데이터의 오차와 테스트 데이터의 오차(또는 손실이라고도 부릅니다.)의 변화를 보여줍니다.\\n[이미지: ]\\n위 그래프는 뒤의 RNN을 이용한 텍스트 분류 챕터의 스팸 메일 분류하기 실습에서 훈련 데이터에 대한 훈련 횟수를 30 에포크로 주어서 의도적으로 과적합을 발생시킨 그래프입니다. y축은 오차(loss), X축의 에포크(epoch)는 전체 훈련 데이터에 대한 훈련 횟수를 의미하며, 사람으로 비유하면 동일한 문제지(훈련 데이터)를 반복해서 푼 횟수입니다. 에포크가 지나치게 크면 훈련 데이터에 과적합이 발생합니다.', '스팸 메일 분류하기 실습은 에포크가 3~4에서 테스트 데이터에 대한 정확도가 가장 높고, 에포크가 그 이상을 넘어가면 과적합이 발생합니다. 위의 그래프는 에포크가 증가할수록 테스트 데이터에 대한 오차가 점차 증가하는 양상을 보여줍니다. 과적합은 다르게 설명하면 훈련 데이터에 대한 정확도는 높지만, 테스트 데이터는 정확도가 낮은 상황이라고 말할 수도 있습니다. 이런 상황을 방지하기 위해서는 테스트 데이터의 오차가 증가하기 전이나, 정확도가 감소하기 전에 훈련을 멈추는 것이 바람직합니다.\\n반면, 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태를 과소적합(Underfitting) 이라고 합니다. 과소 적합은 훈련 자체가 부족한 상태이므로 훈련 횟수인 에포크가 지나치게 적으면 발생할 수 있습니다. 과대 적합과는 달리 과소 적합은 훈련 자체를 너무 적게한 상태이므로 훈련 데이터에 대해서도 정확도가 낮다는 특징이 있습니다.', '이러한 두 가지 현상을 과적합과 과소 적합이라고 부르는 이유는 머신 러닝에서 학습 또는 훈련이라고 하는 과정을 적합(fitting)이라고도 부르기 때문입니다. 모델이 주어진 데이터에 대해서 적합해져가는 과정이기 때문입니다. 이러한 이유로 케라스에서는 기계를 학습시킬 때 fit()을 호출합니다. 바로 뒤의 선형 회귀 실습에서 보게 될 것입니다.\\n딥 러닝을 할 때는 과적합을 막을 수 있는 드롭 아웃(Dropout), 조기 종료(Early Stopping)과 같은 몇 가지 방법이 존재하는데 이는 뒤의 딥 러닝 챕터에서 소개합니다.', '과적합과 과소 적합을 설명하면서 테스트 데이터를 사용하여 판단할 수 있다고 설명하였지만, 더 정확히 설명하면 현업에서는 테스트 데이터를 두 가지 용도로 분리하여 사용하는 것이 더 바람직합니다. 각각의 용도는 과적합 모니터링과 하이퍼파라미터 튜닝을 위한 테스트 데이터와 오직 성능 평가만을 위한 테스트 데이터입니다. 그리고 전자의 테스트 데이터를 검증 데이터라고 부릅니다. 앞에서 데이터를 훈련 데이터, 검증 데이터, 테스트 데이터 세 가지로 나누어야 한다고 언급했던 것을 기억하시나요? 과적합 방지를 고려한 일반적인 딥 러닝 모델의 학습 과정은 다음과 같습니다.\\nStep 1. 주어진 데이터를 훈련 데이터, 검증 데이터, 테스트 데이터로 나눈다. 가령, 6:2:2 비율로 나눌 수 있다.\\nStep 2. 훈련 데이터로 모델을 학습한다. (에포크 +1)\\nStep 3. 검증 데이터로 모델을 평가하여 검증 데이터에 대한 정확도와 오차(loss)를 계산한다.', 'Step 2. 훈련 데이터로 모델을 학습한다. (에포크 +1)\\nStep 3. 검증 데이터로 모델을 평가하여 검증 데이터에 대한 정확도와 오차(loss)를 계산한다.\\nStep 4. 검증 데이터의 오차가 증가하였다면 과적합 징후이므로 학습 종료 후 Step 5로 이동, 아니라면 Step 2.로 재이동한다.\\nStep 5. 모델의 학습이 종료되었으니 테스트 데이터로 모델을 평가한다.\\n==================================================\\n--- 06-03 선형 회귀(Linear Regression) ---', '==================================================\\n--- 06-03 선형 회귀(Linear Regression) ---\\n딥 러닝을 이해하기 위해서는 선형 회귀(Linear Regression)와 로지스틱 회귀(Logsitic Regression)를 이해할 필요가 있습니다. 이번 챕터에서는 머신 러닝에서 쓰이는 용어인 가설(Hypothesis), 손실 함수(Loss Function) 그리고 경사 하강법(Gradient Descent)에 대한 개념과 선형 회귀에 대해서 이해합니다.']\n",
      "['시험 공부하는 시간을 늘리면 늘릴 수록 성적이 잘 나옵니다. 하루에 걷는 횟수를 늘릴 수록, 몸무게는 줄어듭니다. 집의 평수가 클수록, 집의 매매 가격은 비싼 경향이 있습니다. 이는 수학적으로 생각해보면 어떤 요인의 수치에 따라서 특정 요인의 수치가 영향을 받고있다고 말할 수 있습니다. 조금 더 수학적인 표현을 써보면 어떤 변수의 값에 따라서 특정 변수의 값이 영향을 받고 있다고 볼 수 있습니다. 다른 변수의 값을 변하게하는 변수를 $x$, 변수 $x$에 의해서 값이 종속적으로 변하는 변수 $y$라고 해봅시다.\\n이때 변수 $x$의 값은 독립적으로 변할 수 있는 것에 반해, $y$값은 계속해서 $x$의 값에 의해서, 종속적으로 결정되므로 $x$를 독립 변수, $y$를 종속 변수라고도 합니다. 선형 회귀는 한 개 이상의 독립 변수 $x$와 $y$의 선형 관계를 모델링합니다. 만약, 독립 변수 $x$가 1개라면 단순 선형 회귀라고 합니다.', '1) 단순 선형 회귀 분석(Simple Linear Regression Analysis)\\n$$y = {wx + b}$$\\n위의 수식은 단순 선형 회귀의 수식을 보여줍니다. 여기서 독립 변수 $x$와 곱해지는 값 $w$를 머신 러닝에서는 가중치(weight), 별도로 더해지는 값 $b$를 편향(bias)이라고 합니다. 직선의 방정식에서는 각각 직선의 기울기와 절편을 의미합니다. $w$와 $b$가 없이 $y$와 $x$란 수식은 $y$는 $x$와 같다는 하나의 식밖에 표현하지 못합니다. 그래프 상으로 말하면 하나의 직선밖에 표현하지 못합니다.\\n$$y = {x}$$\\n다시 말해 $w$와 $b$의 값에 따라서 $x$와 $y$가 표현하는 직선은 무궁무진해집니다.\\n2) 다중 선형 회귀 분석(Multiple Linear Regression Analysis)\\n$$y = {w_1x_1 + w_2x_2 + ... w_nx_n + b}$$', '2) 다중 선형 회귀 분석(Multiple Linear Regression Analysis)\\n$$y = {w_1x_1 + w_2x_2 + ... w_nx_n + b}$$\\n집의 매매 가격은 단순히 집의 평수가 크다고 결정되는 게 아니라 집의 층의 수, 방의 개수, 지하철 역과의 거리와도 영향이 있습니다. 이러한 다수의 요소를 가지고 집의 매매 가격을 예측해보고 싶습니다. $y$는 여전히 1개이지만 이제 $x$는 1개가 아니라 여러 개가 되었습니다. 이를 다중 선형 회귀 분석이라고 합니다. 이에 대한 실습은 뒤에서 진행합니다.']\n",
      "['단순 선형 회귀를 가지고 문제를 풀어봅시다. 어떤 학생의 공부 시간에 따라서 다음과 같은 점수를 얻었다는 데이터가 있습니다.\\nhours($x$)\\nscore($y$)\\n2\\n25\\n3\\n50\\n4\\n42\\n5\\n61\\n이를 좌표 평면에 그려보면 다음과 같습니다.\\n[이미지: ]\\n알고있는 데이터로부터 $x$와 $y$의 관계를 유추하고, 이 학생이 6시간, 7시간, 8시간을 공부하였을 때의 성적을 예측해보고 싶습니다. $x$와 $y$의 관계를 유추하기 위해서 수학적으로 식을 세워보게 되는데 머신 러닝에서는 이러한 식을 가설(Hypothesis) 이라고 합니다. 아래의 $H(x)$에서 $H$는 Hypothesis를 의미합니다.\\n$$H(x) = {wx + b}$$\\n[이미지: ]', '$$H(x) = {wx + b}$$\\n[이미지: ]\\n위의 그림은 $w$와 $b$의 값에 따라서 천차만별로 그려지는 직선의 모습을 보여줍니다. 중학교 수학 과정인 직선의 방정식을 알고있다면, 위의 가설에서 $w$는 직선의 기울기이고 $b$는 절편으로 직선을 표현함을 알 수 있습니다. 결국 선형 회귀는 주어진 데이터로부터 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그리는 일을 말합니다. 그리고 어떤 직선인지 결정하는 것은 $w$와 $b$의 값이므로 선형 회귀에서 해야할 일은 결국 적절한 $w$와 $b$를 찾아내는 일이 됩니다.', '아직은 방법을 모르지만, 어떤 방법을 사용하여 적절한 $w$와 $b$의 값을 찾은 덕택에 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 위의 좌표 평면 상에서 그렸다고 한 번 가정해보겠습니다. 이 직선을 $x$가 6일때, 7일때, 8일때에 대해서도 계속해서 직선을 그저 이어그린다면 이 학생이 6시간을 공부했을 때, 7시간을 공부했을 때, 8시간을 공부했을 때의 예상 점수를 말할 수 있게 됩니다. 그저 $x$가 각각 6일 때, 7일 때, 8일 때의 $y$값을 확인하면 되기 때문입니다.']\n",
      "['앞서 주어진 데이터에서 $x$와 $y$의 관계를 $w$와 $b$를 이용하여 식을 세우는 일을 가설이라고 언급했습니다. 그리고 이제 해야할 일은 문제에 대한 규칙을 가장 잘 표현하는 $w$와 $b$를 찾는 일입니다. 머신 러닝은 $w$와 $b$를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 $w$와 $b$를 찾아냅니다.', '이때 실제값과 예측값에 대한 오차에 대한 식을 목적 함수(Objective function) 또는 비용 함수(Cost function) 또는 손실 함수(Loss function) 라고 합니다. 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수를 목적 함수(Objective function)라고 합니다. 그리고 값을 최소화하려고 하면 이를 비용 함수(Cost function) 또는 손실 함수(Loss function)라고 합니다. 이 세 가지는 엄밀히는 같은 의미는 아니지만, 이 책에서는 목적 함수, 비용 함수, 손실 함수란 용어를 같은 의미로 혼용해서 사용합니다.', '비용 함수는 단순히 실제값과 예측값에 대한 오차를 표현하면 되는 것이 아니라, 예측값의 오차를 줄이는 일에 최적화 된 식이어야 합니다. 앞으로 배울 러닝, 딥 러닝에는 다양한 문제들이 있고, 각 문제들에는 적합한 비용 함수들이 있습니다. 회귀 문제의 경우에는 주로 평균 제곱 오차(Mean Squared Error, MSE)가 사용됩니다.\\n[이미지: ]\\n위의 그래프에 임의의 $w$의 값 13과 임의의 $b$의 값 1을 가진 직선을 그렸습니다. 임의로 그린 직선으로 정답이 아닙니다. 이제 이 직선으로부터 서서히 $w$와 $b$의 값을 바꾸면서 정답인 직선을 찾아내야 합니다.', '사실 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그린다는 것은 위의 그림에서 모든 점들과 위치적으로 가장 가까운 직선을 그린다는 것과 같습니다. 이제 오차(error)를 정의하겠습니다. 오차는 주어진 데이터에서 각 $x$에서의 실제값 $y$와 위의 직선에서 예측하고 있는 $H(x)$값의 차이를 말합니다. 즉, 위의 그림에서 ↕는 각 점에서의 오차의 크기를 보여줍니다. 오차를 줄여가면서 $w$와 $b$의 값을 찾아내기 위해서는 전체 오차의 크기를 구해야 합니다.\\n오차의 크기를 측정하기 위한 가장 기본적인 방법은 각 오차를 모두 더하는 방법이 있습니다. 위의 $y = {13x + 1}$ 직선이 예측한 예측값을 각각 실제값으로부터 오차를 계산하여 표를 만들어보면 아래와 같습니다.\\nhours($x$)\\n2\\n3\\n4\\n5\\n실제값\\n25\\n50\\n42\\n61\\n예측값\\n27\\n40\\n53\\n66\\n오차\\n-2\\n10\\n-9\\n-5', \"hours($x$)\\n2\\n3\\n4\\n5\\n실제값\\n25\\n50\\n42\\n61\\n예측값\\n27\\n40\\n53\\n66\\n오차\\n-2\\n10\\n-9\\n-5\\n그런데, 수식적으로 단순히 '오차 = 실제값 - 예측값' 이라고 정의한 후에 모든 오차를 더하면 음수 오차도 있고, 양수 오차도 있으므로 오차의 절대적인 크기를 구할 수가 없습니다. 그래서 모든 오차를 제곱하여 더하는 방법을 사용합니다. 다시 말해 위의 그림에서의 모든 점과 직선 사이의 ↕ 거리를 제곱하고 모두 더합니다. 이를 수식으로 표현하면 아래와 같습니다. 단, 여기서 $n$은 갖고 있는 데이터의 개수를 의미합니다.\\n$$\\n\\\\sum_{i=1}^{n} \\\\left[y^{(i)} - H(x^{(i)})\\\\right]^2 = (-2)^{2} + 10^{2} + (-9)^{2} + (-5)^{2} = 210\\n$$\", '$$\\n이때 데이터의 개수인 $n$으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있는데 이를 평균 제곱 오차(Mean Squered Error, MSE)라고 합니다. 수식은 아래와 같습니다.\\n$$\\n\\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left[y^{(i)} - H(x^{(i)})\\\\right]^2 = 210 / 4 = 52.5\\n$$\\n$y = {13x + 1}$의 예측값과 실제값의 평균 제곱 오차의 값은 52.5입니다. 평균 제곱 오차의 값을 최소값으로 만드는 $w$와 $b$를 찾아내는 것이 정답인 직선을 찾아내는 일입니다. 평균 제곱 오차를 $w$와 $b$에 의한 비용 함수(Cost function)로 재정의해보면 다음과 같습니다.\\n$$\\ncost(w, b) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left[y^{(i)} - H(x^{(i)})\\\\right]^2\\n$$', '$$\\ncost(w, b) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left[y^{(i)} - H(x^{(i)})\\\\right]^2\\n$$\\n모든 점들과의 오차가 클 수록 평균 제곱 오차는 커지며, 오차가 작아질 수록 평균 제곱 오차는 작아집니다. 그러므로 이 평균 최소 오차. 즉, $Cost(w, b)$를 최소가 되게 만드는 $w$와 $b$를 구하면 결과적으로 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그릴 수 있습니다.\\n$$w, b → minimize\\\\ cost(w, b)$$']\n",
      "['선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 $w$와 $b$을 찾기 위한 작업을 수행합니다. 이때 사용되는 알고리즘을 옵티마이저(Optimizer) 또는 최적화 알고리즘이라고 부릅니다.\\n그리고 이 옵티마이저를 통해 적절한 $w$와 $b$를 찾아내는 과정을 머신 러닝에서 훈련(training) 또는 학습(learning)이라고 부릅니다. 여기서는 가장 기본적인 옵티마이저 알고리즘인 경사 하강법(Gradient Descent)에 대해서 배웁니다.\\n경사 하강법을 이해하기 위해서 cost와 기울기 $w$와의 관계를 이해해보겠습니다. $w$는 머신 러닝 용어로는 가중치라고 불리지만, 직선의 방정식 관점에서 보면 직선의 기울기를 의미하고 있습니다. 아래의 그래프는 기울기 $w$가 지나치게 높거나, 낮을 때 어떻게 오차가 커지는 모습을 보여줍니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림에서 주황색선은 기울기 $w$가 20일 때, 초록색선은 기울기 $w$가 1일 때를 보여줍니다. 각각 $y = {20x}$, $y = {x}$에 해당되는 직선입니다. ↕는 각 점에서의 실제값과 두 직선의 예측값과의 오차를 보여줍니다. 이는 앞서 예측에 사용했던 $y = {13x + 1}$ 직선보다 확연히 큰 오차값들입니다. 즉, 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커집니다. 사실 $b$ 또한 마찬가지인데 $b$가 지나치게 크거나 작으면 오차가 커집니다.\\n설명의 편의를 위해 편향 $b$가 없이 단순히 가중치 $w$만을 사용한 $y=wx$라는 가설 $H(x)$를 가지고, 경사 하강법을 수행한다고 해보겠습니다. 비용 함수의 값 $cost(w)$는 cost라고 줄여서 표현해보겠습니다. 이에 따라 $w$와 cost의 관계를 그래프로 표현하면 다음과 같습니다.\\n[이미지: ]', '[이미지: ]\\n기울기 $w$가 무한대로 커지면 커질 수록 cost의 값 또한 무한대로 커지고, 반대로 기울기 $w$가 무한대로 작아져도 cost의 값은 무한대로 커집니다. 위의 그래프에서 cost가 가장 작을 때는 볼록한 부분의 맨 아래 부분입니다. 기계가 해야할 일은 cost가 가장 최소값을 가지게 하는 $w$를 찾는 일이므로, 볼록한 부분의 맨 아래 부분의 $w$의 값을 찾아야 합니다.\\n[이미지: ]\\n[이미지: ]\\n위의 그림에서 초록색 선은 $w$가 임의의 값을 가지게 되는 네 가지의 경우에 대해서, 그래프 상으로 접선의 기울기를 보여줍니다. 주목할 것은 맨 아래의 볼록한 부분으로 갈수록 접선의 기울기가 점차 작아진다는 점입니다. 그리고 맨 아래의 볼록한 부분에서는 결국 접선의 기울기가 0이 됩니다. 그래프 상으로는 초록색 화살표가 수평이 되는 지점입니다.', '즉, cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점입니다. 경사 하강법의 아이디어는 비용 함수(Cost function)를 미분하여 현재 $w$에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 $w$의 값을 변경하고 다시 미분하고 이 과정을 접선의 기울기가 0인 곳을 향해 $w$의 값을 변경하는 작업을 반복하는 것에 있습니다.\\n비용 함수(Cost function)는 아래와 같았습니다.\\n$$cost(w, b) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left[y^{(i)} - H(x^{(i)})\\\\right]^2$$\\n이제 비용(cost)를 최소화하는 $w$를 구하기 위해 $w$를 업데이트하는 식은 다음과 같습니다. 이를 접선의 기울기가 0이 될 때까지 반복합니다.\\n$$ w := w - α\\\\frac{∂}{∂w}cost(w) $$', '$$ w := w - α\\\\frac{∂}{∂w}cost(w) $$\\n위의 식은 현재 $w$에서의 접선의 기울기와 $α$와 곱한 값을 현재 $w$에서 빼서 새로운 $w$의 값으로 한다는 것을 의미합니다. $α$는 여기서 학습률(learning rate)이라고 하는데, 우선 $α$는 생각하지 않고 현재 $w$에서 현재 $w$에서의 접선의 기울기를 빼는 행위가 어떤 의미가 있는지 알아보겠습니다.\\n[이미지: ]\\n위의 그림은 접선의 기울기가 음수일 때, 0일때, 양수일 때를 보여줍니다. 접선의 기울기가 음수일 때의 수식은 아래와 같이 표현할 수 있습니다.\\n$$\\nw := w - α(\\\\text{음수 기울기})\\n$$', \"$$\\nw := w - α(\\\\text{음수 기울기})\\n$$\\n기울기가 음수면 '음수를 빼는 것'은 곧 '해당 값을 양수로 바꾸고 더하는 것'과 같습니다. (가령, 어떤 수에서 -2를 뺀다는 것은 해당 숫자에 2를 더하는 것과 같습니다.) 결국 음수 기울기를 빼면 $w$의 값이 증가하게 되는데 이는 결과적으로 접선의 기울기가 0인 방향으로 $w$의 값이 조정됩니다. 만약, 접선의 기울기가 양수라면 위의 수식은 아래와 같이 표현할 수 있습니다.\\n$$\\nw := w - α(\\\\text{양수 기울기})\\n$$\\n기울기가 양수면 $w$의 값이 감소하게 되는데 이는 결과적으로 기울기가 0인 방향으로 $w$의 값이 조정됩니다. 결국, 아래의 수식은 접선의 기울기가 음수거나, 양수일 때 모두 접선의 기울기가 0인 방향으로 $w$의 값을 조정합니다.\\n$$\\nw := w - α\\\\frac{∂}{∂w}cost(w)\\n$$\", '$$\\nw := w - α\\\\frac{∂}{∂w}cost(w)\\n$$\\n그렇다면 여기서 학습률(learning rate)이라고 말하는 $α$는 어떤 의미를 가질까요? 학습률 $α$은 $w$의 값을 변경할 때, 얼마나 크게 변경할지를 결정하며 0과 1사이의 값을 가지도록 합니다. 예를 들어서 0.01이 될 수 있겠습니다. 학습률은 $w$를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정합니다. 직관적으로 생각하기에 학습률 $α$의 값을 무작정 크게 하면 접선의 기울기가 최소값이 되는 $w$를 빠르게 찾을 수 있을 것같지만 그렇지 않습니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림은 학습률 $α$가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 $w$를 찾아가는 것이 아니라 $cost$의 값이 발산하는 상황을 보여줍니다. 반대로 학습률 $α$가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 $α$의 값을 찾아내는 것도 중요합니다. 지금까지는 $b$는 배제시키고 최적의 $w$를 찾아내는 것에만 초점을 맞추어 경사 하강법의 원리에 대해서 배웠는데, 실제 경사 하강법은 $w$와 $b$에 대해서 동시에 경사 하강법을 수행하면서 최적의 $w$와 $b$의 값을 찾아갑니다.\\n정리하자면 가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념입니다. 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있으며 선형 회귀에 가장 적합한 비용 함수와 옵티마이저가 알려져있는데 이번 챕터에서 언급된 MSE와 경사 하강법이 각각 이에 해당됩니다.', '==================================================\\n--- 06-04 자동 미분과 선형 회귀 실습 ---\\n```\\n[[98.556465]]\\n```선형 회귀를 텐서플로우와 케라스를 통해 구현해봅시다.']\n",
      "['import tensorflow as tf\\ntape_gradient()는 자동 미분 기능을 수행합니다. 임의로 $2w^2+5$라는 식을 세워보고, $w$에 대해 미분해보겠습니다.\\nw = tf.Variable(2.)\\ndef f(w):\\ny = w**2\\nz = 2*y + 5\\nreturn z\\ngradients를 출력하면 $w$에 대해 미분한 값이 저장된 것을 확인할 수 있습니다.\\nwith tf.GradientTape() as tape:\\nz = f(w)\\ngradients = tape.gradient(z, [w])\\nprint(gradients)\\n[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\\n이 자동 미분 기능을 사용하여 선형 회귀를 구현해봅시다.']\n",
      "['우선 가중치 변수 w와 b를 선언합니다. 학습될 값이므로 임의의 값인 4와 1로 초기화하였습니다.\\n# 학습될 가중치 변수를 선언\\nw = tf.Variable(4.0)\\nb = tf.Variable(1.0)\\n가설을 함수로서 정의합니다.\\n@tf.function\\ndef hypothesis(x):\\nreturn w*x + b\\n현재의 가설에서 w와 b는 각각 4와 1이므로 임의의 입력값을 넣었을 때의 결과는 다음과 같습니다.\\nx_test = [3.5, 5, 5.5, 6]\\nprint(hypothesis(x_test).numpy())\\n[15. 21. 23. 25.]\\n다음과 같이 평균 제곱 오차를 손실 함수로서 정의합니다.\\n@tf.function\\ndef mse_loss(y_pred, y):\\n# 두 개의 차이값을 제곱을 해서 평균을 취한다.\\nreturn tf.reduce_mean(tf.square(y_pred - y))\\n여기서 사용할 데이터는 x와 y가 약 10배의 차이를 가지는 데이터입니다.', 'return tf.reduce_mean(tf.square(y_pred - y))\\n여기서 사용할 데이터는 x와 y가 약 10배의 차이를 가지는 데이터입니다.\\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\\ny = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\\n옵티마이저는 경사 하강법을 사용하되, 학습률(learning rate)는 0.01을 사용합니다.\\noptimizer = tf.optimizers.SGD(0.01)\\n약 300번에 걸쳐서 경사 하강법을 수행하겠습니다.\\nfor i in range(301):\\nwith tf.GradientTape() as tape:\\n# 현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred\\ny_pred = hypothesis(x)\\n# 평균 제곱 오차를 계산\\ncost = mse_loss(y_pred, y)\\n# 손실 함수에 대한 파라미터의 미분값 계산', 'y_pred = hypothesis(x)\\n# 평균 제곱 오차를 계산\\ncost = mse_loss(y_pred, y)\\n# 손실 함수에 대한 파라미터의 미분값 계산\\ngradients = tape.gradient(cost, [w, b])\\n# 파라미터 업데이트\\noptimizer.apply_gradients(zip(gradients, [w, b]))\\nif i % 10 == 0:\\nprint(\"epoch : {:3} | w의 값 : {:5.4f} | b의 값 : {:5.4} | cost : {:5.6f}\".format(i, w.numpy(), b.numpy(), cost))\\nepoch :   0 | w의 값 : 8.2133 | b의 값 : 1.664 | cost : 1402.555542\\n... 중략 ...\\nepoch : 280 | w의 값 : 10.6221 | b의 값 : 1.191 | cost : 1.091434', '... 중략 ...\\nepoch : 280 | w의 값 : 10.6221 | b의 값 : 1.191 | cost : 1.091434\\nepoch : 290 | w의 값 : 10.6245 | b의 값 : 1.176 | cost : 1.088940\\nepoch : 300 | w의 값 : 10.6269 | b의 값 : 1.161 | cost : 1.086645\\nw와 b값이 계속 업데이트 됨에 따라서 cost가 지속적으로 줄어드는 것을 확인할 수 있습니다. 학습된 w와 b의 값에 대해서 임의 입력을 넣었을 경우의 예측값을 확인해봅시다.\\nx_test = [3.5, 5, 5.5, 6]\\nprint(hypothesis(x_test).numpy())\\n[38.35479  54.295143 59.608593 64.92204 ]', 'print(hypothesis(x_test).numpy())\\n[38.35479  54.295143 59.608593 64.92204 ]\\n모델을 구현하는 방법은 한 가지가 아닙니다. 텐서플로우의 경우, 케라스라는 고수준의 API를 사용하면 모델을 이보다 좀 더 쉽게 구현할 수 있습니다. 이번에는 선형 회귀 모델을 케라스로 구현해봅시다.']\n",
      "['케라스에 대해서는 뒤의 딥 러닝 챕터에서 더 자세히 배우겠지만, 간단하게 케라스를 이용해서 선형 회귀를 구현해봅시다. 케라스로 모델을 만드는 기본적인 형식은 다음과 같습니다. Sequential로 model이라는 이름의 모델을 만들고, 그리고 add를 통해 입력과 출력 벡터의 차원과 같은 필요한 정보들을 추가해갑니다.\\n아래의 예시 코드를 봅시다. 첫번째 인자인 1은 출력의 차원을 정의합니다. 일반적으로 output_dim으로 표현되는 인자입니다. 두번째 인자인 input_dim은 입력의 차원을 정의하는데 이번 실습과 같이 1개의 실수 $x$를 가지고 하는 1개의 실수 $y$를 예측하는 단순 선형 회귀를 구현하는 경우에는 각각 1의 값을 가집니다..\\n# 예시 코드. 실행 불가.\\nmodel = Sequential()\\nmodel.add(keras.layers.Dense(1, input_dim=1))', '# 예시 코드. 실행 불가.\\nmodel = Sequential()\\nmodel.add(keras.layers.Dense(1, input_dim=1))\\n실습을 진행해봅시다. 아래의 코드는 간단하지만, 지금까지 배운 것들이 집대성 된 코드입니다. 우선 공부한 시간을 $x$, 각 공부한 시간에 따른 성적을 $y$라고 해봅시다. activation은 어떤 함수를 사용할 것인지를 의미하는데 선형 회귀를 사용할 경우에는 linear라고 기재합니다.\\n옵티마이저로 기본 경사 하강법을 사용하고 싶다면, sgd라고 기재합니다. 학습률은 0.01로 정하였습니다. 손실 함수로는 평균 제곱 오차를 사\\n용합니다. 그리고 전체 데이터에 대한 훈련 횟수는 300으로 합니다.\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.models import Sequential', \"import numpy as np\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras import optimizers\\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\\ny = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\\nmodel = Sequential()\\n# 출력 y의 차원은 1. 입력 x의 차원(input_dim)은 1\\n# 선형 회귀이므로 activation은 'linear'\\nmodel.add(Dense(1, input_dim=1, activation='linear'))\\n# sgd는 경사 하강법을 의미. 학습률(learning rate, lr)은 0.01.\", \"# sgd는 경사 하강법을 의미. 학습률(learning rate, lr)은 0.01.\\nsgd = optimizers.SGD(lr=0.01)\\n# 손실 함수(Loss function)은 평균제곱오차 mse를 사용합니다.\\nmodel.compile(optimizer=sgd, loss='mse', metrics=['mse'])\\n# 주어진 x와 y데이터에 대해서 오차를 최소화하는 작업을 300번 시도합니다.\\nmodel.fit(x, y, epochs=300)\\n학습이 끝났습니다. 최종적으로 선택된 오차를 최소화하는 직선을 그래프로 그려보겠습니다.\\nplt.plot(x, model.predict(x), 'b', x, y, 'k.')\\n[이미지: ]\", \"plt.plot(x, model.predict(x), 'b', x, y, 'k.')\\n[이미지: ]\\n위의 그래프에서 각 점은 우리가 실제 주었던 실제값에 해당되며, 직선은 실제값으로부터 오차를 최소화하는 $w$와 $b$의 값을 가지는 직선입니다. 이 직선을 통해 9시간 30분을 공부하였을 때의 시험 성적을 예측하게 해봅시다. model.predict()은 학습이 완료된 모델이 입력된 데이터에 대해서 어떤 값을 예측하는지를 보여줍니다.\\nprint(model.predict([9.5]))\\n[[98.556465]]\\n9시간 30분을 공부하면 약 98.5점을 얻는다고 예측하고 있습니다.\\n==================================================\\n--- 06-05 로지스틱 회귀(Logistic Regression) ---\\n```\\ndef sigmoid(x):\\nreturn 1/(1+np.exp(-x))\\nx = np.arange(-5.0, 5.0, 0.1)\", \"```\\ndef sigmoid(x):\\nreturn 1/(1+np.exp(-x))\\nx = np.arange(-5.0, 5.0, 0.1)\\ny1 = sigmoid(x+0.5)\\ny2 = sigmoid(x+1)\\ny3 = sigmoid(x+1.5)\\nplt.plot(x, y1, 'r', linestyle='--') # x + 0.5\\nplt.plot(x, y2, 'g') # x + 1\\nplt.plot(x, y3, 'b', linestyle='--') # x + 1.5\\nplt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\\nplt.title('Sigmoid Function')\\nplt.show()\", \"plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\\nplt.title('Sigmoid Function')\\nplt.show()\\n```일상 속 풀고자하는 많은 문제 중에서는 두 개의 선택지 중에서 정답을 고르는 문제가 많습니다. 예를 들어 시험을 봤는데 이 시험 점수가 합격인지 불합격인지가 궁금할 수도 있고, 어떤 메일을 받았을 때 이게 정상 메일인지 스팸 메일인지를 분류하는 문제도 그렇습니다. 이렇게 둘 중 하나를 결정하는 문제를 이진 분류(Binary Classification)라고 합니다. 그리고 이런 문제를 풀기 위한 대표적인 알고리즘으로 로지스틱 회귀(Logistic Regression)가 있습니다.\"]\n",
      "['앞서 선형 회귀를 설명하며 공부 시간과 성적 간의 관계를 직선의 방정식으로 표현한다는 가설 하에, 주어진 데이터로부터 가중치 $w$와 편향 $b$를 찾아 데이터를 가장 잘 표현하는 직선을 찾았습니다. 그런데 이번에 배울 둘 중 하나의 선택지 중에서 정답을 고르는 이진 분류 문제는 직선으로 표현하는 것이 적절하지 않습니다.\\n학생들이 시험 성적에 따라서 합격, 불합격이 기재된 데이터가 있다고 가정해봅시다. 시험 성적이 $x$라면, 합불 결과는 $y$입니다. 이 데이터로부터 특정 점수를 얻었을 때의 합격, 불합격 여부를 판정하는 모델을 만들고자 합시다.\\nscore($x$)\\nresult($y$)\\n45\\n불합격\\n50\\n불합격\\n55\\n불합격\\n60\\n합격\\n65\\n합격\\n70\\n합격\\n위 데이터에서 합격을 1, 불합격을 0이라고 하였을 때 그래프를 그려보면 아래와 같습니다.\\n[이미지: ]', '45\\n불합격\\n50\\n불합격\\n55\\n불합격\\n60\\n합격\\n65\\n합격\\n70\\n합격\\n위 데이터에서 합격을 1, 불합격을 0이라고 하였을 때 그래프를 그려보면 아래와 같습니다.\\n[이미지: ]\\n이러한 점들을 표현하는 그래프는 알파벳의 S자 형태로 표현됩니다. 이러한 $x$와 $y$의 관계를 표현하기 위해서는 직선을 표현하는 함수가 아니라 S자 형태로 표현할 수 있는 함수가 필요합니다. 직선을 사용할 경우 보통 분류 작업이 제대로 동작하지 않습니다.', '이번 예제의 경우 실제값. 즉, 레이블에 해당하는 $y$가 0 또는 1이라는 두 가지 값만을 가지므로, 이 문제를 풀기 위해서 예측값은 0과 1사이의 값을 가지도록 합니다. 0과 1사이의 값을 확률로 해석하면 문제를 풀기가 훨씬 용이해집니다. 최종 예측값이 0.5보다 작으면 0으로 예측했다고 판단하고, 0.5보다 크면 1로 예측했다고 판단합니다. 만약 $y=wx+b$의 직선을 사용할 경우, $y$값이 음의 무한대부터 양의 무한대와 같은 큰 수들도 가질 수 있는데 이는 직선이 분류 문제에 적합하지 않은 두번째 이유입니다.\\n출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수로 시그모이드 함수(Sigmoid function)가 있습니다.']\n",
      "['시그모이드 함수는 종종 σ로 축약해서 표현하기도 합니다. 로지스틱 회귀를 풀기 위한 가설을 세워봅시다.\\n$$ H(x) = \\\\frac{1}{1 + e^{-(wx + b)}} = sigmoid(wx + b) = σ(wx + b)$$\\n여기서 e(e=2.718281...)는 자연 상수라 불리는 숫자입니다. 여기서 구해야할 것은 여전히 주어진 데이터에 가장 적합한 가중치 $w$(weight)와 편향 $b$(bias)입니다. 인공 지능 알고리즘이 하는 것은 결국 주어진 데이터에 적합한 가중치 $w$와 $b$를 구하는 것입니다.\\n시그모이드 함수를 그래프로 시각화해봅시다.\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n아래의 그래프는 $w$는 1, $b$는 0임을 가정한 그래프입니다.\\ndef sigmoid(x):\\nreturn 1/(1+np.exp(-x))\\nx = np.arange(-5.0, 5.0, 0.1)\\ny = sigmoid(x)', \"def sigmoid(x):\\nreturn 1/(1+np.exp(-x))\\nx = np.arange(-5.0, 5.0, 0.1)\\ny = sigmoid(x)\\nplt.plot(x, y, 'g')\\nplt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\\nplt.title('Sigmoid Function')\\nplt.show()\\n[이미지: ]\\n위의 그래프에서 시그모이드 함수는 출력값을 0과 1사이의 값으로 조정하여 반환합니다. 마치 S자의 모양을 연상시킵니다. $x$가 0일 때 출력값은 0.5의 값을 가집니다. $x$가 증가하면 1에 수렴합니다. 가중치 $w$와 편향 $b$이 출력값에 어떤 영향을 미치는지 시각화를 통해 알아보겠습니다. 우선 $w$의 값을 변화시키고 이에 따른 그래프를 확인해봅시다.\\ndef sigmoid(x):\\nreturn 1/(1+np.exp(-x))\\nx = np.arange(-5.0, 5.0, 0.1)\\ny1 = sigmoid(0.5*x)\", \"def sigmoid(x):\\nreturn 1/(1+np.exp(-x))\\nx = np.arange(-5.0, 5.0, 0.1)\\ny1 = sigmoid(0.5*x)\\ny2 = sigmoid(x)\\ny3 = sigmoid(2*x)\\nplt.plot(x, y1, 'r', linestyle='--') # w의 값이 0.5일때\\nplt.plot(x, y2, 'g') # w의 값이 1일때\\nplt.plot(x, y3, 'b', linestyle='--') # w의 값이 2일때\\nplt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\\nplt.title('Sigmoid Function')\\nplt.show()\\n[이미지: ]\", \"plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\\nplt.title('Sigmoid Function')\\nplt.show()\\n[이미지: ]\\n위 그래프는 $w$의 값이 0.5일때 빨간색선, $w$의 값이 1일때는 초록색선, $w$의 값이 2일때 파란색선이 나오도록 하였습니다. $w$의 값에 따라 그래프의 경사도가 변합니다. 선형 회귀에서 직선을 표현할 때, 가중치 $w$는 직선의 기울기를 의미했지만 여기서는 그래프의 경사도를 결정합니다. $w$의 값이 커지면 경사가 커지고 $w$의 값이 작아지면 경사가 작아집니다.\\n$b$의 값에 따라서 그래프가 어떻게 변하는지 보겠습니다.\\ndef sigmoid(x):\\nreturn 1/(1+np.exp(-x))\\nx = np.arange(-5.0, 5.0, 0.1)\\ny1 = sigmoid(x+0.5)\\ny2 = sigmoid(x+1)\\ny3 = sigmoid(x+1.5)\", \"x = np.arange(-5.0, 5.0, 0.1)\\ny1 = sigmoid(x+0.5)\\ny2 = sigmoid(x+1)\\ny3 = sigmoid(x+1.5)\\nplt.plot(x, y1, 'r', linestyle='--') # x + 0.5\\nplt.plot(x, y2, 'g') # x + 1\\nplt.plot(x, y3, 'b', linestyle='--') # x + 1.5\\nplt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\\nplt.title('Sigmoid Function')\\nplt.show()\\n[이미지: ]\\n위 그래프는 $b$값에 따라서 그래프가 이동하는 것을 보여줍니다. 시그모이드 함수는 입력값이 커지면 1에 수렴하고, 입력값이 작아지면 0에 수렴합니다. 0부터의 1까지의 값을 가지는데 출력값이 0.5 이상이면 1(True), 0.5이하면 0(False)로 만들면 이진 분류 문제를 풀기 위해서 사용할 수 있습니다.\"]\n",
      "['로지스틱 회귀 또한 경사 하강법을 사용하여 가중치 $w$를 찾아내지만, 비용 함수로는 평균 제곱 오차를 사용하지 않습니다. 평균 제곱 오차를 로지스틱 회귀의 비용 함수로 사용했을 때는 좋지 않은 로컬 미니멈에 빠질 가능성이 지나치게 높아 문제 해결이 어렵습니다.\\n[이미지: ]\\n로지스틱 회귀에서 평균 제곱 오차를 비용 함수로 사용하면, 경사 하강법을 사용하였을때 찾고자 하는 최소값이 아닌 잘못된 최소값에 빠질 가능성이 매우 높습니다. 이를 전체 함수에 걸쳐 최소값인 글로벌 미니멈(Global Minimum) 이 아닌 특정 구역에서의 최소값인 로컬 미니멈(Local Minimum) 에 도달했다고 합니다. 로컬 미니멈에 지나치게 쉽게 빠지는 비용 함수는 cost가 가능한한 최소가 되는 가중치 $w$를 찾는다는 목적에는 좋지 않은 선택입니니다. 그리고 로지스틱 회귀에서의 평균 제곱 오차는 바로 그 좋지 않은 선택에 해당합니다.', '로지스틱 회귀라는 문제에서 가중치 $w$를 최소로 만드는 적절한 새로운 비용 함수를 찾아야 합니다. 가중치를 최소화하는 아래의 어떤 함수를 목적 함수라고 합시다. $J$는 목적 함수(objective function)를 의미합니다.\\n$$\\nJ(w) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} f\\\\left(H(x^{(i)}), y^{(i)})\\\\right)\\n$$', '$$\\nJ(w) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} f\\\\left(H(x^{(i)}), y^{(i)})\\\\right)\\n$$\\n아직 완성된 식이 아닙니다. 위의 식에서 샘플 데이터의 개수가 $n$개이고, 어떤 함수 $f$가 실제값 $y_{i}$와 예측값 $H(x_{i})$의 오차를 나타내는 함수라고 할 때, 여기서 새로운 함수 $f$를 어떻게 정의하느냐에 따라서 가중치를 최소화하는 적절한 목적 함수가 완성됩니다. 목적 함수는 전체 데이터에 대해서 어떤 함수 $f$의 값의 평균을 계산하고 있습니다. 적절한 가중치를 찾기 위해서 결과적으로 실제값과 예측값에 대한 오차를 줄여야 하므로 여기서 이 $f$는 비용 함수(cost function)라고 하겠습니다. 식을 다시 쓰면 아래와 같습니다.\\n$$\\nJ(w) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} cost\\\\left(H(x^{(i)}), y^{(i)})\\\\right)\\n$$', '$$\\nJ(w) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} cost\\\\left(H(x^{(i)}), y^{(i)})\\\\right)\\n$$\\n시그모이드 함수는 0과 1사이의 $y$값을 반환합니다. 이는 실제값이 0일 때 $y$값이 1에 가까워지면 오차가 커지며 실제값이 1일 때 $y$값이 0에 가까워지면 오차가 커짐을 의미합니다. 그리고 이를 반영할 수 있는 함수는 로그 함수를 통해 표현 가능합니다.\\n$$\\n\\\\text{if } y=1 → \\\\text{cost}\\\\left( H(x), y \\\\right) = -\\\\log(H(x))\\n$$\\n$$\\n\\\\text{if } y=0 → \\\\text{cost}\\\\left( H(x), y \\\\right) = -\\\\log(1-H(x))\\n$$\\n$y$의 실제값이 1일 때 $-logH(x)$ 그래프를 사용하고 $y$의 실제값이 0일 때 $-log(1-H(x))$ 그래프를 사용해야 합니다. 위의 두 식을 그래프 상으로 표현하면 아래와 같습니다.\\n[이미지: ]', '[이미지: ]\\n실제값 $y$가 1일 때의 그래프를 파란색 선으로 표현하였으며, 실제값 $y$가 0일 때의 그래프를 빨간색 선으로 표현하였습니다. 위의 그래프를 간략히 설명하면, 실제값이 1일 때, 예측값인 $H(x)$의 값이 1이면 오차가 0이므로 당연히 cost는 0이 됩니다. 반면, 실제값이 1일 때, $H(x)$가 0으로 수렴하면 cost는 무한대로 발산합니다. 실제값이 0인 경우는 그 반대로 이해하면 됩니다. 이는 다음과 같이 하나의 식으로 표현할 수 있습니다.\\n$$\\n\\\\text{cost}\\\\left( H(x), y \\\\right) = -[ylogH(x) + (1-y)log(1-H(x))]\\n$$', '$$\\n\\\\text{cost}\\\\left( H(x), y \\\\right) = -[ylogH(x) + (1-y)log(1-H(x))]\\n$$\\n자세히 보면 $y$와 $(1-y)$가 식 중간에 들어갔고, 두 개의 식을 -로 묶은 것 외에는 기존의 두 식이 들어가있는 것을 볼 수 있습니다. $y$가 0이면 $ylogH(x)$가 없어지고, $y$가 1이면 $(1-y)log(1-H(x))$가 없어지는데 이는 각각 $y$가 1일 때와 $y$가 0일 때의 앞서 본 식과 동일합니다.\\n결과적으로 로지스틱 회귀의 목적 함수는 아래와 같습니다.\\n$$\\nJ(w) = -\\\\frac{1}{n} \\\\sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]\\n$$', '$$\\nJ(w) = -\\\\frac{1}{n} \\\\sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]\\n$$\\n이때 로지스틱 회귀에서 찾아낸 비용 함수를 크로스 엔트로피(Cross Entropy)함수라고 합니다. 결론적으로 로지스틱 회귀는 비용 함수로 크로스 엔트로피 함수를 사용하며, 가중치를 찾기 위해서 크로스 엔트로피 함수의 평균을 취한 함수를 사용합니다. 크로스 엔트로피 함수는 소프트맥스 회귀의 비용 함수이기도 하므로 뒤에서 재언급합니다.\\n==================================================\\n--- 06-06 로지스틱 회귀 실습 ---\\n```\\n[[0.21071826]\\n[0.26909265]\\n[0.33673897]\\n[0.41180944]\\n[0.45120454]]\\n[[0.86910886]\\n[0.99398106]\\n[0.99975663]\\n[0.9999902 ]\\n[1.        ]]', '[0.41180944]\\n[0.45120454]]\\n[[0.86910886]\\n[0.99398106]\\n[0.99975663]\\n[0.9999902 ]\\n[1.        ]]\\n```로지스틱 회귀를 케라스를 통해 구현해봅시다.']\n",
      "['독립 변수 데이터를 $x$, 숫자 10 이상인 경우에는 1, 미만인 경우에는 0을 부여한 레이블 데이터를 $y$라고 해봅시다.\\n이번 데이터는 앞서 배운 단순 선형 회귀때와 마찬가지로 1개의 실수 $x$로부터 1개의 실수인 $y$를 예측하는 맵핑 관계를 가지므로 Dense의 output_dim, input_dim 인자값으로 각각 1을 기재합니다. 시그모이드 함수를 사용할 것이므로 activation의 인자값으로는 sigmoid를 기재해줍니다.\\n옵티마이저로는 가장 기본적인 경사 하강법인 sgd를 사용하였습니다. 시그모이드 함수를 사용한 이진 분류 문제에 손실 함수로 크로스 엔트로피 함수를 사용할 경우 binary_crossentropy를 기재해주면 됩니다. 에포크는 200으로 합니다.\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.models import Sequential', \"import numpy as np\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras import optimizers\\nx = np.array([-50, -40, -30, -20, -10, -5, 0, 5, 10, 20, 30, 40, 50])\\ny = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) # 숫자 10부터 1\\nmodel = Sequential()\\nmodel.add(Dense(1, input_dim=1, activation='sigmoid'))\\nsgd = optimizers.SGD(lr=0.01)\", \"model.add(Dense(1, input_dim=1, activation='sigmoid'))\\nsgd = optimizers.SGD(lr=0.01)\\nmodel.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['binary_accuracy'])\\nmodel.fit(x, y, epochs=200)\\n총 200회에 걸쳐 전체 데이터에 대한 오차를 최소화하는 $w$와 $b$를 찾아내는 작업을 합니다. 저자의 경우 약 190회부터 정확도가 100%가 나오기 시작했습니다. 실제값과 오차를 최소화하도록 값이 변경된 $w$와 $b$의 값을 가진 모델을 이용하여 그래프를 그려보겠습니다.\\nplt.plot(x, model.predict(x), 'b', x,y, 'k.')\\n[이미지: ]\", \"plt.plot(x, model.predict(x), 'b', x,y, 'k.')\\n[이미지: ]\\n$x$의 값이 5와 10사이의 어떤 값일때 $y$값이 0.5가 넘기 시작하는 것처럼 보입니다. 정확도가 100%가 나왔었기 때문에 적어도 $x$의 값이 5일때는 $y$값이 0.5보다 작고, $x$의 값이 10일 때는 $y$값이 0.5를 넘을 것입니다. 이제 $x$의 값이 5보다 작은 값일 때와 $x$의 값이 10보다 클 때에 대해서 $y$값을 출력해봅시다.\\nprint(model.predict([1, 2, 3, 4, 4.5]))\\nprint(model.predict([11, 21, 31, 41, 500]))\\n[[0.21071826]\\n[0.26909265]\\n[0.33673897]\\n[0.41180944]\\n[0.45120454]]\\n[[0.86910886]\\n[0.99398106]\\n[0.99975663]\\n[0.9999902 ]\\n[1.        ]]\", '[0.41180944]\\n[0.45120454]]\\n[[0.86910886]\\n[0.99398106]\\n[0.99975663]\\n[0.9999902 ]\\n[1.        ]]\\n$x$의 값이 5보다 작을 때는 0.5보다 작은 값을, $x$의 값이 10보다 클 때는 0.5보다 큰 값을 출력하는 것을 볼 수 있습니다.\\n==================================================\\n--- 06-07 다중 입력에 대한 실습 ---\\n```\\n[[0.23379876]\\n[0.48773268]\\n[0.4808667 ]\\n[0.7481605 ]\\n[0.74294543]\\n[0.7376603 ]]\\n```독립 변수 $x$가 2개 이상인 경우에 대해서 학습합니다. 비용 함수와 옵티마이저 등을 사용하는 방법은 동일합니다.\\nimport numpy as np\\nfrom tensorflow.keras.models import Sequential', 'import numpy as np\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras import optimizers']\n",
      "['딥 러닝 챕터로 들어가게되면 대부분의 입력들은 독립 변수가 2개 이상입니다. 모델을 직접 코딩하는 관점에서는 입력 벡터의 차원이 2이상이라고 할 수 있습니다. $y$를 결정하는데 있어 독립 변수가 3개인 선형 회귀를 풀어봅시다. 중간 고사, 기말 고사, 그리고 추가 점수를 어떤 공식을 통해 최종 점수를 계산한 데이터가 있습니다.\\nMidterm($x_1$)\\nFinal($x_2$)\\nAdded point($x_3$)\\nScore($1000)(y)\\n70\\n85\\n11\\n73\\n71\\n89\\n18\\n82\\n50\\n80\\n20\\n72\\n99\\n20\\n10\\n57\\n50\\n10\\n10\\n34\\n20\\n99\\n10\\n58\\n40\\n50\\n20\\n56\\n$$H(X) = {w_1x_1 + w_2x_2 + w_3x_3 + b}$$\\n3개의 특성을 가진 벡터 $[x_1, x_2, x_3]$를 대문자 $X$로 표기합니다.', \"50\\n20\\n56\\n$$H(X) = {w_1x_1 + w_2x_2 + w_3x_3 + b}$$\\n3개의 특성을 가진 벡터 $[x_1, x_2, x_3]$를 대문자 $X$로 표기합니다.\\n위 데이터의 샘플 중 상위 5개의 데이터만 훈련에 사용하고, 나머지 2개는 테스트에 사용해보겠습니다.   입력의 차원이 3으로 바뀌면서, input_dim의 인자값이 3으로 변경됩니다. 이는 입력 벡터의 $X$의 원소의 개수가 3개라고도 표현할 수 있고, 입력 벡터 $X$의 차원이 3임을 의미합니다.\\n# 중간 고사, 기말 고사, 가산점 점수\\nX = np.array([[70,85,11], [71,89,18], [50,80,20], [99,20,10], [50,10,10]])\\ny = np.array([73, 82 ,72, 57, 34]) # 최종 성적\\nmodel = Sequential()\\nmodel.add(Dense(1, input_dim=3, activation='linear'))\", \"model = Sequential()\\nmodel.add(Dense(1, input_dim=3, activation='linear'))\\nsgd = optimizers.SGD(learning_rate=0.0001)\\nmodel.compile(optimizer=sgd, loss='mse', metrics=['mse'])\\nmodel.fit(X, y, epochs=2000)\\n모델의 학습이 끝났습니다. 학습된 모델에 입력 X에 대한 예측을 해봅시다.\\nprint(model.predict(X))\\n[[73.15294 ]\\n[81.98001 ]\\n[71.93192 ]\\n[57.161617]\\n[33.669353]]\\n실제값에 근접한 예측을 하는 것을 볼 수 있습니다. 훈련할 때 사용하지 않았던 데이터를 가지고 예측 작업을 수행해보겠습니다.\\nX_test = np.array([[20,99,10], [40,50,20]])\\nprint(model.predict(X_test))\\n[[58.08134 ]\", 'X_test = np.array([[20,99,10], [40,50,20]])\\nprint(model.predict(X_test))\\n[[58.08134 ]\\n[55.734634]]']\n",
      "['$y$를 결정하는데 있어 독립 변수 $x$가 2개인 로지스틱 회귀를 풀어봅시다. 꽃받침(Sepal)의 길이와 꽃잎(Petal)의 길이와 해당 꽃이 A인지 B인지가 적혀져 있는 데이터가 있을 때, 새로 조사한 꽃받침의 길이와 꽃잎의 길이로부터 무슨 꽃인지 예측할 수 있는 모델을 만들고자 한다면 이때 독립 변수 $x$는 2개가 됩니다.\\nSepalLengthCm($x_1$)\\nPetalLengthCm($x_2$)\\nSpecies(y)\\n5.1\\n3.5\\nA\\n4.7\\n3.2\\nA\\n5.2\\n1.8\\nB\\n7\\n4.1\\nA\\n5.1\\n2.1\\nB\\n$$H(X) = sigmoid({w_1x_1 + w_2x_2 + b})$$\\n2개의 특성을 가진 벡터 $[x_1, x_2]$를 대문자 $X$로 표기합니다.', 'A\\n5.1\\n2.1\\nB\\n$$H(X) = sigmoid({w_1x_1 + w_2x_2 + b})$$\\n2개의 특성을 가진 벡터 $[x_1, x_2]$를 대문자 $X$로 표기합니다.\\n독립 변수가 2개인 좀 간단한 새로운 예를 들어서 이를 케라스로 구현해봅시다. 두 개의 입력 $x_1$, $x_2$의 합이 2이상이면 출력값 $y$가 1이 되고 두 개의 입력의 합이 2미만인 경우에만 출력값이 0이 되는 로직을 구현해봅시다.\\n앞서 실습한 로지스틱 회귀 코드와 거의 동일한데 달라진 점은 입력의 차원이 2로 바뀌면서 input_dim의 값이 2라는 점입니다. 이는 입력 벡터의 차원이 2임을 의미합니다.\\nX = np.array([[0, 0], [0, 1], [1, 0], [0, 2], [1, 1], [2, 0]])\\ny = np.array([0, 0, 0, 1, 1, 1])\\nmodel = Sequential()', \"y = np.array([0, 0, 0, 1, 1, 1])\\nmodel = Sequential()\\nmodel.add(Dense(1, input_dim=2, activation='sigmoid'))\\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['binary_accuracy'])\\nmodel.fit(X, y, epochs=2000)\\n2000에포크 정도로 학습을 멈추고 각 입력에 대해서 출력값이 0.5보다 크고 작은지를 확인해보겠습니다.\\nprint(model.predict(X))\\n[[0.23379876]\\n[0.48773268]\\n[0.4808667 ]\\n[0.7481605 ]\\n[0.74294543]\\n[0.7376603 ]]\\n입력의 합이 2이상인 경우에는 전부 값이 0.5를 넘는 것을 볼 수 있습니다.\"]\n",
      "['다중 로지스틱 회귀를 인공 신경망의 형태로 표현하면 다음과 같습니다. 아직 인공 신경망을 배우지 않았음에도 이렇게 다이어그램으로 표현해보는 이유는 로지스틱 회귀를 일종의 인공 신경망 구조로 해석해도 무방함을 보여주기 위함입니다.\\n$$ y = sigmoid(w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b) = σ(w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b)$$\\n[이미지: ]\\n==================================================\\n--- 06-08 벡터와 행렬 연산 ---\\n```\\n두 행렬의 행렬곱 :\\n[[23 31]\\n[34 46]]', '--- 06-08 벡터와 행렬 연산 ---\\n```\\n두 행렬의 행렬곱 :\\n[[23 31]\\n[34 46]]\\n```앞서 독립 변수 $x$가 2개 이상인 선형 회귀와 로지스틱 회귀에 대해서 배웠습니다. 그런데 다음 실습인 소프트맥스 회귀에서는 종속 변수 $y$의 종류도 3개 이상이 되면서 더욱 복잡해집니다. 그리고 이러한 식들이 겹겹이 누적되면 인공 신경망의 개념이 됩니다.\\n케라스는 사용하기가 편리해서 이런 고민을 할 일이 상대적으로 적지만, Numpy나 텐서플로우의 로우-레벨(low-level)의 머신 러닝 개발을 하게되면 각 변수들의 연산을 벡터와 행렬 연산으로 이해할 수 있어야 합니다. 다시 말해 사용자가 데이터와 변수의 개수로부터 행렬의 크기, 더 나아가 텐서의 크기를 산정할 수 있어야 합니다. 기본적인 벡터와 행렬 연산에 대해서 이해해보겠습니다.']\n",
      "['벡터는 크기와 방향을 가진 양입니다. 숫자가 나열된 형상이며 파이썬에서는 1차원 배열 또는 리스트로 표현합니다. 반면, 행렬은 행과 열을 가지는 2차원 형상을 가진 구조입니다. 파이썬에서는 2차원 배열로 표현합니다. 가로줄을 행(row)라고 하며, 세로줄을 열(column)이라고 합니다. 3차원부터는 주로 텐서라고 부릅니다. 텐서는 파이썬에서는 3차원 이상의 배열로 표현합니다.']\n",
      "[\"인공 신경망은 복잡한 모델 내의 연산을 주로 행렬 연산을 통해 해결합니다. 그런데 여기서 말하는 행렬 연산이란 단순히 2차원 배열을 통한 행렬 연산만을 의미하는 것이 아닙니다. 머신 러닝의 입, 출력이 복잡해지면 3차원 텐서에 대한 이해가 필수로 요구됩니다. 예를 들어 인공 신경망 모델 중 하나인 RNN에서는 3차원 텐서에 대한 개념 이해 없이는 이해하기가 쉽지 않습니다.\\nNumpy를 사용하여 텐서를 설명해보겠습니다.\\nimport numpy as np\\n1) 0차원 텐서(스칼라)\\n스칼라는 하나의 실수값으로 이루어진 데이터를 말합니다. 이를 0차원 텐서라고 합니다. 차원을 영어로 Dimension이라고 하므로 0D 텐서라고도 합니다.\\nd = np.array(5)\\nprint('텐서의 차원 :',d.ndim)\\nprint('텐서의 크기(shape) :',d.shape)\\n텐서의 차원 : 0\\n텐서의 크기(shape) : ()\", \"print('텐서의 차원 :',d.ndim)\\nprint('텐서의 크기(shape) :',d.shape)\\n텐서의 차원 : 0\\n텐서의 크기(shape) : ()\\nNumpy의 ndim을 출력했을 때 나오는 값에 주목합시다. ndim을 출력했을 때 나오는 값을 우리는 축(axis)의 개수 또는 텐서의 차원이라고 부릅니다. 반드시 이 두 용어를 기억해둡시다.\\n2) 1차원 텐서(벡터)\\n숫자를 배열한 것을 벡터라고합니다. 벡터는 1차원 텐서입니다. 주의할 점은 벡터에서도 차원이라는 용어를 쓰는데, 벡터의 차원과 텐서의 차원은 다른 개념이라는 점입니다. 아래의 예제는 4차원 벡터이지만, 1차원 텐서입니다. 1D 텐서라고도 합니다.\\nd = np.array([1, 2, 3, 4])\\nprint('텐서의 차원 :',d.ndim)\\nprint('텐서의 크기(shape) :',d.shape)\\n텐서의 차원 : 1\\n텐서의 크기(shape) : (4,)\", \"print('텐서의 차원 :',d.ndim)\\nprint('텐서의 크기(shape) :',d.shape)\\n텐서의 차원 : 1\\n텐서의 크기(shape) : (4,)\\n벡터의 차원과 텐서의 차원의 정의로 인해 혼동할 수 있는데 벡터에서의 차원은 하나의 축에 놓인 원소의 개수를 의미하는 것이고, 텐서에서의 차원은 축의 개수를 의미합니다.\\n3) 2차원 텐서(행렬)\\n행과 열이 존재하는 벡터의 배열. 즉, 행렬(matrix)을 2차원 텐서라고 합니다. 2D 텐서라고도 합니다.\\n# 3행 4열의 행렬\\nd = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\\nprint('텐서의 차원 :',d.ndim)\\nprint('텐서의 크기(shape) :',d.shape)\\n텐서의 차원 : 2\\n텐서의 크기(shape) : (3, 4)\", \"print('텐서의 차원 :',d.ndim)\\nprint('텐서의 크기(shape) :',d.shape)\\n텐서의 차원 : 2\\n텐서의 크기(shape) : (3, 4)\\n텐서의 크기(shape)에 대해서도 정리합시다. 텐서의 크기란, 각 축을 따라서 얼마나 많은 차원이 있는지를 나타낸 값입니다. 텐서의 크기를 바로 머릿속으로 떠올릴 수 있으면 모델 설계 시에 유용합니다. 처음에는 어려울 수도 있는데, 순차적으로 확장해나가며 생각하는 것도 방법입니다. 위의 경우 3개의 커다란 데이터가 있는데 그 각각의 커다란 데이터는 작은 데이터 4개로 이루어졌다고 생각할 수 있습니다.\\n4) 3차원 텐서(다차원 배열)\", '4) 3차원 텐서(다차원 배열)\\n행렬 또는 2차원 텐서를 단위로 한 번 더 배열하면 3차원 텐서라고 부릅니다. 3D 텐서라고도 합니다. 사실 위에서 언급한 0차원 ~ 2차원 텐서는 각각 스칼라, 벡터, 행렬이라고 해도 무방하므로 3차원 이상의 텐서부터 본격적으로 텐서라고 부릅니다. 데이터 사이언스 분야 한정으로 주로 3차원 이상의 배열을 텐서라고 부른다고 이해해도 좋습니다. 3D 텐서는 적어도 여기서는 3차원 배열로 이해하면 되겠습니다.  이 3차원 텐서의 구조를 이해하지 않으면, 복잡한 인공 신경망의 입, 출력값을 이해하는 것이 쉽지 않습니다. 개념 자체는 어렵지 않지만 반드시 알아야하는 개념입니다.\\nd = np.array([\\n[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [10, 11, 12, 13, 14]],\\n[[15, 16, 17, 18, 19], [19, 20, 21, 22, 23], [23, 24, 25, 26, 27]]\\n])', \"[[15, 16, 17, 18, 19], [19, 20, 21, 22, 23], [23, 24, 25, 26, 27]]\\n])\\nprint('텐서의 차원 :',d.ndim)\\nprint('텐서의 크기(shape) :',d.shape)\\n텐서의 차원 : 3\\n텐서의 크기(shape) : (2, 3, 5)\\n자연어 처리에서 특히 자주 보게 되는 것이 이 3D 텐서입니다. 3D 텐서는 시퀀스 데이터(sequence data)를 표현할 때 자주 사용되기 때문입니다. 여기서 시퀀스 데이터는  주로 단어의 시퀀스를 의미하며, 시퀀스는 주로 문장이나 문서, 뉴스 기사 등의 텍스트가 될 수 있습니다. 이 경우 3D 텐서는 (samples, timesteps, word_dim)이 됩니다. 또는 일괄로 처리하기 위해 데이터를 묶는 단위인 배치의 개념에 대해서 뒤에서 배울텐데 (batch_size, timesteps, word_dim)이라고도 볼 수 있습니다.\", 'samples 또는 batch_size는 샘플의 개수, timesteps는 시퀀스의 길이, word_dim은 단어를 표현하는 벡터의 차원을 의미합니다. 더 상세한 설명은 RNN 챕터에서 배우게 되겠지만 자연어 처리에서 왜 3D 텐서의 개념이 사용되는지 간단한 예를 들어봅시다. 다음과 같은 3개의 훈련 데이터가 있다고 해봅시다.\\n문서1 : I like NLP\\n문서2 : I like DL\\n문서3 : DL is AI\\n이를 인공 신경망의 모델의 입력으로 사용하기 위해서는 각 단어를 벡터화해야 합니다. 단어를 벡터화하는 방법으로는 원-핫 인코딩이나 워드 임베딩이라는 방법이 대표적입니다. 워드 임베딩은 아직 배우지 않았으므로 원-핫 인코딩으로 각 단어를 벡터화 해보겠습니다.\\n단어\\nOne-hot vector\\nI\\n[1 0 0 0 0 0]\\nlike\\n[0 1 0 0 0 0]\\nNLP\\n[0 0 1 0 0 0]\\nDL\\n[0 0 0 1 0 0]\\nis\\n[0 0 0 0 1 0]\\nAI\\n[0 0 0 0 0 1]', 'like\\n[0 1 0 0 0 0]\\nNLP\\n[0 0 1 0 0 0]\\nDL\\n[0 0 0 1 0 0]\\nis\\n[0 0 0 0 1 0]\\nAI\\n[0 0 0 0 0 1]\\n훈련 데이터의 단어들을 모두 원-핫 벡터로 바꿔서 인공 신경망의 입력으로 한 꺼번에 사용한다고 하면 다음과 같습니다. 이렇게 훈련 데이터를 다수 묶어 입력으로 사용하는 것을 딥 러닝에서는 배치(Batch)라고 합니다.\\n[[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]],\\n[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]],\\n[[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]]]\\n이는 (3, 3, 6)의 크기를 가지는 3D 텐서입니다.\\n5) 그 이상의 텐서', '이는 (3, 3, 6)의 크기를 가지는 3D 텐서입니다.\\n5) 그 이상의 텐서\\n3차원 텐서를 배열로 합치면 4차원 텐서가 됩니다. 4차원 텐서를 배열로 합치면 5차원 텐서가 됩니다. 이렇게 텐서는 다차원 배열로서 계속해서 확장될 수 있습니다.\\n[이미지: ]\\n위의 그림은 각 텐서를 도형으로 시각화한 모습을 보여줍니다.\\n6) 케라스에서의 텐서\\n앞서 Numpy로 각 텐서의 ndim(차원)과 shape(크기)를 출력했었습니다. 예를 들어 위의 예제에서는 3차원 텐서의 크기(shape)는 (2, 3, 5)였습니다. 케라스에서는 신경망의 층에 입력의 크기(shape)를 인자로 줄 때 input_shape라는 인자를 사용합니다.', '실제 예시는 뒤 챕터들에서 보겠지만 input_shape는 배치 크기를 제외하고 차원을 지정하는데, 예를 들어 input_shape(6, 5)라는 인자값을 사용한다면 이 텐서의 크기는 (?, 6, 5)을 의미합니다. 배치 크기는 지정해주기 전까지는 알 수 없기때문에 ?가 됩니다. 만약 배치 크기까지 지정해주고 싶다면 batch_input_shape=(8, 2, 10)와 같이 인자를 주면 이 텐서의 크기는 (8, 2, 10)을 의미합니다.\\n그 외에도 입력의 속성 수를 의미하는 input_dim, 시퀀스 데이터의 길이를 의미하는 input_length 등의 인자도 사용하는데, input_shape의 두 개의 인자는 (input_length, input_dim)입니다.']\n",
      "['벡터와 행렬의 기본적인 연산에 대해서 알아보겠습니다.\\nimport numpy as np\\n1) 벡터와 행렬의 덧셈과 뺄셈\\n같은 크기의 두 개의 벡터나 행렬은 덧셈과 뺄셈을 할 수 있습니다. 이 경우 같은 위치의 원소끼리 연산하면 됩니다. 이러한 연산을 요소별(element-wise) 연산이라고 합니다. 다음과 같이 A와 B라는 두 개의 벡터가 있다고 해봅시다.\\n$$\\nA = \\\\left[\\n\\\\begin{array}{c}\\n8 \\\\\\\\\\n4 \\\\\\\\\\n5 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\\\nB = \\\\left[\\n\\\\begin{array}{c}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n3 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n이때 두 벡터 A와 B의 덧셈과 뺄셈은 아래와 같습니다.\\n$$\\nA + B = \\\\left[\\n\\\\begin{array}{c}\\n8 \\\\\\\\\\n4 \\\\\\\\\\n5 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n+ \\\\left[\\n\\\\begin{array}{c}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n3 \\\\\\\\\\n\\\\end{array}\\n\\\\right]', \"8 \\\\\\\\\\n4 \\\\\\\\\\n5 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n+ \\\\left[\\n\\\\begin{array}{c}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n3 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n= \\\\left[\\n\\\\begin{array}{c}\\n9 \\\\\\\\\\n6 \\\\\\\\\\n8 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n$$\\nA - B = \\\\left[\\n\\\\begin{array}{c}\\n8 \\\\\\\\\\n4 \\\\\\\\\\n5 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n- \\\\left[\\n\\\\begin{array}{c}\\n1 \\\\\\\\\\n2 \\\\\\\\\\n3 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n= \\\\left[\\n\\\\begin{array}{c}\\n7 \\\\\\\\\\n2 \\\\\\\\\\n2 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\nNumpy를 이용하여 이를 구현할 수 있습니다.\\nA = np.array([8, 4, 5])\\nB = np.array([1, 2, 3])\\nprint('두 벡터의 합 :',A+B)\\nprint('두 벡터의 차 :',A-B)\\n두 행렬의 합 : [9 6 8]\", \"B = np.array([1, 2, 3])\\nprint('두 벡터의 합 :',A+B)\\nprint('두 벡터의 차 :',A-B)\\n두 행렬의 합 : [9 6 8]\\n두 행렬의 차 : [7 2 2]\\n행렬도 마찬가지입니다. A와 B라는 두 개의 행렬이 있다고 하였을 때, 두 행렬 A와 B의 덧셈과 뺄셈은 아래와 같습니다.\\n$$\\nA =\\n\\\\left[\\n\\\\begin{array}{c}\\n10\\\\ 20\\\\ 30\\\\ 40\\\\\\\\\\n50\\\\ 60\\\\ 70\\\\ 80\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\ \\\\\\nB =\\n\\\\left[\\n\\\\begin{array}{c}\\n5\\\\ 6\\\\ 7\\\\ 8\\\\\\\\\\n1\\\\ 2\\\\ 3\\\\ 4\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n$$\\nA + B = \\\\left[\\n\\\\begin{array}{c}\\n10\\\\ 20\\\\ 30\\\\ 40\\\\\\\\\\n50\\\\ 60\\\\ 70\\\\ 80\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n+ \\\\left[\\n\\\\begin{array}{c}\\n5\\\\ 6\\\\ 7\\\\ 8\\\\\\\\\\n1\\\\ 2\\\\ 3\\\\ 4\\\\\\\\\\n\\\\end{array}\", '\\\\end{array}\\n\\\\right]\\n+ \\\\left[\\n\\\\begin{array}{c}\\n5\\\\ 6\\\\ 7\\\\ 8\\\\\\\\\\n1\\\\ 2\\\\ 3\\\\ 4\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n= \\\\left[\\n\\\\begin{array}{c}\\n15\\\\ 26\\\\ 37\\\\ 48\\\\\\\\\\n51\\\\ 62\\\\ 73\\\\ 84\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n$$\\nA - B = \\\\left[\\n\\\\begin{array}{c}\\n10\\\\ 20\\\\ 30\\\\ 40\\\\\\\\\\n50\\\\ 60\\\\ 70\\\\ 80\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n- \\\\left[\\n\\\\begin{array}{c}\\n5\\\\ 6\\\\ 7\\\\ 8\\\\\\\\\\n1\\\\ 2\\\\ 3\\\\ 4\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n= \\\\left[\\n\\\\begin{array}{c}\\n5\\\\ 14\\\\ 23\\\\ 32\\\\\\\\\\n49\\\\ 58\\\\ 67\\\\ 76\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\nNumpy를 이용하여 이를 구현할 수 있습니다.\\nA = np.array([[10, 20, 30, 40], [50, 60, 70, 80]])', \"\\\\right]\\n$$\\nNumpy를 이용하여 이를 구현할 수 있습니다.\\nA = np.array([[10, 20, 30, 40], [50, 60, 70, 80]])\\nB = np.array([[5, 6, 7, 8],[1, 2, 3, 4]])\\nprint('두 행렬의 합 :')\\nprint(A + B)\\nprint('두 행렬의 차 :')\\nprint(A - B)\\n두 행렬의 합 :\\n[[15 26 37 48]\\n[51 62 73 84]]\\n두 행렬의 차 :\\n[[ 5 14 23 32]\\n[49 58 67 76]]\\n2) 벡터의 내적과 행렬의 곱셈\\n벡터의 점곱(dot product) 또는 내적(inner product)에 대해 알아봅시다. 벡터의 내적은 연산을 점(dot)으로 표현하여 $a \\\\cdot b$와 같이 표현하기도 합니다.\", \"내적이 성립하기 위해서는 두 벡터의 차원이 같아야 하며, 두 벡터 중 앞의 벡터가 행벡터(가로 방향 벡터)이고 뒤의 벡터가 열벡터(세로 방향 벡터)여야 합니다. 아래는 두 벡터의 차원이 같고 곱셈의 대상이 각각 행벡터이고 열벡터일 때 내적이 이루어지는 모습을 보여줍니다. 벡터의 내적의 결과는 스칼라가 된다는 특징이 있습니다.\\n$$\\nA \\\\cdot B =\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 2\\\\ 3\\n\\\\end{array}\\n\\\\right]\\n\\\\left[\\n\\\\begin{array}{c}\\n4 \\\\\\\\\\n5 \\\\\\\\\\n6 \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n= 1 × 4 + 2 × 5 + 3 × 6 = 32\\\\text{(스칼라)}\\n$$\\nNumpy를 이용하여 이를 구현할 수 있습니다.\\nA = np.array([1, 2, 3])\\nB = np.array([4, 5, 6])\\nprint('두 벡터의 내적 :',np.dot(A, B))\\n두 벡터의 내적 : 32\", \"A = np.array([1, 2, 3])\\nB = np.array([4, 5, 6])\\nprint('두 벡터의 내적 :',np.dot(A, B))\\n두 벡터의 내적 : 32\\n행렬의 곱셈을 이해하기 위해서는 벡터의 내적을 이해해야 합니다. 행렬의 곱셈은 왼쪽 행렬의 행벡터(가로 방향 벡터)와 오른쪽 행렬의 열벡터(세로 방향 벡터)의 내적(대응하는 원소들의 곱의 합)이 결과 행렬의 원소가 되는 것으로 이루어집니다. 다음과 같이 A와 B라는 두 개의 행렬이 있다고 하였을 때, 두 행렬 A와 B의 행렬의 곱셈은 아래와 같습니다.\\n$$\\nA =\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 3\\\\\\\\\\n2\\\\ 4\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\ \\\\\\nB =\\n\\\\left[\\n\\\\begin{array}{c}\\n5\\\\ 7\\\\\\\\\\n6\\\\ 8\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n$$\\nAB =\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 3\\\\\\\\\\n2\\\\ 4\\\\\\\\\\n\\\\end{array}\\n\\\\right]\", \"6\\\\ 8\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n$$\\nAB =\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 3\\\\\\\\\\n2\\\\ 4\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\left[\\n\\\\begin{array}{c}\\n5\\\\ 7\\\\\\\\\\n6\\\\ 8\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n= \\\\left[\\n\\\\begin{array}{c}\\n1 × 5 + 3 × 6\\\\ \\\\ \\\\ 1 × 7 + 3 × 8\\\\\\\\\\n2 × 5 + 4 × 6\\\\ \\\\ \\\\ 2 × 7 + 4 × 8\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n=\\\\left[\\n\\\\begin{array}{c}\\n23\\\\ 31\\\\\\\\\\n34\\\\ 46\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\nNumpy를 이용하여 이를 구현할 수 있습니다.\\nA = np.array([[1, 3],[2, 4]])\\nB = np.array([[5, 7],[6, 8]])\\nprint('두 행렬의 행렬곱 :')\\nprint(np.matmul(A, B))\\n두 행렬의 행렬곱 :\\n[[23 31]\\n[34 46]]\", \"print('두 행렬의 행렬곱 :')\\nprint(np.matmul(A, B))\\n두 행렬의 행렬곱 :\\n[[23 31]\\n[34 46]]\\n행렬의 곱셈은 딥 러닝을 이해하기 위해 필수적인 개념이므로 반드시 숙지해야 합니다. 행렬 곱셈에서의 주요한 두 가지 조건 또한 반드시 기억해둡시다.\\n두 행렬의 곱 A × B이 성립되기 위해서는 행렬 A의 열의 개수와 행렬 B의 행의 개수는 같아야 한다.\\n두 행렬의 곱 A × B의 결과로 나온 행렬 AB의 크기는 A의 행의 개수와 B의 열의 개수를 가진다.\\n벡터와 행렬의 곱 또는 행렬과 벡터의 곱 또한 행렬의 곱셈과 동일한 원리로 이루어집니다.\"]\n",
      "['독립 변수가 2개 이상일 때, 1개의 종속 변수를 예측하는 문제를 행렬의 연산으로 표현한다면 어떻게 될까요? 다중 선형 회귀나 다중 로지스틱 회귀가 이러한 연산의 예인데, 여기서는 다중 선형 회귀를 통해 예를 들어보겠습니다. 다음은 독립 변수 $x$가 n개인 다중 선형 회귀 수식입니다.\\n$$\\ny = w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b\\n$$\\n이는 입력 벡터 $[x_{1},...x_{n}]$와 가중치 벡터 $[w_{1},...,w_{n}]$의 내적으로 표현할 수 있습니다.\\n$$\\ny =\\n\\\\left[\\n\\\\begin{array}{c}\\nx_{1}\\\\ x_{2}\\\\ x_{3}\\\\ \\\\cdot\\\\cdot\\\\cdot\\\\ x_{n}\\n\\\\end{array}\\n\\\\right]\\n\\\\left[\\n\\\\begin{array}{c}\\nw_{1} \\\\\\\\\\nw_{2} \\\\\\\\\\nw_{3} \\\\\\\\\\n\\\\cdot\\\\cdot\\\\cdot \\\\\\\\\\nw_{n}\\n\\\\end{array}\\n\\\\right]\\n+\\nb', '\\\\left[\\n\\\\begin{array}{c}\\nw_{1} \\\\\\\\\\nw_{2} \\\\\\\\\\nw_{3} \\\\\\\\\\n\\\\cdot\\\\cdot\\\\cdot \\\\\\\\\\nw_{n}\\n\\\\end{array}\\n\\\\right]\\n+\\nb\\n= x_1w_1 + x_2w_2 + x_3w_3 + ... + x_nw_n + b\\n$$\\n또는 가중치 벡터 $[w_{1},...,w_{n}]$와 입력 벡터 $[x_{1},...x_{n}]$의 내적으로 표현할 수도 있습니다.\\n$$\\ny =\\n\\\\left[\\n\\\\begin{array}{c}\\nw_{1}\\\\ w_{2}\\\\ w_{3}\\\\ \\\\cdot\\\\cdot\\\\cdot\\\\ w_{n}\\n\\\\end{array}\\n\\\\right]\\n\\\\left[\\n\\\\begin{array}{c}\\nx_{1} \\\\\\\\\\nx_{2} \\\\\\\\\\nx_{3} \\\\\\\\\\n\\\\cdot\\\\cdot\\\\cdot \\\\\\\\\\nx_{n}\\n\\\\end{array}\\n\\\\right]\\n+\\nb\\n= x_1w_1 + x_2w_2 + x_3w_3 + ... + x_nw_n + b\\n$$', '\\\\cdot\\\\cdot\\\\cdot \\\\\\\\\\nx_{n}\\n\\\\end{array}\\n\\\\right]\\n+\\nb\\n= x_1w_1 + x_2w_2 + x_3w_3 + ... + x_nw_n + b\\n$$\\n샘플의 개수가 많을 경우에는 행렬의 곱셈으로 표현이 가능합니다. 다음은 집의 크기, 방의 수, 층의 수, 집이 얼마나 오래되었는지와 집의 가격이 기록된 부동산 데이터라고 가정합시다. 해당 데이터를 학습하여 새로운 집의 정보가 들어왔을 때, 집의 가격을 예측하는 모델을 구현한다고 해봅시다.\\nsize($feet^{2}$)($x_1$)\\nnumber of bedrooms($x_2$)\\nnumber of floors($x_3$)\\nage of home($x_4$)\\nprice($1000)(y)\\n1800\\n2\\n1\\n10\\n207\\n1200\\n4\\n2\\n20\\n176\\n1700\\n3\\n2\\n15\\n213\\n1500\\n5\\n1\\n10\\n234\\n1100\\n2\\n2\\n10\\n155', 'price($1000)(y)\\n1800\\n2\\n1\\n10\\n207\\n1200\\n4\\n2\\n20\\n176\\n1700\\n3\\n2\\n15\\n213\\n1500\\n5\\n1\\n10\\n234\\n1100\\n2\\n2\\n10\\n155\\n위 데이터에 대해서 입력 행렬 $X$와 가중치 벡터 $W$의 곱으로 표현하면 다음과 같습니다.\\n$$\\n\\\\left[\\n\\\\begin{array}{c}\\nx_{11}\\\\ x_{12}\\\\ x_{13}\\\\ x_{14} \\\\\\\\\\nx_{21}\\\\ x_{22}\\\\ x_{23}\\\\ x_{24} \\\\\\\\\\nx_{31}\\\\ x_{32}\\\\ x_{33}\\\\ x_{34} \\\\\\\\\\nx_{41}\\\\ x_{42}\\\\ x_{43}\\\\ x_{44} \\\\\\\\\\nx_{51}\\\\ x_{52}\\\\ x_{53}\\\\ x_{54} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\left[\\n\\\\begin{array}{c}\\nw_{1} \\\\\\\\\\nw_{2} \\\\\\\\\\nw_{3} \\\\\\\\\\nw_{4} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n=\\n\\\\left[\\n\\\\begin{array}{c}', '\\\\begin{array}{c}\\nw_{1} \\\\\\\\\\nw_{2} \\\\\\\\\\nw_{3} \\\\\\\\\\nw_{4} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n=\\n\\\\left[\\n\\\\begin{array}{c}\\nx_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}+ x_{14}w_{4} \\\\\\\\\\nx_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}+ x_{24}w_{4} \\\\\\\\\\nx_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}+ x_{34}w_{4} \\\\\\\\\\nx_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}+ x_{44}w_{4} \\\\\\\\\\nx_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}+ x_{54}w_{4} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n여기에 편향 벡터 $B$를 더 해주면 위 데이터에 대한 전체 가설 수식 $H(X)$를 표현할 수 있습니다.\\n$$\\n\\\\left[\\n\\\\begin{array}{c}', '\\\\right]\\n$$\\n여기에 편향 벡터 $B$를 더 해주면 위 데이터에 대한 전체 가설 수식 $H(X)$를 표현할 수 있습니다.\\n$$\\n\\\\left[\\n\\\\begin{array}{c}\\nx_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}+ x_{14}w_{4} \\\\\\\\\\nx_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}+ x_{24}w_{4} \\\\\\\\\\nx_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}+ x_{34}w_{4} \\\\\\\\\\nx_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}+ x_{44}w_{4} \\\\\\\\\\nx_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}+ x_{54}w_{4} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n+\\n\\\\left[\\n\\\\begin{array}{c}\\nb \\\\\\\\\\nb \\\\\\\\\\nb \\\\\\\\\\nb \\\\\\\\\\nb \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n=\\n\\\\left[\\n\\\\begin{array}{c}', '+\\n\\\\left[\\n\\\\begin{array}{c}\\nb \\\\\\\\\\nb \\\\\\\\\\nb \\\\\\\\\\nb \\\\\\\\\\nb \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n=\\n\\\\left[\\n\\\\begin{array}{c}\\ny_{1}\\\\\\\\ y_{2}\\\\\\\\ y_{3}\\\\\\\\ y_{4}\\\\\\\\ y_{5} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n$$\\nH(X) = XW + B\\n$$\\n위의 수식에서 입력 행렬 $X$는 5행 4열의 크기를 가집니다. 출력 벡터를 $Y$라고 하였을 때 $Y$는 5행 1열의 크기를 가집니다. 여기서 곱셈이 성립하기 위해서 가중치 벡터 $W$의 크기는 4행 1열을 가져야함을 추론할 수 있습니다. 만약 가중치 벡터를 앞에 두고 입력 행렬을 뒤에 두고 행렬 연산을 한다면 이는 아래와 같습니다.\\n$$\\n\\\\left[\\n\\\\begin{array}{c}\\nw_{1}\\\\ w_{2}\\\\ w_{3}\\\\ w_{4} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\left[\\n\\\\begin{array}{c}', '\\\\left[\\n\\\\begin{array}{c}\\nw_{1}\\\\ w_{2}\\\\ w_{3}\\\\ w_{4} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\left[\\n\\\\begin{array}{c}\\nx_{11}\\\\ x_{21}\\\\ x_{31}\\\\ x_{41}\\\\ x_{51}\\\\\\\\\\nx_{12}\\\\ x_{22}\\\\ x_{32}\\\\ x_{42}\\\\ x_{52}\\\\\\\\\\nx_{13}\\\\ x_{23}\\\\ x_{33}\\\\ x_{43}\\\\ x_{53}\\\\\\\\\\nx_{14}\\\\ x_{24}\\\\ x_{34}\\\\ x_{44}\\\\ x_{54}\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n+\\n\\\\left[\\n\\\\begin{array}{c}\\nb\\\\ b\\\\ b\\\\ b\\\\ b \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n=\\n\\\\left[\\n\\\\begin{array}{c}\\ny_{1}\\\\ y_{2}\\\\ y_{3}\\\\ y_{4}\\\\ y_{5} \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n수학적 관례로 아래와 같이 수식으로 표현할 때는 주로 가중치 $W$가 입력 $X$의 앞에 오는 편입니다.', '\\\\end{array}\\n\\\\right]\\n$$\\n수학적 관례로 아래와 같이 수식으로 표현할 때는 주로 가중치 $W$가 입력 $X$의 앞에 오는 편입니다.\\n$H(X) = WX + B$\\n인공 신경망도 본질적으로 위와 같은 행렬 연산입니다.']\n",
      "['훈련 데이터의 입력 행렬을 $X$라고 하였을 때 샘플(Sample)과 특성(Feature)의 정의는 다음과 같습니다.\\n[이미지: ]\\n머신 러닝에서는 데이터를 셀 수 있는 단위로 구분할 때, 각각을 샘플이라고 부르며, 종속 변수 $y$를 예측하기 위한 각각의 독립 변수 $x$를 특성이라고 부릅니다.']\n",
      "['앞서 언급하였던 행렬 곱셈의 두 가지 주요한 조건을 기억해둡시다.\\n두 행렬의 곱 J × K이 성립되기 위해서는 행렬 J의 열의 개수와 행렬 K의 행의 개수는 같아야 한다.\\n두 행렬의 곱 J × K의 결과로 나온 행렬 JK의 크기는 J의 행의 개수와 K의 열의 개수를 가진다.\\n이로부터 입력과 출력의 행렬의 크기로부터 가중치 행렬 W와 편향 행렬 B의 크기를 찾아낼 수 있습니다. 독립 변수 행렬을 X, 종속 변수 행렬을 Y라고 하였을 때, 이때 행렬 X를 입력 행렬(Input Matrix), Y를 출력 행렬(Output Matrix)이라고 합시다.\\n[이미지: ]\\n이제 입력 행렬의 크기와 출력 행렬의 크기로부터 W행렬과 B행렬의 크기를 추론해봅시다.\\n[이미지: ]\\n행렬의 덧셈에 해당되는 B행렬은 Y행렬의 크기에 영향을 주지 않습니다. 그러므로 B행렬의 크기는 Y행렬의 크기와 같습니다.\\n[이미지: ]', '[이미지: ]\\n행렬의 덧셈에 해당되는 B행렬은 Y행렬의 크기에 영향을 주지 않습니다. 그러므로 B행렬의 크기는 Y행렬의 크기와 같습니다.\\n[이미지: ]\\n행렬의 곱셈이 성립되려면 행렬의 곱셈에서 앞에 있는 행렬의 열의 크기와 뒤에 있는 행렬의 행의 크기는 같아야 합니다. 그러므로 입력 행렬 X로부터 W행렬의 행의 크기가 결정됩니다.\\n[이미지: ]\\n두 행렬의 곱의 결과로서 나온 행렬의 열의 크기는 행렬의 곱셈에서 뒤에 있는 행렬의 열의 크기와 동일합니다. 그러므로 출력 행렬 Y로부터 W행렬의 열의 크기가 결정됩니다. 입력 행렬과 출력 행렬의 크기로부터 가중치 행렬과 편향 행렬의 크기를 추정할 수 있다면, 딥 러닝 모델을 구현하였을 때 해당 모델에 존재하는 총 매개변수의 개수를 계산하기 쉽습니다. 어떤 딥 러닝 모델의 총 매개변수의 개수는 해당 모델에 존재하는 가중치 행렬과 편향 행렬의 모든 원소의 수이기 때문입니다.', '==================================================\\n--- 06-09 소프트맥스 회귀(Softmax Regression) ---\\n```\\n{Banana :1, Tomato :2, Apple :3, Strawberry :4, ... Watermelon :10}\\n```앞서 로지스틱 회귀를 통해 2개의 선택지 중에서 1개를 고르는 이진 분류(Binary Classification)를 풀어봤습니다. 이번에는 3개 이상의 선택지 중에서 1개를 고르는 다중 클래스 분류 문제를 위한 소프트맥스 회귀(Softmax Regression)에 대해서 배웁니다.']\n",
      "['앞서 로지스틱 회귀에서 사용한 시그모이드 함수는 입력된 데이터에 대해서 0과 1사이의 값을 출력하여 해당 값이 둘 중 하나에 속할 확률로 해석할 수 있도록 만들어주었습니다. 예를 들어 0이 정상 메일, 1이 스팸 메일이라고 정의해놓는다면 시그모이드 함수의 0과 1사이의 출력값을 스팸 메일일 확률로 해석할 수 있었습니다. 확률값이 0.5를 넘으면 1에 가까우니 스팸 메일로 판단하면 되고, 그 반대라면 정상 메일로 판단하면 됩니다.\\n이진 분류가 두 개의 선택지 중 하나를 고르는 문제였다면, 세 개 이상의 선택지 중 하나를 고르는 문제를 다중 클래스 분류라고 합니다. 아래의 붓꽃 품종 예측 데이터는 꽃받침 길이, 꽃받침 넓이, 꽃잎 길이, 꽃잎 넓이로부터 setosa, versicolor, virginica라는 3개의 품종 중 어떤 품종인지를 예측하는 문제를 위한 데이터로 전형적인 다중 클래스 분류 문제를 위한 데이터입니다.\\nSepalLengthCm($x_1$)', 'SepalLengthCm($x_1$)\\nSepalWidthCm($x_2$)\\nPetalLengthCm($x_3$)\\nPetalWidthCm($x_4$)\\nSpecies(y)\\n5.1\\n3.5\\n1.4\\n0.2\\nsetosa\\n4.9\\n3.0\\n1.4\\n0.2\\nsetosa\\n5.8\\n2.6\\n4.0\\n1.2\\nversicolor\\n6.7\\n3.0\\n5.2\\n2.3\\nvirginica\\n5.6\\n2.8\\n4.9\\n2.0\\nvirginica', '6.7\\n3.0\\n5.2\\n2.3\\nvirginica\\n5.6\\n2.8\\n4.9\\n2.0\\nvirginica\\n여기에 앞서 배운 시그모이드 함수를 사용해본다면 어떨까요? 어쩌면 입력된 샘플 데이터에 대해서 각 정답지에 대해서 시그모이드 함수를 적용해볼 수 있습니다. 만약 그렇게 한다면, setosa가 정답일 확률은 0.8, versicolor가 정답일 확률은 0.2, virginica가 정답일 확률은 0.4 등과 같은 출력을 얻게됩니다. 그런데 이 전체 확률의 합계가 1이 되도록 하여 전체 선택지에 걸친 확률로 바꿀 순 없을까요? 예를 들어 샘플 데이터가 입력으로 들어오면 모델이 setosa일 확률이 0.7, versicolor일 확률 0.05, virginica일 확률이 0.25과 같이 세 개의 확률의 총 합이 1인 예측값을 얻도록 하자는 것입니다. 그리고 이 경우 확률값이 가장 높은 setosa로 예측한 것으로 간주하고자 합니다. 이럴 때 사용할 수 있는 것이 소프트맥스 함수입니다.']\n",
      "['소프트맥스 함수는 선택해야 하는 선택지의 총 개수를 k라고 할 때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정합니다. 우선 수식에 대해 설명하고, 그 후에는 그림으로 이해해보겠습니다.\\n1) 소프트맥스 함수의 이해\\nk차원의 벡터에서 i번째 원소를 $z_{i}$, i번째 클래스가 정답일 확률을 $p_{i}$로 나타낸다고 하였을 때 소프트맥스 함수는 $p_{i}$를 다음과 같이 정의합니다.\\n$$p_{i}=\\\\frac{e^{z_{i}}}{\\\\sum_{j=1}^{k} e^{z_{j}}}\\\\ \\\\ for\\\\ i=1, 2, ... k$$\\n위에서 풀어야하는 문제에 소프트맥스 함수를 차근차근 적용해봅시다. 위에서 풀어야하는 문제의 경우 k=3이므로 3차원 벡터 $z=[z_{1}\\\\ z_{2}\\\\ z_{3}]$의 입력을 받으면 소프트맥스 함수는 아래와 같은 출력을 리턴합니다.', '$$softmax(z)=[\\\\frac{e^{z_{1}}}{\\\\sum_{j=1}^{3} e^{z_{j}}}\\\\ \\\\frac{e^{z_{2}}}{\\\\sum_{j=1}^{3} e^{z_{j}}}\\\\ \\\\frac{e^{z_{3}}}{\\\\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = \\\\hat{y} = \\\\text{예측값}$$', '$p_{1}, p_{2}, p_{3}$ 각각은 1번 클래스가 정답일 확률, 2번 클래스가 정답일 확률, 3번 클래스가 정답일 확률을 나타내며 각각 0과 1사이의 값으로 총 합은 1이 됩니다. 여기서 분류하고자하는 3개의 클래스는 virginica, setosa, versicolor이므로 이는 결국 주어진 입력이 virginica일 확률, setosa일 확률, versicolor일 확률을 나타내는 값을 의미합니다. 여기서는 i가 1일 때는 virginica일 확률을 나타내고, 2일 때는 setosa일 확률, 3일때는 versicolor일 확률이라고 지정하였다고 합시다. 이 지정 순서는 문제를 풀고자 하는 사람의 무작위 선택입니다. 이에따라 식을 문제에 맞게 다시 쓰면 아래와 같습니다.', '$$softmax(z)=[\\\\frac{e^{z_{1}}}{\\\\sum_{j=1}^{3} e^{z_{j}}}\\\\ \\\\frac{e^{z_{2}}}{\\\\sum_{j=1}^{3} e^{z_{j}}}\\\\ \\\\frac{e^{z_{3}}}{\\\\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = [p_{virginica}, p_{setosa}, p_{versicolor}]$$\\n다소 복잡해보이지만 어려운 개념이 아닙니다. 분류하고자 하는 클래스가 k개일 때, k차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 반환한다는 내용을 식으로 기재했습니다. 방금 배운 개념을 그림을 통해 다시 설명해보겠습니다.\\n2) 그림을 통한 이해\\n[이미지: ]\\n위의 그림에 점차 살을 붙여가는 식으로 설명합니다. 여기서는 샘플 데이터를 1개씩 입력으로 받아 처리한다고 가정해봅시다. 즉, 배치 크기가 1입니다.', '[이미지: ]\\n위의 그림에 점차 살을 붙여가는 식으로 설명합니다. 여기서는 샘플 데이터를 1개씩 입력으로 받아 처리한다고 가정해봅시다. 즉, 배치 크기가 1입니다.\\n위의 그림에는 두 가지 의문이 있습니다. 첫번째 질문은 소프트맥스 함수의 입력에 대한 의문입니다. 하나의 샘플 데이터는 4개의 독립 변수 $x$를 가지는데 이는 모델이 4차원 벡터를 입력으로 받음을 의미합니다. 그런데 소프트맥스의 함수의 입력으로 사용되는 벡터는 벡터의 차원이 분류하고자 하는 클래스의 개수가 되어야 하므로 어떤 가중치 연산을 통해 3차원 벡터로 변환되어야 합니다. 위의 그림에서는 소프트맥스 함수의 입력으로 사용되는 3차원 벡터를 $z$로 표현하였습니다.\\n[이미지: ]', '[이미지: ]\\n샘플 데이터 벡터를 소프트맥스 함수의 입력 벡터로 차원을 축소하는 방법은 간단합니다. 소프트맥스 함수의 입력 벡터 $z$의 차원수만큼 결과값이 나오도록 가중치 곱을 진행합니다. 위의 그림에서 화살표는 총  (4 × 3 = 12) 12개이며 전부 다른 가중치를 가지고, 학습 과정에서 점차적으로 오차를 최소화하는 가중치로 값이 변경됩니다.', '두번째 질문은 오차 계산 방법에 대한 의문입니다. 소프트맥스 함수의 출력은 분류하고자하는 클래스의 개수만큼 차원을 가지는 벡터로 각 원소는 0과 1사이의 값을 가집니다. 이 각각은 특정 클래스가 정답일 확률을 나타냅니다. 여기서는 첫번째 원소인 $p_{1}$은 virginica가 정답일 확률, 두번째 원소인 $p_{2}$는 setosa가 정답일 확률, 세번째 원소인 $p_{3}$은 versicolor가 정답일 확률로 고려하고자 합니다. 그렇다면 이 예측값과 비교를 할 수 있는 실제값의 표현 방법이 있어야 합니다. 소프트맥스 회귀에서는 실제값을 원-핫 벡터로 표현합니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림은 소프트맥스 함수의 출력 벡터의 첫번째 원소 $p_{1}$가 virginica가 정답일 확률, 두번째 원소 $p_{2}$가 setosa가 정답일 확률, 세번째 원소 $p_{3}$가 versicolor가 정답일 확률을 의미한다고 하였을 때, 각 실제값의 정수 인코딩은 1, 2, 3이 되고 이에 원-핫 인코딩을 수행하여 실제값을 원-핫 벡터로 수치화한 것을 보여줍니다.\\n[이미지: ]\\n예를 들어 현재 풀고 있는 샘플 데이터의 실제값이 setosa라면 setosa의 원-핫 벡터는 [0 1 0]입니다. 이 경우, 예측값과 실제값의 오차가 0이 되는 경우는 소프트맥스 함수의 결과가 [0 1 0]이 되는 경우입니다. 이 두 벡터의 오차를 계산하기 위해서 소프트맥스 회귀는 비용 함수로 크로스 엔트로피 함수를 사용하는데, 이는 뒤에서 비용 함수를 설명하는 부분에서 다시 언급하겠습니다.\\n[이미지: ]', '[이미지: ]\\n앞서 배운 선형 회귀나 로지스틱 회귀와 마찬가지로 오차로부터 가중치를 업데이트 합니다.\\n[이미지: ]\\n더 정확히는 선형 회귀나 로지스틱 회귀와 마찬가지로 편향 또한 업데이트의 대상이 되는 매개 변수입니다. 소프트맥스 회귀를 벡터와 행렬 연산으로 이해해봅시다. 입력을 특성(feature)의 수만큼의 차원을 가진 입력 벡터 $x$라고 하고, 가중치 행렬을 $W$, 편향을 $b$라고 하였을 때, 소프트맥스 회귀에서 예측값을 구하는 과정을 벡터와 행렬 연산으로 표현하면 아래와 같습니다.\\n[이미지: ]\\n여기서 4는 특성의 수이며 3은 클래스의 개수에 해당됩니다.']\n",
      "['꼭 실제값을 원-핫 벡터로 표현해야만 다중 클래스 분류 문제를 풀 수 있는 것은 아니지만, 대부분의 다중 클래스 분류 문제가 각 클래스 간의 관계가 균등하다는 점에서 원-핫 벡터는 이러한 점을 표현할 수 있는 적절한 표현 방법입니다.', '다수의 클래스를 분류하는 문제에서는 이진 분류처럼 2개의 숫자 레이블이 아니라 클래스의 개수만큼 숫자 레이블이 필요합니다. 이때 직관적으로 생각해볼 수 있는 레이블링 방법은 분류해야 할 클래스 전체에 정수 인코딩을 하는 겁니다. 예를 들어서 분류해야 할 레이블이 {red, green, blue}와 같이 3개라면 각각 0, 1, 2로 레이블을 합니다. 또는 분류해야 할 클래스가 4개고 인덱스를 숫자 1부터 시작하고 싶다고 하면 {baby, child, adolescent, adult}라면 1, 2, 3, 4로 레이블을 해볼 수 있습니다. 그런데 일반적인 다중 클래스 분류 문제에서 레이블링 방법으로는 위와 같은 정수 인코딩이 아니라 원-핫 인코딩을 사용하는 것이 보다 클래스의 성질을 잘 표현하였다고 할 수 있습니다. 그 이유를 알아봅시다.', 'Banana, Tomato, Apple라는 3개의 클래스가 존재하는 문제가 있다고 해봅시다. 레이블은 정수 인코딩을 사용하여 각각 1, 2, 3을 부여하였습니다. 손실 함수로 선형 회귀 실습에서 배운 평균 제곱 오차 MSE를 사용하면 정수 인코딩이 어떤 오해를 불러일으킬 수 있는지 확인할 수 있습니다. 아래의 식은 앞서 선형 회귀에서 배웠던 MSE를 다시 그대로 가져온 것입니다. $\\\\hat{y}$는 예측값을 의미합니다.\\n$$Loss\\\\ function = \\\\frac{1}{n} \\\\sum_i^{n} \\\\left(y_{i} - \\\\hat{y_{i}}\\\\right)^2$$\\n직관적인 오차 크기 비교를 위해 평균을 구하는 수식은 제외하고 제곱 오차로만 판단해봅시다.\\n실제값이 Tomato일때 예측값이 Banana이었다면 제곱 오차는 다음과 같습니다.\\n$(2-1)^{2} = 1$\\n실제값이 Apple일때 예측값이 Banana이었다면 제곱 오차는 다음과 같습니다.\\n$(3-1)^{2} = 4$', '$(2-1)^{2} = 1$\\n실제값이 Apple일때 예측값이 Banana이었다면 제곱 오차는 다음과 같습니다.\\n$(3-1)^{2} = 4$\\n즉, Banana과 Tomato 사이의 오차보다 Banana과 Apple의 오차가 더 큽니다. 이는 기계에게 Banana가 Apple보다는 Tomato에 더 가깝다는 정보를 주는 것과 다름없습니다. 더 많은 클래스에 대해서 정수 인코딩을 수행했다고 해봅시다.\\n{Banana :1, Tomato :2, Apple :3, Strawberry :4, ... Watermelon :10}', '{Banana :1, Tomato :2, Apple :3, Strawberry :4, ... Watermelon :10}\\n이 정수 인코딩은 Banana가 Watermelon보다는 Tomato에 더 가깝다는 의미를 담고 있습니다. 이는 사용자가 부여하고자 했던 정보가 아닙니다. 이러한 정수 인코딩의 순서 정보가 도움이 되는 분류 문제도 물론 있습니다. 바로 각 클래스가 순서의 의미를 갖고 있어서 회귀를 통해서 분류 문제를 풀 수 있는 경우입니다. 예를 들어 {baby, child, adolescent, adult}나 {1층, 2층, 3층, 4층}이나 {10대, 20대, 30대, 40대}와 같은 경우가 이에 해당됩니다. 하지만 일반적인 분류 문제에서는 각 클래스는 순서의 의미를 갖고 있지 않으므로 각 클래스 간의 오차는 균등한 것이 옳습니다. 정수 인코딩과 달리 원-핫 인코딩은 분류 문제 모든 클래스 간의 관계를 균등하게 분배합니다.', '아래는 세 개의 카테고리에 대해서 원-핫 인코딩을 통해서 레이블을 인코딩했을 때 각 클래스 간의 제곱 오차가 균등함을 보여줍니다.\\n$((1,0,0)-(0,1,0))^{2} = (1-0)^{2} + (0-1)^{2} + (0-0)^{2} = 2$\\n$((1,0,0)-(0,0,1))^{2} = (1-0)^{2} + (0-0)^{2} + (0-1)^{2} = 2$\\n다르게 표현하면 모든 클래스에 대해서 원-핫 인코딩을 통해 얻은 원-핫 벡터들은 모든 쌍에 대해서 유클리드 거리를 구해도 전부 유클리드 거리가 동일합니다. 원-핫 벡터는 이처럼 각 클래스의 표현 방법이 무작위성을 가진다는 점을 표현할 수 있습니다. 뒤에서 다시 언급되겠지만 이러한 원-핫 벡터의 관계의 무작위성은 때로는 단어의 유사성을 구할 수 없다는 단점으로 언급되기도 합니다.']\n",
      "['소프트맥스 회귀에서는 비용 함수로 크로스 엔트로피 함수를 사용합니다. 여기서는 소프트맥스 회귀에서의 크로스 엔트로피 함수뿐만 아니라, 다양한 표기 방법에 대해서 이해해보겠습니다.\\n1) 크로스 엔트로피 함수\\n아래에서 $y$는 실제값을 나타내며, $k$는 클래스의 개수로 정의합니다. $y_{j}$는 실제값 원-핫 벡터의 $j$번째 인덱스를 의미하며, $p_{j}$는 샘플 데이터가 $j$번째 클래스일 확률을 나타냅니다. 표기에 따라서 $\\\\hat{y}_{j}$로 표현하기도 합니다.\\n$$cost = -\\\\sum_{j=1}^{k}y_{j}\\\\ log(p_{j})$$', '$$cost = -\\\\sum_{j=1}^{k}y_{j}\\\\ log(p_{j})$$\\n이 함수가 왜 비용 함수로 적합한지 알아보겠습니다. $c$가 실제값 원-핫 벡터에서 1을 가진 원소의 인덱스라고 한다면, $p_{c}=1$은 $\\\\hat{y}$가 $y$를 정확하게 예측한 경우가 됩니다. 이를 식에 대입해보면 $-1 log(1) = 0$이 되기 때문에, 결과적으로 $\\\\hat{y}$가 $y$를 정확하게 예측한 경우의 크로스 엔트로피 함수의 값은 0이 됩니다. 즉, $-\\\\sum_{j=1}^{k}y_{j}\\\\ log(p_{j})$ 이 값을 최소화하는 방향으로 학습해야 합니다.\\n이를 $n$개의 전체 데이터에 대한 평균을 구한다고 하면 최종 비용 함수는 다음과 같습니다.\\n$$\\ncost = -\\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\sum_{j=1}^{k}y_{j}^{(i)}\\\\ log(p_{j}^{(i)})\\n$$\\n2) 이진 분류에서의 크로스 엔트로피 함수', '$$\\n2) 이진 분류에서의 크로스 엔트로피 함수\\n로지스틱 회귀에서 배운 크로스 엔트로피 함수식과 달라보이지만, 본질적으로는 동일한 함수식입니다. 로지스틱 회귀의 크로스 엔트로피 함수식으로부터 소프트맥스 회귀의 크로스 엔트로피 함수식을 도출해봅시다.\\n$$\\ncost = -(y\\\\ logH(X) + (1-y)\\\\ log(1-H(X)))\\n$$\\n위의 식은 앞서 로지스틱 회귀에서 배웠던 크로스 엔트로피의 함수식을 보여줍니다. 위의 식에서 $y$를 $y_{1}$, $1-y$를 $y_{2}$로 치환하고 $H(X)$를 $p_{1}$, $1-H(X)$를 $p_{2}$로 치환해봅시다. 결과적으로 아래의 식을 얻을 수 있습니다.\\n$$\\n-(y_{1}\\\\ log(p_{1})+y_{2}\\\\ log(p_{2}))\\n$$\\n이 식은 아래와 같이 표현할 수 있습니다.\\n$$\\n-(\\\\sum_{i=1}^{2}y_{i}\\\\ log\\\\ p_{i})\\n$$\\n소프트맥스 회귀에서는 k의 값이 고정된 값이 아니므로 2를 k로 변경합니다.\\n$$', '$$\\n-(\\\\sum_{i=1}^{2}y_{i}\\\\ log\\\\ p_{i})\\n$$\\n소프트맥스 회귀에서는 k의 값이 고정된 값이 아니므로 2를 k로 변경합니다.\\n$$\\n-(\\\\sum_{i=1}^{k}y_{i}\\\\ log\\\\ p_{i})\\n$$\\n위의 식은 결과적으로 소프트맥스 회귀의 식과 동일합니다. 역으로 소프트맥스 회귀에서 로지스틱 회귀의 크로스 엔트로피 함수식을 얻는 것은 k를 2로 하고, $y_{1}$과 $y_{2}$를 각각 $y$와 $1-y$로 치환하고, $p_{1}$와 $p_{2}$를 각각 $H(X)$와 $1-H(X)$로 치환하면 됩니다.\\n정리하면 소프트맥스 함수의 최종 비용 함수에서 $k$가 2라고 가정하면 결국 로지스틱 회귀의 비용 함수와 같습니다.\\n$$', '정리하면 소프트맥스 함수의 최종 비용 함수에서 $k$가 2라고 가정하면 결국 로지스틱 회귀의 비용 함수와 같습니다.\\n$$\\ncost = -\\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\sum_{j=1}^{k}y_{j}^{(i)}\\\\ log(p_{j}^{(i)}) = -\\\\frac{1}{n} \\\\sum_{i=1}^{n} [y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})]\\n$$']\n",
      "['n개의 특성을 가지고 m개의 클래스를 분류하는 소프트맥스 회귀를 뒤에서 배우게 되는 인공 신경망의 형태로 표현하면 다음과 같습니다. 소프트맥스 회귀 또한 하나의 인공 신경망으로 볼 수 있습니다.\\n[이미지: ]\\n사실 위의 그림은 앞서 소프트맥스 함수를 사용하기 위해 설명했던 아래의 그림에서 특성의 개수를 $n$으로 하고, 클래스의 개수를 $m$으로 일반화한 뒤에 그림을 좀 더 요약해서 표현한 것으로 봐도 무방합니다.\\n[이미지: ]\\n==================================================\\n--- 06-10 소프트맥스 회귀 실습 ---\\n```\\n테스트 정확도: 0.9667\\n```이번에 실습할 데이터는 앞서 다중 클래스 분류를 설명하기 위해 예시로 들었던 붓꽃 품종 분류 문제입니다. 데이터를 직접 다운로드 받고, 데이터에 대한 탐색 과정을 거친 후 모델을 설계해보겠습니다.', '데이터 다운로드 링크 : https://www.kaggle.com/saurabh00007/iriscsv']\n",
      "['import pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport urllib.request\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.utils import to_categorical\\niris.csv 파일을 데이터프레임으로 로드한 후 5개의 샘플을 출력해보겠습니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/06.%20Machine%20Learning/dataset/Iris.csv\", filename=\"Iris.csv\")\\ndata = pd.read_csv(\\'Iris.csv\\', encoding=\\'latin1\\')\\nprint(\\'샘플의 개수 :\\', len(data))', \"data = pd.read_csv('Iris.csv', encoding='latin1')\\nprint('샘플의 개수 :', len(data))\\nprint(data[:5])\\n샘플의 개수 : 150\\nId  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\", '3   4            4.6           3.1            1.5           0.2  Iris-setosa\\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\\n데이터는 6개의 열로 구성된  총 150개의 샘플로 구성되어져 있습니다. 각 샘플의 인덱스를 의미하는 첫번째 열인 Id는 실질적으로 의미는 없는 열입니다. 그 후 특성(feature)에 해당하는 SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm 4개의 열이 존재합니다. 마지막 열 Species는 해당 샘플이 어떤 품종인지를 의미하며 여기서 예측해야 하는 레이블에 해당됩니다. Species열에서 품종이 몇 개 존재하는지 출력합니다.\\n# 중복을 허용하지 않고, 있는 데이터의 모든 종류를 출력', '# 중복을 허용하지 않고, 있는 데이터의 모든 종류를 출력\\nprint(\"품종 종류:\", data[\"Species\"].unique(), sep=\"\\\\n\")\\n품종 종류:\\n[\\'Iris-setosa\\' \\'Iris-versicolor\\' \\'Iris-virginica\\']\\nSpecies는 Iris-setosa, Iris-versicolor, Iris-virginica라는 3개의 품종으로 구성되어져 있습니다. 즉, 이번 데이터를 가지고 푸는 문제는 주어진 샘플 데이터의 4개의 특성으로부터 3개 중 어떤 품종인지를 예측하는 문제가 되겠습니다. 3개의 품종이 4개의 특성에 대해서 어떤 분포를 가지고 있는지 시각화해봅시다. seaborn의 pairplot은 데이터프레임을 입력으로 받아 데이터프레임의 각 열의 조합에 따라서 산점도(scatter plot)를 그립니다.\\nsns.set(style=\"ticks\", color_codes=True)', 'sns.set(style=\"ticks\", color_codes=True)\\ng = sns.pairplot(data, hue=\"Species\", palette=\"husl\")\\n[이미지: ]\\n해당 입력의 경우에는 4개의 특성에 해당하는 SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm에 대해서 모든 쌍(pair)의 조합인 16개의 경우에 대해서 산점도를 그립니다. 만약 동일한 특성의 쌍일 경우에는 히스토그램으로 나타내는데 가령, SepalLengthCm와 SepalLengthCm의 조합이 그렇습니다. seaborn의 barplot을 통해 종과 특성에 대한 연관 관계를 출력할 수도 있습니다. 예를 들어 각 종에 따른 SepalWidthCm의 값을 확인해봅시다.\\n# 각 종과 특성에 대한 연관 관계\\nsns.barplot(x=\\'Species\\', y=\\'SepalWidthCm\\', data=data, ci=None)\\n[이미지: ]', \"# 각 종과 특성에 대한 연관 관계\\nsns.barplot(x='Species', y='SepalWidthCm', data=data, ci=None)\\n[이미지: ]\\n150개의 샘플 데이터 중에서 Species열에서 각 품종이 몇 개있는지 확인합니다.\\ndata['Species'].value_counts().plot(kind='bar')\\n[이미지: ]\\n동일하게 50개씩 존재합니다. 각 레이블에 대한 분포가 균일합니다. 소프트맥스 회귀 모델을 구성하기 위해 전처리를 진행해야 합니다. 레이블에 해당하는 Species열에 대해서 전부 수치화를 진행해봅시다. 우선 원-핫 인코딩을 수행하기 전 정수 인코딩을 수행합니다. 정상적으로 정수 인코딩이 수행되었는지 확인하기 위하여 다시 한 번 값의 분포를 출력합니다.\\n# Iris-virginica는 0, Iris-setosa는 1, Iris-versicolor는 2가 됨.\", \"# Iris-virginica는 0, Iris-setosa는 1, Iris-versicolor는 2가 됨.\\ndata['Species'] = data['Species'].replace(['Iris-virginica','Iris-setosa','Iris-versicolor'],[0,1,2])\\ndata['Species'].value_counts().plot(kind='bar')\\n[이미지: ]\\n여전히 동일한 분포를 보입니다. 특성과 품종을 각각 종속 변수와 독립 변수 데이터로 분리하는 작업을 수행하고, 정확하게 분리가 되었는지 확인하기 위해 데이터 중 상위 5개씩 출력해보겠습니다.\\n# X 데이터. 특성은 총 4개.\\ndata_X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\\n# Y 데이터. 예측 대상.\\ndata_y = data['Species'].values\\nprint(data_X[:5])\", \"# Y 데이터. 예측 대상.\\ndata_y = data['Species'].values\\nprint(data_X[:5])\\nprint(data_y[:5])\\n[[5.1 3.5 1.4 0.2]\\n[4.9 3.  1.4 0.2]\\n[4.7 3.2 1.3 0.2]\\n[4.6 3.1 1.5 0.2]\\n[5.  3.6 1.4 0.2]]\\n[1 1 1 1 1]\\n훈련 데이터와 테스트 데이터를 분리하고 레이블에 대한 원-핫 인코딩을 수행해합니다. 그리고 원-핫 인코딩이 진행되었는지 확인하기 위해 훈련 데이터의 레이블과 테스트 데이터의 레이블을 상위 5개씩 출력합니다.\\n# 훈련 데이터와 테스트 데이터를 8:2로 나눈다.\\n(X_train, X_test, y_train, y_test) = train_test_split(data_X, data_y, train_size=0.8, random_state=1)\\n# 원-핫 인코딩\\ny_train = to_categorical(y_train)\", '# 원-핫 인코딩\\ny_train = to_categorical(y_train)\\ny_test = to_categorical(y_test)\\nprint(y_train[:5])\\nprint(y_test[:5])\\n[[0. 0. 1.]\\n[1. 0. 0.]\\n[0. 0. 1.]\\n[1. 0. 0.]\\n[1. 0. 0.]]\\n[[0. 1. 0.]\\n[0. 0. 1.]\\n[0. 0. 1.]\\n[0. 1. 0.]\\n[1. 0. 0.]]\\n전처리 단계가 모두 끝이 났습니다.']\n",
      "[\"입력의 차원이 4이므로 input_dim의 인자값이 4로 변경되었습니다. 출력의 차원이 3이므로 input_dim=4 앞의 인자값이 3입니다. 또한 활성화 함수로는 소프트맥스 함수를 사용하므로 activation의 인자값으로 'softmax'를 사용합니다.\\n오차 함수로는 크로스 엔트로피 함수를 사용합니다. 시그모이드 함수를 사용한 이진 분류 문제에서는 binary_crossentropy를 사용하였지만, 다중 클래스 분류 문제에서는 'categorical_crossentropy를 사용합니다. 옵티마이저로는 경사 하강법의 일종인 아담(adam)을 사용해보았습니다. 아담에 대한 설명은 딥 러닝 챕터에서 다룹니다.\", \"전체 데이터에 대한 훈련 횟수는 200입니다. 이번에는 테스트 데이터를 별도로 분리해서 평가에 사용하였는데, validation_data=()에 테스트 데이터를 기재해주면 실제로는 훈련에는 사용하지 않으면서 각 훈련 횟수마다 테스트 데이터에 대한 정확도를 출력합니다. 즉, 정확도가 전체 데이터에 대한 훈련 1회(1 에포크)마다 측정되고는 있지만 기계는 해당 데이터를 가지고 가중치를 업데이트하지 않습니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nmodel = Sequential()\\nmodel.add(Dense(3, input_dim=4, activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\", \"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nhistory = model.fit(X_train, y_train, epochs=200, batch_size=1, validation_data=(X_test, y_test))\\n출력에서 accuracy은 훈련 데이터에 대한 정확도이고, val_accuracy은 테스트 데이터에 대한 정확도를 의미합니다. 이번에는 각 에포크당 훈련 데이터와 테스트 데이터에 대한 정확도를 측정했으므로 한 번 에포크에 따른 정확도를 그래프로 출력해보겠습니다.\\nepochs = range(1, len(history.history['accuracy']) + 1)\\nplt.plot(epochs, history.history['loss'])\\nplt.plot(epochs, history.history['val_loss'])\\nplt.title('model loss')\", 'plt.plot(epochs, history.history[\\'val_loss\\'])\\nplt.title(\\'model loss\\')\\nplt.ylabel(\\'loss\\')\\nplt.xlabel(\\'epoch\\')\\nplt.legend([\\'train\\', \\'val\\'], loc=\\'upper left\\')\\nplt.show()\\n[이미지: ]\\n에포크가 증가함에 따라 오차(loss)가 점차적으로 줄어드는 것을 볼 수 있습니다. 케라스에서 테스트 데이터의 정확도를 측정하는 용도로 제공하고 있는 evaluate()를 통해 테스트 데이터에 대한 정확도를 다시 출력해보겠습니다.\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\\n테스트 정확도: 0.9667\\n테스트 데이터에 대해서 96.67%의 정확도를 얻었습니다.\\n==================================================', '테스트 정확도: 0.9667\\n테스트 데이터에 대해서 96.67%의 정확도를 얻었습니다.\\n==================================================\\n--- 07. 딥 러닝(Deep Learning) 개요 ---\\n마지막 편집일시 : 2022년 1월 4일 4:01 오후\\n==================================================\\n--- 07-01 퍼셉트론(Perceptron) ---\\n```\\n(0, 1, 1, 1)\\n```인공 신경망은 수많은 머신 러닝 방법 중 하나입니다. 하지만 최근 인공 신경망을 복잡하게 쌓아 올린 딥 러닝이 다른 머신 러닝 방법들을 뛰어넘는 성능을 보여주는 사례가 늘면서, 전통적인 머신 러닝과 딥 러닝을 구분해서 이해해야 한다는 목소리가 커지고 있습니다. 딥 러닝을 이해하기 위해서는 우선 인공 신경망에 대한 이해가 필요한데, 여기서는 초기의 인공 신경망인 퍼셉트론(Perceptron)에 대해서 이해합니다.']\n",
      "['퍼셉트론(Perceptron)은 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 제안한 초기 형태의 인공 신경망으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘입니다. 퍼셉트론은 실제 뇌를 구성하는 신경 세포 뉴런의 동작과 유사한데, 신경 세포 뉴런의 그림을 먼저 보도록 하겠습니다. 뉴런은 가지돌기에서 신호를 받아들이고, 이 신호가 일정치 이상의 크기를 가지면 축삭돌기를 통해서 신호를 전달합니다.\\n[이미지: ]\\n다수의 입력을 받는 퍼셉트론의 그림을 보겠습니다. 신경 세포 뉴런의 입력 신호와 출력 신호가 퍼셉트론에서 각각 입력값과 출력값에 해당됩니다.\\n[이미지: ]', '[이미지: ]\\n다수의 입력을 받는 퍼셉트론의 그림을 보겠습니다. 신경 세포 뉴런의 입력 신호와 출력 신호가 퍼셉트론에서 각각 입력값과 출력값에 해당됩니다.\\n[이미지: ]\\n$x$는 입력값을 의미하며, $w$는 가중치(Weight), $y$는 출력값입니다. 그림 안의 원은 인공 뉴런에 해당됩니다. 실제 신경 세포 뉴런에서의 신호를 전달하는 축삭돌기의 역할을 퍼셉트론에서는 가중치가 대신합니다. 각각의 인공 뉴런에서 보내진 입력값 $x$는 각각의 가중치 $w$와 함께 종착지인 인공 뉴런에 전달되고 있습니다.\\n각각의 입력값에는 각각의 가중치가 존재하는데, 이때 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미합니다.', '각각의 입력값에는 각각의 가중치가 존재하는데, 이때 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미합니다.\\n각 입력값이 가중치와 곱해져서 인공 뉴런에 보내지고, 각 입력값과 그에 해당되는 가중치의 곱의 전체 합이 임계치(threshold)를 넘으면 종착지에 있는 인공 뉴런은 출력 신호로서 1을 출력하고, 그렇지 않을 경우에는 0을 출력합니다. 이러한 함수를 계단 함수(Step function)라고 하며, 아래는 그래프는 계단 함수의 하나의 예를 보여줍니다.\\n[이미지: ]\\n이때 계단 함수에 사용된 이 임계치값을 수식으로 표현할 때는 보통 세타(Θ)로 표현합니다. 식으로 표현하면 다음과 같습니다.\\n$$if \\\\sum_i^{n} w_{i}x_{i}\\\\ ≥ \\\\theta → y=1$$\\n$$if \\\\sum_i^{n} w_{i}x_{i}\\\\ < \\\\theta → y=0$$', '$$if \\\\sum_i^{n} w_{i}x_{i}\\\\ ≥ \\\\theta → y=1$$\\n$$if \\\\sum_i^{n} w_{i}x_{i}\\\\ < \\\\theta → y=0$$\\n위의 식에서 임계치를 좌변으로 넘기고 편향 $b$(bias)로 표현할 수도 있습니다. 편향 $b$ 또한 퍼셉트론의 입력으로 사용됩니다. 보통 그림으로 표현할 때는 입력값이 1로 고정되고 편향 $b$가 곱해지는 변수로 표현됩니다.\\n[이미지: ]\\n$$if \\\\sum_i^{n} w_{i}x_{i} + b ≥ 0 → y=1$$\\n$$if \\\\sum_i^{n} w_{i}x_{i} + b < 0 → y=0$$\\n이 책을 포함한 많은 인공 신경망 자료에서 편의상 편향 $b$가 그림이나 수식에서 생략되서 표현되기도 하지만 실제로는 편향 $b$ 또한 딥 러닝이 최적의 값을 찾아야 할 변수 중 하나입니다.', '뒤에서 배우겠지만 이렇게 뉴런에서 출력값을 변경시키는 함수를 활성화 함수(Activation Function)라고 합니다. 초기 인공 신경망 모델인 퍼셉트론은 활성화 함수로 계단 함수를 사용하였지만, 그 뒤에 등장한 여러가지 발전된 신경망들은 계단 함수 외에도 여러 다양한 활성화 함수를 사용하기 시작했습니다. 사실 앞서 배운 시그모이드 함수나 소프트맥스 함수 또한 활성화 함수 중 하나입니다.\\n퍼셉트론을 배우기 전에 로지스틱 회귀를 먼저 배운 이유도 여기에 있습니다. 퍼셉트론의 활성화 함수는 계단 함수이지만 여기서 활성화 함수를 시그모이드 함수로 변경하면 방금 배운 퍼셉트론은 곧 이진 분류를 수행하는 로지스틱 회귀와 동일함을 알 수 있습니다.\\n다시 말하면 로지스틱 회귀 모델이 인공 신경망에서는 하나의 인공 뉴런으로 볼 수 있습니다. 로지스틱 회귀를 수행하는 인공 뉴런과 위에서 배운 퍼셉트론의 차이는 오직 활성화 함수의 차이입니다.', '인공 뉴런 : 활성화 함수 $f(\\\\sum_i^{n} w_{i}x_{i} + b)$\\n위의 퍼셉트론(인공 뉴런 종류 중 하나) : 계단 함수 $f(\\\\sum_i^{n} w_{i}x_{i} + b)$']\n",
      "['위에서 배운 퍼셉트론을 단층 퍼셉트론이라고 합니다. 퍼셉트론은 단층 퍼셉트론과 다층 퍼셉트론으로 나누어지는데, 단층 퍼셉트론은 값을 보내는 단계과 값을 받아서 출력하는 두 단계로만 이루어집니다. 이때 이 각 단계를 보통 층(layer)이라고 부르며, 이 두 개의 층을 입력층(input layer)과 출력층(output layer)이라고 합니다.\\n[이미지: ]', '[이미지: ]\\n단층 퍼셉트론이 어떤 일을 할 수 있으며 한계는 무엇인지 학습해보겠습니다. 컴퓨터는 두 개의 값 0과 1을 입력해 하나의 값을 출력하는 회로가 모여 만들어지는데, 이 회로를 게이트(gate)라고 부릅니다. 초기 형태의 인공 신경망인 단층 퍼셉트론은 간단한 XOR 게이트조차도 구현할 수 없는 부족한 인공 신경망이라는 지적을 받았습니다. 단층 퍼셉트론을 이용하면 AND, NAND, OR 게이트는 구현가능합니다. 게이트 연산에 쓰이는 것은 두 개의 입력값과 하나의 출력값입니다. AND 게이트란 두 개의 입력값 $x_{1}, x_{2}$이 각각 0 또는 1의 값을 가질 수 있으면서 모두 1인 경우에만 출력값 $y$가 1이 나오는 구조를 말합니다.\\n[이미지: ]', '[이미지: ]\\n단층 퍼셉트론의 식을 통해 AND 게이트를 만족하는 두 개의 가중치와 편향 값에는 뭐가 있을까요? 각각 $w_{1}$, $w_{2}$, $b$라고 한다면 [0.5, 0.5, -0.7], [0.5, 0.5, -0.8] 또는 [1.0, 1.0, -1.0] 등 이 외에도 다양한 가중치와 편향의 조합이 나올 수 있습니다. 이해를 돕기 위해서 AND 게이트를 위한 매개변수 값을 가진 단층 퍼셉트론의 식을 파이썬 코드로 간단하게 구현해봅시다.\\ndef AND_gate(x1, x2):\\nw1 = 0.5\\nw2 = 0.5\\nb = -0.7\\nresult = x1*w1 + x2*w2 + b\\nif result <= 0:\\nreturn 0\\nelse:\\nreturn 1\\n위의 함수에 AND 게이트의 입력값을 모두 넣어보면 오직 두 개의 입력값이 1인 경우에만 1을 출력합니다.\\nAND_gate(0, 0), AND_gate(0, 1), AND_gate(1, 0), AND_gate(1, 1)', 'AND_gate(0, 0), AND_gate(0, 1), AND_gate(1, 0), AND_gate(1, 1)\\n(0, 0, 0, 1)\\n그렇다면 두 개의 입력값이 1인 경우에만 출력값이 0, 나머지 입력값의 쌍(pair)에 대해서는 모두 출력값이 1이 나오는 NAND 게이트는 어떨까요?\\n[이미지: ]\\n앞서 언급했던 AND 게이트를 충족하는 가중치와 편향값인 [0.5, 0.5, -0.7]에 -를 붙여서 [-0.5, -0.5, +0.7]을 단층 퍼셉트론의 식에 넣어보면 NAND 게이트를 충족합니다. 파이썬 코드를 통해서 이를 확인해봅시다.\\ndef NAND_gate(x1, x2):\\nw1 = -0.5\\nw2 = -0.5\\nb = 0.7\\nresult = x1*w1 + x2*w2 + b\\nif result <= 0:\\nreturn 0\\nelse:\\nreturn 1\\n단지 같은 코드에 함수 이름과 가중치와 편향만 바꿨을 뿐입니다. 퍼셉트론의 구조는 같기때문입니다.', 'if result <= 0:\\nreturn 0\\nelse:\\nreturn 1\\n단지 같은 코드에 함수 이름과 가중치와 편향만 바꿨을 뿐입니다. 퍼셉트론의 구조는 같기때문입니다.\\nNAND_gate(0, 0), NAND_gate(0, 1), NAND_gate(1, 0), NAND_gate(1, 1)\\n(1, 1, 1, 0)\\nNAND 게이트를 구현한 파이썬 코드에 입력값을 넣자, 두 개의 입력값이 1인 경우에만 0이 나오는 것을 확인할 수 있습니다. 퍼셉트론으로 NAND 게이트를 구현한 것입니다. [-0.5, -0.5, -0.7] 외에도 퍼셉트론이 NAND 게이트의 동작을 하도록 하는 다양한 가중치와 편향의 값들이 있을 것입니다.\\n두 개의 입력이 모두 0인 경우에 출력값이 0이고 나머지 경우에는 모두 출력값이 1인 OR 게이트 또한 적절한 가중치 값과 편향 값만 찾으면 단층 퍼셉트론의 식으로 구현할 수 있습니다.\\n[이미지: ]', '[이미지: ]\\n예를 들어 각각 가중치와 편향에 대해서 [0.6, 0.6, -0.5]를 선택하면 OR 게이트를 충족합니다.\\ndef OR_gate(x1, x2):\\nw1 = 0.6\\nw2 = 0.6\\nb = -0.5\\nresult = x1*w1 + x2*w2 + b\\nif result <= 0:\\nreturn 0\\nelse:\\nreturn 1\\nOR_gate(0, 0), OR_gate(0, 1), OR_gate(1, 0), OR_gate(1, 1)\\n(0, 1, 1, 1)\\n이 외에도 이를 충족하는 다양한 가중치와 편향의 값이 있습니다.', '(0, 1, 1, 1)\\n이 외에도 이를 충족하는 다양한 가중치와 편향의 값이 있습니다.\\n이처럼 단층 퍼셉트론은 AND 게이트, NAND 게이트, OR 게이트를 구현할 수 있으나 지금부터 설명할 XOR 게이트는 구현할 수 없습니다. XOR 게이트는 입력값 두 개가 서로 다른 값을 갖고 있을때에만 출력값이 1이 되고, 입력값 두 개가 서로 같은 값을 가지면 출력값이 0이 되는 게이트입니다. 위의 파이썬 코드에 아무리 수많은 가중치와 편향을 넣어봐도 XOR 게이트를 구현하는 것은 불가능합니다. 그 이유는 단층 퍼셉트론은 직선 하나로 두 영역을 나눌 수 있는 문제에 대해서만 구현이 가능하기 때문입니다.\\n예를 들어 AND 게이트에 대한 단층 퍼셉트론을 시각화해보면 다음과 같습니다.\\n[이미지: ]', '예를 들어 AND 게이트에 대한 단층 퍼셉트론을 시각화해보면 다음과 같습니다.\\n[이미지: ]\\n그림에서는 출력값 0을 하얀색 원, 1을 검은색 원으로 표현했습니다. AND 게이트를 충족하려면 하얀색 원과 검은색 원을 직선으로 나누게 됩니다. 마찬가지로 NAND 게이트나 OR 게이트에 대해서도 시각화를 했을 때 직선으로 나누는 것이 가능합니다.\\n[이미지: ]\\n그렇다면 XOR 게이트는 어떨까요? XOR 게이트는 입력값 두 개가 서로 다른 값을 갖고 있을때에만 출력값이 1이 되고, 입력값 두 개가 서로 같은 값을 가지면 출력값이 0이 되는 게이트입니다. XOR 게이트를 시각화해보면 다음과 같습니다.\\n[이미지: ]', '[이미지: ]\\n하얀색 원과 검은색 원을 직선 하나로 나누는 것은 불가능하므로 단층 퍼셉트론으로는 XOR 게이트를 구현할 수 없습니다. 위의 좌측 그림과 같이 적어도 두 개의 선이 필요합니다. 이를 어떻게 해결할 수 있을까요? 이에 대한 해답은 다층 퍼셉트론입니다. 다층 퍼셉트론을 사용하면 여러 개의 선으로 분류하는 효과를 얻을 수 있습니다.']\n",
      "['XOR 게이트는 기존의 AND, NAND, OR 게이트를 조합하면 만들 수 있습니다. 퍼셉트론 관점에서 말하면 층을 더 쌓으면 만들 수 있습니다. 다층 퍼셉트론과 단층 퍼셉트론의 차이는 단층 퍼셉트론은 입력층과 출력층만 존재하지만, 다층 퍼셉트론은 중간에 층을 더 추가하였다는 점입니다. 이렇게 입력층과 출력층 사이에 존재하는 층을 은닉층(hidden layer)이라고 합니다. 즉, 다층 퍼셉트론은 중간에 은닉층이 존재한다는 점이 단층 퍼셉트론과 다릅니다. 다층 퍼셉트론은 줄여서 MLP라고도 부릅니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림은 AND, NAND, OR 게이트를 조합하여 XOR 게이트를 구현한 다층 퍼셉트론의 예입니다. (실제 구현은 숙제로 남겨두겠습니다. 힌트를 드리자면 위의 단층 퍼셉트론에서 사용한 함수들을 그대로 사용하면 됩니다.) XOR 예제에서는 은닉층 1개만으로 문제를 해결할 수 있었지만, 다층 퍼셉트론은 본래 은닉층이 1개 이상인 퍼셉트론을 말합니다. 즉, XOR 문제나 기타 복잡한 문제를 해결하기 위해서 다층 퍼셉트론은 중간에 수많은 은닉층을 더 추가할 수 있습니다. 은닉층의 개수는 2개일 수도 있고, 수십 개일수도 있고 사용자가 설정하기 나름입니다. 아래는 더 어려운 문제를 풀기 위해서 은닉층이 하나 더 추가되고(이 경우에는 은닉층이 2개), 뉴런의 개수를 늘린 다층 퍼셉트론의 모습을 보여줍니다.\\n[이미지: ]', '[이미지: ]\\n위와 같이 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN) 이라고 합니다. 심층 신경망은 다층 퍼셉트론만 이야기 하는 것이 아니라, 여러 변형된 다양한 신경망들도 은닉층이 2개 이상이 되면 심층 신경망이라고 합니다.\\n지금까지는 OR, AND, XOR 게이트 등. 퍼셉트론이 제대로 된 정답을 출력할 때까지 저자가 직접 가중치를 바꿔보면서 적절한 가중치를 수동으로 찾았습니다. 하지만 이제는 기계가 가중치를 스스로 찾아내도록 자동화시켜야하는데, 이것이 머신 러닝에서 말하는 훈련(training) 또는 학습(learning) 단계에 해당됩니다. 앞서 선형 회귀와 로지스틱 회귀에서 보았듯이 손실 함수(Loss function)와 옵티마이저(Optimizer)를 사용합니다. 그리고 만약 학습을 시키는 인공 신경망이 심층 신경망일 경우에는 이를 심층 신경망을 학습시킨다고 하여, 딥 러닝(Deep Learning)이라고 합니다.', \"==================================================\\n--- 07-02 인공 신경망(Artificial Neural Network) 훑어보기 ---\\n```\\nx = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성\\ny = np.exp(x) / np.sum(np.exp(x))\\nplt.plot(x, y)\\nplt.title('Softmax Function')\\nplt.show()\\n```인공 신경망에 대한 기본적인 내용들을 정리합니다.\"]\n",
      "['[이미지: ]\\n위 그림의 다층 퍼셉트론(MLP)과 같이 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망을 피드 포워드 신경망(Feed-Forward Neural Network, FFNN)이라고 합니다.\\n[이미지: ]\\n위의 그림은 FFNN에 속하지 않는 RNN이라는 신경망을 보여줍니다. 이 신경망은 은닉층의 출력값을 출력층으로도 값을 보내지만, 동시에 은닉층의 출력값이 다시 은닉층의 입력으로 사용됩니다.']\n",
      "['다층 퍼셉트론은 은닉층과 출력층에 있는 모든 뉴런은 바로 이전 층의 모든 뉴런과 연결돼 있었습니다. 그와 같이 어떤 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결돼 있는 층을 전결합층(Fully-connected layer) 또는 완전연결층이라고 합니다. 줄여서 FC라고 부르기도 합니다. 앞서 본 다층 퍼셉트론의 모든 은닉층과 출력층은 전결합층입니다. 동일한 의미로 밀집층(Dense layer) 이라고 부르기도 하는데, 케라스에서는 밀집층을 구현할 때 Dense()를 사용합니다.']\n",
      "['[이미지: ]\\n앞서 배운 퍼셉트론에서는 계단 함수(Step function)를 통해 출력값이 0이 될지, 1이 될지를 결정했습니다. 이러한 매커니즘은 실제 뇌를 구성하는 신경 세포 뉴런이 전위가 일정치 이상이 되면 시냅스가 서로 화학적으로 연결되는 모습을 모방한 것입니다. 이렇게 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수를 활성화 함수(Activation function)라고 하는데 계단 함수는 이러한 활성화 함수의 하나의 예제에 불과합니다.\\n다양한 활성화 함수에 대해서 정리해봅시다. 일부는 머신 러닝 챕터에서 이미 봤던 함수들입니다.\\n(1) 활성화 함수의 특징 - 비선형 함수(Nonlinear function)', '다양한 활성화 함수에 대해서 정리해봅시다. 일부는 머신 러닝 챕터에서 이미 봤던 함수들입니다.\\n(1) 활성화 함수의 특징 - 비선형 함수(Nonlinear function)\\n활성화 함수의 특징은 선형 함수가 아닌 비선형 함수여야 한다는 점입니다. 선형 함수란 출력이 입력의 상수배만큼 변하는 함수를 선형함수라고 합니다. 예를 들어 $f(x) = wx + b$라는 함수가 있을 때, $w$와 $b$는 상수입니다. 이 식을 그래프로 시각화하면 직선입니다. 반대로 비선형 함수는 직선 1개로는 그릴 수 없는 함수를 말합니다.', '인공 신경망에서 활성화 함수는 비선형 함수여야 합니다. 앞서 퍼셉트론에서도 계단 함수라는 활성화 함수를 사용했는데 계단 함수 또한 비선형 함수에 속합니다. 인공 신경망의 능력을 높이기 위해서는 은닉층을 계속해서 추가해야 합니다. 그런데 만약 활성화 함수로 선형 함수를 사용하게 되면 은닉층을 쌓을 수가 없습니다. 예를 들어 활성화 함수로 선형 함수를 선택하고, 층을 계속 쌓는다고 가정해보겠습니다. 활성화 함수는 $f(x) = wx$라고 가정합니다. 여기다가 은닉층을 두 개 추가한다고하면 출력층을 포함해서 $y(x) = f(f(f(x)))$가 됩니다. 이를 식으로 표현하면 $w × w × w × x$입니다. 그런데 이는 잘 생각해보면 $w$의 세 제곱값을 $k$라고 정의해버리면 $y(x) = kx$와 같이 다시 표현이 가능합니다. 이 경우, 선형 함수로 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이가 없음을 알 수 있습니다.', '활성화 함수가 존재하지 않는 선형 함수 층을 사용하지 않는다는 의미는 아닙니다. 종종 활성화 함수를 사용하지 않는 층을 비선형 층들과 함께 인공 신경망의 일부로서 추가하는 경우도 있는데, 학습 가능한 가중치가 새로 생긴다는 점에서 의미가 있습니다. 이와 같이 선형 함수를 사용한 층을 활성화 함수를 사용하는 은닉층과 구분하기 위해서 이 책에서는 선형층(linear layer)이나 투사층(projection layer) 등의 다른 표현을 사용하여 표현합니다. 뒤의 챕터에서 언급할 임베딩 층(embedding layer)도 일종의 선형층입니다. 임베딩 층에는 활성화 함수가 존재하지 않습니다. 활성화 함수를 사용하는 일반적인 은닉층을 선형층과 대비되는 표현을 사용하면 비선형층(nonlinear layer)입니다.\\n파이썬을 통해 주로 사용되는 활성화 함수를 직접 그려봅시다.\\nimport numpy as np\\nimport matplotlib.pyplot as plt', \"파이썬을 통해 주로 사용되는 활성화 함수를 직접 그려봅시다.\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n(2) 계단 함수(Step function)\\n[이미지: ]\\ndef step(x):\\nreturn np.array(x > 0, dtype=np.int)\\nx = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성\\ny = step(x)\\nplt.title('Step Function')\\nplt.plot(x,y)\\nplt.show()\\n계단 함수는 거의 사용되지 않지만 퍼셉트론을 통해 인공 신경망을 처음 배울 때 접하게 되는 활성화 함수입니다.\\n(3) 시그모이드 함수(Sigmoid function)와 기울기 소실\\n시그모이드 함수를 사용한 인공 신경망이 있다고 가정해보겠습니다.\\n[이미지: ]\", '(3) 시그모이드 함수(Sigmoid function)와 기울기 소실\\n시그모이드 함수를 사용한 인공 신경망이 있다고 가정해보겠습니다.\\n[이미지: ]\\n위 인공 신경망의 학습 과정은 다음과 같습니다. 우선 인공 신경망은 입력에 대해서 순전파(forward propagation) 연산을 하고, 그리고 순전파 연산을 통해 나온 예측값과 실제값의 오차를 손실 함수(loss function)을 통해 계산하고, 그리고 이 손실(오차라고도 부릅니다. loss)을 미분을 통해서 기울기(gradient)를 구하고, 이를 통해 출력층에서 입력층 방향으로 가중치와 편향을 업데이트 하는 과정인 역전파(back propagation)를 수행합니다. 역전파에 대해서는 뒤에서 더 자세히 설명하겠지만 일단 여기에서는 인공 신경망에서 출력층에서 입력층 방향으로 가중치와 편향을 업데이트 하는 과정이라고만 언급해두겠습니다. 역전파 과정에서 인공 신경망은 경사 하강법을 사용합니다.', \"이 시그모이드 함수의 문제점은 미분을 해서 기울기(gradient)를 구할 때 발생합니다.\\n# 시그모이드 함수 그래프를 그리는 코드\\ndef sigmoid(x):\\nreturn 1/(1+np.exp(-x))\\nx = np.arange(-5.0, 5.0, 0.1)\\ny = sigmoid(x)\\nplt.plot(x, y)\\nplt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\\nplt.title('Sigmoid Function')\\nplt.show()\\n[이미지: ]\\n위 그래프는 시그모이드 함수의 그래프를 보여줍니다. 시그모이드 함수의 출력값이 0 또는 1에 가까워지면, 그래프의 기울기가 완만해지는 모습을 볼 수 있습니다. 기울기가 완만해지는 구간을 주황색, 그렇지 않은 구간을 초록색으로 칠해보겠습니다.\\n[이미지: ]\", '[이미지: ]\\n주황색 구간에서는 미분값이 0에 가까운 아주 작은 값입니다. 초록색 구간에서의 미분값은 최대값이 0.25입니다. 다시 말해 시그모이드 함수를 미분한 값은 적어도 0.25 이하의 값입니다. 시그모이드 함수를 활성화 함수로하는 인공 신경망의 층을 쌓는다면, 가중치와 편향을 업데이트 하는 과정인 역전파 과정에서 0에 가까운 값이 누적해서 곱해지게 되면서, 앞단에는 기울기(미분값)가 잘 전달되지 않게 됩니다. 이러한 현상을 기울기 소실(Vanishing Gradient) 문제라고 합니다.\\n시그모이드 함수를 사용하는 은닉층의 개수가 다수가 될 경우에는 0에 가까운 기울기가 계속 곱해지면 앞단에서는 거의 기울기를 전파받을 수 없게 됩니다. 다시 말해 매개변수 $w$가 업데이트 되지 않아 학습이 되지를 않습니다.\\n[이미지: ]', \"[이미지: ]\\n위의 그림은 은닉층이 깊은 신경망에서 기울기 소실 문제로 인해 출력층과 가까운 은닉층에서는 기울기가 잘 전파되지만, 앞단으로 갈수록 기울기가 제대로 전파되지 않는 모습을 보여줍니다. 결론적으로 시그모이드 함수의 은닉층에서의 사용은 지양됩니다. 시그모이드 함수는 주로 이진 분류를 위해 출력층에서 사용합니다.\\n(4) 하이퍼볼릭탄젠트 함수(Hyperbolic tangent function)\\n하이퍼볼릭탄젠트 함수(tanh)는 입력값을 -1과 1사이의 값으로 변환합니다. 그래프를 그려보겠습니다.\\nx = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성\\ny = np.tanh(x)\\nplt.plot(x, y)\\nplt.plot([0,0],[1.0,-1.0], ':')\\nplt.axhline(y=0, color='orange', linestyle='--')\\nplt.title('Tanh Function')\\nplt.show()\\n[이미지: ]\", \"plt.axhline(y=0, color='orange', linestyle='--')\\nplt.title('Tanh Function')\\nplt.show()\\n[이미지: ]\\n하이퍼볼릭탄젠트 함수도 -1과 1에 가까운 출력값을 출력할 때, 시그모이드 함수와 같은 문제가 발생합니다. 그러나 하이퍼볼릭탄젠트 함수의 경우에는 시그모이드 함수와는 달리 0을 중심으로 하고있으며 하이퍼볼릭탄젠트 함수를 미분했을 때의 최대값은 1로 시그모이드 함수의 최대값인 0.25보다는 큽니다. 다시 말해 미분했을 때 시그모이드 함수보다는 전반적으로 큰 값이 나오게 됩니다. 그래서 시그모이드 함수보다는 기울기 소실 증상이 적은 편이며 은닉층에서 시그모이드 함수보다는 선호됩니다.\\n(5) 렐루 함수(ReLU)\\n인공 신경망의 은닉층에서 가장 인기있는 함수입니다. 수식은 $f(x) = max(0, x)$로 아주 간단합니다.\\ndef relu(x):\\nreturn np.maximum(0, x)\", \"인공 신경망의 은닉층에서 가장 인기있는 함수입니다. 수식은 $f(x) = max(0, x)$로 아주 간단합니다.\\ndef relu(x):\\nreturn np.maximum(0, x)\\nx = np.arange(-5.0, 5.0, 0.1)\\ny = relu(x)\\nplt.plot(x, y)\\nplt.plot([0,0],[5.0,0.0], ':')\\nplt.title('Relu Function')\\nplt.show()\\n[이미지: ]\\n렐루 함수는 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환하는 것이 특징인 함수로 출력값이 특정 양수값에 수렴하지 않습니다. 0이상의 입력값의 경우에는 미분값이 항상 1입니다. 깊은 신경망의 은닉층에서 시그모이드 함수보다 훨씬 더 잘 작동합니다. 뿐만 아니라, 렐루 함수는 시그모이드 함수와 하이퍼볼릭탄젠트 함수와 같이 어떤 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠릅니다.\", \"하지만 여전히 문제점이 존재하는데, 입력값이 음수면 기울기. 즉, 미분값도 0이 됩니다. 그리고 이 뉴런은 다시 회생하는 것이 매우 어렵습니다. 이 문제를 죽은 렐루(dying ReLU)라고 합니다.\\n(6) 리키 렐루(Leaky ReLU)\\n죽은 렐루를 보완하기 위해 ReLU의 변형 함수들이 등장하기 시작했습니다. 변형 함수는 여러 개가 있지만 여기서는 Leaky ReLU에 대해서만 소개합니다. Leaky ReLU는 입력값이 음수일 경우에 0이 아니라 0.001과 같은 매우 작은 수를 반환하도록 되어있습니다. 수식은 $f(x) = max(ax, x)$로 아주 간단합니다. a는 하이퍼파라미터로 Leaky('새는') 정도를 결정하며 일반적으로는 0.01의 값을 가집니다. 여기서 말하는 '새는 정도'라는 것은 입력값의 음수일 때의 기울기를 비유하고 있습니다.\\na = 0.1\\ndef leaky_relu(x):\\nreturn np.maximum(a*x, x)\", \"a = 0.1\\ndef leaky_relu(x):\\nreturn np.maximum(a*x, x)\\nx = np.arange(-5.0, 5.0, 0.1)\\ny = leaky_relu(x)\\nplt.plot(x, y)\\nplt.plot([0,0],[5.0,0.0], ':')\\nplt.title('Leaky ReLU Function')\\nplt.show()\\n[이미지: ]\\n위의 그래프에서는 새는 모습을 확실히 보여주기 위해 a를 0.1로 잡았습니다. 위와 같이 입력값이 음수라도 기울기가 0이 되지 않으면 ReLU는 죽지 않습니다.\\n(7) 소프트맥스 함수(Softmax function)\", '(7) 소프트맥스 함수(Softmax function)\\n은닉층에서는 ReLU(또는 ReLU 변형) 함수들을 사용하는 것이 일반적입니다. 반면, 소프트맥스 함수는 시그모이드 함수처럼 출력층에서 주로 사용됩니다. 시그모이드 함수가 두 가지 선택지 중 하나를 고르는 이진 분류 (Binary Classification) 문제에 사용된다면 소프트맥스 함수는 세 가지 이상의 (상호 배타적인) 선택지 중 하나를 고르는 다중 클래스 분류(MultiClass Classification) 문제에 주로 사용됩니다. 다시 말해서 딥 러닝으로 이진 분류를 할 때는 출력층에 앞서 배운 로지스틱 회귀를 사용하고, 딥 러닝으로 다중 클래스 분류 문제를 풀 때는 출력층에 소프트맥스 회귀를 사용한다고 생각할 수 있습니다.\\nx = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성\\ny = np.exp(x) / np.sum(np.exp(x))\\nplt.plot(x, y)', \"y = np.exp(x) / np.sum(np.exp(x))\\nplt.plot(x, y)\\nplt.title('Softmax Function')\\nplt.show()\\n[이미지: ]\\n==================================================\\n--- 07-03 행렬곱으로 이해하는 신경망 ---\\n```\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nmodel = Sequential()\\n# 4개의 입력과 8개의 출력\\nmodel.add(Dense(8, input_dim=4, activation='relu'))\\n# 이어서 8개의 출력\\nmodel.add(Dense(8, activation='relu'))\\n# 이어서 3개의 출력\\nmodel.add(Dense(3, activation='softmax'))\", \"model.add(Dense(8, activation='relu'))\\n# 이어서 3개의 출력\\nmodel.add(Dense(3, activation='softmax'))\\n```인공 신경망에서 입력층에서 출력층 방향으로 연산을 진행하는 과정을 순전파(Forward Propagation)라고 합니다. 다르게 말하면 주어진 입력이 입력층으로 들어가서 은닉층을 지나 출력층에서 예측값을 얻는 과정을 순전파라고 합니다. 여기서는 신경망의 순전파는 결과적으로 행렬의 곱셈으로 이해할 수 있다는 것과 다층 퍼셉트론 내의 학습 가능한 매개변수인 가중치 $w$와 편향 $b$의 개수를 추정하는 방법에 대해서 학습합니다.\"]\n",
      "['[이미지: ]\\n활성화 함수, 은닉층의 수, 각 은닉층의 뉴런 수 등 딥 러닝 모델을 설계하고나면 입력값은 입력층, 은닉층을 지나면서 각 층에서의 가중치와 함께 연산되며 출력층으로 향합니다. 그리고 출력층에서 모든 연산을 마친 예측값이 나오게 됩니다. 이와 같이 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정을 순전파라고 합니다.']\n",
      "[\"[이미지: ]\\n위와 같은 인공 신경망이 있다고 해봅시다. 입력의 차원이 3, 출력의 차원이 2인 위 인공 신경망을 구현해본다면 다음과 같습니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nmodel = Sequential()\\n# 3개의 입력과 2개의 출력\\nmodel.add(Dense(2, input_dim=3, activation='softmax'))\\n소프트맥스 회귀를 한다고 가정하고 활성화 함수는 소프트맥스 함수를 임의로 기재하였습니다. 인공 신경망이란 표현이 아직 어색한다면 앞에서 배운 소프트맥스 회귀 모델을 만들었다고 생각해도 되겠습니다. 소프트맥스 회귀는 출력 벡터의 차원을 2로 두면 이진 분류를 수행하는 모델이 됩니다. 로지스틱 회귀가 아닌 소프트맥스 회귀로도 이진 분류는 수행 가능함을 기억해둡시다.\", '케라스에서는 .summary()를 사용하면 해당 모델에 존재하는 모든 매개변수(가중치 $w$와 편향 $b$의 개수)를 확인할 수 있습니다.\\nmodel.summary()\\nModel: \"sequential\"\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #\\n=================================================================\\ndense (Dense)                (None, 2)                 8\\n=================================================================\\nTotal params: 8\\nTrainable params: 8\\nNon-trainable params: 0', 'Total params: 8\\nTrainable params: 8\\nNon-trainable params: 0\\n_________________________________________________________________\\n매개변수의 수가 8개라고 나옵니다. 위 신경망에서 학습가능한 매개변수 인 $w$와 $b$의 개수가 총 합해서 8개라는 의미입니다. 실제로 그런지 위 신경망을 행렬의 곱셈 관점에서 이해해봅시다.\\n[이미지: ]\\n위 모델은 입력의 차원이 3, 출력의 차원이 2입니다. 또는 신경망의 용어로서 표현한다면, 입력층의 뉴런이 3개, 출력층의 뉴런이 2개라고 말할 수 있습니다. 위 신경망 그림에서 화살표 각각은 가중치 $w$를 의미하고 있습니다. 3개의 뉴런과 2개의 뉴런 사이에는 총 6개의 화살표가 존재하는데, 이는 위 신경망에서 가중치 $w$의 개수가 6개임을 의미합니다.', '이를 행렬곱 관점에서는 3차원 벡터에서 2차원 벡터가 되기 위해서 3 × 2 행렬을 곱했다고 이해할 수 있습니다. 그리고 이 행렬 각각의 원소가 각각의 $w$가 되는 것입니다. 위 그림에서는 $y_{1}$에 연결되는 화살표 $w_{1}$, $w_{2}$, $w_{3}$를 주황색으로 표현하고, $y_{2}$에 연결되는 화살표 $w_{4}$, $w_{5}$, $w_{6}$를 초록색으로 표현했습니다.', '일반적으로 동그란 뉴런과 화살표로 표현하는 인공 신경망의 그림에서는 편향 $b$의 경우에는 편의상 생략되는 경우가 많지만, 인공 신경망 내부적으로는 편향 $b$의 연산 또한 존재합니다. 위 그림에서 뉴런과 화살표로 표현한 인공 신경망의 그림에서는 편향을 표현하지 않았지만, 행렬 연산식에서는 $b_{1}$과 $b_{2}$를 표현하였습니다. 편향 $b$의 개수는 항상 출력의 차원을 기준으로 개수를 확인하면 됩니다. 위의 인공 신경망의 경우에는 출력의 차원이 2인데, 이에 따라서 편향 또한 $b_{1}$과 $b_{2}$로 두 개입니다.\\n가중치 $w$의 개수가 $w_{1}$, $w_{2}$, $w_{3}$, $w_{4}$, $w_{5}$, $w_{6}$로 총 6개이며 편향 $b$의 개수가 $b_{1}$과 $b_{2}$로 두 개이므로 총 학습가능한 매개변수의 수는 8개입니다. 이는 앞서 model.summary()를 하였을 때 확인한 매개변수의 수인 8개와 일치합니다.', '$y_{1}$과 $y_{2}$를 구하는 과정을 수식으로 표현한다면 다음과 같이 표현할 수 있습니다.\\n$$h_{1} = x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} + b_{1}$$\\n$$h_{2} = x_{1}w_{4} + x_{2}w_{5} + x_{3}w_{6} + b_{2}$$\\n$$[y_{1}, y_{2}] = softmax([h_{1}, h_{2}])$$\\n좀 더 간단하게 식을 표현해보겠습니다. 입력 $x_{1}$, $x_{2}$, $x_{3}$을 벡터 $X$로 명명합니다.\\n$$X=[x_{1}, x_{2}, x_{3}]$$', '좀 더 간단하게 식을 표현해보겠습니다. 입력 $x_{1}$, $x_{2}$, $x_{3}$을 벡터 $X$로 명명합니다.\\n$$X=[x_{1}, x_{2}, x_{3}]$$\\n그리고 $w_{1}$, $w_{2}$, $w_{3}$, $w_{4}$, $w_{5}$, $w_{6}$를 원소로 하는 3 × 2 행렬을 가중치 행렬 $W$, 그리고 편향 $b_{1}$ $b_{2}$를 원소로 하는 벡터를 $B$, 그리고 $y_{1}$, $y_{2}$를 원소로하는 출력 벡터를 $Y$로 명명합시다. 이 경우, 위의 인공 신경망은 다음과 같이 표현할 수 있습니다.\\n[이미지: ]\\n다시 말해 수식은 다음과 같습니다.\\n$Y = XW + B$']\n",
      "[\"인공 신경망을 행렬곱으로 구현할 때의 흥미로운 점은 행렬곱을 사용하면 병렬 연산도 가능하다는 점입니다. 위의 예시에서는 데이터 중 1개의 샘플만을 처리한다고 가정했습습니다. 이번에는 인공 신경망이 4개의 샘플을 동시에 처리해본다고 가정해봅시다. 4개의 샘플을 하나의 행렬 $X$로 정의하고 인공 신경망의 순전파를 행렬곱으로 표현하면 다음과 같습니다.\\n[이미지: ]\\n여기서 혼동하지 말아야 할 것은 인공 신경망의 4개의 샘플을 동시에 처리하고 있지만, 여기서 학습가능한 매개변수의 수는 여전히 8개라는 점입니다. 이렇게 인공 신경망이 다수의 샘플을 동시에 처리하는 것을 우리는 '배치 연산'이라고 부릅니다.\\n난이도를 올려서 중간에 층을 더 추가해봅시다.\"]\n",
      "[\"[이미지: ]\\n위와 같은 인공 신경망이 있다고 합시다. 주어진 인공 신경망을 케라스로 구현해본다면 아래와 같이 구현할 수 있습니다.\\n1) 코드로 구현하기\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nmodel = Sequential()\\n# 4개의 입력과 8개의 출력\\nmodel.add(Dense(8, input_dim=4, activation='relu'))\\n# 이어서 8개의 출력\\nmodel.add(Dense(8, activation='relu'))\\n# 이어서 3개의 출력\\nmodel.add(Dense(3, activation='softmax'))\", \"model.add(Dense(8, activation='relu'))\\n# 이어서 3개의 출력\\nmodel.add(Dense(3, activation='softmax'))\\n위의 코드의 주석에서 () 괄호 안의 값은 각 층에서의 뉴런의 수를 의미하며 입력층부터 출력층까지 순차적으로 인공 신경망의 층을 한 층씩 추가하였습니다. 케라스를 사용하면 이렇게 간단하게 층을 딥하게 쌓은 딥 러닝 모델을 구현할 수 있습니다.\\n2) 행렬의 크기 추정해보기\\n우선 각 층을 기준으로 입력과 출력의 개수를 정리하면 다음과 같습니다.\\n입력층 : 4개의 입력과 8개의 출력\\n은닉층1 : 8개의 입력과 8개의 출력\\n은닉층2 : 8개의 입력과 3개의 출력\\n출력층 : 3개의 입력과 3개의 출력\\n위의 정보를 가지고 층마다 생기는 가중치와 편향 행렬의 크기를 추정해봅시다. 단, 배치 크기는 1을 가정합니다.\"]\n",
      "['앞서 벡터와 행렬 연산을 설명하며 배운 바에 따르면, 입력 행렬, 가중치 행렬, 편향 행렬, 출력 행렬은 다음과 같은 크기 관계를 가집니다.\\n$$X_{m\\\\ \\\\text{×}\\\\ n} × W_{n\\\\ \\\\text{×}\\\\ j} + B_{m\\\\ \\\\text{×}\\\\ j} = Y_{m\\\\ \\\\text{×}\\\\ j}$$\\nlayer 1의 입력 행렬 $X$ 의 크기는 1 × 4입니다. layer 1의 출력은 8개이므로, 그에 따라 출력 행렬 $Y$의 크기는 1 × 8이 됩니다.\\n$$X_{1\\\\ \\\\text{×}\\\\ 4} × W_{n\\\\ \\\\text{×}\\\\ j} + B_{m\\\\ \\\\text{×}\\\\ j} = Y_{1\\\\ \\\\text{×}\\\\ 8}$$\\n그런데 가중치 행렬 $W$의 행은 입력 행렬 $X$의 열과 같아야 하므로 아래와 같습니다.\\n$$X_{1\\\\ \\\\text{×}\\\\ 4} × W_{4\\\\ \\\\text{×}\\\\ j} + B_{m\\\\ \\\\text{×}\\\\ j} = Y_{1\\\\ \\\\text{×}\\\\ 8}$$', '$$X_{1\\\\ \\\\text{×}\\\\ 4} × W_{4\\\\ \\\\text{×}\\\\ j} + B_{m\\\\ \\\\text{×}\\\\ j} = Y_{1\\\\ \\\\text{×}\\\\ 8}$$\\n편향 행렬 $B$는 출력 행렬 $Y$의 크기에 영향을 주지 않으므로 편향 행렬 $B$의 크기는 출력 행렬 $Y$의 크기와 같습니다.\\n$$X_{1\\\\ \\\\text{×}\\\\ 4} × W_{4\\\\ \\\\text{×}\\\\ j} + B_{1\\\\ \\\\text{×}\\\\ 8} = Y_{1\\\\ \\\\text{×}\\\\ 8}$$\\n가중치 행렬 $W$의 열은 출력 행렬 $Y$의 열과 동일해야 합니다.\\n$$X_{1\\\\ \\\\text{×}\\\\ 4} × W_{4\\\\ \\\\text{×}\\\\ 8} + B_{1\\\\ \\\\text{×}\\\\ 8} = Y_{1\\\\ \\\\text{×}\\\\ 8}$$\\n입력층과 은닉층1 사이의 가중치 행렬과 편향 행렬의 크기를 구했습니다.']\n",
      "['이제 입력층 ⇒ 은닉층1에서의 출력 행렬 $Y$는 은닉층2에서는 입력 행렬 $X$로 다시 명명해봅시다.\\n은닉층1 ⇒ 은닉층2 : $X_{1\\\\ \\\\text{×}\\\\ 8} × W_{8\\\\ \\\\text{×}\\\\ 8} + B_{1\\\\ \\\\text{×}\\\\ 8} = Y_{1\\\\ \\\\text{×}\\\\ 8}$']\n",
      "['이제 은닉층2 ⇒ 은닉층3에서의 출력 행렬 $Y$는 은닉층3에서는 입력 행렬 $X$로 다시 명명해봅시다.\\n은닉층2 ⇒ 은닉층3 : $X_{1\\\\ \\\\text{×}\\\\ 8} × W_{8\\\\ \\\\text{×}\\\\ 3} + B_{1\\\\ \\\\text{×}\\\\ 3} = Y_{1\\\\ \\\\text{×}\\\\ 3}$\\n은닉층과 출력층에 활성화 함수 relu와 softmax가 존재하지만 활성화 함수는 행렬의 크기에 영향을 주지 않습니다.', '은닉층과 출력층에 활성화 함수 relu와 softmax가 존재하지만 활성화 함수는 행렬의 크기에 영향을 주지 않습니다.\\n인공 신경망이 입력층에서 은닉층을 지나 출력층에서 예측값을 계산하기까지의 과정을 행렬 연산으로 가정하고 행렬의 크기를 추정해보았습니다. 이와 같이 순전파를 진행하고 예측값을 구하고나서 이 다음에 인공 신경망이 해야할 일은 예측값과 실제값으로부터 오차를 계산하고, 오차로부터 가중치와 편향을 업데이트하는 일입니다. 즉, 인공 신경망의 학습 단계에 해당됩니다. 이때 인공 신경망은 순전파와는 반대 방향으로 연산을 진행하며 가중치를 업데이트하는데, 이 과정을 역전파(BackPropagation)라고 합니다. 인공 신경망의 역전파에 대해서는 뒤에서 다룹니다.\\n==================================================\\n--- 07-04 딥 러닝의 학습 방법 ---\\n```', \"==================================================\\n--- 07-04 딥 러닝의 학습 방법 ---\\n```\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\\n```딥 러닝의 학습 방법의 이해를 위해 필요한 개념인 손실 함수, 옵티마이저, 에포크의 개념에 대해서 정리합니다.\"]\n",
      "[\"[이미지: ]\\n손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수입니다. 이 두 값의 차이. 즉, 오차가 클 수록 손실 함수의 값은 크고 오차가 작을 수록 손실 함수의 값은 작아집니다. 회귀에서는 평균 제곱 오차, 분류 문제에서는 크로스 엔트로피를 주로 손실 함수로 사용합니다. 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 $w$와 편향 $b$의 값을 찾는 것이 딥 러닝의 학습 과정이므로 손실 함수의 선정은 매우 중요합니다. 앞서 설명했던 손실 함수를 정리해봅시다.\\n1) MSE(Mean Squared Error, MSE)\\n평균 제곱 오차는 선형 회귀를 학습할 때 배웠던 손실 함수입니다. 연속형 변수를 예측할 때 사용됩니다.\\n다음과 같이 compile의 loss에 문자열 'mse'라고 기재하여 사용할 수 있습니다.\\nmodel.compile(optimizer='adam', loss='mse', metrics=['mse'])\", \"model.compile(optimizer='adam', loss='mse', metrics=['mse'])\\ncompile의 loss는 tf.keras.losses.Loss 인스턴스를 호출하므로 위 코드는 아래와 같이 사용할 수도 있습니다.\\nmodel.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])\\n딥 러닝 자연어 처리는 대부분 분류 문제이므로 평균 제곱 오차보다는 아래의 크로스 엔트로피 함수들을 주로 사용합니다.\\n2) 이진 크로스 엔트로피(Binary Cross-Entropy)\", \"2) 이진 크로스 엔트로피(Binary Cross-Entropy)\\n이항 교차 엔트로피라고도 부르는 손실 함수입니다. 출력층에서 시그모이드 함수를 사용하는 이진 분류 (Binary Classification)의 경우 binary_crossentropy를 사용합니다. compile의 loss에 문자열로 'binary_crossentropy'를 기재해주면 됩니다. 이는 로지스틱 회귀에서 사용했던 손실 함수입니다.\\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\\ncompile의 loss는 tf.keras.losses.Loss 인스턴스를 호출하므로 위 코드는 아래와 같이 사용할 수도 있습니다.\\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['acc'])\", \"model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['acc'])\\n3) 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy)\\n범주형 교차 엔트로피라고도 부르는 손실 함수입니다. 출력층에서 소프트맥스 함수를 사용하는 다중 클래스 분류(Multi-Class Classification)일 경우 categorical_crossentropy를 사용합니다. compile의 loss에 문자열로 'categorical_crossentropy'를 기재해주면 됩니다. 소프트맥스 회귀에서 사용했던 손실 함수입니다.\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\", \"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\\ncompile의 loss는 tf.keras.losses.Loss 인스턴스를 호출하므로 위 코드는 아래와 같이 사용할 수도 있습니다.\\nmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer='adam', metrics=['acc'])\\n만약 레이블에 대해서 원-핫 인코딩 과정을 생략하고, 정수값을 가진 레이블에 대해서 다중 클래스 분류를 수행하고 싶다면 다음과 같이 'sparse_categorical_crossentropy'를 사용합니다.\\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\\n위 코드는 아래와 같이 사용할 수도 있습니다.\", \"위 코드는 아래와 같이 사용할 수도 있습니다.\\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['acc'])\\n4) 그 외에 다양한 손실 함수들\\n아래의 텐서플로우 공식 문서 링크에서 방금 언급하지 않은 손실 함수 외에도 다양한 손실 함수들을 확인할 수 있습니다.\\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses\\n지금까지 자주 사용하는 손실 함수 몇 가지에 대해서 정리해봤습니다. 위 compile 코드에서 optimizer='adam' 이라는 부분에 주목해봅시다. 이는 아담이라는 옵티마이저를 사용했다라는 의미입니다. 손실 함수의 선정만큼이나 옵티마이저의 선정 또한 중요합니다. 이어서 옵티마이저에 대해서 정리해봅시다.\"]\n",
      "['[이미지: ]\\n손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라집니다. 여기서 배치(Batch)라는 개념에 대한 이해가 필요합니다. 배치는 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양을 말합니다. 전체 데이터를 가지고 매개 변수의 값을 조정할 수도 있고, 정해준 양의 데이터만 가지고도 매개 변수의 값을 조정할 수 있습니다.\\n1) 배치 경사 하강법(Batch Gradient Descent)', '1) 배치 경사 하강법(Batch Gradient Descent)\\n배치 경사 하강법(Batch Gradient Descent)은 가장 기본적인 경사 하강법입니다. 배치 경사 하강법은 옵티마이저 중 하나로 오차(loss)를 구할 때 전체 데이터를 고려합니다. 딥 러닝에서는 전체 데이터에 대한  한 번의 훈련 횟수를 1 에포크라고 하는데, 배치 경사 하강법은 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행합니다. 배치 경사 하강법은 전체 데이터를 고려해서 학습하므로 한 번의 매개 변수 업데이트에 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있습니다.\\nmodel.fit(X_train, y_train, batch_size=len(X_train))\\n2) 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD)', '2) 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\\n기존의 배치 경사 하강법은 전체 데이터에 대해서 계산을 하다보니 시간이 너무 오래걸린다는 단점이 있습니다. 배치 크기가 1인 확률적 경사 하강법은 매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법입니다. 더 적은 데이터를 사용하므로 더 빠르게 계산할 수 있습니다.\\n[이미지: ]\\n위 그림에서 좌측은 배치 경사 하강법, 우측은 배치 크기가 1인 확률적 경사 하강법이 최적해를 찾아가는 모습을 보여주고 있습니다. 확률적 경사 하강법은 매개변수의 변경폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮을 수도 있지만 하나의 데이터에 대해서만 메모리에 저장하면 되므로 자원이 적은 컴퓨터에서도 쉽게 사용가능 하다는 장점이 있습니다. 케라스에서는 아래와 같이 사용합니다.', 'model.fit(X_train, y_train, batch_size=1)\\n3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent)\\n전체 데이터도, 1개의 데이터도 아닐 때, 배치 크기를 지정하여 해당 데이터 개수만큼에 대해서 계산하여 매개 변수의 값을 조정하는 경사 하강법을 미니 배치 경사 하강법이라고 합니다. 미니 배치 경사 하강법은 전체 데이터를 계산하는 것보다 빠르며, SGD보다 안정적이라는 장점이 있습니다. 가장 많이 사용되는 경사 하강법으로 앞으로 이 책에서도 주로 배치 크기를 지정하여 미니 배치 경사 하강법으로 학습하게 될 것입니다. 아래의 코드는 배치 크기를 128로 지정했을 경우를 보여줍니다.\\nmodel.fit(X_train, y_train, batch_size=128)', 'model.fit(X_train, y_train, batch_size=128)\\n배치 크기는 일반적으로 2의 n제곱에 해당하는 숫자로 선택하는 것이 보편적입니다. 만약, model.fit()에서 배치 크기를 별도로 지정해주지 않을 경우에 기본값은 2의 5제곱에 해당하는 숫자인 32로 설정됩니다. 지금까지 배치 크기에 따른 학습 방법의 차이를 알아봤습니다.  앞으로는 경사 하강법의 알고리즘 자체를 조금씩 달리한 다양한 옵티마이저에 대해서 설명합니다.']\n",
      "['1) 모멘텀(Momentum)\\n모멘텀(Momentum)은 관성이라는 물리학의 법칙을 응용한 방법입니다. 모멘텀 경사 하강법에 관성을 더 해줍니다. 모멘텀은 경사 하강법에서 계산된 접선의 기울기에 한 시점 전의 접선의 기울기값을 일정한 비율만큼 반영합니다. 이렇게 하면 마치 언덕에서 공이 내려올 때, 중간에 작은 웅덩이에 빠지더라도 관성의 힘으로 넘어서는 효과를 줄 수 있습니다.\\n[이미지: ]\\n전체 함수에 걸쳐 최소값을 글로벌 미니멈(Global Minimum) 이라고 하고, 글로벌 미니멈이 아닌 특정 구역에서의 최소값인 로컬 미니멈(Local Minimum) 이라고 합니다. 로컬 미니멈에 도달하였을 때 글로벌 미니멈으로 잘못 인식하여 탈출하지 못하였을 상황에서 모멘텀. 즉, 관성의 힘을 빌리면 값이 조절되면서 현재의 로컬 미니멈에서 탈출하고 글로벌 미니멈 내지는 더 낮은 로컬 미니멈으로 갈 수 있는 효과를 얻을 수도 있습니다.', 'tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)\\n2) 아다그라드(Adagrad)\\n매개변수들은 각자 의미하는 바가 다른데, 모든 매개변수에 동일한 학습률(learning rate)을 적용하는 것은 비효율적입니다. 아다그라드는 각 매개변수에 서로 다른 학습률을 적용시킵니다. 이때 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정시킵니다.\\ntf.keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)\\n3) 알엠에스프롭(RMSprop)\\n아다그라드는 학습을 계속 진행한 경우에는, 나중에 가서는 학습률이 지나치게 떨어진다는 단점이 있는데 이를 다른 수식으로 대체하여 이러한 단점을 개선하였습니다.\\ntf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\\n4) 아담(Adam)', 'tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\\n4) 아담(Adam)\\n아담은 알엠에스프롭과 모멘텀 두 가지를 합친 듯한 방법으로, 방향과 학습률 두 가지를 모두 잡기 위한 방법입니다.\\ntf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\\n5) 사용 방법\\n각 옵티마이저 인스턴스는 compile의 optimizer에서 호출합니다. 예를 들어 아담(adam)은 다음과 같이 코드를 작성합니다.\\nadam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)', \"model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\\n하지만 다음과 같이 단순히 문자열로 'adam'으로 작성하더라도 동작합니다.\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\\n다른 옵티마이저들도 마찬가지입니다. optimizer='sgd', optimizer='rmsprop'와 같이 각 옵티마이저를 문자열로 호출할 수 있습니다. 케라스의 옵티마이저 사용법은 아래의 링크에서 좀 더 상세히 확인할 수 있습니다.\\n링크 : https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\"]\n",
      "['이 부분은 05) 역전파 챕터로 별도 작성되었습니다.']\n",
      "['기계는 실제값과 예측값의 오차로부터 옵티마이저를 통해서 가중치를 업데이트합니다. 머신 러닝에서는 이 과정을 학습이라고 합니다. 이를 현실의 학습에 비유하면 사람은 문제지의 문제를 풀고, 정답지의 정답을 보면서 채점을 하면서 부족했던 점을 깨달으며 머릿속의 지식이 업데이트되는 과정입니다.\\n그런데 사람마다 동일한 문제지와 정답지를 주더라도 공부 방법은 사실 천차만별입니다. 어떤 사람은 문제지 하나를 다 풀고 나서 정답을 채점하는데 어떤 사람은 문제지의 문제를 10개 단위로 끊어서 공부합니다. 문제 10개를 풀고 채점하고 다시 다음 문제 10개를 풀고 채점하고 반복하는 방식으로 학습하는 방식입니다. 또한 게으른 사람은 문제지를 세 번 공부하는데, 성실한 사람은 문제지의 문제를 달달 외울만큼 문제지를 100번 공부합니다. 기계도 똑같습니다. 같은 문제지와 정답지를 주더라도 공부 방법을 다르게 설정할 수 있습니다.\\n[이미지: ]\\n위 그림은 에포크와 배치 크기와 이터레이션의 차이를 보여줍니다.', '[이미지: ]\\n위 그림은 에포크와 배치 크기와 이터레이션의 차이를 보여줍니다.\\n1) 에포크(Epoch)\\n에포크란 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태를 말합니다. 전체 데이터를 하나의 문제지에 비유한다면 문제지의 모든 문제를 끝까지 다 풀고, 정답지로 채점을 하여 문제지에 대한 공부를 한 번 끝낸 상태를 말합니다.\\n만약 에포크가 50이라고 하면, 전체 데이터 단위로는 총 50번 학습합니다. 문제지에 비유하면 문제지를 50번 푼 셈입니다. 이 에포크 횟수가 지나치거나 너무 적으면 앞서 배운 과적합과 과소적합이 발생할 수 있습니다.\\n2) 배치 크기(Batch size)', '2) 배치 크기(Batch size)\\n배치 크기는 몇 개의 데이터 단위로 매개변수를 업데이트 하는지를 말합니다. 현실에 비유하면 문제지에서 몇 개씩 문제를 풀고나서 정답지를 확인하느냐의 문제입니다. 사람은 문제를 풀고 정답을 보는 순간 부족했던 점을 깨달으며 지식이 업데이트 된다고 하였습니다. 기계 입장에서는 실제값과 예측값으로부터 오차를 계산하고 옵티마이저가 매개변수를 업데이트합니다. 여기서 중요한 포인트는 업데이트가 시작되는 시점이 정답지/실제값을 확인하는 시점이라는 겁니다.\\n사람이 2,000 문제가 수록되어있는 문제지의 문제를 200개 단위로 풀고 채점한다고 하면 이때 배치 크기는 200입니다. 기계는 배치 크기가 200이면 200개의 샘플 단위로 가중치를 업데이트 합니다.', '여기서 주의할 점은 배치 크기와 배치의 수는 다른 개념이라는 점입니다. 전체 데이터가 2,000일때 배치 크기를 200으로 준다면 배치의 수는 10입니다. 이는 에포크에서 배치 크기를 나눠준 값(2,000/200 = 10)이기도 합니다. 이때 배치의 수를 이터레이션이라고 합니다.\\n3) 이터레이션(Iteration) 또는 스텝(Step)', '3) 이터레이션(Iteration) 또는 스텝(Step)\\n이터레이션이란 한 번의 에포크를 끝내기 위해서 필요한 배치의 수를 말합니다. 또는 한 번의 에포크 내에서 이루어지는 매개변수의 업데이트 횟수이기도 합니다. 전체 데이터가 2,000일 때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10입니다. 이는 한 번의 에포크 당 매개변수 업데이트가 10번 이루어진다는 것을 의미합니다. 배치 크기가 1인 확률적 경사 하강법을 이 개념을 가지고 다시 설명하면 배치 크기가 1이므로 모든 이터레이션마다 하나의 데이터를 선택하여 경사 하강법을 수행합니다. 이터레이션은 스텝(Step)이라고 부르기도 하므로 두 용어 모두 기억해둡시다.\\n==================================================\\n--- 07-05 역전파(BackPropagation) 이해하기 ---', '==================================================\\n--- 07-05 역전파(BackPropagation) 이해하기 ---\\n인공 신경망이 순전파 과정을 진행하여 예측값과 실제값의 오차를 계산하였을 때 어떻게 역전파 과정에서 경사 하강법을 사용하여 가중치를 업데이트하는지 직접 계산을 통해 이해해봅시다.']\n",
      "['우선 예제를 위해 사용될 인공 신경망을 소개합니다. 역전파의 이해를 위해서 여기서 사용할 인공 신경망은 입력층, 은닉층, 출력층 이렇게 3개의 층을 가집니다. 또한 해당 인공 신경망은 두 개의 입력과, 두 개의 은닉층 뉴런, 두 개의 출력층 뉴런을 사용합니다. 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수를 사용합니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림은 여기서 사용할 인공 신경망의 모습을 보여줍니다. 은닉층과 출력층의 모든 뉴런에서 변수 $z$가 존재하는데 여기서 변수 $z$는 이전층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합을 의미합니다. 이 값은 뉴런에서 아직 시그모이드 함수를 거치지 않은 상태입니다. 즉, 활성화 함수의 입력을 의미합니다. $z$ 우측의 |를 지나서 존재하는 변수 $h$ 또는 $o$는 $z$가 시그모이드 함수를 지난 후의 값으로 각 뉴런의 출력값을 의미합니다. 이번 역전파 예제에서는 인공 신경망에 존재하는 모든 가중치 $w$에 대해서 역전파를 통해 업데이트하는 것을 목표로합니다. 해당 인공 신경망은 편향 $b$는 고려하지 않습니다.']\n",
      "['[이미지: ]\\n주어진 값이 위의 그림과 같을 때 순전파를 진행해봅시다. 위의 그림에서 소수점 앞의 0은 생략하였습니다. 예를 들어 .25는 0.25를 의미합니다. 파란색 숫자는 입력값을 의미하며, 빨간색 숫자는 각 가중치의 값을 의미합니다. 앞으로 진행하는 계산의 결과값은 소수점 아래 여덟번째 자리까지 반올림하여 표기합니다.\\n각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 됩니다. $z_{1}$과 $z_{2}$는 시그모이드 함수의 입력으로 사용되는 각각의 값에 해당됩니다.\\n$$z_{1}=w_{1}x_{1} + w_{2}x_{2}=0.3 \\\\text{×} 0.1 + 0.25 \\\\text{×} 0.2= 0.08$$\\n$$z_{2}=w_{3}x_{1} + w_{4}x_{2}=0.4 \\\\text{×} 0.1 + 0.35 \\\\text{×} 0.2= 0.11$$', '$$z_{2}=w_{3}x_{1} + w_{4}x_{2}=0.4 \\\\text{×} 0.1 + 0.35 \\\\text{×} 0.2= 0.11$$\\n$z_{1}$과 $z_{2}$는 각각의 은닉층 뉴런에서 시그모이드 함수를 지나게 되는데 시그모이드 함수가 리턴하는 결과값은 은닉층 뉴런의 최종 출력값입니다. 식에서는 각각 $h_{1}$과 $h_{2}$에 해당되며, 아래의 결과와 같습니다.\\n$$h_{1}=sigmoid(z_{1}) = 0.51998934$$\\n$$h_{2}=sigmoid(z_{2}) = 0.52747230$$\\n$h_{1}$과 $h_{2}$ 이 두 값은 다시 출력층의 뉴런으로 향하게 되는데 이때 다시 각각의 값에 해당되는 가중치와 곱해지고, 다시 가중합 되어 출력층 뉴런의 시그모이드 함수의 입력값이 됩니다. 식에서는 각각 $z_{3}$과 $z_{4}$에 해당됩니다.', '$$z_{3}=w_{5}h_{1}+w_{6}h_{2} = 0.45 \\\\text{×} h_{1} + 0.4 \\\\text{×} h_{2} = 0.44498412$$\\n$$z_{4}=w_{7}h_{1}+w_{8}h_{2} = 0.7 \\\\text{×} h_{1} + 0.6 \\\\text{×} h_{2} = 0.68047592$$\\n$z_{3}$과 $z_{4}$이 출력층 뉴런에서 시그모이드 함수를 지난 값은 이 인공 신경망이 최종적으로 계산한 출력값입니다. 실제값을 예측하기 위한 값으로서 예측값이라고도 부릅니다.\\n$$o_{1}=sigmoid(z_{3})=0.60944600$$\\n$$o_{2}=sigmoid(z_{4})=0.66384491$$', '$$o_{1}=sigmoid(z_{3})=0.60944600$$\\n$$o_{2}=sigmoid(z_{4})=0.66384491$$\\n이제 해야할 일은 예측값과 실제값의 오차를 계산하기 위한 오차 함수를 선택하는 것입니다. 오차(Error)를 계산하기 위한 손실 함수(Loss function)로는 평균 제곱 오차 MSE를 사용합니다. 식에서는 실제값을 target이라고 표현하였으며, 순전파를 통해 나온 예측값을 output으로 표현하였습니다. 그리고 각 오차를 모두 더하면 전체 오차 $E_{total}$가 됩니다.\\n$$E_{o1}=\\\\frac{1}{2}(target_{o1}-output_{o1})^{2}=0.02193381$$\\n$$E_{o2}=\\\\frac{1}{2}(target_{o2}-output_{o2})^{2}=0.00203809$$\\n$$E_{total}=E_{o1}+E_{o2}=0.02397190$$']\n",
      "['순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트해갑니다. 출력층 바로 이전의 은닉층을 N층이라고 하였을 때, 출력층과 N층 사이의 가중치를 업데이트하는 단계를 역전파 1단계, 그리고 N층과 N층의 이전층 사이의 가중치를 업데이트 하는 단계를 역전파 2단계라고 해봅시다.\\n[이미지: ]\\n역전파 1단계에서 업데이트 해야 할 가중치는 $w_{5}, w_{6}, w_{7}, w_{8}$ 총 4개입니다. 원리 자체는 동일하므로 우선 $w_{5}$에 대해서 먼저 업데이트를 진행해보겠습니다. 경사 하강법을 수행하려면 가중치 $w_{5}$를 업데이트 하기 위해서 $\\\\frac{∂E_{total}}{∂w_{5}}$를 계산해야 합니다.\\n$\\\\frac{∂E_{total}}{∂w_{5}}$를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있습니다.', '$\\\\frac{∂E_{total}}{∂w_{5}}$를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있습니다.\\n$$\\\\frac{∂E_{total}}{∂w_{5}} = \\\\frac{∂E_{total}}{∂o_{1}} \\\\text{×} \\\\frac{∂o_{1}}{∂z_{3}} \\\\text{×} \\\\frac{∂z_{3}}{∂w_{5}}$$\\n위의 식에서 우변의 세 개의 각 항에 대해서 순서대로 계산해봅시다. 우선 첫번째 항에 대해서 계산해보겠습니다. 미분을 진행하기 전에 $E_{total}$의 값을 상기해봅시다. $E_{total}$은 앞서 순전파를 진행하고 계산했던 전체 오차값입니다. 식은 다음과 같습니다.\\n$$E_{total}=\\\\frac{1}{2}(target_{o1}-output_{o1})^{2} + \\\\frac{1}{2}(target_{o2}-output_{o2})^{2}$$\\n이에 $\\\\frac{∂E_{total}}{∂o_{1}}$는 다음과 같습니다.', '이에 $\\\\frac{∂E_{total}}{∂o_{1}}$는 다음과 같습니다.\\n$$\\\\frac{∂E_{total}}{∂o_{1}}=2 \\\\text{×} \\\\frac{1}{2}(target_{o1}-output_{o1})^{2-1} \\\\text{×} (-1) + 0$$\\n$$\\\\frac{∂E_{total}}{∂o_{1}}=-(target_{o1}-output_{o1})=-(0.4-0.60944600)=0.20944600$$\\n이제 두번째 항을 주목해봅시다. $o_{1}$이라는 값은 시그모이드 함수의 출력값입니다. 그런데 시그모이드 함수의 미분은 $f(x) \\\\text{×} (1-f(x))$입니다. 앞으로의 계산 과정에서도 계속해서 시그모이드 함수를 미분해야 하는 상황이 생기므로 기억해둡시다. 이에 따라서 두번째 항의 미분 결과는 다음과 같습니다.\\n(시그모이드 함수 미분 참고 링크 : https://en.wikipedia.org/wiki/Logistic_function#Derivative)', '(시그모이드 함수 미분 참고 링크 : https://en.wikipedia.org/wiki/Logistic_function#Derivative)\\n$$\\\\frac{∂o_{1}}{∂z_{3}}=o_{1}\\\\text{×}(1-o_{1})=0.60944600(1-0.60944600)=0.23802157$$\\n마지막으로 세번째 항은 $h_{1}$의 값과 동일합니다.\\n$$\\\\frac{∂z_{3}}{∂w_{5}}=h_{1}=0.51998934$$\\n우변의 모든 항을 계산하였습니다. 이제 이 값을 모두 곱해주면 됩니다.\\n$$\\\\frac{∂E_{total}}{∂w_{5}} = 0.20944600 \\\\text{×} 0.23802157 \\\\text{×} 0.51998934 = 0.02592286$$\\n이제 앞서 배웠던 경사 하강법을 통해 가중치를 업데이트 할 때가 왔습니다! 하이퍼파라미터에 해당되는 학습률(learning rate) $α$는 0.5라고 가정합니다.', '이제 앞서 배웠던 경사 하강법을 통해 가중치를 업데이트 할 때가 왔습니다! 하이퍼파라미터에 해당되는 학습률(learning rate) $α$는 0.5라고 가정합니다.\\n$$w_{5}^{+}=w_{5}-α\\\\frac{∂E_{total}}{∂w_{5}}=0.45- 0.5 \\\\text{×} 0.02592286=0.43703857$$\\n이와 같은 원리로 $w_{6}^{+},\\\\ w_{7}^{+},\\\\ w_{8}^{+}$을 계산할 수 있습니다.\\n$$\\\\frac{∂E_{total}}{∂w_{6}} = \\\\frac{∂E_{total}}{∂o_{1}} \\\\text{×} \\\\frac{∂o_{1}}{∂z_{3}} \\\\text{×} \\\\frac{∂z_{3}}{∂w_{6}} → w_{6}^{+}=0.38685205$$', '$$\\\\frac{∂E_{total}}{∂w_{7}} = \\\\frac{∂E_{total}}{∂o_{2}} \\\\text{×} \\\\frac{∂o_{2}}{∂z_{4}} \\\\text{×} \\\\frac{∂z_{4}}{∂w_{7}} → w_{7}^{+}=0.69629578$$\\n$$\\\\frac{∂E_{total}}{∂w_{8}} = \\\\frac{∂E_{total}}{∂o_{2}} \\\\text{×} \\\\frac{∂o_{2}}{∂z_{4}} \\\\text{×} \\\\frac{∂z_{4}}{∂w_{8}} → w_{8}^{+}=0.59624247$$']\n",
      "['[이미지: ]\\n1단계를 완료하였다면 이제 입력층 방향으로 이동하며 다시 계산을 이어갑니다. 위의 그림에서 빨간색 화살표는 순전파의 정반대 방향인 역전파의 방향을 보여줍니다. 현재 인공 신경망은 은닉층이 1개밖에 없으므로 이번 단계가 마지막 단계입니다. 하지만 은닉층이 더 많은 경우라면 입력층 방향으로 한 단계씩 계속해서 계산해가야 합니다.\\n이번 단계에서 계산할 가중치는 $w_{1}, w_{2}, w_{3}, w_{4}$입니다. 원리 자체는 동일하므로 우선 $w_{1}$에 대해서 먼저 업데이트를 진행해보겠습니다. 경사 하강법을 수행하려면 가중치 $w_{1}$를 업데이트 하기 위해서 $\\\\frac{∂E_{total}}{∂w_{1}}$를 계산해야 합니다.\\n$\\\\frac{∂E_{total}}{∂w_{1}}$를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있습니다.', '$\\\\frac{∂E_{total}}{∂w_{1}}$를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있습니다.\\n$$\\\\frac{∂E_{total}}{∂w_{1}} = \\\\frac{∂E_{total}}{∂h_{1}} \\\\text{×} \\\\frac{∂h_{1}}{∂z_{1}} \\\\text{×} \\\\frac{∂z_{1}}{∂w_{1}}$$\\n위의 식에서 우변의 첫번째항인 $\\\\frac{∂E_{total}}{∂h_{1}}$는 다음과 같이 다시 식을 풀어서 쓸 수 있습니다.\\n$$\\\\frac{∂E_{total}}{∂h_{1}} = \\\\frac{∂E_{o1}}{∂h_{1}} + \\\\frac{∂E_{o2}}{∂h_{1}}$$\\n위의 식의 우변의 두 항을 각각 구해봅시다. 우선 첫번째 항 $\\\\frac{∂E_{o1}}{∂h_{1}}$에 대해서 항을 분해 및 계산해보겠습니다.', '위의 식의 우변의 두 항을 각각 구해봅시다. 우선 첫번째 항 $\\\\frac{∂E_{o1}}{∂h_{1}}$에 대해서 항을 분해 및 계산해보겠습니다.\\n$$\\\\frac{∂E_{o1}}{∂h_{1}} = \\\\frac{∂E_{o1}}{∂z_{3}} \\\\text{×} \\\\frac{{∂z_{3}}}{∂h_{1}} = \\\\frac{∂E_{o1}}{∂o_{1}} \\\\text{×} \\\\frac{∂o_{1}}{∂z_{3}} \\\\text{×} \\\\frac{{∂z_{3}}}{∂h_{1}}$$\\n$$= -(target_{o1}-output_{o1}) \\\\text{×} o_{1}\\\\text{×}(1-o_{1}) \\\\text{×} w_{5}$$\\n$$= 0.20944600 \\\\text{×} 0.23802157 \\\\text{×} 0.45 = 0.02243370$$\\n이와 같은 원리로 $\\\\frac{∂E_{o2}}{∂h_{1}}$ 또한 구합니다.', '이와 같은 원리로 $\\\\frac{∂E_{o2}}{∂h_{1}}$ 또한 구합니다.\\n$$\\\\frac{∂E_{o2}}{∂h_{1}} = \\\\frac{∂E_{o2}}{∂z_{4}} \\\\text{×} \\\\frac{{∂z_{4}}}{∂h_{1}} = \\\\frac{∂E_{o2}}{∂o_{2}} \\\\text{×} \\\\frac{∂o_{2}}{∂z_{4}} \\\\text{×} \\\\frac{{∂z_{4}}}{∂h_{1}} = 0.00997311$$\\n$$\\\\frac{∂E_{total}}{∂h_{1}} = 0.02243370 + 0.00997311 = 0.03240681$$\\n이제 $\\\\frac{∂E_{total}}{∂w_{1}}$를 구하기 위해서 필요한 첫번째 항을 구했습니다. 나머지 두 항에 대해서 구해보도록 하겠습니다.\\n$$\\\\frac{∂h_{1}}{∂z_{1}} = h_{1}\\\\text{×}(1-h_{1}) = 0.51998934(1-0.51998934)=0.24960043$$', '$$\\\\frac{∂h_{1}}{∂z_{1}} = h_{1}\\\\text{×}(1-h_{1}) = 0.51998934(1-0.51998934)=0.24960043$$\\n$$\\\\frac{∂z_{1}}{∂w_{1}} = x_{1} = 0.1$$\\n즉, $\\\\frac{∂E_{total}}{∂w_{1}}$는 다음과 같습니다.\\n$$\\\\frac{∂E_{total}}{∂w_{1}} = 0.03240681 \\\\text{×} 0.24960043 \\\\text{×} 0.1 = 0.00080888$$\\n이제 앞서 배웠던 경사 하강법을 통해 가중치를 업데이트 할 수 있습니다.\\n$$w_{1}^{+}=w_{1}-α\\\\frac{∂E_{total}}{∂w_{1}}=0.3- 0.5 \\\\text{×} 0.00080888=0.29959556$$\\n이와 같은 원리로 $w_{2}^{+},\\\\ w_{3}^{+},\\\\ w_{4}^{+}$을 계산할 수 있습니다.', '이와 같은 원리로 $w_{2}^{+},\\\\ w_{3}^{+},\\\\ w_{4}^{+}$을 계산할 수 있습니다.\\n$$\\\\frac{∂E_{total}}{∂w_{2}} = \\\\frac{∂E_{total}}{∂h_{1}} \\\\text{×} \\\\frac{∂h_{1}}{∂z_{1}} \\\\text{×} \\\\frac{∂z_{1}}{∂w_{2}}  → w_{2}^{+}=0.24919112$$\\n$$\\\\frac{∂E_{total}}{∂w_{3}} = \\\\frac{∂E_{total}}{∂h_{2}} \\\\text{×} \\\\frac{∂h_{2}}{∂z_{2}} \\\\text{×} \\\\frac{∂z_{2}}{∂w_{3}}  → w_{3}^{+}=0.39964496$$\\n$$\\\\frac{∂E_{total}}{∂w_{4}} = \\\\frac{∂E_{total}}{∂h_{2}} \\\\text{×} \\\\frac{∂h_{2}}{∂z_{2}} \\\\text{×} \\\\frac{∂z_{2}}{∂w_{4}} → w_{4}^{+}=0.34928991$$']\n",
      "['[이미지: ]\\n업데이트 된 가중치에 대해서 다시 한 번 순전파를 진행하여 오차가 감소하였는지 확인해보겠습니다.\\n$$z_{1}=w_{1}x_{1} + w_{2}x_{2}=0.29959556 \\\\text{×} 0.1 + 0.24919112 \\\\text{×} 0.2= 0.07979778$$\\n$$z_{2}=w_{3}x_{1} + w_{4}x_{2}=0.39964496 \\\\text{×} 0.1 + 0.34928991 \\\\text{×} 0.2= 0.10982248$$\\n$$h_{1}=sigmoid(z_{1}) = 0.51993887$$\\n$$h_{2}=sigmoid(z_{2}) = 0.52742806$$\\n$$z_{3}=w_{5}h_{1}+w_{6}h_{2} = 0.43703857 \\\\text{×} h_{1} + 0.38685205 \\\\text{×} h_{2} = 0.43126996$$', '$$z_{4}=w_{7}h_{1}+w_{8}h_{2} = 0.69629578 \\\\text{×} h_{1} + 0.59624247 \\\\text{×} h_{2} = 0.67650625$$\\n$$o_{1}=sigmoid(z_{3})=0.60617688$$\\n$$o_{2}=sigmoid(z_{4})=0.66295848$$\\n$$E_{o1}=\\\\frac{1}{2}(target_{o1}-output_{o1})^{2}=0.02125445$$\\n$$E_{o2}=\\\\frac{1}{2}(target_{o2}-output_{o2})^{2}=0.00198189$$\\n$$E_{total}=E_{o1}+E_{o2}=0.02323634$$\\n기존의 전체 오차 $E_{total}$가 0.02397190였으므로 1번의 역전파로 오차가 감소한 것을 확인할 수 있습니다. 인공 신경망의 학습은 오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복하는 것을 말합니다.', \"==================================================\\n--- 07-06 과적합(Overfitting)을 막는 방법들 ---\\n```\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dropout, Dense\\nmax_words = 10000\\nnum_classes = 46\\nmodel = Sequential()\\nmodel.add(Dense(256, input_shape=(max_words,), activation='relu'))\\nmodel.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\\nmodel.add(Dense(num_classes, activation='softmax'))\", \"model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\\nmodel.add(Dense(num_classes, activation='softmax'))\\n```학습 데이터에 모델이 과적합되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도, 새로운 데이터. 즉, 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다. 이는 모델이 학습 데이터를 불필요할정도로 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다. 이번에는 모델의 과적합을 막을 수 있는 여러가지 방법에 대해서 논의합니다.\\n특히 이 책은 딥 러닝을 다루고 있으므로, 인공 신경망의 과적합을 막는 방법에 초점을 둡니다.\"]\n",
      "['모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하기 되므로 과적합 현상이 발생할 확률이 늘어납니다. 그렇기 때문에 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있습니다.\\n만약, 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증강(Data Augmentation)이라고 합니다. 이미지의 경우에는 데이터 증식이 많이 사용되는데 이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정하는 등으로 데이터를 증식시킵니다. 텍스트 데이터의 경우에는 데이터를 증강하는 방법으로 번역 후 재번역을 통해 새로운 데이터를 만들어내는 역번역(Back Translation) 등의 방법이 있습니다.']\n",
      "['인공 신경망의 복잡도는 은닉층(hidden layer)의 수나 매개변수의 수 등으로 결정됩니다. 과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한 가지 조치는 인공 신경망의 복잡도를 줄이는 것 입니다.\\n인공 신경망에서는 모델에 있는 매개변수들의 수를 모델의 수용력(capacity)이라고 하기도 합니다.']\n",
      "['복잡한 모델이 간단한 모델보다 과적합될 가능성이 높습니다. 그리고 간단한 모델은 적은 수의 매개변수를 가진 모델을 말합니다. 복잡한 모델을 좀 더 간단하게 하는 방법으로 가중치 규제(Regularization)가 있습니다.\\nL1 규제 : 가중치 w들의 절대값 합계를 비용 함수에 추가합니다. L1 노름이라고도 합니다.\\nL2 규제 : 모든 가중치 w들의 제곱합을 비용 함수에 추가합니다. L2 노름이라고도 합니다.', 'L2 규제 : 모든 가중치 w들의 제곱합을 비용 함수에 추가합니다. L2 노름이라고도 합니다.\\nL1 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\\\lambda \\\\mid w \\\\mid$를 더 한 값을 비용 함수로 하고, L2 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\\\frac{1}{2} \\\\lambda w^2$를 더 한 값을 비용 함수로 합니다. $\\\\lambda$는 규제의 강도를 정하는 하이퍼파라미터입니다. $\\\\lambda$가 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미가 됩니다.', '이 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 w들의 값이 작아져야 한다는 특징이 있습니다. L1 규제로 예를 들어봅시다. L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 합니다. 이렇게 되면, 가중치 w의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않게 됩니다.\\n예를 들어 $H(X) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4}$라는 수식이 있다고 해봅시다. 여기에 L1 규제를 사용하였더니, $w_{3}$의 값이 0이 되었다고 해봅시다. 이는 $x_{3}$ 특성은 사실 모델의 결과에 별 영향을 주지 못하는 특성임을 의미합니다.', 'L2 규제는 L1 규제와는 달리 가중치들의 제곱을 최소화하므로 w의 값이 완전히 0이 되기보다는 0에 가까워지기는 경향을 띕니다. L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용합니다. 만약, 이런 판단이 필요없다면 경험적으로는 L2 규제가 더 잘 동작하므로 L2 규제를 더 권장합니다. 인공 신경망에서 L2 규제는 가중치 감쇠(weight decay)라고도 부릅니다.\\n책에 따라서는 Regularization를 정규화로 번역하기도 하지만, 이는 정규화(Normalization)와 혼동될 수 있으므로 규제 또는 정형화라는 번역이 바람직한 것 같습니다.\\n인공 신경망에서 정규화(Normalization)라는 용어가 쓰이는 기법으로는 또 배치 정규화, 층 정규화 등이 있습니다.']\n",
      "['드롭아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법입니다.\\n[이미지: ]\\n위의 그림은 드롭아웃 전과 후의 신경망을 비교하고 있습니다. 예를 들어 드롭아웃의 비율을 0.5로 한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용합니다.\\n드롭아웃은 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적입니다. 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지합니다.\\n케라스에서는 다음과 같은 방법으로 드롭아웃을 모델에 추가할 수 있습니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dropout, Dense\\nmax_words = 10000\\nnum_classes = 46', \"from tensorflow.keras.layers import Dropout, Dense\\nmax_words = 10000\\nnum_classes = 46\\nmodel = Sequential()\\nmodel.add(Dense(256, input_shape=(max_words,), activation='relu'))\\nmodel.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\\nmodel.add(Dense(num_classes, activation='softmax'))\\n==================================================\\n--- 07-07 기울기 소실(Gradient Vanishing)과 폭주(Exploding) ---\\n```\", '--- 07-07 기울기 소실(Gradient Vanishing)과 폭주(Exploding) ---\\n```\\nfrom tensorflow.keras import optimizers\\nAdam = optimizers.Adam(lr=0.0001, clipnorm=1.)\\n```깊은 인공 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient)가 점차적으로 작아지는 현상이 발생할 수 있습니다. 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 됩니다. 이를 기울기 소실(Gradient Vanishing) 이라고 합니다.', '반대의 경우도 있습니다. 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 결국 발산되기도 합니다. 이를 기울기 폭주(Gradient Exploding) 라고 하며, 다음 챕터에서 배울 순환 신경망(Recurrent Neural Network, RNN)에서 쉽게 발생할 수 있습니다. 여기서는 기울기 소실 또는 기울기 폭주를 막는 방법들에 대해서 다룹니다.']\n",
      "['앞에서 배운 내용을 간단히 복습해봅시다. 시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워집니다. 그래서 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈 수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생할 수 있습니다.\\n기울기 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신에 ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용하는 것입니다.\\n은닉층에서는 시그모이드 함수를 사용하지 마세요.\\nLeaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제를 해결합니다.\\n은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 함수의 변형들을 사용하세요.']\n",
      "['그래디언트 클리핑은 말 그대로 기울기 값을 자르는 것을 의미합니다. 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자릅니다. 다시 말해서 임계치만큼 크기를 감소시킵니다. 이는 뒤에서 배울 신경망인 RNN에서 유용합니다. RNN은 역전파 과정에서 시점을 역행하면서 기울기를 구하는데, 이때 기울기가 너무 커질 수 있기 때문입니다. 케라스에서는 다음과 같은 방법으로 그래디언트 클리핑을 수행합니다.\\nfrom tensorflow.keras import optimizers\\nAdam = optimizers.Adam(lr=0.0001, clipnorm=1.)']\n",
      "['같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 합니다. 다시 말해 가중치 초기화만 적절히 해줘도 기울기 소실 문제과 같은 문제를 완화시킬 수 있습니다.\\n1) 세이비어 초기화(Xavier Initialization)\\n논문 : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\\n2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안했습니다. 이 초기화 방법은 제안한 사람의 이름을 따서 세이비어(Xavier Initialization) 초기화 또는 글로럿 초기화(Glorot Initialization)라고 합니다.', '이 방법은 균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution)로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세웁니다. 이전 층의 뉴런의 개수를 $n_{in}$, 다음 층의 뉴런의 개수를 $n_{out}$이라고 해봅시다.\\n글로럿과 벤지오의 논문에서는 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용하라고 합니다.\\n$$W \\\\sim Uniform(-\\\\sqrt{\\\\frac{6}{ n_{in} + n_{out} }}, +\\\\sqrt{\\\\frac{6}{ n_{in} + n_{out} }})$$\\n다시 말해 $\\\\sqrt{\\\\frac{6}{ n_{in} + n_{out} }}$를 $m$이라고 하였을 때, $-m$과 $+m$ 사이의 균등 분포를 의미합니다.\\n정규 분포로 초기화할 경우에는 평균이 0이고, 표준 편차 σ가 다음을 만족하도록 합니다.', '정규 분포로 초기화할 경우에는 평균이 0이고, 표준 편차 σ가 다음을 만족하도록 합니다.\\n$$σ=\\\\sqrt{\\\\frac { 2 }{ n_{ in }+n_{ out } } }$$\\n세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막습니다. 그런데 세이비어 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에는 좋은 성능을 보이지만, ReLU와 함께 사용할 경우에는 성능이 좋지 않습니다. ReLU 함수 또는 ReLU의 변형 함수들을 활성화 함수로 사용할 경우에는 다른 초기화 방법을 사용하는 것이 좋은데, 이를 He 초기화(He initialization)라고 합니다.\\n2) He 초기화(He initialization)', '2) He 초기화(He initialization)\\n논문 : https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf\\nHe 초기화(He initialization)는 세이비어 초기화와 유사하게 정규 분포와 균등 분포 두 가지 경우로 나뉩니다. 다만, He 초기화는 세이비어 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않습니다. 전과 같이 이전 층의 뉴런의 개수를 $n_{in}$이라고 해봅시다.\\nHe 초기화는 균등 분포로 초기화 할 경우에는 다음과 같은 균등 분포 범위를 가지도록 합니다.\\n$$W\\\\sim Uniform(- \\\\sqrt{\\\\frac { 6 }{ n_{in} } } , \\\\space\\\\space + \\\\sqrt{\\\\frac { 6 }{ n_{ in } } } )$$\\n정규 분포로 초기화할 경우에는 표준 편차 σ가 다음을 만족하도록 합니다.', '정규 분포로 초기화할 경우에는 표준 편차 σ가 다음을 만족하도록 합니다.\\n$$σ=\\\\sqrt{\\\\frac { 2 }{ n_{ in } } }$$\\n시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적입니다.\\nReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적입니다.\\nReLU + He 초기화 방법이 좀 더 보편적입니다.']\n",
      "['ReLU 계열의 함수와 He 초기화를 사용하는 것만으로도 어느 정도 기울기 소실과 폭주를 완화시킬 수 있지만, 이 두 방법을 사용하더라도 훈련 중에 언제든 다시 발생할 수 있습니다. 기울기 소실이나 폭주를 예방하는 또 다른 방법은 배치 정규화(Batch Normalization)입니다. 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만듭니다.\\n1) 내부 공변량 변화(Internal Covariate Shift)', '1) 내부 공변량 변화(Internal Covariate Shift)\\n배치 정규화를 이해하기 위해서는 내부 공변량 변화(Internal Covariate Shift)를 이해할 필요가 있습니다. 내부 공변량 변화란 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상을 말합니다. 이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생합니다. 배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥 러닝 모델의 불안전성이 층마다 입력의 분포가 달라지기 때문이라고 주장합니다. (배치 정규화를 제안한 논문에서는 이렇게 주장했지만, 뒤에 이어서는 이에 대한 반박들이 나오기는 했습니다. 하지만 그 이유가 어찌되었든 배치 정규화가 학습을 돕는다는 것은 명백합니다.)\\n공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미합니다.', '공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미합니다.\\n내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미합니다.\\n2) 배치 정규화(Batch Normalization)\\n배치 정규화(Batch Normalization)는 표현 그대로 한 번에 들어오는 배치 단위로 정규화하는 것을 말합니다. 배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행됩니다. 배치 정규화를 요약하면 다음과 같습니다. 입력에 대해 평균을 0으로 만들고, 정규화를 합니다. 그리고 정규화 된 데이터에 대해서 스케일과 시프트를 수행합니다. 이때 두 개의 매개변수 γ와 β를 사용하는데, γ는 스케일을 위해 사용하고, β는 시프트를 하는 것에 사용하며 다음 레이어에 일정한 범위의 값들만 전달되게 합니다.\\n배치 정규화의 수식은 다음과 같습니다. 아래에서 $BN$은 배치 정규화를 의미합니다.', '배치 정규화의 수식은 다음과 같습니다. 아래에서 $BN$은 배치 정규화를 의미합니다.\\nInput : 미니 배치 $B = \\\\{{x}^{(1)}, {x}^{(2)}, ..., {x}^{(m)}\\\\}$\\nOutput : $y^{(i)} = BN_{γ, β}(x^{(i)})$\\n$$\\nμ_{B} ← \\\\frac{1}{m} \\\\sum_{i=1}^{m} x^{(i)} \\\\text{ # 미니 배치에 대한 평균 계산}\\n$$\\n$$\\nσ^2_{B} ← \\\\frac{1}{m} \\\\sum_{i=1}^{m} (x^{(i)} - μ_{B})^{2}\\\\text{ # 미니 배치에 대한 분산 계산}\\n$$\\n$$\\n\\\\hat{x}^{(i)} ← \\\\frac{x^{(i)} - μ_{B}}{\\\\sqrt{σ^2_{B}+ε}}\\\\text{ # 정규화}\\n$$\\n$$\\ny^{(i)} ← γ\\\\hat{x}^{(i)} + β = BN_{γ, β}(x^{(i)}) \\\\text{ # 스케일 조정(γ)과 시프트(β)를 통한 선형 연산}\\n$$', '$$\\n$$\\ny^{(i)} ← γ\\\\hat{x}^{(i)} + β = BN_{γ, β}(x^{(i)}) \\\\text{ # 스케일 조정(γ)과 시프트(β)를 통한 선형 연산}\\n$$\\n$m$은 미니 배치에 있는 샘플의 수\\n$μ_{B}$는 미니 배치 $B$에 대한 평균.\\n$σ_{B}$는 미니 배치 $B$에 대한 표준편차.\\n$\\\\hat{x}^{(i)}$은 평균이 0이고 정규화 된 입력 데이터.\\n$ε$은 $σ^{2}$가 0일 때, 분모가 0이 되는 것을 막는 작은 양수. 보편적으로 $10^{-5}$\\n$γ$는 정규화 된 데이터에 대한 스케일 매개변수로 학습 대상\\n$β$는 정규화 된 데이터에 대한 시프트 매개변수로 학습 대상\\n$y^{(i)}$는 스케일과 시프트를 통해 조정한 $BN$의 최종 결과\\n배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 합니다.', '배치 정규화를 사용하면 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용하더라도 기울기 소실 문제가 크게 개선됩니다.\\n가중치 초기화에 훨씬 덜 민감해집니다.\\n훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킵니다.\\n미니 배치마다 평균과 표준편차를 계산하여 사용하므로 훈련 데이터에 일종의 잡음 주입의 부수 효과로 과적합을 방지하는 효과도 냅니다. 다시 말해, 마치 드롭아웃과 비슷한 효과를 냅니다. 물론, 드롭 아웃과 함께 사용하는 것이 좋습니다.\\n배치 정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려집니다. 그래서 서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한지 고민이 필요합니다.\\n배치 정규화의 효과는 굉장하지만 내부 공변량 변화때문은 아니라는 논문도 있습니다. : https://arxiv.org/pdf/1805.11604.pdf\\n3) 배치 정규화의 한계', '배치 정규화의 효과는 굉장하지만 내부 공변량 변화때문은 아니라는 논문도 있습니다. : https://arxiv.org/pdf/1805.11604.pdf\\n3) 배치 정규화의 한계\\n배치 정규화는 뛰어난 방법이지만 몇 가지 한계가 존재합니다.']\n",
      "['배치 정규화는 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있습니다. 단적으로 배치 크기를 1로 하게되면 분산은 0이 됩니다. 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있습니다. 배치 정규화를 적용할때는 작은 미니 배치보다는 크기가 어느정도 되는 미니 배치에서 하는 것이 좋습니다. 이처럼 배치 정규화는 배치 크기에 의존적인 면이 있습니다.']\n",
      "['뒤에서 배우겠지만, RNN은 각 시점(time step)마다 다른 통계치를 가집니다. 이는 RNN에 배치 정규화를 적용하는 것을 어렵게 만듭니다. RNN에서 배치 정규화를 적용하기 위한 몇 가지 논문이 제시되어 있지만, 여기서는 이를 소개하는 대신 배치 크기에도 의존적이지 않으며, RNN에도 적용하는 것이 수월한 층 정규화(layer normalization)라는 방법을 소개하고자 합니다.']\n",
      "['층 정규화를 이해하기에 앞서 배치 정규화를 시각화해보겠습니다. 다음은 $m$이 3이고, 특성의 수가 4일 때의 배치 정규화를 보여줍니다. 미니 배치란 동일한 특성(feature) 개수들을 가진 다수의 샘플들을 의미함을 상기합시다.\\n[이미지: ]\\n반면, 층 정규화는 다음과 같습니다.\\n[이미지: ]\\n==================================================\\n--- 07-08 케라스(Keras) 훑어보기 ---\\n```\\nfrom tensorflow.keras.models import load_model\\nmodel = load_model(\"model_name.h5\")\\n```이 책에서는 딥 러닝을 쉽게 할 수 있는 파이썬 라이브러리인 케라스(Keras)를 사용합니다. 케라스는 유저가 손쉽게 딥 러닝을 구현할 수 있도록 도와주는 상위 레벨의 인터페이스로 딥 러닝을 쉽게 구현할 수 있도록 해줍니다.', '케라스의 모든 기능들을 열거하는 것만으로도 한 권의 책의 분량이므로 여기서 전부 다룰 수는 없습니다. 가장 좋은 방법은 케라스나 텐서플로우 공식 문서( https://keras.io/ or https://www.tensorflow.org/guide/keras?hl=ko )를 참고하는 것입니다. 여기서는 대표적으로 사용되는 케라스의 도구들을 이해합니다.']\n",
      "['Tokenizer() : 토큰화와 정수 인코딩을 위해 사용됩니다. 다음은 훈련 데이터로부터 단어 집합을 생성하고, 해당 단어 집합으로 임의의 문장을 정수 인코딩하는 과정을 보여줍니다.\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\ntokenizer = Tokenizer()\\ntrain_text = \"The earth is an awesome place live\"\\n# 단어 집합 생성\\ntokenizer.fit_on_texts([train_text])\\n# 정수 인코딩\\nsub_text = \"The earth is an great place live\"\\nsequences = tokenizer.texts_to_sequences([sub_text])[0]\\nprint(\"정수 인코딩 : \",sequences)', 'sequences = tokenizer.texts_to_sequences([sub_text])[0]\\nprint(\"정수 인코딩 : \",sequences)\\nprint(\"단어 집합 : \",tokenizer.word_index)\\n정수 인코딩 :  [1, 2, 3, 4, 6, 7]\\n단어 집합 :  {\\'the\\': 1, \\'earth\\': 2, \\'is\\': 3, \\'an\\': 4, \\'awesome\\': 5, \\'place\\': 6, \\'live\\': 7}\\n출력 결과를 보면 great는 단어 집합(vocabulary)에 없으므로 출력되지 않습니다.', \"출력 결과를 보면 great는 단어 집합(vocabulary)에 없으므로 출력되지 않습니다.\\npad_sequence() : 전체 훈련 데이터에서 각 샘플의 길이는 서로 다를 수 있습니다. 또는 각 문서 또는 각 문장은 단어의 수가 제각각입니다. 모델의 입력으로 사용하려면 모든 샘플의 길이를 동일하게 맞추어야할 때가 있습니다. 이를 자연어 처리에서는 패딩(padding) 작업이라고 하는데, 보통 숫자 0을 넣어서 길이가 다른 샘플들의 길이를 맞춰줍니다. 케라스에서는 pad_sequence()를 사용합니다. pad_sequence()는 정해준 길이보다 길이가 긴 샘플은 값을 일부 자르고, 정해준 길이보다 길이가 짧은 샘플은 값을 0으로 채웁니다.\\npad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='pre')\\narray([[1, 2, 3],\\n[4, 5, 6],\\n[0, 7, 8]], dtype=int32)\", \"array([[1, 2, 3],\\n[4, 5, 6],\\n[0, 7, 8]], dtype=int32)\\n첫번째 인자 = 패딩을 진행할 데이터\\nmaxlen = 모든 데이터에 대해서 정규화 할 길이\\npadding = 'pre'를 선택하면 앞에 0을 채우고 'post'를 선택하면 뒤에 0을 채움.\"]\n",
      "['워드 임베딩 챕터에서 다루겠지만, 워드 임베딩이란 텍스트 내의 단어들을 밀집 벡터(dense vector)로 만드는 것을 말합니다. 앞서 배운 개념인 원-핫 벡터와 비교해봅시다. 원-핫 벡터는 대부분이 0의 값을 가지고, 단 하나의 1의 값을 가지는 벡터이며 벡터의 차원이 대체적으로 크다는 성질을 가졌습니다.\\nEx) [0 1 0 0 0 0 ... 중략 ... 0 0 0 0 0 0 0] # 차원이 굉장히 크면서 대부분의 값이 0\\n원-핫 벡터는 단어 집합의 크기만큼 벡터의 차원을 가지며 단어 벡터 간의 유의미한 유사도를 구할 수 없다는 단점이 있습니다. 반면 워드 임베딩으로부터 얻은 임베딩 벡터는 상대적으로 저차원을 가지며 모든 원소의 값이 실수입니다.\\nEx) [0.1 -1.2 0.8 0.2 1.8] # 상대적으로 저차원이며 실수값을 가짐\\n간단히 표로 정리하면 아래와 같습니다.\\n-----\\n원-핫 벡터\\n임베딩 벡터\\n차원\\n고차원(단어 집합의 크기)\\n저차원\\n다른 표현', '간단히 표로 정리하면 아래와 같습니다.\\n-----\\n원-핫 벡터\\n임베딩 벡터\\n차원\\n고차원(단어 집합의 크기)\\n저차원\\n다른 표현\\n대부분의 값이 0이 대부분인 희소 벡터\\n모든 값이 실수인 밀집 벡터\\n표현 방법\\n수동\\n훈련 데이터로부터 학습함\\n값의 타입\\n1과 0\\n실수\\n단어를 원-핫 벡터로 만드는 과정을 원-핫 인코딩이라고 한다면, 단어를 밀집 벡터로 만드는 작업을 워드 임베딩(word embedding) 이라고 합니다. 밀집 벡터는 워드 임베딩 과정을 통해 나온 결과므로 임베딩 벡터(embedding vector)라고도 합니다. 원-핫 벡터의 차원이 주로 20,000 이상을 넘어가는 것과는 달리 임베딩 벡터는 주로 256, 512, 1024 등의 차원을 가집니다. 임베딩 벡터는 초기에는 랜덤값을 가지지만, 인공 신경망의 가중치가 학습되는 방법과 같은 방식으로 값이 학습되며 변경됩니다.', 'Embedding() :  Embedding()은 단어를 밀집 벡터로 만드는 역할을 합니다. 인공 신경망 용어로는 임베딩 층(embedding layer)을 만드는 역할을 합니다. Embedding()은 정수 인코딩이 된 단어들을 입력을 받아서 임베딩을 수행합니다.\\nEmbedding()은 (number of samples, input_length)인 2D 정수 텐서를 입력받습니다. 이때 각 sample은 정수 인코딩이 된 결과로, 정수의 시퀀스입니다. Embedding()은 워드 임베딩 작업을 수행하고 (number of samples, input_length, embedding word dimensionality)인 3D 텐서를 리턴합니다.\\n아래의 코드는 실제 동작되는 코드가 아니라 의사 코드(pseudo-code)로 임베딩의 개념 이해를 돕기 위해서 작성되었습니다.\\n# 1. 토큰화', \"아래의 코드는 실제 동작되는 코드가 아니라 의사 코드(pseudo-code)로 임베딩의 개념 이해를 돕기 위해서 작성되었습니다.\\n# 1. 토큰화\\ntokenized_text = [['Hope', 'to', 'see', 'you', 'soon'], ['Nice', 'to', 'see', 'you', 'again']]\\n# 2. 각 단어에 대한 정수 인코딩\\nencoded_text = [[0, 1, 2, 3, 4],[5, 1, 2, 3, 6]]\\n# 3. 위 정수 인코딩 데이터가 아래의 임베딩 층의 입력이 된다.\\nvocab_size = 7\\nembedding_dim = 2\\nEmbedding(vocab_size, embedding_dim, input_length=5)\\n# 각 정수는 아래의 테이블의 인덱스로 사용되며 Embedding()은 각 단어마다 임베딩 벡터를 리턴한다.\\n+------------+------------+\\n|   index    | embedding  |\", '+------------+------------+\\n|   index    | embedding  |\\n+------------+------------+\\n|     0      | [1.2, 3.1] |\\n|     1      | [0.1, 4.2] |\\n|     2      | [1.0, 3.1] |\\n|     3      | [0.3, 2.1] |\\n|     4      | [2.2, 1.4] |\\n|     5      | [0.7, 1.7] |\\n|     6      | [4.1, 2.0] |\\n+------------+------------+\\n# 위의 표는 임베딩 벡터가 된 결과를 예로서 정리한 것이고 Embedding()의 출력인 3D 텐서를 보여주는 것이 아님.\\nEmbedding()의 대표적인 인자는 다음과 같습니다.\\n첫번째 인자 = 단어 집합의 크기. 즉, 총 단어의 개수\\n두번째 인자 = 임베딩 벡터의 출력 차원. 결과로서 나오는 임베딩 벡터의 크기', '첫번째 인자 = 단어 집합의 크기. 즉, 총 단어의 개수\\n두번째 인자 = 임베딩 벡터의 출력 차원. 결과로서 나오는 임베딩 벡터의 크기\\ninput_length = 입력 시퀀스의 길이']\n",
      "['Sequential() : 인공 신경망 챕터에서 입력층, 은닉층, 출력층에 대해서 배웠습니다. 케라스에서는 이러한 층을 구성하기 위해 Sequential()을 사용합니다. Sequential()을 model로 선언한 뒤에 model.add()라는 코드를 통해 층을 단계적으로 추가합니다. 아래는 model.add()로 층을 추가하는 예제 코드를 보여줍니다. 실제로는 세 개의 온점 대신에 층의 이름을 기재해야 합니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nmodel = Sequential()\\nmodel.add(...) # 층 추가\\nmodel.add(...) # 층 추가\\nmodel.add(...) # 층 추가\\nEmbedding()을 통해 생성하는 임베딩 층(embedding layer)을 추가하는 예시를 봅시다.\\nmodel = Sequential()', \"Embedding()을 통해 생성하는 임베딩 층(embedding layer)을 추가하는 예시를 봅시다.\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, output_dim, input_length))\\n전결합층(fully-connected layer)을 추가하는 예시를 보겠습니다.\\nmodel = Sequential()\\nmodel.add(Dense(1, input_dim=3, activation='relu'))\\n위의 코드에서 Dense()는 한번 사용되었지만 더 많은 층을 추가할 수 있습니다. Dense()의 대표적인 인자를 보겠습니다.\\n첫번째 인자 = 출력 뉴런의 수.\\ninput_dim = 입력 뉴런의 수. (입력의 차원)\\nactivation = 활성화 함수.\\n- linear : 디폴트 값으로 별도 활성화 함수 없이 입력 뉴런과 가중치의 계산 결과 그대로 출력.\", 'activation = 활성화 함수.\\n- linear : 디폴트 값으로 별도 활성화 함수 없이 입력 뉴런과 가중치의 계산 결과 그대로 출력.\\n- sigmoid : 이진 분류 문제에서 출력층에 주로 사용되는 활성화 함수.\\n- softmax : 셋 이상의 선택지 중 하나를 택하는 다중 클래스 분류 문제에서 출력층에 주로 사용되는 활성화 함수.\\n- relu : 은닉층에 주로 사용되는 활성화 함수.\\n위 코드에서 사용된 Dense()의 의미를 보겠습니다. 첫번째 인자의 값은 1인데 이는 총 1개의 출력 뉴런을 의미합니다. Dense()의 두번째 인자인 input_dim은 입력층의 뉴런 수를 의미합니다. 이 경우에는 3입니다. 3개의 입력층 뉴런과 1개의 출력층 뉴런을 만들었습니다. 이를 시각화하면 다음과 같습니다.\\n[이미지: ]\\nDense()를 사용하여 전결합층을 하나 더 추가해보겠습니다.\\nmodel = Sequential()', \"[이미지: ]\\nDense()를 사용하여 전결합층을 하나 더 추가해보겠습니다.\\nmodel = Sequential()\\nmodel.add(Dense(8, input_dim=4, activation='relu'))\\nmodel.add(Dense(1, activation='sigmoid')) # 출력층\\n이번에는 Dense()가 두 번 사용되었습니다. Dense()가 처음 사용되었을 때와 추가로 사용되었을 때의 인자는 조금 다릅니다. 첫번째 사용된 Dense()의 8이라는 값은 더 이상 출력층의 뉴런이 아니라 은닉층의 뉴런입니다. 층이 하나 더 생겼기 때문입니다.\\n두번째 Dense()는 input_dim 인자가 없는데, 이는 이미 이전 층의 뉴런 수가 8개임을 알고있기 때문입니다. 위 코드에서 두번째 Dense()는 마지막 층이므로, 첫번째 인자 1은 결국 출력층의 뉴런 개수가 됩니다. 이를 시각화하면 다음과 같습니다.\\n[이미지: ]\", '[이미지: ]\\n이 외에도 LSTM, GRU, Convolution2D, BatchNormalization 등 다양한 층을 만들 수 있습니다. 일부는 뒤에서 배웁니다.\\nsummary() : 모델의 정보를 요약해서 보여줍니다.\\nmodel.summary()\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #\\n=================================================================\\ndense_1 (Dense)              (None, 8)                 40\\n_________________________________________________________________', '_________________________________________________________________\\ndense_2 (Dense)              (None, 1)                 9\\n=================================================================\\nTotal params: 49\\nTrainable params: 49\\nNon-trainable params: 0\\n_________________________________________________________________']\n",
      "[\"아래의 코드는 RNN을 이용하여 이진 분류를 하는 전형적인 코드를 보여줍니다. RNN은 다음 챕터에서 학습합니다. 임베딩층, 은닉층, 출력층을 추가하여 모델을 설계한 후에, 마지막으로 컴파일을 합니다.\\ncompile() : 모델을 기계가 이해할 수 있도록 컴파일 합니다. 손실 함수와 옵티마이저, 메트릭 함수를 선택합니다.\\nfrom tensorflow.keras.layers import SimpleRNN, Embedding, Dense\\nfrom tensorflow.keras.models import Sequential\\nvocab_size = 10000\\nembedding_dim = 32\\nhidden_units = 32\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(SimpleRNN(hidden_units))\\nmodel.add(Dense(1, activation='sigmoid'))\", \"model.add(SimpleRNN(hidden_units))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\noptimizer = 훈련 과정을 설정하는 옵티마이저를 설정합니다.\\nloss = 훈련 과정에서 사용할 손실 함수(loss function)를 설정합니다.\\nmetrics = 훈련을 모니터링하기 위한 지표를 선택합니다.\\n대표적으로 사용되는 손실 함수와 활성화 함수의 조합은 다음과 같습니다. 더 많은 함수는 공식문서를 참고바랍니다.\\n문제 유형\\n손실 함수명\\n출력층의 활성화 함수명\\n참고 실습\\n회귀 문제\\nmean_squared_error\\n-\\n선형 회귀 실습\\n다중 클래스 분류\\ncategorical_crossentropy\\n소프트맥스\\n로이터 뉴스 분류하기\\n다중 클래스 분류\", '회귀 문제\\nmean_squared_error\\n-\\n선형 회귀 실습\\n다중 클래스 분류\\ncategorical_crossentropy\\n소프트맥스\\n로이터 뉴스 분류하기\\n다중 클래스 분류\\nsparse_categorical_crossentropy\\n소프트맥스\\n양방향 LSTM를 이용한 품사 태깅\\n이진 분류\\nbinary_crossentropy\\n시그모이드\\nIMDB 리뷰 감성 분류하기\\nsparse_categorical_crossentropy는 categorical_crossentropy와 동일하게 다중 클래스 분류에서 사용하지만, 레이블을 원-핫 인코딩하지 않고 정수 인코딩 된 상태에서 수행 가능하다는 점이 다릅니다.\\nfit() : 모델을 학습합니다. 모델이 오차로부터 매개 변수를 업데이트 시키는 과정을 학습, 훈련, 또는 적합(fitting)이라고 하는데, 모델이 데이터에 적합해가는 과정이기 때문입니다. 그런 의미에서 fit()은 모델의 훈련을 시작합니다.', '# 위의 compile() 코드의 연장선상인 코드\\nmodel.fit(X_train, y_train, epochs=10, batch_size=32)\\n첫번째 인자 = 훈련 데이터에 해당됩니다.\\n두번째 인자 = 지도 학습에서 레이블 데이터에 해당됩니다.\\nepochs = 에포크. 에포크 1은 전체 데이터를 한 차례 훑고 지나갔음을 의미함. 정수값 기재 필요. 총 훈련 횟수를 정의합니다.\\nbatch_size = 배치 크기. 기본값은 32. 미니 배치 경사 하강법을 사용하고 싶지 않을 경우에는 batch_size=None을 기재합니다.\\n좀 더 많은 인자를 사용할 때를 보겠습니다.\\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data(X_val, y_val))', 'model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data(X_val, y_val))\\nvalidation_data(x_val, y_val) = 검증 데이터(validation data)를 사용합니다. 일반적으로 검증 데이터를 사용하면 각 에포크마다 검증 데이터의 정확도나 오차를 함께 출력하는데, 이 정확도는 훈련이 잘 되고 있는지를 보여줄 뿐이며 실제로 모델이 검증 데이터를 학습하지는 않습니다. 검증 데이터의 오차(loss)가 낮아지다가 높아지기 시작하면 이는 과적합(overfitting)의 신호입니다.', 'validation_split = validation_data와 동일하게 검증 데이터를 사용하기 위한 용도로 validation_data 대신 사용할 수 있습니다. 검증 데이터를 지정하는 것이 아니라 훈련 데이터와 훈련 데이터의 레이블인 X_train과 y_train에서 일정 비율 분리하여 이를 검증 데이터로 사용합니다.\\n# 훈련 데이터의 20%를 검증 데이터로 사용.\\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_split=0.2))\\nverbose = 학습 중 출력되는 문구를 설정합니다.\\n- 0 : 아무 것도 출력하지 않습니다.\\n- 1 : 훈련의 진행도를 보여주는 진행 막대를 보여줍니다.\\n- 2 : 미니 배치마다 손실 정보를 출력합니다.\\n아래는 verbose의 값이 1일 때와 2일 때를 보여줍니다.\\n# verbose = 1일 경우.\\nEpoch 88/100', '- 2 : 미니 배치마다 손실 정보를 출력합니다.\\n아래는 verbose의 값이 1일 때와 2일 때를 보여줍니다.\\n# verbose = 1일 경우.\\nEpoch 88/100\\n7/7 [==============================] - 0s 143us/step - loss: 0.1029 - acc: 1.0000\\n# verbose = 2일 경우.\\nEpoch 88/100\\n- 0s - loss: 0.1475 - acc: 1.0000']\n",
      "['evaluate() : 테스트 데이터를 통해 학습한 모델에 대한 정확도를 평가합니다.\\n# 위의 fit() 코드의 연장선상인 코드\\nmodel.evaluate(X_test, y_test, batch_size=32)\\n첫번째 인자 = 테스트 데이터에 해당됩니다.\\n두번째 인자 = 지도 학습에서 레이블 테스트 데이터에 해당됩니다.\\nbatch_size = 배치 크기.\\npredict() : 임의의 입력에 대한 모델의 출력값을 확인합니다.\\n# 위의 fit() 코드의 연장선상인 코드\\nmodel.predict(X_input, batch_size=32)\\n첫번째 인자 = 예측하고자 하는 데이터.\\nbatch_size = 배치 크기.']\n",
      "['복습을 위한 스터디나 실제 어플리케이션 개발 단계에서 구현한 모델을 저장하고 불러오는 일은 중요합니다. 모델을 저장한다는 것은 학습이 끝난 신경망의 구조를 보존하고 계속해서 사용할 수 있다는 의미입니다.\\nsave() : 인공 신경망 모델을 hdf5 파일에 저장합니다.\\nmodel.save(\"model_name.h5\")\\nload_model() : 저장해둔 모델을 불러옵니다.\\nfrom tensorflow.keras.models import load_model\\nmodel = load_model(\"model_name.h5\")\\n==================================================\\n--- 07-09 케라스의 함수형 API(Keras Functional API) ---\\n```\\ndense = Dense(128)\\nresult = dense(input)', '--- 07-09 케라스의 함수형 API(Keras Functional API) ---\\n```\\ndense = Dense(128)\\nresult = dense(input)\\n```앞서 구현한 선형, 로지스틱, 소프트맥스 회귀 모델들과 케라스 훑어보기 실습에서 배운 케라스의 모델 설계 방식은 Sequential API을 사용한 것입니다. 그런데 Sequential API는 여러층을 공유하거나 다양한 종류의 입력과 출력을 사용하는 등의 복잡한 모델을 만드는 일에는 한계가 있습니다. 이번에는 더욱 복잡한 모델을 생성할 수 있는 방식인 Functional API(함수형 API)에 대해서 알아봅니다.\\nFunctional API에 대한 자세한 소개는 케라스 공식 문서에서도 확인할 수 있습니다.\\n링크 : https://keras.io/getting-started/functional-api-guide/']\n",
      "[\"두 가지 API의 차이를 이해하기 위해서 앞서 배운 Sequential API를 사용하여 기본적인 모델을 만들어봅시다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nmodel = Sequential()\\nmodel.add(Dense(3, input_dim=4, activation='softmax'))\\n위와 같은 방식은 직관적이고 편리하지만 단순히 층을 쌓는 것만으로는 구현할 수 없는 복잡한 신경망을 구현할 수 없습니다. 따라서 초심자에게 적합한 API이지만, 전문가가 되기 위해서는 결과적으로 Functional API를 학습해야 합니다.\"]\n",
      "['Functional API는 각 층을 일종의 함수(function)로서 정의합니다. 그리고 각 함수를 조합하기 위한 연산자들을 제공하는데, 이를 이용하여 신경망을 설계합니다. Functional API로 FFNN, RNN 등 다양한 모델을 만들면서 기존의 sequential API와의 차이를 이해해봅시다.\\nFunctional API는 입력의 크기(shape)를 명시한 입력층(Input layer)을 모델의 앞단에 정의해주어야 합니다.\\n1) 전결합 피드 포워드 신경망(Fully-connected FFNN)\\nSequential API와는 다르게 functional API에서는 입력 데이터의 크기(shape)를 인자로 입력층을 정의해주어야 합니다. 피드 포워드 신경망(Fully-connected FFNN)을 만든다고 가정해보겠습니다.\\nfrom tensorflow.keras.layers import Input, Dense', \"from tensorflow.keras.layers import Input, Dense\\nfrom tensorflow.keras.models import Model\\ninputs = Input(shape=(10,))\\n위의 코드는 10개의 입력을 받는 입력층을 보여줍니다. 위의 코드에 은닉층과 출력층을 추가해봅시다.\\ninputs = Input(shape=(10,))\\nhidden1 = Dense(64, activation='relu')(inputs)  # <- 새로 추가\\nhidden2 = Dense(64, activation='relu')(hidden1) # <- 새로 추가\\noutput = Dense(1, activation='sigmoid')(hidden2) # <- 새로 추가\\n위의 코드를 하나의 모델로 구성해보겠습니다. 이는 Model에 입력 텐서와 출력 텐서를 정의하여 완성됩니다.\\ninputs = Input(shape=(10,))\", \"위의 코드를 하나의 모델로 구성해보겠습니다. 이는 Model에 입력 텐서와 출력 텐서를 정의하여 완성됩니다.\\ninputs = Input(shape=(10,))\\nhidden1 = Dense(64, activation='relu')(inputs)\\nhidden2 = Dense(64, activation='relu')(hidden1)\\noutput = Dense(1, activation='sigmoid')(hidden2)\\nmodel = Model(inputs=inputs, outputs=output) # <- 새로 추가\\n지금까지의 내용을 정리하면 다음과 같습니다.\\nInput() 함수에 입력의 크기를 정의합니다.\\n이전층을 다음층 함수의 입력으로 사용하고, 변수에 할당합니다.\\nModel() 함수에 입력과 출력을 정의합니다.\\n이를 model로 저장하면 sequential API를 사용할 때와 마찬가지로 model.compile, model.fit 등을 사용 가능합니다.\", '이를 model로 저장하면 sequential API를 사용할 때와 마찬가지로 model.compile, model.fit 등을 사용 가능합니다.\\nmodel.compile(optimizer=\\'rmsprop\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n# model.fit(data, labels)\\n이번에는 변수명을 달리해서 FFNN을 만들어보겠습니다. 이번에는 은닉층과 출력층의 변수를 전부 x로 통일하였습니다.\\ninputs = Input(shape=(10,))\\nx = Dense(8, activation=\"relu\")(inputs)\\nx = Dense(4, activation=\"relu\")(x)\\nx = Dense(1, activation=\"linear\")(x)\\nmodel = Model(inputs, x)\\n이번에는 위에서 배운 내용을 바탕으로 선형 회귀와 로지스틱 회귀를 Functional API로 구현해봅시다.', 'model = Model(inputs, x)\\n이번에는 위에서 배운 내용을 바탕으로 선형 회귀와 로지스틱 회귀를 Functional API로 구현해봅시다.\\n2) 선형 회귀(Linear Regression)\\n앞서 ( https://wikidocs.net/111472 ) Sequential API로 구현했던 선형 회귀를 Functional API로 구현해봅시다.\\nfrom tensorflow.keras.layers import Input, Dense\\nfrom tensorflow.keras import optimizers\\nfrom tensorflow.keras.models import Model\\nX = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\\ny = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\\ninputs = Input(shape=(1,))', \"y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\\ninputs = Input(shape=(1,))\\noutput = Dense(1, activation='linear')(inputs)\\nlinear_model = Model(inputs, output)\\nsgd = optimizers.SGD(lr=0.01)\\nlinear_model.compile(optimizer=sgd, loss='mse', metrics=['mse'])\\nlinear_model.fit(X, y, epochs=300)\\n그 외에 다양한 다른 예제들을 구현해봅시다.\\n3) 로지스틱 회귀(Logistic Regression)\\nfrom tensorflow.keras.layers import Input, Dense\\nfrom tensorflow.keras.models import Model\\ninputs = Input(shape=(3,))\", \"from tensorflow.keras.models import Model\\ninputs = Input(shape=(3,))\\noutput = Dense(1, activation='sigmoid')(inputs)\\nlogistic_model = Model(inputs, output)\\n4) 다중 입력을 받는 모델(model that accepts multiple inputs)\\nfunctional API를 사용하면 아래와 같이 다중 입력과 다중 출력을 가지는 모델도 만들 수 있습니다.\\n# 최종 완성된 다중 입력, 다중 출력 모델의 예\\nmodel = Model(inputs=[a1, a2], outputs=[b1, b2, b3])\\n이번에는 다중 입력을 받는 모델을 입력층부터 출력층까지 설계해보겠습니다.\\nfrom tensorflow.keras.layers import Input, Dense, concatenate\\nfrom tensorflow.keras.models import Model\", 'from tensorflow.keras.models import Model\\n# 두 개의 입력층을 정의\\ninputA = Input(shape=(64,))\\ninputB = Input(shape=(128,))\\n# 첫번째 입력층으로부터 분기되어 진행되는 인공 신경망을 정의\\nx = Dense(16, activation=\"relu\")(inputA)\\nx = Dense(8, activation=\"relu\")(x)\\nx = Model(inputs=inputA, outputs=x)\\n# 두번째 입력층으로부터 분기되어 진행되는 인공 신경망을 정의\\ny = Dense(64, activation=\"relu\")(inputB)\\ny = Dense(32, activation=\"relu\")(y)\\ny = Dense(8, activation=\"relu\")(y)\\ny = Model(inputs=inputB, outputs=y)\\n# 두개의 인공 신경망의 출력을 연결(concatenate)', 'y = Model(inputs=inputB, outputs=y)\\n# 두개의 인공 신경망의 출력을 연결(concatenate)\\nresult = concatenate([x.output, y.output])\\nz = Dense(2, activation=\"relu\")(result)\\nz = Dense(1, activation=\"linear\")(z)\\nmodel = Model(inputs=[x.input, y.input], outputs=z)\\n위 모델은 두 개의 입력층으로부터 분기되어 진행된 후 마지막에는 하나의 출력을 예측하는 모델입니다.\\n5) RNN(Recurrence Neural Network) 은닉층 사용하기\\n이번에는 RNN 은닉층을 가지는 모델을 설계해봅시다. 여기서는 하나의 특성(feature)에 50개의 시점(time-step)을 입력으로 받는 모델을 설계해보겠습니다. RNN에 대한 구체적인 내용은 다음 챕터인 RNN 챕터에서 배웁니다.', \"from tensorflow.keras.layers import Input, Dense, LSTM\\nfrom tensorflow.keras.models import Model\\ninputs = Input(shape=(50,1))\\nlstm_layer = LSTM(10)(inputs)\\nx = Dense(10, activation='relu')(lstm_layer)\\noutput = Dense(1, activation='sigmoid')(x)\\nmodel = Model(inputs=inputs, outputs=output)\\n다수의 입력과 다수의 출력을 가지는 좀 더 다양한 예제는 앞서 소개한 케라스 공식 문서에서 확인할 수 있습니다.\\n6) 다르게 보이지만 동일한 표기\", '다수의 입력과 다수의 출력을 가지는 좀 더 다양한 예제는 앞서 소개한 케라스 공식 문서에서 확인할 수 있습니다.\\n6) 다르게 보이지만 동일한 표기\\n케라스의 Functional API가 익숙하지 않은 상태에서 Functional API를 사용한 코드를 보다가 혼동할 수 있는 점이 한 가지 있습니다. 바로 동일한 의미를 가지지만, 하나의 줄로 표현할 수 있는 코드를 두 개의 줄로 표현한 경우입니다.\\nresult = Dense(128)(input)\\n위 코드는 아래와 같이 두 개의 줄로 표현할 수 있습니다.\\ndense = Dense(128)\\nresult = dense(input)\\n==================================================\\n--- 07-10 케라스 서브클래싱 API(Keras Subclassing API) ---\\n```\\n# 선형 회귀 구현 코드의 일부 발췌\\ninputs = Input(shape=(1,)) # <-- 해당 부분', \"```\\n# 선형 회귀 구현 코드의 일부 발췌\\ninputs = Input(shape=(1,)) # <-- 해당 부분\\noutput = Dense(1, activation='linear')(inputs)\\nlinear_model = Model(inputs, output)\\nsgd = optimizers.SGD(lr=0.01)\\nlinear_model.compile(optimizer=sgd, loss='mse', metrics=['mse'])\\nlinear_model.fit(X, y, epochs=300)\\n```케라스의 구현 방식에는 Sequential API, Functional API 외에도 Subclassing API라는 구현 방식이 존재합니다.\\n[이미지: ]\"]\n",
      "[\"앞서 ( https://wikidocs.net/111472 ) 에서 Sequential API로 구현했던 선형 회귀를 Subclassing API로 구현한다면 다음과 같습니다.\\nimport tensorflow as tf\\nclass LinearRegression(tf.keras.Model):\\ndef __init__(self):\\nsuper(LinearRegression, self).__init__()\\nself.linear_layer = tf.keras.layers.Dense(1, input_dim=1, activation='linear')\\ndef call(self, x):\\ny_pred = self.linear_layer(x)\\nreturn y_pred\\nmodel = LinearRegression()\\nX = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\\ny = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\", \"y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\\nsgd = tf.keras.optimizers.SGD(lr=0.01)\\nmodel.compile(optimizer=sgd, loss='mse', metrics=['mse'])\\nmodel.fit(X, y, epochs=300)\\n클래스(class) 형태의 모델은 tf.keras.Model을 상속받습니다. 그리고 init()에서 모델의 구조와 동적을 정의하는 생성자를 정의합니다. 이는 파이썬에서 객체가 갖는 속성값을 초기화하는 역할로, 객체가 생성될 때 자동으호 호출됩니다. super() 함수를 부르면 여기서 만든 클래스는 tf.keras.Model 클래스의 속성들을 가지고 초기화 됩니다. call() 함수는 모델이 데이터를 입력받아 예측값을 리턴하는 포워드(forward) 연산을 진행시키는 함수입니다.\", '$H(x)$ 식에 입력 $x$로부터 예측된 $y$를 얻는 것을 forward 연산이라고 합니다.']\n",
      "['Sequential API는 간단한 모델을 구현하기에 적합합니다. Functional API로는 Sequential API로 구현할 수 없는 복잡한 모델들을 구현가능합니다. 그런데 Subclassing API로는 Functional API가 구현할 수 없는 모델들조차 구현할 수 있는 경우가 있습니다. Functional API는 기본적으로 딥 러닝 모델을 DAG(directed acyclic graph)로 취급합니다. 실제로 대부분의 딥 러닝 모델이 이에 속하기는 하지만, 항상 그렇지는 않습니다.  예를 들어서 재귀 네트워크나 트리 RNN은 이 가정을 따르지 않으며 Functional API에서 구현할 수 없습니다.\\n이를 반대로 해석하면 대부분의 딥 러닝 모델은 Functional API 수준에서도 전부 구현이 가능하다는 의미이기도 합니다. 그래서 Subclassing API는 밑바닥부터 새로운 수준의 아키텍처를 구현해야 하는 실험적 연구를 하는 연구자들에게 적합합니다.']\n",
      "[\"1) Sequential API\\n장점 : 단순하게 층을 쌓는 방식으로 쉽고 사용하기가 간단합니다.\\n단점 : 다수의 입력(multi-input), 다수의 출력(multi-output)을 가진 모델 또는 층 간의 연결(concatenate)이나 덧셈(Add)과 같은 연산을 하는 모델을 구현하기에는 적합하지 않습니다. 이런 모델들의 구현은 Functional API를 사용해야 합니다.\\n2) Functional API\\n장점 : Sequential API로는 구현하기 어려운 복잡한 모델들을 구현할 수 있습니다.\\n단점 : 입력의 크기(shape)를 명시한 입력층(Input layer)을 모델의 앞단에 정의해주어야 합니다. 가령, 아래의 코드를 봅시다.\\n# 선형 회귀 구현 코드의 일부 발췌\\ninputs = Input(shape=(1,)) # <-- 해당 부분\\noutput = Dense(1, activation='linear')(inputs)\", \"inputs = Input(shape=(1,)) # <-- 해당 부분\\noutput = Dense(1, activation='linear')(inputs)\\nlinear_model = Model(inputs, output)\\nsgd = optimizers.SGD(lr=0.01)\\nlinear_model.compile(optimizer=sgd, loss='mse', metrics=['mse'])\\nlinear_model.fit(X, y, epochs=300)\\n3) Subclassing API\\n장점 : Functional API로도 구현할 수 없는 모델들조차 구현이 가능합니다.\\n단점 : 객체 지향 프로그래밍(Object-oriented programming)에 익숙해야 하므로 코드 사용이 가장 까다롭습니다.\\n==================================================\", '==================================================\\n--- 07-11 다층 퍼셉트론(MultiLayer Perceptron, MLP)으로 텍스트 분류하기 ---\\n```\\nbinary 모드의 테스트 정확도: 0.8312533\\ncount 모드의 테스트 정확도: 0.8239511\\ntfidf 모드의 테스트 정확도: 0.8381572\\nfreq 모드의 테스트 정확도: 0.6902549\\n```다층 퍼셉트론(Multilayer Perceptron, MLP)으로 텍스트 분류를 수행합니다.']\n",
      "['앞서 단층 퍼셉트론의 형태에서 은닉층이 1개 이상 추가된 신경망을 다층 퍼셉트론(MLP)이라고 한다고 배웠습니다. 다층 퍼셉트론은 피드 포워드 신경망(Feed Forward Neural Network, FFNN)의 가장 기본적인 형태입니다. 피드 포워드 신경망은 입력층에서 출력층으로 오직 한 방향으로만 연산 방향이 정해져 있는 신경망을 말합니다.\\n뒤에서는 순환 신경망(RNN)과 분산 표현(distributed representation)이라는 새로운 개념들을 사용하여 각종 자연어 처리 실습을 하게 될텐데, 이번 실습의 목적은 위 두 가지 개념없이 지금까지 배운 개념만으로도 자연어 처리를 할 수 있다는 것을 보여주기 위함입니다.']\n",
      "[\"MLP로 텍스트 분류를 수행하기 전에 이번에 사용할 도구인 케라스 Tokenizer의 texts_to_matrix()를 이해해봅시다.\\nimport numpy as np\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\n우선 케라스의 전처리 도구인 Tokenizer를 임포트합니다.\\ntexts = ['먹고 싶은 사과', '먹고 싶은 바나나', '길고 노란 바나나 바나나', '저는 과일이 좋아요']\\n위 텍스트 데이터에 대해서 정수 인코딩을 수행합니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(texts)\\nprint(tokenizer.word_index)\\n{'바나나': 1, '먹고': 2, '싶은': 3, '사과': 4, '길고': 5, '노란': 6, '저는': 7, '과일이': 8, '좋아요': 9}\", \"{'바나나': 1, '먹고': 2, '싶은': 3, '사과': 4, '길고': 5, '노란': 6, '저는': 7, '과일이': 8, '좋아요': 9}\\n각 단어에 숫자 1부터 시작하는 정수 인덱스가 부여되었습니다. 텍스트 데이터에 texts_to_matrix()를 사용해보겠습니다. texts_to_matrix()란 이름에서 알 수 있지만, 이 도구는 입력된 텍스트 데이터로부터 행렬(matrix)를 만드는 도구입니다. texts_to_matrx()는 총 4개의 모드를 지원하는데 각 모드는 'binary', 'count', 'freq', 'tfidf'로 총 4개입니다. 우선 'count' 모드를 사용해봅시다.\\nprint(tokenizer.texts_to_matrix(texts, mode = 'count')) # texts_to_matrix의 입력으로 texts를 넣고, 모드는 'count'\\n[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\", \"[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\\n[0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\\n[0. 2. 0. 0. 0. 1. 1. 0. 0. 0.]\\n[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\\n위의 경우는 총 4개의 모드 중에서 'count' 모드를 사용했을 경우입니다. 'count'를 사용하면 우리가 앞서 배운 문서 단어 행렬(Document-Term Matrix, DTM)을 생성합니다. DTM에서의 인덱스는 앞서 확인한 word_index의 결과입니다.\\n다만 주의할 점은 각 단어에 부여되는 인덱스는 1부터 시작하는 반면에 완성되는 행렬의 인덱스는 0부터 시작합니다. 실제로 단어의 개수는 9개였지만 완성된 행렬의 열의 개수는 10개인 것과 첫번째 열은 모든 행에서 값이 0인 것을 볼 수 있습니다. 인덱스 0에는 그 어떤 단어도 할당되지 않았기 때문입니다.\", \"우선, 네번째 행을 보겠습니다. 네번째 행은 테스트 데이터에서 네번째 문장을 의미합니다. 네번째 행은 8번째 열, 9번째 열, 10번째 열에서 1의 값을 가집니다. 이는 7번 단어, 8번 단어, 9번 단어가 네번째 문장에서 1개씩 존재함을 의미합니다. 위에서 정수 인코딩 된 결과를 보면 7번 단어는 '저는', 8번 단어는 '과일이', 9번 단어는 '좋아요'입니다. 세번째 행의 첫번째 열의 값은 2인데, 이는 세번째 문장에서 1번 인덱스를 가진 바나나가 두 번 등장했기 때문입니다.\\n앞서 배웠듯이 DTM은 bag of words를 기반으로 하므로 단어 순서 정보는 보존되지 않습니다. 사실 더 구체적으로는 4개의 모든 모드에서 단어 순서 정보는 보존되지 않습니다. 'binary' 모드를 보겠습니다.\\nprint(tokenizer.texts_to_matrix(texts, mode = 'binary'))\\n[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\", \"print(tokenizer.texts_to_matrix(texts, mode = 'binary'))\\n[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\\n[0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\\n[0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\\n[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\\nDTM과 결과가 매우 유사해보입니다. 다만 세번째 행, 두번째 열의 값이 DTM에서는 2였는데 여기서는 1로 바뀌었습니다. 그 이유는 'binary' 모드는 해당 단어가 존재하는지만 관심을 가지고 해당 단어가 몇 개였는지는 무시하기 때문입니다. 해당 단어가 존재하면 1, 단어가 존재하지 않으면 0의 값을 가집니다. 즉, 단어의 존재 유무로만 행렬을 표현합니다. 'tfidf' 모드를 보겠습니다.\\nprint(tokenizer.texts_to_matrix(texts, mode = 'tfidf').round(2)) # 둘째 자리까지 반올림하여 출력\", \"print(tokenizer.texts_to_matrix(texts, mode = 'tfidf').round(2)) # 둘째 자리까지 반올림하여 출력\\n[[0.   0.   0.85 0.85 1.1  0.   0.   0.   0.   0.  ]\\n[0.   0.85 0.85 0.85 0.   0.   0.   0.   0.   0.  ]\\n[0.   1.43 0.   0.   0.   1.1  1.1  0.   0.   0.  ]\\n[0.   0.   0.   0.   0.   0.   0.   1.1  1.1  1.1 ]]\", \"[0.   0.   0.   0.   0.   0.   0.   1.1  1.1  1.1 ]]\\n'tfidf' 모드는 말 그대로 TF-IDF 행렬을 만듭니다. 다만, TF-IDF 실습에서 배운 기본식이나 사이킷런의 TfidfVectorizer에서 사용하는 식이랑 또 조금 다릅니다. 앞서 배운 기본식에서 TF는 각 문서에서의 각 단어의 빈도였다면 'tfidf' 모드에서는 TF를 각 문서에서의 각 단어의 빈도에 자연 로그를 씌우고 1을 더한 값으로 정의했습니다. idf에서는 앞서 배운 기본식에서 로그는 자연 로그를 사용하고, 로그 안의 분수에 1을 추가로 더했습니다. 물론, 이러한 식을 굳이 기억할 필요는 없고 여전히 TF-IDF의 기존 의도를 갖고 있다고 이해하면 됩니다.\\nprint(tokenizer.texts_to_matrix(texts, mode = 'freq').round(2)) # 둘째 자리까지 반올림하여 출력\", \"print(tokenizer.texts_to_matrix(texts, mode = 'freq').round(2)) # 둘째 자리까지 반올림하여 출력\\n[[0.   0.   0.33 0.33 0.33 0.   0.   0.   0.   0.  ]\\n[0.   0.33 0.33 0.33 0.   0.   0.   0.   0.   0.  ]\\n[0.   0.5  0.   0.   0.   0.25 0.25 0.   0.   0.  ]\\n[0.   0.   0.   0.   0.   0.   0.   0.33 0.33 0.33]]\", \"[0.   0.   0.   0.   0.   0.   0.   0.33 0.33 0.33]]\\n마지막으로 'freq' 모드입니다. 'freq' 모드는 각 문서에서의 각 단어의 등장 횟수를 분자로, 각 문서의 크기(각 문서에서 등장한 모든 단어의 개수의 총 합)를 분모로 하는 표현 방법입니다. 예를 들어 세번째 행을 보겠습니다. 세번째 문장은 '길고 노란 바나나 바나나' 였습니다. 문서의 크기는 4인데, 바나나는 총 2회 등장했습니다. 이에 따라서 세번째 문장에서의 단어 '바나나'의 값은 위의 행렬에서 0.5가 됩니다. 반면에 '길고', '노란'이라는 두 단어는 각 1회 등장했으므로 각자 1/4의 값인 0.25의 값을 가집니다.\"]\n",
      "[\"import pandas as pd\\nfrom sklearn.datasets import fetch_20newsgroups\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.utils import to_categorical\\n사이킷런에서는 20개의 다른 주제를 가진 18,846개의 뉴스 그룹 이메일 데이터를 제공합니다.\\nnewsdata = fetch_20newsgroups(subset = 'train') # 'train'을 기재하면 훈련 데이터만 리턴한다.\", \"newsdata = fetch_20newsgroups(subset = 'train') # 'train'을 기재하면 훈련 데이터만 리턴한다.\\n위의 subset의 값으로 'all'을 넣으면 전체 데이터인 18,846개의 샘플을 다운로드할 수 있으며, 'train'을 넣으면 훈련 데이터를, 'test'를 넣으면 테스트 데이터를 다운로드할 수 있습니다. newsdata.keys()를 출력하면 해당 데이터의 속성을 확인할 수 있습니다.\\nprint(newsdata.keys())\\ndict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\", \"print(newsdata.keys())\\ndict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\\n해당 데이터는 data, filenames, target_names, target, DESCR, description이라는 6개 속성을 갖고 있습니다. 이 중 실제로 훈련에 사용할 속성은 이메일 본문인 data와 메일이 어떤 주제인지 기재된 숫자 레이블인 target입니다. 우선 훈련용 샘플의 개수를 보겠습니다.\\nprint('훈련용 샘플의 개수 : {}'.format(len(newsdata.data)))\\n훈련용 샘플의 개수 : 11314\\n훈련 샘플은 11,314개가 존재합니다. target_names에는 20개의 주제의 이름을 담고있습니다. 어떤 주제가 있는지 확인해보겠습니다.\\nprint('총 주제의 개수 : {}'.format(len(newsdata.target_names)))\", \"print('총 주제의 개수 : {}'.format(len(newsdata.target_names)))\\nprint(newsdata.target_names)\\n총 주제의 개수 : 20\\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\", \"이번 실습의 목적은 테스트 데이터에서 이메일 본문을 보고 20개의 주제 중 어떤 주제인지를 맞추는 것입니다. 레이블인 target에는 총 0부터 19까지의 숫자가 들어가있는데 첫번째 샘플의 경우에는 몇 번 주제인지 확인해보겠습니다.\\nprint('첫번째 샘플의 레이블 : {}'.format(newsdata.target[0]))\\n첫번째 샘플의 레이블 : 7\\n첫번째 샘플의 레이블의 값은 7입니다. 숫자만으로는 앞서 target_names를 통해 확인한 20개의 주제 중에서 어떤 주제를 의미하는지 알 수가 없습니다. 7이 실제로 어떤 주제를 나타내는지는 target_names[] 안에 숫자를 입력하여 알 수 있습니다.\\nprint('7번 레이블이 의미하는 주제 : {}'.format(newsdata.target_names[7]))\\n7번 레이블이 의미하는 주제 : rec.autos\", \"print('7번 레이블이 의미하는 주제 : {}'.format(newsdata.target_names[7]))\\n7번 레이블이 의미하는 주제 : rec.autos\\n7번 레이블은 rec.autos라는 주제입니다. 즉, 첫번째 샘플의 주제는 rec.autos입니다. 첫번째 샘플의 본문 내용을 확인해봅시다.\\nprint(newsdata.data[0]) # 첫번째 샘플 출력\\nFrom: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\nI was wondering if anyone out there could enlighten me on this car I saw\", 'Lines: 15\\nI was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is\\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\nThanks,\\n- IL', \"have on this funky looking car, please e-mail.\\nThanks,\\n- IL\\n---- brought to you by your neighborhood Lerxst ----\\n이메일의 내용을 보니 스포츠 카에 대한 글로 보입니다. 이 글의 레이블은 7이고, 7번 레이블은 rec.autos란 주제를 의미합니다. 훈련에 사용될 메일 본문인 data와 레이블인 target을 데이터프레임으로 만들어서 데이터에 대한 통계적인 정보들을 알아보겠습니다.\\ndata로부터 데이터프레임을 생성하고, target 열을 추가한 뒤에 상위 5개의 행을 출력합니다.\\ndata = pd.DataFrame(newsdata.data, columns = ['email'])\\ndata['target'] = pd.Series(newsdata.target)\\ndata[:5]\\n[이미지: ]\", \"data['target'] = pd.Series(newsdata.target)\\ndata[:5]\\n[이미지: ]\\n메일 본문에 해당하는 email열과 레이블에 해당되는 target 열, 2개의 열로 구성된 데이터프레임이 생성된 것을 볼 수 있습니다.\\ndata.info()\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 11314 entries, 0 to 11313\\nData columns (total 2 columns):\\nnews      11314 non-null object\\ntarget    11314 non-null int32\\ndtypes: int32(1), object(1)\\nmemory usage: 132.7+ KB\\nnews열은 문자열, target열은 정수형 데이터입니다. 혹시 Null 값을 가진 샘플이 있는지 isnull().values.any()로도 확인 가능합니다.\\ndata.isnull().values.any()\\nFalse\", \"data.isnull().values.any()\\nFalse\\nFalse는 데이터에 별도의 Null 값은 없음을 의미합니다. nunique()를 통해 샘플 중 중복을 제거한 개수를 확인할 수 있습니다.\\nprint('중복을 제외한 샘플의 수 : {}'.format(data['email'].nunique()))\\nprint('중복을 제외한 주제의 수 : {}'.format(data['target'].nunique()))\\n중복을 제외한 샘플의 수 : 11314\\n중복을 제외한 주제의 수 : 20\\n레이블 값의 분포를 시각화해보겠습니다.\\ndata['target'].value_counts().plot(kind='bar');\\n[이미지: ]\\n10번 레이블의 수가 가장 많고, 19번 레이블의 수가 가장 적으며 대체적으로 400 ~ 600개 사이의 분포를 보입니다. 이번에는 각 레이블이 몇 개 있는지 구체적인 수치로 확인해보겠습니다.\", \"print(data.groupby('target').size().reset_index(name='count'))\\ntarget  count\\n0        0    480\\n1        1    584\\n2        2    591\\n3        3    590\\n4        4    578\\n5        5    593\\n6        6    585\\n7        7    594\\n8        8    598\\n9        9    597\\n10      10    600\\n11      11    595\\n12      12    591\\n13      13    594\\n14      14    593\\n15      15    599\\n16      16    546\\n17      17    564\\n18      18    465\\n19      19    377\", \"15      15    599\\n16      16    546\\n17      17    564\\n18      18    465\\n19      19    377\\n데이터프레임으로부터 다시 메일 본문과 레이블을 분리하고, 테스트 데이터 또한 불러오겠습니다. subset에 'test'를 기재하면 테스트 데이터를 불러옵니다. 훈련 데이터와 테스트 데이터의 본문과 레이블을 각각 저장합니다.\\nnewsdata_test = fetch_20newsgroups(subset='test', shuffle=True)\\ntrain_email = data['email']\\ntrain_label = data['target']\\ntest_email = newsdata_test.data\\ntest_label = newsdata_test.target\\n훈련 데이터와 테스트 데이터가 모두 준비되었습니다. 케라스의 토크나이저 도구를 사용하여 전처리를 진행해봅시다.\\nvocab_size = 10000\\nnum_classes = 20\", '훈련 데이터와 테스트 데이터가 모두 준비되었습니다. 케라스의 토크나이저 도구를 사용하여 전처리를 진행해봅시다.\\nvocab_size = 10000\\nnum_classes = 20\\n우선 필요한 변수들을 정의합니다. vocab_size는 이번 실습에서 사용할 최대 단어 개수를 정의하는 변수입니다. 뒤에서 케라스 토크나이저를 사용하면 빈도수 순으로 인덱스를 부여하므로, 빈도수가 가장 높은 상위 vocab_size 개수만큼의 단어를 사용합니다.\\ndef prepare_data(train_data, test_data, mode): # 전처리 함수\\ntokenizer = Tokenizer(num_words = vocab_size) # vocab_size 개수만큼의 단어만 사용한다.\\ntokenizer.fit_on_texts(train_data)\\nX_train = tokenizer.texts_to_matrix(train_data, mode=mode) # 샘플 수 × vocab_size 크기의 행렬 생성', \"X_train = tokenizer.texts_to_matrix(train_data, mode=mode) # 샘플 수 × vocab_size 크기의 행렬 생성\\nX_test = tokenizer.texts_to_matrix(test_data, mode=mode) # 샘플 수 × vocab_size 크기의 행렬 생성\\nreturn X_train, X_test, tokenizer.index_word\\n케라스 토크나이저로 전처리를 수행하는 함수인 prepare_data를 만들었습니다. 해당 함수는 케라스 토크나이저를 통해 단어 토큰화를 수행하고, 앞서 배운 texts_to_matrix()를 사용하여 훈련 데이터와 테스트 데이터를 'binary', 'count', 'tfidf', 'freq' 4개의 모드 중 사용자가 정한 모드로 변환합니다.\", \"X_train, X_test, index_to_word = prepare_data(train_email, test_email, 'binary') # binary 모드로 변환\\ny_train = to_categorical(train_label, num_classes) # 원-핫 인코딩\\ny_test = to_categorical(test_label, num_classes) # 원-핫 인코딩\\n메일 본문에 대해서는 'binary' 모드로 변환하고, 훈련 데이터와 테스트 데이터의 레이블은 원-핫 인코딩을 수행하였습니다.\\nprint('훈련 샘플 본문의 크기 : {}'.format(X_train.shape))\\nprint('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\\nprint('테스트 샘플 본문의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\", \"print('테스트 샘플 본문의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\\n훈련 샘플 본문의 크기 : (11314, 10000)\\n훈련 샘플 레이블의 크기 : (11314, 20)\\n테스트 샘플 본문의 크기 : (7532, 10000)\\n테스트 샘플 레이블의 크기 : (7532, 20)\", \"훈련 샘플 레이블의 크기 : (11314, 20)\\n테스트 샘플 본문의 크기 : (7532, 10000)\\n테스트 샘플 레이블의 크기 : (7532, 20)\\n훈련 데이터와 테스트 데이터 모두 메일 본문의 크기가 샘플의 수 × 10,000의 행렬로 변환되었는데, 열의 개수가 10,000인 것은 위의 prepard_data 함수 내부에서 Tokenizer의 num_words의 인자로 vocab_size를 지정해주었기 때문입니다. 사실 단어의 정수 인덱스는 1부터 시작하지만, 행렬의 인덱스는 0부터 시작하여 0번 인덱스는 사용되지 않으므로 실제로 행렬에는 빈도수 기준 상위 9,999개의 단어가 표현된 셈입니다. 빈도수 상위 1번 단어와 9,999번 단어를 확인해보겠습니다.\\nprint('빈도수 상위 1번 단어 : {}'.format(index_to_word[1]))\\nprint('빈도수 상위 9999번 단어 : {}'.format(index_to_word[9999]))\", \"print('빈도수 상위 9999번 단어 : {}'.format(index_to_word[9999]))\\n빈도수 상위 1번 단어 : the\\n빈도수 상위 9999번 단어 : mic\\n불용어에 해당되는 단어 'the'가 빈도수 상위 1번 단어가 된 것을 확인할 수 있습니다.\"]\n",
      "[\"모델을 설계해보겠습니다. 우선 모델 설계에 필요한 도구들을 임포트합니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Dropout\\n다층 퍼셉트론을 설계합니다.\\ndef fit_and_evaluate(X_train, y_train, X_test, y_test):\\nmodel = Sequential()\\nmodel.add(Dense(256, input_shape=(vocab_size,), activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(num_classes, activation='softmax'))\", \"model.add(Dropout(0.5))\\nmodel.add(Dense(num_classes, activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.1)\\nscore = model.evaluate(X_test, y_test, batch_size=128, verbose=0)\\nreturn score[1]\\n모델 설계를 fit_and_evaluate라는 함수 내에 정의하였는데, 모델을 함수 내에 정의한 이유는 이번 실습에서는 입력값을 바꿔가면서 모델을 여러번 호출하기 위함입니다. 우선은 모델의 아키텍처에 집중해보겠습니다.\\n[이미지: ]\", '[이미지: ]\\n위의 그림은 현재 설계한 신경망의 구조를 보여줍니다. 현재 설계한 다층 퍼셉트론은 총 4개의 층을 가지고 있습니다. vocab_size의 크기를 가진 입력층, 256개의 뉴런을 가진 첫번째 은닉층, 128개의 뉴런을 가진 두번째 은닉층, num_classes의 크기를 가진 출력층입니다. 또한 이번에 설계한 다층 퍼셉트론은 은닉층이 2개이므로 깊은 신경망(Deep Neural Network, DNN)입니다.', \"코드로 돌아가보겠습니다. 위 모델에서는 과적합을 막기 위해서 두 번의 드롭아웃(Dropout)을 적용하였습니다. 이 문제는 다중 클래스 분류 문제입니다. 여러 개의 선택지 중에서 하나의 선택지를 고르는 문제인데, 이 경우 20개의 주제 중에서 모델은 자신이 정답이라고 생각하는 1개의 주제를 예측해야 합니다. 다중 클래스 분류 문제이므로 출력층의 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로는 크로스 엔트로피(categorical_crossentropy) 함수를 사용하였습니다.\\n모델을 훈련시켜보겠습니다. 이번에는 앞서 배운 texts_to_matrix()의 4개의 모드에 대해서 전부 모델의 결과를 확인해보겠습니다.\\nmodes = ['binary', 'count', 'tfidf', 'freq'] # 4개의 모드를 리스트에 저장.\\nfor mode in modes: # 4개의 모드에 대해서 각각 아래의 작업을 반복한다.\", \"for mode in modes: # 4개의 모드에 대해서 각각 아래의 작업을 반복한다.\\nX_train, X_test, _ = prepare_data(train_email, test_email, mode) # 모드에 따라서 데이터를 전처리\\nscore = fit_and_evaluate(X_train, y_train, X_test, y_test) # 모델을 훈련하고 평가.\\nprint(mode+' 모드의 테스트 정확도:', score)\\n각 모드에 대해서 총 5회의 에포크를 수행하는데, 각 모드에 대한 정확도는 다음과 같이 나왔습니다.\\nbinary 모드의 테스트 정확도: 0.8312533\\ncount 모드의 테스트 정확도: 0.8239511\\ntfidf 모드의 테스트 정확도: 0.8381572\\nfreq 모드의 테스트 정확도: 0.6902549\", \"count 모드의 테스트 정확도: 0.8239511\\ntfidf 모드의 테스트 정확도: 0.8381572\\nfreq 모드의 테스트 정확도: 0.6902549\\n대체적으로 82% ~ 83%의 비슷한 정확도를 보이는데, 'freq' 모드에서만 정확도가 69%가 나왔습니다. 아무래도 'freq' 모드는 이번 문제를 풀기위한 적절한 전처리 방법이 아니었던 것 같습니다.\\n==================================================\\n--- 07-12 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM) ---\\n```\\nwhat = [1, 0, 0, 0, 0, 0, 0]\\nwill = [0, 1, 0, 0, 0, 0, 0]\\nthe = [0, 0, 1, 0, 0, 0, 0]\\nfat = [0, 0, 0, 1, 0, 0, 0]\\ncat = [0, 0, 0, 0, 1, 0, 0]\\nsit = [0, 0, 0, 0, 0, 1, 0]\", 'fat = [0, 0, 0, 1, 0, 0, 0]\\ncat = [0, 0, 0, 0, 1, 0, 0]\\nsit = [0, 0, 0, 0, 0, 1, 0]\\non = [0, 0, 0, 0, 0, 0, 1]\\n```파이썬 등과 같은 프로그래밍 언어를 사용할 때는 명세되어져 있는 튜플, 클래스 등과 같은 용어와 작성할 때 지켜야 하는 문법을 바탕으로 코드를 작성합니다. 문법에 맞지 않으면 에러가 발생하므로 명세된 규칙을 지키는 것은 필수적입니다.\\n자연어는 어떨까요? 자연어에도 문법이라는 규칙이 있기는 하지만, 많은 예외 사항, 시간에 따른 언어의 변화, 중의성과 모호성 문제 등을 전부 명세하기란 어렵습니다. 기계가 자연어를 표현하도록 규칙으로 명세하기가 어려운 상황에서 대안은 규칙 기반 접근이 아닌 기계가 주어진 자연어 데이터를 학습하게 하는 것입니다.', '과거에는 기계가 자연어를 학습하게 하는 방법으로 통계적인 접근을 사용했으나, 최근에는 인공 신경망을 사용하는 방법이 자연어 처리에서 더 좋은 성능을 얻고 있습니다. 번역기, 음성 인식 등과 같이 자연어 생성(Natural Language Generation, NLG)의 기반으로 사용되는 언어 모델도 마찬가지입니다. 통계적 언어 모델(Statistical Language Model, SLM)에서 다양한 구조의 인공 신경망을 사용한 언어 모델들로 대체되기 시작했습니다.\\n여기서는 신경망 언어 모델의 시초인 피드 포워드 신경망 언어 모델(Feed Forward Neural Network Language Model)에 대해서 학습합니다. 여기서는 간단히 줄여 NNLM이라고 합시다. 뒤에서 RNNLM, BiLM 등 보다 발전된 신경망 언어 모델들을 배웁니다.\\n이 모델은 제안 되었을 당시 NPLM(Neural Probabilistic Language Model)이라는 이름을 갖고 있었습니다.']\n",
      "['언어 모델은 문장에 확률을 할당하는 모델이며, 주어진 문맥으로부터 아직 모르는 단어를 예측하는 것을 언어 모델링이라고 한다고 언급한 바 있습니다. 다음은 이전 단어들로부터 다음 단어를 예측하는 언어 모델링(Language Modeling) 의 예를 보여줍니다.\\n# 다음 단어 예측하기\\nAn adorable little boy is spreading ____\\n위 문장을 가지고 앞서 배운 n-gram 언어 모델이 언어 모델링을 하는 방법을 복습해봅시다.\\n[이미지: ]\\nn-gram 언어 모델은 언어 모델링에 바로 앞 n-1개의 단어를 참고합니다. 4-gram 언어 모델이라고 가정해봅시다. 모델은 바로 앞 3개의 단어만 참고하며 더 앞의 단어들은 무시합니다. 위 예제에서 다음 단어 예측에 사용되는 단어는 boy, is, spreading입니다.', '$$P(w\\\\text{|boy is spreading}) = \\\\frac{\\\\text{count(boy is spreading}\\\\ w)}{\\\\text{count(boy is spreading)}}$$\\n그 후에는 훈련 코퍼스에서 (n-1)-gram을 카운트한 것을 분모로, n-gram을 카운트한 것을 분자로 하여 다음 단어가 등장 확률을 예측했습니다. 예를 들어 갖고있는 코퍼스에서 boy is spreading가 1,000번, boy is spreading insults가 500번, boy is spreading smiles가 200번 등장했다면 각 확률은 아래와 같습니다.\\n$$P(\\\\text{insults|boy is spreading}) = 0.500$$\\n$$P(\\\\text{smiles|boy is spreading}) = 0.200$$', \"$$P(\\\\text{insults|boy is spreading}) = 0.500$$\\n$$P(\\\\text{smiles|boy is spreading}) = 0.200$$\\n하지만 이러한 n-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소 문제(sparsity problem) 가 있었습니다. 예를 들어 훈련 코퍼스에 '$\\\\text{boy is spreading smile}$'라는 단어 시퀀스가 존재하지 않으면 n-gram 언어 모델에서 해당 단어 시퀀스의 확률 $P(\\\\text{smiles|boy is spreading})$는 0이 됩니다. 이는 언어 모델이 판단하기에 boy is spreading 다음에는 smiles이란 단어가 나올 수 없다는 의미이지만 해당 단어 시퀀스는 현실에서 존재 가능한 시퀀스이므로 적절한 모델링이 아닙니다.\"]\n",
      "[\"희소 문제는 기계가 단어의 의미적 유사성을 알수 있다면 해결할 수 있는 문제입니다. 실제 사람의 사례를 들어 이야기해보겠습니다. 저자는 최근 '톺아보다'라는 생소한 단어를 배웠고, '톺아보다'가 '샅샅이 살펴보다'와 유사한 의미임을 학습했습니다. 그리고 '발표 자료를 살펴보다'라는 표현 대신 '발표 자료를 톺아보다'라는 표현을 써봤습니다. 저는 '발표 자료를 톺아보다'라는 예문을 어디서 읽은 적은 없지만 두 단어가 유사함을 학습하였으므로 단어를 대신 선택하여 자연어 생성을 할 수 있었습니다.\\n기계도 마찬가지입니다. '발표 자료를 살펴보다'라는 단어 시퀀스는 존재하지만, '발표 자료를 톺아보다'라는 단어 시퀀스는 존재하지 않는 코퍼스를 학습한 언어 모델이 있다고 가정해봅시다. 언어 모델은 아래 선택지에서 다음 단어를 예측해야 합니다.\\n$$P(\\\\text{톺아보다|발표 자료를})$$\\n$$P(\\\\text{냠냠하다|발표 자료를})$$\", \"$$P(\\\\text{톺아보다|발표 자료를})$$\\n$$P(\\\\text{냠냠하다|발표 자료를})$$\\n저자의 경우에는 '살펴보다'와 '톺아보다'의 유사성을 학습하였고 이를 근거로 두 선택지 중에서 '톺아보다'가 더 맞는 선택이라고 판단할 수 있습니다. 하지만 n-gram 언어 모델은 '발표 자료를' 다음에 '톺아보다'가 나올 확률 $P(\\\\text{톺아보다|발표 자료를})$를 0으로 연산합니다. n-gram 언어 모델은 '살펴보다'와 '톺아보다'의 단어의 유사도를 알 수 없으므로 예측에 고려할 수 없습니다.\", '만약 언어 모델 또한 단어의 의미적 유사성을 학습할 수 있도록 설계한다면, 훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측을 할 수 있습니다. 그리고 이러한 아이디어를 반영한 언어 모델이 신경망 언어 모델 NNLM입니다. 그리고 이 아이디어는 단어 벡터 간 유사도를 구할 수 있는 벡터를 얻어내는 워드 임베딩(word embedding) 의 아이디어이기도 합니다. NNLM이 어떻게 훈련 과정에서 단어의 유사도를 학습할 수 있는지 알아봅시다.']\n",
      "['NNLM이 언어 모델링을 학습하는 과정을 보겠습니다. 이해를 위해 간소화 된 형태로 설명합니다.\\n예문 : \"what will the fat cat sit on\"\\n예를 들어 훈련 코퍼스에 위와 같은 문장이 있다고 해봅시다. 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측합니다. 훈련 과정에서는 \\'what will the fat cat\\'이라는 단어 시퀀스가 입력으로 주어지면, 다음 단어 \\'sit\\'을 예측하는 방식으로 훈련됩니다.\\n훈련 코퍼스가 준비된 상태에서 가장 먼저 해야 할 일은 기계가 단어를 인식할 수 있도록 모든 단어를 수치화하는 것입니다. 훈련 코퍼스에 7개의 단어만 존재한다고 가정했을 때 위 단어들을 다음과 같이 원-핫 인코딩 할 수 있습니다.\\nwhat = [1, 0, 0, 0, 0, 0, 0]\\nwill = [0, 1, 0, 0, 0, 0, 0]\\nthe = [0, 0, 1, 0, 0, 0, 0]\\nfat = [0, 0, 0, 1, 0, 0, 0]', \"will = [0, 1, 0, 0, 0, 0, 0]\\nthe = [0, 0, 1, 0, 0, 0, 0]\\nfat = [0, 0, 0, 1, 0, 0, 0]\\ncat = [0, 0, 0, 0, 1, 0, 0]\\nsit = [0, 0, 0, 0, 0, 1, 0]\\non = [0, 0, 0, 0, 0, 0, 1]\\n모든 단어가 단어 집합(vocabulary)의 크기인 7의 차원을 가지는 원-핫 벡터가 되었습니다. 이 원-핫 벡터들이 훈련을 위한 NNLM의 입력이면서 예측을 위한 레이블이 됩니다. 'what will the fat cat'를 입력을 받아서 'sit'을 예측하는 일은 기계에게 what, will, the, fat, cat의 원-핫 벡터를 입력받아 sit의 원-핫 벡터를 예측하는 문제입니다.\", \"NNLM은 n-gram 언어 모델처럼 다음 단어를 예측할 때, 앞의 모든 단어를 참고하는 것이 아니라 정해진 개수의 단어만을 참고합니다. 이 개수를 n이라고 하고 n을 4라고 해봅시다. 이때, 언어 모델은 'what will the fat cat'라는 단어 시퀀스가 주어졌을 때, 다음 단어를 예측하기 위해 앞의 4개 단어 'will the fat cat'까지만 참고하고 그 앞 단어인 what은 무시합니다. 이 범위를 윈도우(window)라고 하기도 하는데, 여기서 윈도우의 크기인 n은 4입니다.\\n[이미지: ]\", \"[이미지: ]\\nNNLM의 구조를 보겠습니다. NNLM은 위의 그림과 같이 총 4개의 층(layer)으로 이루어진 인공 신경망입니다. 입력층(input layer)을 보면 앞에서 윈도우의 크기는 4로 정하였으므로 입력은 4개의 단어 'will, the, fat, cat'의 원-핫 벡터입니다. 출력층(output layer)을 보면 모델이 예측해야하는 정답에 해당되는 단어 sit의 원-핫 벡터는 모델이 예측한 값의 오차를 구하기 위해 레이블로서 사용됩니다. 그리고 오차로부터 손실 함수를 사용하여 인공 신경망이 학습을 하게 됩니다.\\n내부 메커니즘을 따라가봅시다. 4개의 원-핫 벡터를 입력 받은 NNLM은 다음층인 투사층(projection layer)을 지나게 됩니다. 인공 신경망에서 입력층과 출력층 사이의 층은 보통 은닉층이라고 부르는데, 여기서 투사층이라고 명명한 이 층은 일반 은닉층과 다르게 가중치 행렬과의 곱셈은 이루어지지만 활성화 함수가 존재하지 않습니다.\", '투사층의 크기를 M으로 설정하면, 각 입력 단어들은 투사층에서 V × M 크기의 가중치 행렬과 곱해집니다. 여기서 V는 단어 집합의 크기를 의미합니다. 만약 원-핫 벡터의 차원이 7이고, M이 5라면 가중치 행렬 W는 7 × 5 행렬이 됩니다.\\n[이미지: ]\\n각 단어의 원-핫 벡터와 가중치 W 행렬의 곱이 어떻게 이루어지는지 보겠습니다. 위 그림에서는 각 원-핫 벡터를 $x$로 표기하였습니다. 원-핫 벡터의 특성으로 인해 i번째 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 원-핫 벡터와 가중치 W 행렬의 곱은 사실 W행렬의 i번째 행을 그대로 읽어오는 것과(lookup) 동일합니다. 그래서 이 작업을 룩업 테이블(lookup table)이라고 합니다.', '룩업 테이블 후에는 V차원을 가지는 원-핫 벡터는 이보다 더 차원이 작은 M차원의 벡터로 맵핑됩니다. 위 그림에서 단어 fat을 의미하는 원-핫 벡터를 $x_{fat}$으로 표현했고, 테이블 룩업 과정을 거친 후의 단어 벡터는 $e_{fat}$으로 표현했습니다. 이 벡터들은 초기에는 랜덤한 값을 가지지만 학습 과정에서 값이 계속 변경되는데 이 단어 벡터를 임베딩 벡터(embedding vector) 라고 합니다.\\n[이미지: ]', '[이미지: ]\\n각 단어가 테이블 룩업을 통해 임베딩 벡터로 변경되고, 투사층에서 모든 임베딩 벡터들의 값은 연결됩니다(concatenate). 여기서 벡터의 연결 연산은 벡터들을 이어붙이는 것을 의미합니다. 가령, 5차원 벡터 4개를 연결한다는 의미는 20차원 벡터를 얻는다는 의미입니다. $x$를 각 단어의 원-핫 벡터, NNLM이 예측하고자 하는 단어가 문장에서 $t$번째 단어라고 하고, 윈도우의 크기를 $n$, 룩업 테이블을 의미하는 함수를 $lookup$, 세미콜론(;)을 연결 기호로 하였을 때 투사층을 식으로 표현하면 아래와 같습니다.\\n투사층 : $p^{layer} = (lookup(x_{t-n}); ...; lookup(x_{t-2}); lookup(x_{t-1})) = (e_{t-n}; ...; e_{t-2}; e_{t-1})$', '일반적인 은닉층이 활성화 함수를 사용하는 비선형층(nonlinear layer)인 것과는 달리 투사층은 활성화 함수가 존재하지 않는 선형층(linear layer)이라는 점이 다소 생소하지만, 이 다음은 다시 은닉층을 사용하는 일반적인 피드 포워드 신경망과 동일합니다.\\n[이미지: ]\\n투사층의 결과는 h의 크기를 가지는 은닉층을 지납니다. 일반적인 피드 포워드 신경망에서 은닉층을 지난다는 것은 은닉층의 입력은 가중치 곱해진 후 편향이 더해져 활성화 함수의 입력이 된다는 의미입니다. 이때의 가중치와 편향을 $W_{h}$와 $b_{h}$이라고 하고, 은닉층의 활성화 함수를 하이퍼볼릭탄젠트 함수라고 하였을 때, 은닉층을 식으로 표현하면 아래와 같습니다.\\n은닉층 : $h^{layer} = tanh(W_{h}p^{layer} + b_{h})$\\n[이미지: ]', '은닉층 : $h^{layer} = tanh(W_{h}p^{layer} + b_{h})$\\n[이미지: ]\\n은닉층의 출력은 V의 크기를 가지는 출력층으로 향합니다. 이 과정에서 다시 또 다른 가중치와 곱해지고 편향이 더해지면, 입력이었던 원-핫 벡터들과 동일하게 V차원의 벡터를 얻습니다. 만약 입력 벡터의 차원이 7이었다면 해당 벡터도 동일한 차원 수를 가집니다. 출력층에서는 활성화 함수로 소프트맥스(softmax) 함수를 사용하는데, V차원의 벡터는 소프트맥스 함수를 지나면서 벡터의 각 원소는 0과 1사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀝니다. 이 벡터를 NNLM의 예측값이라는 의미에서 $\\\\hat{y}$라고 합시다. 이를 식으로 표현하면 아래와 같습니다.\\n출력층 : $\\\\hat{y} = softmax(W_{y}h^{layer} + b_{y})$', \"출력층 : $\\\\hat{y} = softmax(W_{y}h^{layer} + b_{y})$\\n벡터 $\\\\hat{y}$의 각 차원 안에서의 값이 의미하는 것은 이와 같습니다. $\\\\hat{y}$의 j번째 인덱스가 가진 0과 1사이의 값은 j번째 단어가 다음 단어일 확률을 나타냅니다. 그리고 $\\\\hat{y}$는 실제값. 즉, 실제 정답에 해당되는 단어인 원-핫 벡터의 값에 가까워져야 합니다. 실제값에 해당되는 다음 단어를 $y$라고 했을 때, 이 두 벡터가 가까워지게 하기위해서 NNLM는 손실 함수로 크로스 엔트로피(cross-entropy) 함수를 사용합니다. 해당 문제는 단어 집합의 모든 단어라는 V개의 선택지 중 정답인 'sit'을 예측해야하는 다중 클래스 분류 문제입니다. 그리고 역전파가 이루어지면 모든 가중치 행렬들이 학습되는데, 여기에는 투사층에서의 가중치 행렬도 포함되므로 임베딩 벡터값 또한 학습됩니다.\", \"이번 예제에서는 7개의 단어만 사용했지만, 만약 충분한 훈련 데이터가 있다는 가정 하에 NNLM이 얻을 수 있는 이점은 무엇일까요? NNLM의 핵심은 충분한 양의 훈련 코퍼스를 위와 같은 과정으로 학습한다면 결과적으로 수많은 문장에서 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터값을 얻게되는 것에 있습니다. 이렇게 되면 훈련이 끝난 후 다음 단어를 예측 과정에서 (마치 앞서 언급한 저자의 '톺아보기'와 같은 예시처럼) 훈련 코퍼스에서 없던 단어 시퀀스라 하더라도 다음 단어를 선택할 수 있습니다.\\n단어 간 유사도를 구할 수 있는 임베딩 벡터의 아이디어는 Word2Vec, FastText, GloVe 등으로 발전되어서 딥 러닝 자연어 처리 모델에서는 필수적으로 사용되는 방법이 되었습니다. 임베딩 벡터에 대해서는 워드 임베딩 챕터에서 좀 더 자세히 다룹니다.\"]\n",
      "['NNLM은 기존 n-gram 언어 모델의 한계를 개선하였지만 여전히 가지는 문제점이 있습니다.\\n1) 기존 모델에서의 개선점\\nNNLM은 단어를 표현하기 위해 임베딩 벡터를 사용하므로서 단어의 유사도를 계산할 수 있었습니다. 그리고 이를 통해 희소 문제(sparsity problem)를 해결하였습니다.\\n2) 고정된 길이의 입력(Fixed-length input)\\nNNLM이 극복하지 못한 한계 또한 존재합니다. NNLM은 n-gram 언어 모델과 마찬가지로 다음 단어를 예측하기 위해 모든 이전 단어를 참고하는 것이 아니라 정해진 n개의 단어만을 참고할 수 있습니다. 이 한계를 극복할 수 있는 언어 모델이 있는데, 다음 챕터에서 배우게 될 RNN(Recurrent Neural Network)을 사용한 RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)입니다.', '==================================================\\n--- 08. 순환 신경망(Recurrent Neural Network) ---\\n마지막 편집일시 : 2022년 1월 2일 2:49 오후\\n==================================================\\n--- 08-01 순환 신경망(Recurrent Neural Network, RNN) ---\\n```\\nmodel = Sequential()\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))', 'model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\\n```RNN(Recurrent Neural Network)은 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델입니다. 번역기를 생각해보면 입력은 번역하고자 하는 단어의 시퀀스인 문장입니다. 출력에 해당되는 번역된 문장 또한 단어의 시퀀스입니다. 이와 같이 시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델이라고 합니다. 그 중 RNN은 가장 기본적인 인공 신경망 시퀀스 모델입니다.', \"뒤에서 배우는 LSTM이나 GRU 또한 근본적으로 RNN에 속합니다. RNN을 이해하고 'RNN을 이용한 텍스트 분류' 챕터, '태깅 작업' 챕터, 'RNN을 이용한 인코더-디코더' 챕터에서 실습을 진행합니다.\\n용어는 비슷하지만 순환 신경망과 재귀 신경망(Recursive Neural Network)은 전혀 다른 개념입니다.\"]\n",
      "['앞서 배운 신경망들은 전부 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향했습니다. 이와 같은 신경망들을 피드 포워드 신경망(Feed Forward Neural Network)이라고 합니다. 그런데 그렇지 않은 신경망들이 있습니다. RNN(Recurrent Neural Network) 또한 그 중 하나입니다. RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고있습니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림을 보겠습니다. $x$는 입력층의 입력 벡터, $y$는 출력층의 출력 벡터입니다. 실제로는 편향 $b$도 입력으로 존재할 수 있지만 앞으로의 그림에서는 생략합니다. RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 셀(cell)이라고 합니다. 이 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 메모리 셀 또는 RNN 셀이라고 표현합니다.\\n은닉층의 메모리 셀은 각각의 시점(time step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 하고 있습니다. 앞으로는 현재 시점을 변수 t로 표현하겠습니다. 이는 현재 시점 t에서의 메모리 셀이 갖고있는 값은 과거의 메모리 셀들의 값에 영향을 받은 것임을 의미합니다. 그렇다면 메모리 셀이 갖고 있는 이 값은 뭐라고 부를까요?', '메모리 셀이 출력층 방향 또는 다음 시점인 t+1의 자신에게 보내는 값을 은닉 상태(hidden state) 라고 합니다. 다시 말해 t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉 상태값을 t 시점의 은닉 상태 계산을 위한 입력값으로 사용합니다.\\n[이미지: ]\\nRNN을 표현할 때는 일반적으로 위의 그림에서 좌측과 같이 화살표로 사이클을 그려서 재귀 형태로 표현하기도 하지만, 우측과 같이 사이클을 그리는 화살표 대신 여러 시점으로 펼쳐서 표현하기도 합니다. 두 그림은 동일한 그림으로 단지 사이클을 그리는 화살표를 사용하여 표현하였느냐, 시점의 흐름에 따라서 표현하였느냐의 차이일 뿐 둘 다 동일한 RNN을 표현하고 있습니다.', '피드 포워드 신경망에서는 뉴런이라는 단위를 사용했지만, RNN에서는 뉴런이라는 단위보다는 입력층과 출력층에서는 각각 입력 벡터와 출력 벡터, 은닉층에서는 은닉 상태라는 표현을 주로 사용합니다. 위의 그림에서 회색과 초록색으로 표현한 각 네모들은 기본적으로 벡터 단위를 가정하고 있습니다. 피드 포워드 신경망과의 차이를 비교하기 위해서 RNN을 뉴런 단위로 시각화해보겠습니다.\\n[이미지: ]\\n위의 그림은 입력 벡터의 차원이 4, 은닉 상태의 크기가 2, 출력층의 출력 벡터의 차원이 2인 RNN이 시점이 2일 때의 모습을 보여줍니다. 다시 말해 뉴런 단위로 해석하면 입력층의 뉴런 수는 4, 은닉층의 뉴런 수는 2, 출력층의 뉴런 수는 2입니다.\\n[이미지: ]', \"[이미지: ]\\nRNN은 입력과 출력의 길이를 다르게 설계 할 수 있으므로 다양한 용도로 사용할 수 있습니다. 위 그림은 입력과 출력의 길이에 따라서 달라지는 RNN의 다양한 형태를 보여줍니다. 위 구조가 자연어 처리에서 어떻게 사용될 수 있는지 예를 들어봅시다. RNN 셀의 각 시점의 입, 출력의 단위는 사용자가 정의하기 나름이지만 가장 보편적인 단위는 '단어 벡터'입니다.\\n예를 들어 하나의 입력에 대해서 여러개의 출력을 의미하는 일 대 다(one-to-many) 구조의 모델은 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝(Image Captioning) 작업에 사용할 수 있습니다. 사진의 제목은 단어들의 나열이므로 시퀀스 출력입니다.\\n[이미지: ]\", \"[이미지: ]\\n또한 단어 시퀀스에 대해서 하나의 출력을 하는 다 대 일(many-to-one) 구조의 모델은 입력 문서가 긍정적인지 부정적인지를 판별하는 감성 분류(sentiment classification), 또는 메일이 정상 메일인지 스팸 메일인지 판별하는 스팸 메일 분류(spam detection) 등에 사용할 수 있습니다. 위 그림은 RNN으로 스팸 메일을 분류할 때의 아키텍처를 보여줍니다. 이러한 예제들은 'RNN을 이용한 텍스트 분류' 챕터에서 배웁니다.\\n[이미지: ]\\n다 대 다(many-to-many) 구조의 모델의 경우에는 사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇과 입력 문장으로부터 번역된 문장을 출력하는 번역기, 또는 '태깅 작업' 챕터에서 배우는 개체명 인식이나 품사 태깅과 같은 작업이 속합니다. 위 그림은 개체명 인식을 수행할 때의 RNN 아키텍처를 보여줍니다.\\nRNN에 대한 수식을 정의해보겠습니다.\\n[이미지: ]\", 'RNN에 대한 수식을 정의해보겠습니다.\\n[이미지: ]\\n현재 시점 t에서의 은닉 상태값을 $h_{t}$라고 정의하겠습니다. 은닉층의 메모리 셀은 $h_{t}$를 계산하기 위해서 총 두 개의 가중치를 가집니다. 하나는 입력층을 위한 가중치 $W_{x}$이고, 하나는 이전 시점 t-1의 은닉 상태값인 $h_{t-1}$을 위한 가중치 $W_{h}$입니다.\\n이를 식으로 표현하면 다음과 같습니다.\\n은닉층 : $h_{t} = tanh(W_{x} x_{t} + W_{h}h_{t−1} + b)$\\n출력층 : $y_{t} = f(W_{y}h_{t} + b)$\\n단, $f$는 비선형 활성화 함수 중 하나.\\nRNN의 은닉층 연산을 벡터와 행렬 연산으로 이해해봅시다. 자연어 처리에서 RNN의 입력 $x_{t}$는 대부분의 경우 단어 벡터로 간주할 수 있는데, 단어 벡터의 차원을 $d$라고 하고, 은닉 상태의 크기를 $D_{h}$라고 하였을 때 각 벡터와 행렬의 크기는 다음과 같습니다.', '$x_{t}$ : $(d × 1)$\\n$W_{x}$ : $(D_{h} × d)$\\n$W_{h}$ : $(D_{h} × D_{h})$\\n$h_{t-1}$ : $(D_{h} × 1)$\\n$b$ : $(D_{h} × 1)$\\n배치 크기가 1이고, $d$와 $D_{h}$ 두 값 모두를 4로 가정하였을 때, RNN의 은닉층 연산을 그림으로 표현하면 아래와 같습니다.\\n[이미지: ]\\n이때 $h_{t}$를 계산하기 위한 활성화 함수로는 주로 하이퍼볼릭탄젠트 함수(tanh)가 사용됩니다. 위의 식에서 각각의 가중치 $W_{x}$, $W_{h}$, $W_{y}$의 값은 하나의 층에서는 모든 시점에서 값을 동일하게 공유합니다. 하지만 은닉층이 2개 이상일 경우에는 각 은닉층에서의 가중치는 서로 다릅니다.', '출력층은 결과값인 $y_{t}$를 계산하기 위한 활성화 함수로는 푸는 문제에 따라서 다를텐데, 예를 들어서 이진 분류를 해야하는 경우라면 출력층에 로지스틱 회귀를 사용하여 시그모이드 함수를 사용할 수 있고 다중 클래스 분류를 해야하는 경우라면 출력층에 소프트맥스 회귀를 사용하여로 소프트맥스 함수를 사용할 수 있습니다.']\n",
      "['케라스로 RNN 층을 추가하는 코드는 다음과 같습니다.\\nfrom tensorflow.keras.layers import SimpleRNN\\nmodel.add(SimpleRNN(hidden_units))\\n인자를 사용할 때를 보겠습니다.\\n# 추가 인자를 사용할 때\\nmodel.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))\\n# 다른 표기\\nmodel.add(SimpleRNN(hidden_units, input_length=M, input_dim=N))\\nhidden_units = 은닉 상태의 크기를 정의. 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(output_dim)와도 동일. RNN의 용량(capacity)을 늘린다고 보면 되며, 중소형 모델의 경우 보통 128, 256, 512, 1024 등의 값을 가진다.', 'timesteps = 입력 시퀀스의 길이(input_length)라고 표현하기도 함. 시점의 수.\\ninput_dim = 입력의 크기.\\n[이미지: ]\\nRNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받습니다. batch_size는 한 번에 학습하는 데이터의 개수를 말합니다. 여기서부터는 텐서의 개념을 반드시 이해해야 하므로 벡터와 행렬 연산 챕터의 텐서 설명을 참고하시기 바랍니다. 다만, 이러한 표현은 사람이나 문헌에 따라서, 또는 풀고자 하는 문제에 따라서 종종 다르게 기재되는데 의 그림은 문제와 상황에 따라서 다르게 표현되는 입력 3D 텐서의 대표적인 표현들을 보여줍니다.', '주의할 점은 위 코드는 출력층까지 포함한 인공 신경망 코드가 아니라 주로 은닉층으로 간주할 수 있는 하나의 RNN 층에 대한 코드입니다. 해당 코드가 리턴하는 결과값은 하나의 은닉 상태 또는 정의하기에 따라 여러 개의 시점의 은닉 상태 입니다. 아래의 그림은 만약 전결합층(Fully-connected layer)을 출력층으로 사용하였을 경우의 인공 신경망 그림과 은닉층까지만 표현한 그림의 차이를 보여줍니다.\\n[이미지: ]', '[이미지: ]\\n그렇다면 RNN 층은 위에서 설명한 입력 3D 텐서를 입력받아서 어떻게 은닉 상태를 출력할까요? RNN 층은 사용자의 설정에 따라 두 가지 종류의 출력을 내보냅니다. 메모리 셀의 최종 시점의 은닉 상태만을 리턴하고자 한다면 (batch_size, output_dim) 크기의 2D 텐서를 리턴합니다. 하지만, 메모리 셀의 각 시점(time step)의 은닉 상태값들을 모아서 전체 시퀀스를 리턴하고자 한다면 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴합니다. 이는 RNN 층의 return_sequences 매개 변수에 True를 설정하여 설정이 가능합니다. output_dim은 앞서 코드에서 정의한 hidden_units의 값으로 설정됩니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림은 time step=3일 때, return_sequences = True를 설정했을 때와 그렇지 않았을 때 어떤 차이가 있는지를 보여줍니다. return_sequences=True를 선택하면 메모리 셀이 모든 시점(time step)에 대해서 은닉 상태값을 출력하며, 별도 기재하지 않거나 return_sequences=False로 선택할 경우에는 메모리 셀은 하나의 은닉 상태값만을 출력합니다. 그리고 이 하나의 값은 마지막 시점(time step)의 메모리 셀의 은닉 상태값입니다.\\n마지막 은닉 상태만 전달하도록 하면 다 대 일(many-to-one) 문제를 풀 수 있고, 모든 시점의 은닉 상태를 전달하도록 하면, 다음층에 RNN 은닉층이 하나 더 있는 경우이거나 다 대 다(many-to-many) 문제를 풀 수 있습니다.', '뒤에서 배우는 LSTM이나 GRU도 내부 메커니즘은 다르지만 model.add()를 통해서 층을 추가하는 코드는 SimpleRNN 코드와 같은 형태를 가집니다. 실습을 통해 모델 내부적으로 출력 결과를 어떻게 정의하는지 이해해봅시다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import SimpleRNN\\nmodel = Sequential()\\nmodel.add(SimpleRNN(3, input_shape=(2,10)))\\n# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\\nmodel.summary()\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #', 'Layer (type)                 Output Shape              Param #\\n=================================================================\\nsimple_rnn_1 (SimpleRNN)     (None, 3)                 42\\n=================================================================\\nTotal params: 42\\nTrainable params: 42\\nNon-trainable params: 0\\n_________________________________________________________________', 'Non-trainable params: 0\\n_________________________________________________________________\\n출력값이 (batch_size, output_dim) 크기의 2D 텐서일 때, output_dim은 hidden_units의 값인 3입니다. 이 경우 batch_size를 현 단계에서는 알 수 없으므로 (None, 3)이 됩니다. 이번에는 batch_size를 미리 정의해보겠습니다.\\nmodel = Sequential()\\nmodel.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\\nmodel.summary()\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #', 'Layer (type)                 Output Shape              Param #\\n=================================================================\\nsimple_rnn_2 (SimpleRNN)     (8, 3)                    42\\n=================================================================\\nTotal params: 42\\nTrainable params: 42\\nNon-trainable params: 0\\n_________________________________________________________________', 'Non-trainable params: 0\\n_________________________________________________________________\\nbatch_size를 8로 기재하면 출력의 크기가 (8, 3)이 됩니다. return_sequences 매개 변수에 True를 기재하여 출력값으로 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴하도록 모델을 만들어 보겠습니다.\\nmodel = Sequential()\\nmodel.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\\nmodel.summary()\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #', 'Layer (type)                 Output Shape              Param #\\n=================================================================\\nsimple_rnn_3 (SimpleRNN)    (8, 2, 3)                 42\\n=================================================================\\nTotal params: 42\\nTrainable params: 42\\nNon-trainable params: 0\\n_________________________________________________________________\\n출력의 크기가 (8, 2, 3)이 됩니다.']\n",
      "['직접 Numpy로 RNN 층을 구현해보겠습니다. 앞서 메모리 셀에서 은닉 상태를 계산하는 식을 다음과 같이 정의하였습니다.\\n$h_{t} = tanh(W_{x}X_{t} + W_{h}h_{t−1} + b)$\\n실제 구현에 앞서 간단히 가상의 코드(pseudocode)를 작성해보겠습니다.\\n# 아래의 코드는 가상의 코드(pseudocode)로 실제 동작하는 코드가 아님.\\nhidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화\\nfor input_t in input_length: # 각 시점마다 입력을 받는다.\\noutput_t = tanh(input_t, hidden_state_t) # 각 시점에 대해서 입력과 은닉 상태를 가지고 연산\\nhidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태가 된다.', 'hidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태가 된다.\\n우선 t 시점의 은닉 상태를 hidden_state_t라는 변수로 선언하였고, 입력 데이터의 길이를 input_length로 선언하였습니다. 이 경우, 입력 데이터의 길이는 곧 총 시점의 수(timesteps)가 됩니다. 그리고 t 시점의 입력값을 input_t로 선언하였습니다. 각 메모리 셀은 각 시점마다 input_t와 hidden_sate_t(이전 상태의 은닉 상태)를 입력으로 활성화 함수인 하이퍼볼릭탄젠트 함수를 통해 현 시점의 hidden_state_t를 계산합니다.', '가상의 코드를 통해 간단히 개념 정립을 해보았습니다. RNN 층을 실제 동작되는 코드로 구현해보겠습니다. 아래의 코드는 이해를 돕기 위해 (timesteps, input_dim) 크기의 2D 텐서를 입력으로 받았다고 가정하였으나, 실제로 케라스에서는 (batch_size, timesteps, input_dim)의 크기의 3D 텐서를 입력으로 받는 것을 기억합시다.\\ntimesteps는 시점의 수입니다. 자연어 처리에서는 보통 문장의 길이입니다. input_dim은 입력의 차원입니다. 자연어 처리에서는 보통 단어 벡터의 차원입니다. hidden_units는 은닉 상태의 크기로 메모리 셀의 용량입니다. 초기 은닉 상태는 0의 값을 가지는 벡터로 초기화합니다. 초기 은닉 상태를 출력해보겠습니다.\\nimport numpy as np\\ntimesteps = 10\\ninput_dim = 4\\nhidden_units = 8\\n# 입력에 해당되는 2D 텐서', \"import numpy as np\\ntimesteps = 10\\ninput_dim = 4\\nhidden_units = 8\\n# 입력에 해당되는 2D 텐서\\ninputs = np.random.random((timesteps, input_dim))\\n# 초기 은닉 상태는 0(벡터)로 초기화\\nhidden_state_t = np.zeros((hidden_units,))\\nprint('초기 은닉 상태 :',hidden_state_t)\\n초기 은닉 상태 : [0. 0. 0. 0. 0. 0. 0. 0.]\\n은닉 상태의 크기를 8로 정의하였으므로 8의 차원을 가지는 0의 값으로 구성된 벡터가 출력됩니다. 가중치와 편향을 각 크기에 맞게 정의하고 크기를 출력해보겠습니다.\\nWx = np.random.random((hidden_units, input_dim))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\", \"Wx = np.random.random((hidden_units, input_dim))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\\nWh = np.random.random((hidden_units, hidden_units)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\\nb = np.random.random((hidden_units,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).\\nprint('가중치 Wx의 크기(shape) :',np.shape(Wx))\\nprint('가중치 Wh의 크기(shape) :',np.shape(Wh))\\nprint('편향의 크기(shape) :',np.shape(b))\\n가중치 Wx의 크기(shape) : (8, 4)\\n가중치 Wh의 크기(shape) : (8, 8)\\n편향의 크기(shape) : (8,)\", '가중치 Wx의 크기(shape) : (8, 4)\\n가중치 Wh의 크기(shape) : (8, 8)\\n편향의 크기(shape) : (8,)\\n각 가중치와 편향의 크기는 다음과 같습니다. Wx는 (은닉 상태의 크기 × 입력의 차원), Wh는 (은닉 상태의 크기 × 은닉 상태의 크기), b는 (은닉 상태의 크기)의 크기를 가집니다. 이제 모든 시점의 은닉 상태를 출력한다고 가정하고, RNN 층을 동작시켜봅시다.\\ntotal_hidden_states = []\\n# 각 시점 별 입력값.\\nfor input_t in inputs:\\n# Wx * Xt + Wh * Ht-1 + b(bias)\\noutput_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b)\\n# 각 시점 t별 메모리 셀의 출력의 크기는 (timestep t, output_dim)\\n# 각 시점의 은닉 상태의 값을 계속해서 누적', \"# 각 시점 t별 메모리 셀의 출력의 크기는 (timestep t, output_dim)\\n# 각 시점의 은닉 상태의 값을 계속해서 누적\\ntotal_hidden_states.append(list(output_t))\\nhidden_state_t = output_t\\n# 출력 시 값을 깔끔하게 해주는 용도.\\ntotal_hidden_states = np.stack(total_hidden_states, axis = 0)\\n# (timesteps, output_dim)\\nprint('모든 시점의 은닉 상태 :')\\nprint(total_hidden_states)\\n모든 시점의 은닉 상태 :\\n[[0.85575076 0.71627213 0.87703694 0.83938496 0.81045543 0.86482715 0.76387233 0.60007514]\", '[[0.85575076 0.71627213 0.87703694 0.83938496 0.81045543 0.86482715 0.76387233 0.60007514]\\n[0.99982366 0.99985897 0.99928638 0.99989791 0.99998252 0.99977656 0.99997677 0.9998397 ]\\n[0.99997583 0.99996057 0.99972541 0.99997993 0.99998684 0.99954936 0.99997638 0.99993143]\\n[0.99997782 0.99996494 0.99966651 0.99997989 0.99999115 0.99980087 0.99999107 0.9999622 ]\\n[0.99997231 0.99996091 0.99976218 0.99998483 0.9999955  0.99989239 0.99999339 0.99997324]', '[0.99997231 0.99996091 0.99976218 0.99998483 0.9999955  0.99989239 0.99999339 0.99997324]\\n[0.99997082 0.99998754 0.99962158 0.99996278 0.99999331 0.99978731 0.99998831 0.99993414]\\n[0.99997427 0.99998367 0.99978331 0.99998173 0.99999579 0.99983689 0.99999058 0.99995531]\\n[0.99992591 0.99996115 0.99941212 0.99991593 0.999986   0.99966571 0.99995842 0.99987795]\\n[0.99997139 0.99997192 0.99960794 0.99996751 0.99998795 0.9996674 0.99998177 0.99993016]', '[0.99997139 0.99997192 0.99960794 0.99996751 0.99998795 0.9996674 0.99998177 0.99993016]\\n[0.99997659 0.99998915 0.99985392 0.99998726 0.99999773 0.99988295 0.99999316 0.99996326]]']\n",
      "['[이미지: ]\\n앞서 RNN도 다수의 은닉층을 가질 수 있다고 언급한 바 있습니다. 위의 그림은 순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은(deep) 순환 신경망의 모습을 보여줍니다. 은닉층을 2개 추가하는 경우 코드는 아래와 같습니다.\\nmodel = Sequential()\\nmodel.add(SimpleRNN(hidden_units, input_length=10, input_dim=5, return_sequences=True))\\nmodel.add(SimpleRNN(hidden_units, return_sequences=True))\\n위의 코드에서 첫번째 은닉층은 다음 은닉층이 존재하므로 return_sequences = True를 설정하여 모든 시점에 대해서 은닉 상태 값을 다음 은닉층으로 보내주고 있습니다.']\n",
      "[\"양방향 순환 신경망은 시점 t에서의 출력값을 예측할 때 이전 시점의 입력뿐만 아니라, 이후 시점의 입력 또한 예측에 기여할 수 있다는 아이디어에 기반합니다. 빈칸 채우기 문제에 비유하여 보겠습니다.\\n운동을 열심히 하는 것은 [        ]을 늘리는데 효과적이다.\\n1) 근육\\n2) 지방\\n3) 스트레스\\n'운동을 열심히 하는 것은 [ ]을 늘리는데 효과적이다.' 라는 문장에서 문맥 상으로 정답은 '근육'입니다. 위의 빈 칸 채우기 문제를 풀 때 이전에 나온 단어들만으로 빈 칸을 채우려고 시도해보면 정보가 부족합니다. '운동을 열심히 하는 것은' 까지만 주고 뒤의 단어들은 가린 채 빈 칸의 정답이 될 수 있는 세 개의 선택지 중 고르는 것은 뒤의 단어들까지 알고있는 상태보다 명백히 정답을 결정하기가 어렵습니다.\", 'RNN이 풀고자 하는 문제 중에서는 과거 시점의 입력 뿐만 아니라 미래 시점의 입력에 힌트가 있는 경우도 많습니다. 그래서 이전과 이후의 시점 모두를 고려해서 현재 시점의 예측을 더욱 정확하게 할 수 있도록 고안된 것이 양방향 RNN입니다.\\n[이미지: ]\\n양방향 RNN은 하나의 출력값을 예측하기 위해 기본적으로 두 개의 메모리 셀을 사용합니다. 첫번째 메모리 셀은 앞에서 배운 것처럼 앞 시점의 은닉 상태(Forward States) 를 전달받아 현재의 은닉 상태를 계산합니다. 위의 그림에서는 주황색 메모리 셀에 해당됩니다. 두번째 메모리 셀은 앞에서 배운 것과는 다릅니다. 앞 시점의 은닉 상태가 아니라 뒤 시점의 은닉 상태(Backward States) 를 전달 받아 현재의 은닉 상태를 계산합니다. 입력 시퀀스를 반대 방향으로 읽는 것입니다. 위의 그림에서는 초록색 메모리 셀에 해당됩니다. 그리고 이 두 개의 값 모두가 현재 시점의 출력층에서 출력값을 예측하기 위해 사용됩니다.', 'from tensorflow.keras.layers import Bidirectional\\ntimesteps = 10\\ninput_dim = 5\\nmodel = Sequential()\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))\\n양방향 RNN도 다수의 은닉층을 가질 수 있습니다. 아래의 그림은 양방향 순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은(deep) 양방향 순환 신경망의 모습을 보여줍니다.\\n[이미지: ]\\n다른 인공 신경망 모델들도 마찬가지이지만, 은닉층을 무조건 추가한다고 해서 모델의 성능이 좋아지는 것은 아닙니다. 은닉층을 추가하면 학습할 수 있는 양이 많아지지만 반대로 훈련 데이터 또한 많은 양이 필요합니다. 아래의 코드는 은닉층이 4개인 경우를 보여줍니다.\\nmodel = Sequential()', 'model = Sequential()\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\\nmodel.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\\n양방향 RNN은 태깅 작업 챕터의 실습에서 사용해보겠습니다.']\n",
      "['RNN을 제대로 이해했는지 퀴즈를 통해서 확인해보세요! 모델에 대한 설명이 다음과 같을 때, 총 파라미터 개수를 구해보세요.\\nEmbedding을 사용하며, 단어 집합(Vocabulary)의 크기가 5,000이고 임베딩 벡터의 차원은 100입니다.\\n은닉층에서는 Simple RNN을 사용하며, 은닉 상태의 크기는 128입니다.\\n훈련에 사용하는 모든 샘플의 길이는 30으로 가정합니다.\\n이진 분류를 수행하는 모델로, 출력층의 뉴런은 1개로 시그모이드 함수를 사용합니다.\\n은닉층은 1개입니다.\\n정답은 위키독스 웹 사이트의 댓글에 남겨두었습니다.\\n==================================================\\n--- 08-02 장단기 메모리(Long Short-Term Memory, LSTM) ---', '--- 08-02 장단기 메모리(Long Short-Term Memory, LSTM) ---\\n바닐라 아이스크림이 가장 기본적인 맛을 가진 아이스크림인 것처럼, 앞서 배운 RNN을 가장 단순한 형태의 RNN이라고 하여 바닐라 RNN(Vanilla RNN)이라고 합니다. (케라스에서는 SimpleRNN) 바닐라 RNN 이후 바닐라 RNN의 한계를 극복하기 위한 다양한 RNN의 변형이 나왔습니다. 이번에 배우게 될 LSTM도 그 중 하나입니다. 앞으로의 설명에서 LSTM과 비교하여 RNN을 언급하는 것은 전부 바닐라 RNN을 말합니다.']\n",
      "['[이미지: ]\\n앞에서 바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 위의 그림은 첫번째 입력값인 $x_{1}$의 정보량을 짙은 남색으로 표현했을 때, 색이 점차 얕아지는 것으로 시점이 지날수록 $x_{1}$의 정보량이 손실되어가는 과정을 표현하였습니다. 뒤로 갈수록 $x_{1}$의 정보량은 손실되고, 시점이 충분히 긴 상황에서는 $x_{1}$의 전체 정보에 대한 영향력은 거의 의미가 없을 수도 있습니다.', \"어쩌면 가장 중요한 정보가 시점의 앞 쪽에 위치할 수도 있습니다. RNN으로 만든 언어 모델이 다음 단어를 예측하는 과정을 생각해봅시다. 예를 들어 ''모스크바에 여행을 왔는데 건물도 예쁘고 먹을 것도 맛있었어. 그런데 글쎄 직장 상사한테 전화가 왔어. 어디냐고 묻더라구 그래서 나는 말했지. 저 여행왔는데요. 여기 ___''\\n다음 단어를 예측하기 위해서는 장소 정보가 필요합니다. 그런데 장소 정보에 해당되는 단어인 '모스크바'는 앞에 위치하고 있고, RNN이 충분한 기억력을 가지고 있지 못한다면 다음 단어를 엉뚱하게 예측합니다.\\n이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 합니다.\"]\n",
      "['[이미지: ]\\nLSTM에 대해서 이해해보기 전에 바닐라 RNN의 뚜껑을 열어보겠습니다. 위의 그림은 바닐라 RNN의 내부 구조를 보여줍니다. 이 책에서는 RNN 계열의 인공 신경망의 그림에서는 편향 $b$를 생략합니다. 위의 그림에 편향 $b$를 그린다면 $x_{t}$ 옆에 tanh로 향하는 또 하나의 입력선을 그리면 됩니다.\\n$h_{t} = tanh(W_{x}x_{t} + W_{h}h_{t−1} + b)$\\n바닐라 RNN은 $x_{t}$와 $h_{t-1}$이라는 두 개의 입력이 각각의 가중치와 곱해져서 메모리 셀의 입력이 됩니다. 그리고 이를 하이퍼볼릭탄젠트 함수의 입력으로 사용하고 이 값은 은닉층의 출력인 은닉 상태가 됩니다.']\n",
      "['[이미지: ]\\n위의 그림은 LSTM의 전체적인 내부의 모습을 보여줍니다. 전통적인 RNN의 이러한 단점을 보완한 RNN의 일종을 장단기 메모리(Long Short-Term Memory)라고 하며, 줄여서 LSTM이라고 합니다. LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정합니다. 요약하면 LSTM은 은닉 상태(hidden state)를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 셀 상태(cell state)라는 값을 추가하였습니다. 위의 그림에서는 t시점의 셀 상태를 $C_{t}$로 표현하고 있습니다. LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보입니다.\\n[이미지: ]\\n셀 상태는 위의 그림에서 왼쪽에서 오른쪽으로 가는 굵은 선입니다. 셀 상태 또한 이전에 배운 은닉 상태처럼 이전 시점의 셀 상태가 다음 시점의 셀 상태를 구하기 위한 입력으로서 사용됩니다.', '은닉 상태의 값과 셀 상태의 값을 구하기 위해서 새로 추가 된 3개의 게이트를 사용합니다. 각 게이트는 삭제 게이트, 입력 게이트, 출력 게이트라고 부르며 이 3개의 게이트에는 공통적으로 시그모이드 함수가 존재합니다. 시그모이드 함수를 지나면 0과 1사이의 값이 나오게 되는데 이 값들을 가지고 게이트를 조절합니다. 아래의 내용을 참고로 각 게이트에 대해서 알아보겠습니다.\\n이하 식에서 σ는 시그모이드 함수를 의미합니다.\\n이하 식에서 tanh는 하이퍼볼릭탄젠트 함수를 의미합니다.\\n$W_{xi}, W_{xg}, W_{xf}, W_{xo}$는 $x_{t}$와 함께 각 게이트에서 사용되는 4개의 가중치입니다.\\n$W_{hi}, W_{hg}, W_{hf}, W_{ho}$는 $h_{t-1}$와 함께 각 게이트에서 사용되는 4개의 가중치입니다.\\n$b_{i}, b_{g}, b_{f}, b_{o}$는 각 게이트에서 사용되는 4개의 편향입니다.\\n(1) 입력 게이트\\n[이미지: ]', '$b_{i}, b_{g}, b_{f}, b_{o}$는 각 게이트에서 사용되는 4개의 편향입니다.\\n(1) 입력 게이트\\n[이미지: ]\\n$i_{t}=σ(W_{xi}x_{t}+W_{hi}h_{t-1}+b_{i})$\\n$g_{t}=tanh(W_{xg}x_{t}+W_{hg}h_{t-1}+b_{g})$', '입력 게이트는 현재 정보를 기억하기 위한 게이트입니다. 우선 현재 시점 t의 $x$값과 입력 게이트로 이어지는 가중치 $W_{xi}$를 곱한 값과 이전 시점 t-1의 은닉 상태가 입력 게이트로 이어지는 가중치 $W_{hi}$를 곱한 값을 더하여 시그모이드 함수를 지납니다. 이를 $i_{t}$라고 합니다. 그리고 현재 시점 t의 $x$값과 입력 게이트로 이어지는 가중치 $W_{xg}$를 곱한 값과 이전 시점 t-1의 은닉 상태가 입력 게이트로 이어지는 가중치 $W_{hg}$를 곱한 값을 더하여 하이퍼볼릭탄젠트 함수를 지납니다. 이를 $g_{t}$라고 합니다. 시그모이드 함수를 지나 0과 1사이의 값을 가지는 $i_{t}$과 하이퍼볼릭탄젠트 함수를 지나 -1과 1사이의 값을 가지는 $g_{t}$. 이 두 개의 값을 가지고 이번에 선택된 기억할 정보의 양을 정하는데, 구체적으로 어떻게 결정하는지는 아래에서 배우게 될 셀 상태 수식을 참고합니다.\\n(2) 삭제 게이트\\n[이미지: ]', '(2) 삭제 게이트\\n[이미지: ]\\n$f_{t}=σ(W_{xf}x_{t}+W_{hf}h_{t-1}+b_{f})$\\n삭제 게이트는 기억을 삭제하기 위한 게이트입니다. 현재 시점 t의 $x$값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지나게 됩니다. 시그모이드 함수를 지나면 0과 1 사이의 값이 나오게 되는데, 이 값이 곧 삭제 과정을 거친 정보의 양입니다. 0에 가까울수록 정보가 많이 삭제된 것이고 1에 가까울수록 정보를 온전히 기억한 것입니다. 이를 가지고 셀 상태를 구하게 되는데, 구체적으로는 아래에서 배우게 될 셀 상태 수식을 참고합니다.\\n(3) 셀 상태\\n[이미지: ]\\n$C_{t}=f_{t}∘C_{t-1}+i_{t}∘g_{t}$\\n셀 상태 $C_{t}$를 구하는 방법을 알아보겠습니다. 삭제 게이트에서 일부 기억을 잃은 상태입니다.', '[이미지: ]\\n$C_{t}=f_{t}∘C_{t-1}+i_{t}∘g_{t}$\\n셀 상태 $C_{t}$를 구하는 방법을 알아보겠습니다. 삭제 게이트에서 일부 기억을 잃은 상태입니다.\\n입력 게이트에서 구한 $i_{t}$, $g_{t}$ 이 두 개의 값에 대해서 원소별 곱(entrywise product)을 진행합니다. 다시 말해 같은 크기의 두 행렬이 있을 때 같은 위치의 성분끼리 곱하는 것을 말합니다. 여기서는 식으로 $∘$ 로 표현합니다. 이것이 이번에 선택된 기억할 값입니다.\\n입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더합니다. 이 값을 현재 시점 t의 셀 상태라고 하며, 이 값은 다음 t+1 시점의 LSTM 셀로 넘겨집니다.', '삭제 게이트와 입력 게이트의 영향력을 이해해봅시다. 만약 삭제 게이트의 출력값인 $f_{t}$가 0이 된다면, 이전 시점의 셀 상태의 값인 $C_{t-1}$은 현재 시점의 셀 상태의 값을 결정하기 위한 영향력이 0이 되면서, 오직 입력 게이트의 결과만이 현재 시점의 셀 상태의 값 $C_{t}$을 결정할 수 있습니다. 이는 삭제 게이트가 완전히 닫히고 입력 게이트를 연 상태를 의미합니다. 반대로 입력 게이트의 $i_{t}$값을 0이라고 한다면, 현재 시점의 셀 상태의 값 $C_{t}$는 오직 이전 시점의 셀 상태의 값 $C_{t-1}$의 값에만 의존합니다. 이는 입력 게이트를 완전히 닫고 삭제 게이트만을 연 상태를 의미합니다. 결과적으로 삭제 게이트는 이전 시점의 입력을 얼마나 반영할지를 의미하고, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지를 결정합니다.\\n(4) 출력 게이트와 은닉 상태\\n[이미지: ]\\n$o_{t}=σ(W_{xo}x_{t}+W_{ho}h_{t-1}+b_{o})$', '(4) 출력 게이트와 은닉 상태\\n[이미지: ]\\n$o_{t}=σ(W_{xo}x_{t}+W_{ho}h_{t-1}+b_{o})$\\n$h_{t}=o_{t}∘tanh(c_{t})$\\n출력 게이트는 현재 시점 t의 $x$값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지난 값입니다. 해당 값은 현재 시점 t의 은닉 상태를 결정하는 일에 쓰이게 됩니다. 셀 상태의 값이 하이퍼볼릭탄젠트 함수를 지나 -1과 1사이의 값이 되고, 해당 값은 출력 게이트의 값과 연산되면서, 값이 걸러지는 효과가 발생하여 은닉 상태가 됩니다. 은닉 상태의 값은 또한 출력층으로도 향합니다.\\n==================================================\\n--- 08-03 게이트 순환 유닛(Gated Recurrent Unit, GRU) ---\\n```\\nmodel.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))', '```\\nmodel.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))\\n```GRU(Gated Recurrent Unit)는 2014년 뉴욕대학교 조경현 교수님이 집필한 논문에서 제안되었습니다. GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였습니다. 다시 말해서, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화 시켰습니다.']\n",
      "['LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했습니다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재합니다. GRU는 LSTM보다 학습 속도가 빠르다고 알려져있지만 여러 평가에서 GRU는 LSTM과 비슷한 성능을 보인다고 알려져 있습니다.\\n[이미지: ]\\n$r_{t}=σ(W_{xr}x_{t}+W_{hr}h_{t-1}+b_{r})$\\n$z_{t}=σ(W_{xz}x_{t}+W_{hz}h_{t-1}+b_{z})$\\n$g_{t}=tanh(W_{hg}(r_{t}∘h_{t-1})+W_{xg}x_{t}+b_{g})$\\n$h_{t}=(1-z_{t})∘g_{t}+z_{t}∘h_{t-1}$\\nGRU와 LSTM 중 어떤 것이 모델의 성능면에서 더 낫다라고 단정지어 말할 수 없으며, 기존에 LSTM을 사용하면서 최적의 하이퍼파라미터를 찾아낸 상황이라면 굳이 GRU로 바꿔서 사용할 필요는 없습니다.', '경험적으로 데이터 양이 적을 때는 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 낫다고도 합니다. GRU보다 LSTM에 대한 연구나 사용량이 더 많은데, 이는 LSTM이 더 먼저 나온 구조이기 때문입니다.']\n",
      "['케라스에서는 역시 GRU에 대한 구현을 지원합니다. 사용 방법은 SimpleRNN이나 LSTM과 같습니다.\\nmodel.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))\\n==================================================\\n--- 08-04 케라스의 SimpleRNN과 LSTM 이해하기 ---\\n```\\nhidden states : [[[0.3590648  0.3590648  0.3590648  0.70387346 0.70387346 0.70387346]\\n[0.5511133  0.5511133  0.5511133  0.5886358  0.5886358  0.5886358 ]\\n[0.5911575  0.5911575  0.5911575  0.39516988 0.39516988 0.39516988]', '[0.5911575  0.5911575  0.5911575  0.39516988 0.39516988 0.39516988]\\n[0.6303139  0.6303139  0.6303139  0.21942243 0.21942243 0.21942243]]], shape: (1, 4, 6)\\nforward state : [[0.6303139 0.6303139 0.6303139]], shape: (1, 3)\\nbackward state : [[0.70387346 0.70387346 0.70387346]], shape: (1, 3)\\n```케라스의 SimpleRNN과 LSTM을 이해해봅니다.']\n",
      "['import numpy as np\\nimport tensorflow as tf\\nfrom tensorflow.keras.layers import SimpleRNN, LSTM, Bidirectional\\n우선 RNN과 LSTM을 테스트하기 위한 임의의 입력을 만듭니다.\\ntrain_X = [[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]\\nprint(np.shape(train_X))\\n(4, 5)', 'print(np.shape(train_X))\\n(4, 5)\\n위 입력은 단어 벡터의 차원은 5이고, 문장의 길이가 4인 경우를 가정한 입력입니다. 다시 말해 4번의 시점(timesteps)이 존재하고, 각 시점마다 5차원의 단어 벡터가 입력으로 사용됩니다. 그런데 앞서 RNN은 2D 텐서가 아니라 3D 텐서를 입력을 받는다고 언급한 바 있습니다. 즉, 위에서 만든 2D 텐서를 3D 텐서로 변경합니다. 이는 배치 크기 1을 추가해주므로서 해결합니다.\\ntrain_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]]\\ntrain_X = np.array(train_X, dtype=np.float32)\\nprint(train_X.shape)\\n(1, 4, 5)', 'train_X = np.array(train_X, dtype=np.float32)\\nprint(train_X.shape)\\n(1, 4, 5)\\n(batch_size, timesteps, input_dim)에 해당되는 (1, 4, 5)의 크기를 가지는 3D 텐서가 생성되었습니다. batch_size는 한 번에 RNN이 학습하는 데이터의 양을 의미하지만, 여기서는 샘플이 1개 밖에 없으므로 batch_size는 1입니다.']\n",
      "['위에서 생성한 데이터를 SimpleRNN의 입력으로 사용하여 SimpleRNN의 출력값을 이해해보겠습니다.  SimpleRNN에는 여러 인자가 있으며 대표적인 인자로 return_sequences와 return_state가 있습니다. 기본값으로는 둘 다 False로 지정되어져 있으므로 별도 지정을 하지 않을 경우에는 False로 처리됩니다. 우선, 은닉 상태의 크기를 3으로 지정하고, 두 인자 값이 모두 False일 때의 출력값을 보겠습니다.\\n앞으로의 실습에서 SimpleRNN을 매번 재선언하므로 은닉 상태의 값 자체는 매번 초기화되어 이전 출력과 값의 일관성은 없습니다. 그래서 출력값 자체보다는 해당 값의 크기(shape)에 주목해야합니다.\\nrnn = SimpleRNN(3)\\n# rnn = SimpleRNN(3, return_sequences=False, return_state=False)와 동일.\\nhidden_state = rnn(train_X)', \"# rnn = SimpleRNN(3, return_sequences=False, return_state=False)와 동일.\\nhidden_state = rnn(train_X)\\nprint('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\\nhidden state : [[-0.866719    0.95010996 -0.99262357]], shape: (1, 3)\\n(1, 3) 크기의 텐서가 출력되는데, 이는 마지막 시점의 은닉 상태입니다. 은닉 상태의 크기를 3으로 지정했음을 주목합시다. 기본적으로 return_sequences가 False인 경우에는 SimpleRNN은 마지막 시점의 은닉 상태만 출력합니다. 이번에는 return_sequences를 True로 지정하여 모든 시점의 은닉 상태를 출력해봅시다.\\nrnn = SimpleRNN(3, return_sequences=True)\", \"rnn = SimpleRNN(3, return_sequences=True)\\nhidden_states = rnn(train_X)\\nprint('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\\nhidden states : [[[ 0.92948604 -0.9985648   0.98355013]\\n[ 0.89172053 -0.9984244   0.191779  ]\\n[ 0.6681082  -0.96070355  0.6493537 ]\\n[ 0.95280755 -0.98054564  0.7224146 ]]], shape: (1, 4, 3)\\n(1, 4, 3) 크기의 텐서가 출력됩니다. 앞서 입력 데이터는 (1, 4, 5)의 크기를 가지는 3D 텐서였고, 그 중 4가 시점(timesteps)에 해당하는 값이므로 모든 시점에 대해서 은닉 상태의 값을 출력하여 (1, 4, 3) 크기의 텐서를 출력하는 것입니다.\", \"return_state가 True일 경우에는 return_sequences의 True/False 여부와 상관없이 마지막 시점의 은닉 상태를 출력합니다. 가령, return_sequences가 True이면서, return_state를 True로 할 경우 SimpleRNN은 두 개의 출력을 리턴합니다.\\nrnn = SimpleRNN(3, return_sequences=True, return_state=True)\\nhidden_states, last_state = rnn(train_X)\\nprint('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\\nprint('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))\\nhidden states : [[[ 0.29839835 -0.99608386  0.2994854 ]\", 'hidden states : [[[ 0.29839835 -0.99608386  0.2994854 ]\\n[ 0.9160876   0.01154806  0.86181474]\\n[-0.20252597 -0.9270214   0.9696659 ]\\n[-0.5144398  -0.5037417   0.96605766]]], shape: (1, 4, 3)\\nlast hidden state : [[-0.5144398  -0.5037417   0.96605766]], shape: (1, 3)', 'last hidden state : [[-0.5144398  -0.5037417   0.96605766]], shape: (1, 3)\\n첫번째 출력은 return_sequences=True로 인한 출력으로 모든 시점의 은닉 상태입니다. 두번째 출력은 return_state=True로 인한 출력으로 마지막 시점의 은닉 상태입니다. 실제로 출력을 보면 모든 시점의 은닉 상태인 (1, 4, 3) 텐서의 마지막 벡터값이 return_state=True로 인해 출력된 벡터값과 일치하는 것을 볼 수 있습니다. (둘 다 [-0.5144398  -0.5037417   0.96605766])\\n그렇다면 return_sequences는 False인데, retun_state가 True인 경우는 어떨까요?\\nrnn = SimpleRNN(3, return_sequences=False, return_state=True)\\nhidden_state, last_state = rnn(train_X)', \"hidden_state, last_state = rnn(train_X)\\nprint('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\\nprint('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))\\nhidden state : [[0.07532981 0.97772664 0.97351676]], shape: (1, 3)\\nlast hidden state : [[0.07532981 0.97772664 0.97351676]], shape: (1, 3)\\n두 개의 출력 모두 마지막 시점의 은닉 상태를 출력하게 됩니다.\"]\n",
      "[\"실제로 SimpleRNN이 사용되는 경우는 거의 없습니다. 이보다는 LSTM이나 GRU을 주로 사용하는데, 이번에는 임의의 입력에 대해서 LSTM을 사용할 경우를 보겠습니다. 우선 return_sequences를 False로 두고, return_state가 True인 경우를 봅시다.\\nlstm = LSTM(3, return_sequences=False, return_state=True)\\nhidden_state, last_state, last_cell_state = lstm(train_X)\\nprint('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\\nprint('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))\", \"print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))\\nprint('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))\\nhidden state : [[-0.00263056  0.20051427 -0.22501363]], shape: (1, 3)\\nlast hidden state : [[-0.00263056  0.20051427 -0.22501363]], shape: (1, 3)\\nlast cell state : [[-0.04346419  0.44769213 -0.2644241 ]], shape: (1, 3)\", 'last cell state : [[-0.04346419  0.44769213 -0.2644241 ]], shape: (1, 3)\\n이번에는 SimpleRNN 때와는 달리, 세 개의 결과를 반환합니다. return_sequences가 False이므로 우선 첫번째 결과는 마지막 시점의 은닉 상태입니다. 그런데 LSTM이 SimpleRNN과 다른 점은 return_state를 True로 둔 경우에는 마지막 시점의 은닉 상태뿐만 아니라 셀 상태까지 반환한다는 점입니다. 이번에는 return_sequences를 True로 바꿔보겠습니다.\\nlstm = LSTM(3, return_sequences=True, return_state=True)\\nhidden_states, last_hidden_state, last_cell_state = lstm(train_X)', \"hidden_states, last_hidden_state, last_cell_state = lstm(train_X)\\nprint('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\\nprint('last hidden state : {}, shape: {}'.format(last_hidden_state, last_hidden_state.shape))\\nprint('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))\\nhidden states : [[[ 0.1383949   0.01107763 -0.00315794]\\n[ 0.0859854   0.03685492 -0.01836833]\\n[-0.02512104  0.12305924 -0.0891041 ]\", '[ 0.0859854   0.03685492 -0.01836833]\\n[-0.02512104  0.12305924 -0.0891041 ]\\n[-0.27381724  0.05733536 -0.04240693]]], shape: (1, 4, 3)\\nlast hidden state : [[-0.27381724  0.05733536 -0.04240693]], shape: (1, 3)\\nlast cell state : [[-0.39230722  1.5474017  -0.6344505 ]], shape: (1, 3)\\nreturn_state가 True이므로 두번째 출력값이 마지막 은닉 상태, 세번째 출력값이 마지막 셀 상태인 것은 변함없지만 return_sequences가 True이므로 첫번째 출력값은 모든 시점의 은닉 상태가 출력됩니다.']\n",
      "['난이도를 조금 올려서 양방향 LSTM의 출력값을 확인해보겠습니다. return_sequences가 True인 경우와 False인 경우에 대해서 은닉 상태의 값이 어떻게 바뀌는지 직접 비교하기 위해서 이번에는 출력되는 은닉 상태의 값을 고정시켜주겠습니다.\\nk_init = tf.keras.initializers.Constant(value=0.1)\\nb_init = tf.keras.initializers.Constant(value=0)\\nr_init = tf.keras.initializers.Constant(value=0.1)\\n우선 return_sequences가 False이고, return_state가 True인 경우입니다.\\nbilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\\\', \"bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\\\\\nkernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\\nhidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\\nprint('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\\nprint('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\\nprint('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))\", \"print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))\\nhidden states : [[0.6303139  0.6303139  0.6303139  0.70387346 0.70387346 0.70387346]], shape: (1, 6)\\nforward state : [[0.6303139 0.6303139 0.6303139]], shape: (1, 3)\\nbackward state : [[0.70387346 0.70387346 0.70387346]], shape: (1, 3)\", 'backward state : [[0.70387346 0.70387346 0.70387346]], shape: (1, 3)\\n이번에는 무려 5개의 값을 반환합니다. return_state가 True인 경우에는 정방향 LSTM의 은닉 상태와 셀 상태, 역방향 LSTM의 은닉 상태와 셀 상태 4가지를 반환하기 때문입니다. 다만, 셀 상태는 각각 forward_c와 backward_c에 저장만 하고 출력하지 않았습니다. 첫번째 출력값의 크기가 (1, 6)인 것에 주목합시다. 이는 return_sequences가 False인 경우 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태가 연결된 채 반환되기 때문입니다. 그림으로 표현하면 아래와 같이 연결되어 다음층에서 사용됩니다.\\n[이미지: ]', \"[이미지: ]\\n마찬가지로 return_state가 True인 경우에 반환한 은닉 상태의 값인 forward_h와 backward_h는 각각 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태값입니다. 그리고 이 두 값을 연결한 값이 hidden_states에 출력되는 값입니다. 이를 이용한 실습은 'RNN을 이용한 텍스트 분류 챕터'에서의 한국어 스팀 리뷰 분류하기 실습( https://wikidocs.net/94748 )을 참고하세요.\\n정방향 LSTM의 마지막 시점의 은닉 상태값과 역방향 LSTM의 첫번째 은닉 상태값을 기억해둡시다.\\n정방향 LSTM의 마지막 시점의 은닉 상태값 : [0.6303139 0.6303139 0.6303139]\\n역방향 LSTM의 첫번째 시점의 은닉 상태값 : [0.70387346 0.70387346 0.70387346]\", \"역방향 LSTM의 첫번째 시점의 은닉 상태값 : [0.70387346 0.70387346 0.70387346]\\n현재 은닉 상태의 값을 고정시켜두었기 때문에 return_sequences를 True로 할 경우, 출력이 어떻게 바뀌는지 비교가 가능합니다.\\nbilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True, \\\\\\nkernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\\nhidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\\nprint('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\", \"print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\\nprint('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\\nprint('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))\\nhidden states : [[[0.3590648  0.3590648  0.3590648  0.70387346 0.70387346 0.70387346]\\n[0.5511133  0.5511133  0.5511133  0.5886358  0.5886358  0.5886358 ]\\n[0.5911575  0.5911575  0.5911575  0.39516988 0.39516988 0.39516988]\", '[0.5911575  0.5911575  0.5911575  0.39516988 0.39516988 0.39516988]\\n[0.6303139  0.6303139  0.6303139  0.21942243 0.21942243 0.21942243]]], shape: (1, 4, 6)\\nforward state : [[0.6303139 0.6303139 0.6303139]], shape: (1, 3)\\nbackward state : [[0.70387346 0.70387346 0.70387346]], shape: (1, 3)\\nhidden states의 출력값에서는 이제 모든 시점의 은닉 상태가 출력됩니다. 역방향 LSTM의 첫번째 시점의 은닉 상태는 더 이상 정방향 LSTM의 마지막 시점의 은닉 상태와 연결되는 것이 아니라 정방향 LSTM의 첫번째 시점의 은닉 상태와 연결됩니다.\\n그림으로 표현하면 다음과 같이 연결되어 다음층의 입력으로 사용됩니다.\\n[이미지: ]', '그림으로 표현하면 다음과 같이 연결되어 다음층의 입력으로 사용됩니다.\\n[이미지: ]\\n==================================================\\n--- 08-05 RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM) ---\\nRNN을 이용하여 언어 모델을 구현한 RNN 언어 모델에 대해서 배웁니다.']\n",
      "[\"앞서 n-gram 언어 모델과 NNLM은 고정된 개수의 단어만을 입력으로 받아야한다는 단점이 있었습니다. 하지만 시점(time step)이라는 개념이 도입된 RNN으로 언어 모델을 만들면 입력의 길이를 고정하지 않을 수 있습니다. 이처럼 RNN으로 만든 언어 모델을 RNNLM(Recurrent Neural Network Language Model)이라고 합니다.\\nRNNLM이 언어 모델링을 학습하는 과정을 보겠습니다. 이해를 위해 간소화 된 형태로 설명합니다.\\n예문 : 'what will the fat cat sit on'\\n예를 들어 훈련 코퍼스에 위와 같은 문장이 있다고 해봅시다. 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측하는 모델입니다. 아래의 그림은 RNNLM이 어떻게 이전 시점의 단어들과 현재 시점의 단어로 다음 단어를 예측하는지를 보여줍니다.\\n[이미지: ]\", '[이미지: ]\\nRNNLM은 기본적으로 예측 과정에서 이전 시점의 출력을 현재 시점의 입력으로 합니다. RNNLM은 what을 입력받으면, will을 예측하고 이 will은 다음 시점의 입력이 되어 the를 예측합니다. 그리고 the는 또 다시 다음 시점의 입력이 되고 해당 시점에서는 fat을 예측합니다. 그리고 이 또한 다시 다음 시점의 입력이 됩니다. 결과적으로 세번째 시점에서 fat은 앞서 나온 what, will, the라는 시퀀스로 인해 결정된 단어이며, 네번째 시점의 cat은 앞서 나온 what, will, the, fat이라는 시퀀스로 인해 결정된 단어입니다.', '사실 위 과정은 훈련이 끝난 모델의 테스트 과정 동안(실제 사용할 때)의 이야기입니다. 훈련 과정에서는 이전 시점의 예측 결과를 다음 시점의 입력으로 넣으면서 예측하는 것이 아니라, what will the fat cat sit on라는 훈련 샘플이 있다면, what will the fat cat sit 시퀀스를 모델의 입력으로 넣으면, will the fat cat sit on를 예측하도록 훈련됩니다. will, the, fat, cat, sit, on는 각 시점의 레이블입니다.', '이러한 RNN 훈련 기법을 교사 강요(teacher forcing)라고 합니다. 교사 강요(teacher forcing)란, 테스트 과정에서 t 시점의 출력이 t+1 시점의 입력으로 사용되는 RNN 모델을 훈련시킬 때 사용하는 훈련 기법입니다. 훈련할 때 교사 강요를 사용할 경우, 모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고, t 시점의 레이블. 즉, 실제 알고있는 정답을 t+1 시점의 입력으로 사용합니다.\\n물론, 훈련 과정에서도 이전 시점의 출력을 다음 시점의 입력으로 사용하면서 훈련 시킬 수도 있지만 이는 한 번 잘못 예측하면 뒤에서의 예측까지 영향을 미쳐 훈련 시간이 느려지게 되므로 교사 강요를 사용하여 RNN을 좀 더 빠르고 효과적으로 훈련시킬 수 있습니다.\\n[이미지: ]', '[이미지: ]\\n훈련 과정 동안 출력층에서 사용하는 활성화 함수는 소프트맥스 함수입니다. 그리고 모델이 예측한 값과 실제 레이블과의 오차를 계산하기 위해서 손실 함수로 크로스 엔트로피 함수를 사용합니다. 이해를 돕기 위해 앞서 배운 NNLM의 그림과 유사한 형태로 RNNLM을 다시 시각화해보겠습니다.\\n[이미지: ]\\nRNNLM의 구조를 보겠습니다. RNNLM은 위의 그림과 같이 총 4개의 층(layer)으로 이루어진 인공 신경망입니다. 우선 입력층(input layer)을 봅시다. RNNLM의 현 시점(timestep)은 4로 가정합니다. 그래서 4번째 입력 단어인 fat의 원-핫 벡터가 입력이 됩니다.', '출력층(output layer)을 봅시다. 모델이 예측해야하는 정답에 해당되는 단어 cat의 원-핫 벡터는 출력층에서 모델이 예측한 값의 오차를 구하기 위해 사용될 예정입니다. 그리고 이 오차로부터 손실 함수를 사용해 인공 신경망이 학습을 하게 됩니다. 조금 더 구체적으로 살펴보겠습니다.\\n[이미지: ]\\n현 시점의 입력 단어의 원-핫 벡터 $x_{t}$를 입력 받은 RNNLM은 우선 임베딩층(embedding layer)을 지납니다. 이 임베딩층은 기본적으로 NNLM에서 배운 투사층(projection layer)입니다. NNLM에서는 룩업 테이블을 수행하는 층을 투사층라고 표현했지만, 이미 투사층의 결과로 얻는 벡터를 임베딩 벡터라고 부른다고 NNLM에서 학습하였으므로, 앞으로는 임베딩 벡터를 얻는 투사층을 임베딩층(embedding layer)이라는 표현을 사용할 겁니다.', '단어 집합의 크기가 V일 때, 임베딩 벡터의 크기를 M으로 설정하면, 각 입력 단어들은 임베딩층에서 V × M 크기의 임베딩 행렬과 곱해집니다. 여기서 V는 단어 집합의 크기를 의미합니다. 만약 원-핫 벡터의 차원이 7이고, M이 5라면 임베딩 행렬은 7 × 5 행렬이 됩니다. 그리고 이 임베딩 행렬은 역전파 과정에서 다른 가중치들과 함께 학습됩니다. 이는 NNLM에서 이미 배운 개념입니다.\\n임베딩층 :\\n$$\\ne_{t} = lookup(x_{t})\\n$$\\n여기서부터는 다시 RNN을 복습하는 것과 같습니다.\\n이 임베딩 벡터는 은닉층에서 이전 시점의 은닉 상태인 $h_{t-1}$과 함께 다음의 연산을 하여 현재 시점의 은닉 상태 $h_{t}$를 계산하게 됩니다.\\n은닉층 :\\n$$\\nh_{t} = tanh(W_{x} e_{t} + W_{h}h_{t−1} + b)\\n$$', '은닉층 :\\n$$\\nh_{t} = tanh(W_{x} e_{t} + W_{h}h_{t−1} + b)\\n$$\\n출력층에서는 활성화 함수로 소프트맥스(softmax) 함수를 사용하는데, V차원의 벡터는 소프트맥스 함수를 지나면서 각 원소는 0과 1사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀝니다. 이렇게 나온 벡터를 RNNLM의 t시점의 예측값이라는 의미에서 $\\\\hat{y_{t}}$라고 합시다. 이를 식으로 표현하면 아래와 같습니다.\\n출력층 :\\n$$\\n\\\\hat{y_{t}} = softmax(W_{y}h_{t} + b)\\n$$', '출력층 :\\n$$\\n\\\\hat{y_{t}} = softmax(W_{y}h_{t} + b)\\n$$\\n벡터 $\\\\hat{y_{t}}$의 각 차원 안에서의 값이 의미하는 것은 이와 같습니다. $\\\\hat{y_{t}}$의 j번째 인덱스가 가진 0과 1사이의 값은 j번째 단어가 다음 단어일 확률을 나타냅니다. 그리고 $\\\\hat{y_{t}}$는 실제값. 즉, 실제 정답에 해당되는 단어인 원-핫 벡터의 값에 가까워져야 합니다. 실제값에 해당되는 다음 단어를 $y$라고 했을 때, 이 두 벡터가 가까워지게 하기위해서 RNNLM는 손실 함수로 cross-entropy 함수를 사용합니다. 그리고 역전파가 이루어지면서 가중치 행렬들이 학습되는데, 이 과정에서 임베딩 벡터값들도 학습이 됩니다.', '룩업 테이블의 대상이 되는 테이블인 임베딩 행렬을 $E$라고 하였을 때, 결과적으로 RNNLM에서 학습 과정에서 학습되는 가중치 행렬은 다음의 $E$, $W_{x}$, $W_{h}$, $W_{y}$ 4개 입니다. 뒤의 글자 단위 RNN 실습에서 RNN 언어 모델을 구현해보면서 훈련 과정과 테스트 과정의 차이를 이해해보겠습니다.\\n==================================================\\n--- 08-06 RNN을 이용한 텍스트 생성(Text Generation using RNN) ---\\n```\\nhow to make facebook more accountable will so your neighbor chasing\\n```다 대 일(many-to-one) 구조의 RNN을 사용하여 문맥을 반영해서 텍스트를 생성하는 모델을 만들어봅시다.']\n",
      "[\"예를 들어서 '경마장에 있는 말이 뛰고 있다'와 '그의 말이 법이다'와 '가는 말이 고와야 오는 말이 곱다'라는 세 가지 문장이 있다고 해봅시다. 모델이 문맥을 학습할 수 있도록 전체 문장의 앞의 단어들을 전부 고려하여 학습하도록 데이터를 재구성한다면 아래와 같이 총 11개의 샘플이 구성됩니다.\\nsamples\\n$X$\\n$y$\\n1\\n경마장에\\n있는\\n2\\n경마장에 있는\\n말이\\n3\\n경마장에 있는 말이\\n뛰고\\n4\\n경마장에 있는 말이 뛰고\\n있다\\n5\\n그의\\n말이\\n6\\n그의 말이\\n법이다\\n7\\n가는\\n말이\\n8\\n가는 말이\\n고와야\\n9\\n가는 말이 고와야\\n오는\\n10\\n가는 말이 고와야 오는\\n말이\\n11\\n가는 말이 고와야 오는 말이\\n곱다\\n1) 데이터에 대한 이해와 전처리\\nimport numpy as np\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\", 'from tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\n우선 예제로 언급한 3개의 한국어 문장을 저장합니다.\\ntext = \"\"\"경마장에 있는 말이 뛰고 있다\\\\n\\n그의 말이 법이다\\\\n\\n가는 말이 고와야 오는 말이 곱다\\\\n\"\"\"\\n단어 집합을 생성하고 크기를 확인해보겠습니다. 단어 집합의 크기를 저장할 때는 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만, 패딩을 위한 0을 고려하여 +1을 해줍니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([text])\\nvocab_size = len(tokenizer.word_index) + 1\\nprint(\\'단어 집합의 크기 : %d\\' % vocab_size)\\n단어 집합의 크기 : 12\\n각 단어와 단어에 부여된 정수 인덱스를 출력해보겠습니다.', \"print('단어 집합의 크기 : %d' % vocab_size)\\n단어 집합의 크기 : 12\\n각 단어와 단어에 부여된 정수 인덱스를 출력해보겠습니다.\\nprint(tokenizer.word_index)\\n{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\\n훈련 데이터를 만들어보겠습니다.\\nsequences = list()\\nfor line in text.split('\\\\n'): # 줄바꿈 문자를 기준으로 문장 토큰화\\nencoded = tokenizer.texts_to_sequences([line])[0]\\nfor i in range(1, len(encoded)):\\nsequence = encoded[:i+1]\\nsequences.append(sequence)\\nprint('학습에 사용할 샘플의 개수: %d' % len(sequences))\", \"sequence = encoded[:i+1]\\nsequences.append(sequence)\\nprint('학습에 사용할 샘플의 개수: %d' % len(sequences))\\n학습에 사용할 샘플의 개수: 11\\n샘플의 개수는 총 11개가 나옵니다. 전체 샘플을 출력해봅시다.\\nprint(sequences)\\n[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\\n위의 데이터는 아직 레이블로 사용될 단어를 분리하지 않은 훈련 데이터입니다. [2, 3]은 [경마장에, 있는]에 해당되며 [2, 3, 1]은 [경마장에, 있는, 말이]에 해당됩니다. 전체 훈련 데이터에 대해서 맨 우측에 있는 단어에 대해서만 레이블로 분리해야 합니다.\", \"우선 전체 샘플에 대해서 길이를 일치시켜 줍니다. 가장 긴 샘플의 길이를 기준으로 합니다. 현재 육안으로 봤을 때, 길이가 가장 긴 샘플은 [8, 1, 9, 10, 1, 11]이고 길이는 6입니다. 이를 코드로는 다음과 같이 구할 수 있습니다.\\nmax_len = max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\\nprint('샘플의 최대 길이 : {}'.format(max_len))\\n샘플의 최대 길이 : 6\\n전체 훈련 데이터에서 가장 긴 샘플의 길이가 6임을 확인하였습니다. 전체 샘플의 길이를 6으로 패딩합니다.\\nsequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\", \"sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\\npad_sequences()는 모든 샘플에 대해서 0을 사용하여 길이를 맞춰줍니다. maxlen의 값으로 6을 주면 모든 샘플의 길이를 6으로 맞춰주며, padding의 인자로 'pre'를 주면 길이가 6보다 짧은 샘플의 앞에 0으로 채웁니다. 전체 훈련 데이터를 출력해봅니다.\\nprint(sequences)\\n[[ 0  0  0  0  2  3]\\n[ 0  0  0  2  3  1]\\n[ 0  0  2  3  1  4]\\n[ 0  2  3  1  4  5]\\n[ 0  0  0  0  6  1]\\n[ 0  0  0  6  1  7]\\n[ 0  0  0  0  8  1]\\n[ 0  0  0  8  1  9]\\n[ 0  0  8  1  9 10]\\n[ 0  8  1  9 10  1]\\n[ 8  1  9 10  1 11]]\", '[ 0  0  0  8  1  9]\\n[ 0  0  8  1  9 10]\\n[ 0  8  1  9 10  1]\\n[ 8  1  9 10  1 11]]\\n길이가 6보다 짧은 모든 샘플에 대해서 앞에 0을 채워서 모든 샘플의 길이를 6으로 바꿨습니다. 이제 각 샘플의 마지막 단어를 레이블로 분리합시다. 레이블의 분리는 Numpy를 이용해서 가능합니다. 리스트의 마지막 값을 제외하고 저장한 것은 X, 리스트의 마지막 값만 저장한 것은 y. 이는 레이블에 해당됩니다.\\nsequences = np.array(sequences)\\nX = sequences[:,:-1]\\ny = sequences[:,-1]\\n분리된 X와 y에 대해서 출력해보면 다음과 같습니다.\\nprint(X)\\n[[ 0  0  0  0  2]\\n[ 0  0  0  2  3]\\n[ 0  0  2  3  1]\\n[ 0  2  3  1  4]\\n[ 0  0  0  0  6]\\n[ 0  0  0  6  1]\\n[ 0  0  0  0  8]', '[ 0  0  2  3  1]\\n[ 0  2  3  1  4]\\n[ 0  0  0  0  6]\\n[ 0  0  0  6  1]\\n[ 0  0  0  0  8]\\n[ 0  0  0  8  1]\\n[ 0  0  8  1  9]\\n[ 0  8  1  9 10]\\n[ 8  1  9 10  1]]\\nprint(y)\\n[ 3  1  4  5  1  7  1  9 10  1 11]\\n레이블이 분리되었습니다. RNN 모델에 훈련 데이터를 훈련 시키기 전에 레이블에 대해서 원-핫 인코딩을 수행합니다.\\ny = to_categorical(y, num_classes=vocab_size)\\n원-핫 인코딩이 수행되었는지 출력합니다.\\nprint(y)\\n[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] # 3에 대한 원-핫 벡터\\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터', '[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터\\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] # 4에 대한 원-핫 벡터\\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] # 5에 대한 원-핫 벡터\\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터\\n[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] # 7에 대한 원-핫 벡터\\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터\\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] # 9에 대한 원-핫 벡터\\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] # 10에 대한 원-핫 벡터\\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터', '[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # 1에 대한 원-핫 벡터\\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] # 11에 대한 원-핫 벡터\\n정상적으로 원-핫 인코딩이 수행된 것을 볼 수 있습니다.\\n2) 모델 설계하기\\nRNN 모델에 데이터를 훈련시킵니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Embedding, Dense, SimpleRNN', 'from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\\n하이퍼파라미터인 임베딩 벡터의 차원은 10, 은닉 상태의 크기는 32입니다. 다 대 일 구조의 RNN을 사용합니다. 전결합층(Fully Connected Layer)을 출력층으로 단어 집합 크기만큼의 뉴런을 배치하여 모델을 설계합니다. 해당 모델은 마지막 시점에서 모든 가능한 단어 중 하나의 단어를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용하여 200 에포크를 수행합니다.\\nembedding_dim = 10\\nhidden_units = 32\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))', \"hidden_units = 32\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(SimpleRNN(hidden_units))\\nmodel.add(Dense(vocab_size, activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.fit(X, y, epochs=200, verbose=2)\\n모델이 정확하게 예측하고 있는지 문장을 생성하는 함수를 만들어서 출력해봅시다.\\ndef sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\\ninit_word = current_word\\nsentence = ''\\n# n번 반복\\nfor _ in range(n):\", \"init_word = current_word\\nsentence = ''\\n# n번 반복\\nfor _ in range(n):\\n# 현재 단어에 대한 정수 인코딩과 패딩\\nencoded = tokenizer.texts_to_sequences([current_word])[0]\\nencoded = pad_sequences([encoded], maxlen=5, padding='pre')\\n# 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\\nresult = model.predict(encoded, verbose=0)\\nresult = np.argmax(result, axis=1)\\nfor word, index in tokenizer.word_index.items():\\n# 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\\nif index == result:\\nbreak\\n# 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\", \"# 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\\nif index == result:\\nbreak\\n# 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\\ncurrent_word = current_word + ' '  + word\\n# 예측 단어를 문장에 저장\\nsentence = sentence + ' ' + word\\nsentence = init_word + sentence\\nreturn sentence\\n입력된 단어로부터 다음 단어를 예측해서 문장을 생성하는 함수를 만들었습니다. '경마장에' 라는 단어 뒤에는 총 4개의 단어가 있으므로 4번 예측해봅시다.\\nprint(sentence_generation(model, tokenizer, '경마장에', 4))\\n경마장에 있는 말이 뛰고 있다\\nprint(sentence_generation(model, tokenizer, '그의', 2))\\n그의 말이 법이다\", \"경마장에 있는 말이 뛰고 있다\\nprint(sentence_generation(model, tokenizer, '그의', 2))\\n그의 말이 법이다\\nprint(sentence_generation(model, tokenizer, '가는', 5))\\n가는 말이 고와야 오는 말이 곱다\\n앞의 문맥을 기준으로 '말이' 라는 단어 다음에 나올 단어를 기존의 훈련 데이터와 일치하게 예측함을 보여줍니다. 이 모델은 충분한 훈련 데이터를 갖고 있지 못하므로 위에서 문장의 길이에 맞게 적절하게 예측해야하는 횟수 4, 2, 5를 각각 인자값으로 주었습니다. 이 이상의 숫자를 주면 기계는 '있다', '법이다', '곱다' 다음에 나오는 단어가 무엇인지 배운 적이 없으므로 임의 예측을 합니다. 이번에는 더 많은 훈련 데이터를 가지고 실습해봅시다.\"]\n",
      "['이번에는 LSTM을 통해 보다 많은 데이터로 텍스트를 생성해보겠습니다. 본질적으로 앞에서 한 것과 동일한 실습입니다.\\n1) 데이터에 대한 이해와 전처리\\n사용할 데이터는 뉴욕 타임즈 기사의 제목입니다. 아래의 링크에서 ArticlesApril2018.csv 데이터를 다운로드 합니다.\\n파일 다운로드 링크 : https://www.kaggle.com/aashita/nyt-comments\\nimport pandas as pd\\nimport numpy as np\\nfrom string import punctuation\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\n다운로드한 훈련 데이터를 데이터프레임에 저장합니다.', \"from tensorflow.keras.utils import to_categorical\\n다운로드한 훈련 데이터를 데이터프레임에 저장합니다.\\ndf = pd.read_csv('ArticlesApril2018.csv')\\ndf.head()\\n책의 지면의 한계로 이번 출력 화면은 생략\\n열의 개수가 굉장히 많기 때문에 한 눈에 보기 어렵습니다. 어떤 열이 있고, 열이 총 몇 개가 있는지 출력해봅시다.\\nprint('열의 개수: ',len(df.columns))\\nprint(df.columns)\\n열의 개수:  15\\nIndex(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\\n'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\", \"'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\\n'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'], dtype='object')\\n총 15개의 열이 존재합니다. 여기서 사용할 열은 제목에 해당되는 headline 열입니다. Null 값이 있는지 확인해봅시다.\\nprint(df['headline'].isnull().values.any())\\nFalse\\nNull 값은 별도로 없는 것으로 보입니다. headline 열에서 모든 신문 기사의 제목을 뽑아서 하나의 리스트로 저장해보도록 하겠습니다.\\nheadline = []\\n# 헤드라인의 값들을 리스트로 저장\\nheadline.extend(list(df.headline.values))\\nheadline[:5]\\nheadline이라는 리스트에 모든 신문 기사의 제목을 저장했습니다. 저장한 리스트에서 상위 5개만 출력해보았습니다.\", \"headline[:5]\\nheadline이라는 리스트에 모든 신문 기사의 제목을 저장했습니다. 저장한 리스트에서 상위 5개만 출력해보았습니다.\\n['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\\n'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\\n'The New Noma, Explained',\\n'Unknown',\\n'Unknown']\\n네번째와 다섯 번째 샘플에 Unknown 값이 들어가있습니다. headline 전체에 걸쳐서 Unknown 값을 가진 샘플이 있을 것으로 추정됩니다. 비록 Null 값은 아니지만 실습에 도움이 되지 않는 노이즈 데이터이므로 제거해줄 필요가 있습니다. 제거하기 전에 현재 샘플의 개수를 확인해보고 제거 전, 후의 샘플의 개수를 비교해봅시다.\", 'print(\\'총 샘플의 개수 : {}\\'.format(len(headline)))\\n총 샘플의 개수 : 1324\\n노이즈 데이터를 제거하기 전 신문 기사의 제목 샘플은 총 1,324개입니다. Unknown 값을 가진 샘플을 제거합니다.\\nheadline = [word for word in headline if word != \"Unknown\"]\\nprint(\\'노이즈값 제거 후 샘플의 개수 : {}\\'.format(len(headline)))\\n노이즈값 제거 후 샘플의 개수 : 1214\\n샘플의 수가 1,324에서 1,214로 110개의 샘플이 제거되었는데 기존에 출력했던 5개의 샘플을 출력해봅시다.\\nheadline[:5]\\n[\\'Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell\\',\\n\\'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.\\',', '\\'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.\\',\\n\\'The New Noma, Explained\\',\\n\\'How a Bag of Texas Dirt  Became a Times Tradition\\',\\n\\'Is School a Place for Self-Expression?\\']\\n기존에 네번째, 다섯 번째 샘플에서는 Unknown 값이 있었는데 현재는 제거가 되었습니다. 이제 데이터 전처리를 수행합니다. 여기서 선택한 전처리는 구두점 제거와 단어의 소문자화입니다. 전처리를 수행하고, 다시 샘플 5개를 출력합니다.\\ndef repreprocessing(raw_sentence):\\npreproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",\\'ignore\\')\\n# 구두점 제거와 동시에 소문자화', 'preproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",\\'ignore\\')\\n# 구두점 제거와 동시에 소문자화\\nreturn \\'\\'.join(word for word in preproceseed_sentence if word not in punctuation).lower()\\npreprocessed_headline = [repreprocessing(x) for x in headline]\\npreprocessed_headline[:5]\\n[\\'former nfl cheerleaders settlement offer 1 and a meeting with goodell\\',\\n\\'epa to unveil a new rule its effect less science in policymaking\\',\\n\\'the new noma explained\\',\\n\\'how a bag of texas dirt  became a times tradition\\',', \"'the new noma explained',\\n'how a bag of texas dirt  became a times tradition',\\n'is school a place for selfexpression']\\n기존의 출력과 비교하면 모든 단어들이 소문자화되었으며 N.F.L.이나 Cheerleaders’ 등과 같이 기존에 구두점이 붙어있던 단어들에서 구두점이 제거되었습니다. 단어 집합(vocabulary)을 만들고 크기를 확인합니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(preprocessed_headline)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint('단어 집합의 크기 : %d' % vocab_size)\\n단어 집합의 크기 : 3494\\n총 3,494개의 단어가 존재합니다. 정수 인코딩을 진행하는 동시에 하나의 문장을 여러 줄로 분해하여 훈련 데이터를 구성합니다.\", '단어 집합의 크기 : 3494\\n총 3,494개의 단어가 존재합니다. 정수 인코딩을 진행하는 동시에 하나의 문장을 여러 줄로 분해하여 훈련 데이터를 구성합니다.\\nsequences = list()\\nfor sentence in preprocessed_headline:\\n# 각 샘플에 대한 정수 인코딩\\nencoded = tokenizer.texts_to_sequences([sentence])[0]\\nfor i in range(1, len(encoded)):\\nsequence = encoded[:i+1]\\nsequences.append(sequence)\\nsequences[:11]\\n[[99, 269], # former nfl\\n[99, 269, 371], # former nfl cheerleaders\\n[99, 269, 371, 1115], # former nfl cheerleaders settlement', \"[99, 269, 371, 1115], # former nfl cheerleaders settlement\\n[99, 269, 371, 1115, 582], # former nfl cheerleaders settlement offer\\n[99, 269, 371, 1115, 582, 52], # 'former nfl cheerleaders settlement offer 1\\n[99, 269, 371, 1115, 582, 52, 7], # former nfl cheerleaders settlement offer 1 and\\n[99, 269, 371, 1115, 582, 52, 7, 2], # ... 이하 생략 ...\\n[99, 269, 371, 1115, 582, 52, 7, 2, 372],\\n[99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\", \"[99, 269, 371, 1115, 582, 52, 7, 2, 372],\\n[99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\\n[99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116], # 모든 단어가 사용된 완전한 첫번째 문장\\n# 바로 위의 줄은 : former nfl cheerleaders settlement offer 1 and a meeting with goodell\\n[100, 3]] # epa to에 해당되며 두번째 문장이 시작됨.\\n이해를 돕기 위해 출력 결과에 주석을 추가하였습니다. 왜 하나의 문장을 저렇게 나눌까요? 예를 들어 '경마장에 있는 말이 뛰고 있다' 라는 문장 하나가 있을 때, 최종적으로 원하는 훈련 데이터의 형태는 다음과 같습니다. 하나의 단어를 예측하기 위해 이전에 등장한 단어들을 모두 참고하는 것입니다.\\nsamples\\n$X$\\n$y$\\n1\\n경마장에\\n있는\\n2\\n경마장에 있는\\n말이\\n3\", \"samples\\n$X$\\n$y$\\n1\\n경마장에\\n있는\\n2\\n경마장에 있는\\n말이\\n3\\n경마장에 있는 말이\\n뛰고\\n4\\n경마장에 있는 말이 뛰고\\n있다\\n위의 sequences는 모든 문장을 각 단어가 각 시점(time step)마다 하나씩 추가적으로 등장하는 형태로 만들기는 했지만, 아직 예측할 단어에 해당되는 레이블을 분리하는 작업까지는 수행하지 않은 상태입니다. 어떤 정수가 어떤 단어를 의미하는지 알아보기 위해 인덱스로부터 단어를 찾는 index_to_word를 만듭니다.\\nindex_to_word = {}\\nfor key, value in tokenizer.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\\nindex_to_word[value] = key\\nprint('빈도수 상위 582번 단어 : {}'.format(index_to_word[582]))\\n빈도수 상위 582번 단어 : offer\", \"print('빈도수 상위 582번 단어 : {}'.format(index_to_word[582]))\\n빈도수 상위 582번 단어 : offer\\n582이라는 인덱스를 가진 단어는 본래 offer라는 단어였습니다. 원한다면 다른 숫자로도 시도해보세요. 이제 $y$데이터를 분리하기 전에 전체 샘플의 길이를 동일하게 만드는 패딩 작업을 수행합니다. 패딩 작업을 수행하기 전에 가장 긴 샘플의 길이를 확인합니다.\\nmax_len = max(len(l) for l in sequences)\\nprint('샘플의 최대 길이 : {}'.format(max_len))\\n샘플의 최대 길이 : 24\\n가장 긴 샘플의 길이인 24로 모든 샘플의 길이를 패딩하겠습니다.\\nsequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\\nprint(sequences[:3])\", \"sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\\nprint(sequences[:3])\\n[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0    0    0   99  269]\\n[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0    0   99  269  371]\\n[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   99  269  371 1115]\\npadding='pre'를 설정하여 샘플의 길이가 24보다 짧은 경우에 앞에 0으로 패딩되었습니다. 이제 맨 우측 단어만 레이블로 분리합니다.\\nsequences = np.array(sequences)\\nX = sequences[:,:-1]\\ny = sequences[:,-1]\", 'sequences = np.array(sequences)\\nX = sequences[:,:-1]\\ny = sequences[:,-1]\\nprint(X[:3])\\n[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0    0    0   99]\\n[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0    0   99  269]\\n[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   99  269  371]\\n훈련 데이터 X에서 3개의 샘플만 출력해보았는데, 맨 우측에 있던 정수값 269, 371, 1115가 사라진 것을 볼 수 있습니다. 뿐만 아니라, 각 샘플의 길이가 24에서 23으로 줄었습니다.\\nprint(y[:3])\\n[ 269  371 1115]', 'print(y[:3])\\n[ 269  371 1115]\\n훈련 데이터 y 중 3개의 샘플만 출력해보았는데, 기존 훈련 데이터에서 맨 우측에 있던 정수들이 별도로 저장되었습니다.\\ny = to_categorical(y, num_classes=vocab_size)\\n레이블 데이터 y에 대해서 원-핫 인코딩을 수행하였습니다. 이제 모델을 설계합니다.\\n2) 모델 설계하기\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM', 'from tensorflow.keras.layers import Embedding, Dense, LSTM\\n하이퍼파라미터인 임베딩 벡터의 차원은 10, 은닉 상태의 크기는 128입니다. 다 대 일 구조의 LSTM을 사용합니다. 전결합층(Fully Connected Layer)을 출력층으로 단어 집합 크기만큼의 뉴런을 배치하여 모델을 설계합니다. 해당 모델은 마지막 시점에서 모든 가능한 단어 중 하나의 단어를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용하여 200 에포크를 수행합니다.\\nembedding_dim = 10\\nhidden_units = 128\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))', \"hidden_units = 128\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(LSTM(hidden_units))\\nmodel.add(Dense(vocab_size, activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.fit(X, y, epochs=200, verbose=2)\\n문장을 생성하는 함수 sentence_generation을 만들어서 문장을 생성해봅시다.\\ndef sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\\ninit_word = current_word\\nsentence = ''\\n# n번 반복\\nfor _ in range(n):\", \"init_word = current_word\\nsentence = ''\\n# n번 반복\\nfor _ in range(n):\\nencoded = tokenizer.texts_to_sequences([current_word])[0]\\nencoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre')\\n# 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\\nresult = model.predict(encoded, verbose=0)\\nresult = np.argmax(result, axis=1)\\nfor word, index in tokenizer.word_index.items():\\n# 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\\nif index == result:\\nbreak\\n# 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\", \"# 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\\nif index == result:\\nbreak\\n# 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\\ncurrent_word = current_word + ' '  + word\\n# 예측 단어를 문장에 저장\\nsentence = sentence + ' ' + word\\nsentence = init_word + sentence\\nreturn sentence\\n임의의 단어 'i'에 대해서 10개의 단어를 추가 생성해봅시다.\\nprint(sentence_generation(model, tokenizer, 'i', 10))\\ni disapprove of school vouchers can i still apply for them\\n임의의 단어 'how'에 대해서 10개의 단어를 추가 생성해봅시다.\\nprint(sentence_generation(model, tokenizer, 'how', 10))\", \"임의의 단어 'how'에 대해서 10개의 단어를 추가 생성해봅시다.\\nprint(sentence_generation(model, tokenizer, 'how', 10))\\nhow to make facebook more accountable will so your neighbor chasing\\n==================================================\\n--- 08-07 문자 단위 RNN(Char RNN) ---\\n```\\nI get on with life as a programmer, I like to hang out with programming and deep learning.\\n```지금까지 배운 RNN은 전부 입력과 출력의 단위가 단어 벡터였습니다. 하지만 입출력의 단위를 단어 레벨(word-level)에서 문자 레벨(character-level)로 변경하여 RNN을 구현할 수 있습니다.\\n[이미지: ]\", '[이미지: ]\\n위의 그림은 문자 단위 RNN을 다 대 다(Many-to-Many) 구조로 구현한 경우, 다 대 일(Many-to-One) 구조로 구현한 경우 두 가지를 보여줍니다. 여기서는 이 두 가지 모두 구현해보겠습니다. 첫번째로 구현할 것은 다 대 다 구조를 이용한 언어 모델입니다.']\n",
      "[\"이전 시점의 예측 문자를 다음 시점의 입력으로 사용하는 문자 단위 RNN 언어 모델을 구현해봅시다. 앞서 배운 단어 단위 RNN 언어 모델과 다른 점은 단어 단위가 아니라 문자 단위를 입, 출력으로 사용하므로 임베딩층(embedding layer)을 여기서는 사용하지 않겠습니다. 여기서는 언어 모델의 훈련 과정과 테스트 과정의 차이를 이해하는데 집중합니다.\\n다운로드 링크 : http://www.gutenberg.org/files/11/11-0.txt\\n고전 소설들은 저작권에 보호받지 않으므로 무료로 다운받을 수 있습니다. 위의 링크에서 '이상한 나라의 앨리스(Alice’s Adventures in Wonderland)'라는 소설을 다운로드 합니다.\\n1) 데이터에 대한 이해와 전처리\\n데이터를 로드하고 특수문자를 제거하고 단어를 소문자화하는 간단한 전처리를 수행합니다.\\nimport numpy as np\\nimport urllib.request\", '데이터를 로드하고 특수문자를 제거하고 단어를 소문자화하는 간단한 전처리를 수행합니다.\\nimport numpy as np\\nimport urllib.request\\nfrom tensorflow.keras.utils import to_categorical\\n# 데이터 로드\\nurllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\\nf = open(\\'11-0.txt\\', \\'rb\\')\\nsentences = []\\nfor sentence in f: # 데이터로부터 한 줄씩 읽는다.\\nsentence = sentence.strip() # strip()을 통해 \\\\r, \\\\n을 제거한다.\\nsentence = sentence.lower() # 소문자화.\\nsentence = sentence.decode(\\'ascii\\', \\'ignore\\') # \\\\xe2\\\\x80\\\\x99 등과 같은 바이트 열 제거', \"sentence = sentence.decode('ascii', 'ignore') # \\\\xe2\\\\x80\\\\x99 등과 같은 바이트 열 제거\\nif len(sentence) > 0:\\nsentences.append(sentence)\\nf.close()\\n전처리가 수행된 결과가 리스트에 저장되었습니다. 리스트에서 5개의 원소만 출력해보겠습니다.\\nsentences[:5]\\n['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\\n'this ebook is for the use of anyone anywhere in the united states and',\\n'most other parts of the world at no cost and with almost no restrictions',\", \"'most other parts of the world at no cost and with almost no restrictions',\\n'whatsoever. you may copy it, give it away or re-use it under the terms',\\n'of the project gutenberg license included with this ebook or online at']\\n리스트의 원소는 문자열로 구성되어져 있는데 의미있게 문장 토큰화가 된 상태는 아닙니다. 이를 하나의 문자열로 통합하겠습니다.\\ntotal_data = ' '.join(sentences)\\nprint('문자열의 길이 또는 총 문자의 개수: %d' % len(total_data))\\n문자열의 길이 또는 총 문자의 개수: 159484\\n하나의 문자열로 통합되었고, 문자열의 길이는 약 15만 9천입니다. 일부 출력해보겠습니다.\\nprint(total_data[:200])\", \"하나의 문자열로 통합되었고, 문자열의 길이는 약 15만 9천입니다. 일부 출력해보겠습니다.\\nprint(total_data[:200])\\nthe project gutenberg ebook of alices adventures in wonderland, by lewis carroll this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with\\n이 문자열로부터 문자 집합을 만들겠습니다. 기존에는 중복을 제거한 단어들의 모음인 단어 집합(vocabulary)을 만들었으나, 이번에 만들 집합은 단어 집합이 아니라 문자 집합입니다.\\nchar_vocab = sorted(list(set(total_data)))\\nvocab_size = len(char_vocab)\\nprint ('문자 집합의 크기 : {}'.format(vocab_size))\", \"vocab_size = len(char_vocab)\\nprint ('문자 집합의 크기 : {}'.format(vocab_size))\\n문자 집합의 크기 : 56\\n영어가 훈련 데이터일 때 문자 집합의 크기는 단어 집합을 사용했을 경우보다 집합의 크기가 현저히 작습니다. 아무리 훈련 코퍼스에 수십만 개 이상의 많은 영어 단어가 존재한다고 하더라도, 영어 단어를 표현하기 위해서 사용되는 문자는 26개의 알파벳뿐이기 때문입니다. 만약 훈련 데이터의 알파벳이 대, 소문자가 구분된 상태라고 하더라도 모든 영어 단어는 총 52개의 알파벳으로 표현 가능합니다.\\n어떤 방대한 양의 텍스트라도 집합의 크기를 적게 가져갈 수 있다는 것은 구현과 테스트를 굉장히 쉽게 할 수 있다는 이점을 가지므로, RNN의 동작 메커니즘 이해를 위한 토이 프로젝트 용도로 유용합니다. 문자 집합의 각 문자에 정수를 부여하고 출력해보겠습니다.\\n# 문자에 고유한 정수 부여\", \"# 문자에 고유한 정수 부여\\nchar_to_index = dict((char, index) for index, char in enumerate(char_vocab))\\nprint('문자 집합 :',char_to_index)\", '문자 집합 : {\\' \\': 0, \\'!\\': 1, \\'\"\\': 2, \\'#\\': 3, \\'$\\': 4, \\'%\\': 5, \"\\'\": 6, \\'(\\': 7, \\')\\': 8, \\'*\\': 9, \\',\\': 10, \\'-\\': 11, \\'.\\': 12, \\'/\\': 13, \\'0\\': 14, \\'1\\': 15, \\'2\\': 16, \\'3\\': 17, \\'4\\': 18, \\'5\\': 19, \\'6\\': 20, \\'7\\': 21, \\'8\\': 22, \\'9\\': 23, \\':\\': 24, \\';\\': 25, \\'?\\': 26, \\'[\\': 27, \\']\\': 28, \\'_\\': 29, \\'a\\': 30, \\'b\\': 31, \\'c\\': 32, \\'d\\': 33, \\'e\\': 34, \\'f\\': 35, \\'g\\': 36, \\'h\\': 37, \\'i\\': 38, \\'j\\': 39, \\'k\\': 40, \\'l\\': 41, \\'m\\': 42, \\'n\\': 43, \\'o\\': 44, \\'p\\': 45, \\'q\\': 46, \\'r\\': 47, \\'s\\': 48, \\'t\\': 49, \\'u\\': 50, \\'v\\': 51, \\'w\\': 52, \\'x\\': 53, \\'y\\': 54, \\'z\\':', \"44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55}\", '정수 0부터 28까지는 공백을 포함한 각종 구두점, 특수문자가 존재하고, 정수 29부터 54까지는 a부터 z까지 총 26개의 알파벳 소문자가 문자 집합에 포함되어져 있습니다. 반대로 정수로부터 문자를 리턴하는 index_to_char을 만듭니다.\\nindex_to_char = {}\\nfor key, value in char_to_index.items():\\nindex_to_char[value] = key', \"index_to_char = {}\\nfor key, value in char_to_index.items():\\nindex_to_char[value] = key\\n훈련 데이터를 구성해보겠습니다. 훈련 데이터 구성을 위한 간소화 된 예를 들어봅시다. 훈련 데이터에 apple이라는 시퀀스가 있고, 입력의 길이를 4라고 정하였을 때 데이터의 구성은 어떻게 될까요? 입력의 길이가 4이므로 입력 시퀀스와 예측해야 하는 출력 시퀀스 모두 길이는 4가 됩니다. 다시 말해 RNN은 총 네 번의 시점을(timestep)을 가질 수 있다는 의미입니다. apple은 다섯 글자이지만 입력의 길이는 4이므로 'appl'까지만 입력으로 사용할 수 있습니다. 그리고 언어 모델은 다음 시점의 입력을 예측해야하는 모델이므로 'pple'를 예측하도록 데이터가 구성됩니다.\\n# appl (입력 시퀀스) -> pple (예측해야하는 시퀀스)\\ntrain_X = 'appl'\\ntrain_y = 'pple'\", \"# appl (입력 시퀀스) -> pple (예측해야하는 시퀀스)\\ntrain_X = 'appl'\\ntrain_y = 'pple'\\n이제 15만 9천의 길이를 가진 문자열로부터 다수의 샘플들을 만들어보겠습니다. 데이터를 만드는 방법은 문장 샘플의 길이를 정하고, 해당 길이만큼 문자열 전체를 등분하는 것입니다. 여기서는 문장의 길이를 60으로 정했는데, 결국 15만 9천을 60으로 나눈 수가 샘플의 수가 됩니다. 몇 개의 샘플을 만들 수 있을지 그 개수를 계산해봅시다.\\nseq_length = 60\\n# 문자열의 길이를 seq_length로 나누면 전처리 후 생겨날 샘플 수\\nn_samples = int(np.floor((len(total_data) - 1) / seq_length))\\nprint ('샘플의 수 : {}'.format(n_samples))\\n샘플의 수 : 2658\", \"print ('샘플의 수 : {}'.format(n_samples))\\n샘플의 수 : 2658\\n여기서는 총 샘플의 수가 2,658개가 될 예정입니다. 이제 전처리를 진행합니다. 전처리가 어떻게 진행되었는지는 전처리 후 얻은 train_X와 train_y를 통해 설명하겠습니다.\\ntrain_X = []\\ntrain_y = []\\nfor i in range(n_samples):\\n# 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 pick.\\nX_sample = total_data[i * seq_length: (i + 1) * seq_length]\\n# 정수 인코딩\\nX_encoded = [char_to_index[c] for c in X_sample]\\ntrain_X.append(X_encoded)\\n# 오른쪽으로 1칸 쉬프트\\ny_sample = total_data[i * seq_length + 1: (i + 1) * seq_length + 1]\", \"# 오른쪽으로 1칸 쉬프트\\ny_sample = total_data[i * seq_length + 1: (i + 1) * seq_length + 1]\\ny_encoded = [char_to_index[c] for c in y_sample]\\ntrain_y.append(y_encoded)\\ntrain_X와 train_y의 첫번째 샘플을 출력해봅시다.\\nprint('X 데이터의 첫번째 샘플 :',train_X[0])\\nprint('y 데이터의 첫번째 샘플 :',train_y[0])\\nprint('-'*50)\\nprint('X 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_X[0]])\\nprint('y 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_y[0]])\", \"print('y 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_y[0]])\\nX 데이터의 첫번째 샘플 : [49, 37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30]\", 'y 데이터의 첫번째 샘플 : [37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30, 43]\\n--------------------------------------------------', \"--------------------------------------------------\\nX 데이터의 첫번째 샘플 디코딩 : ['t', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a']\", \"y 데이터의 첫번째 샘플 디코딩 : ['h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n']\\ntrain_y[0]은 train_X[0]에서 오른쪽으로 한 칸 쉬프트 된 문장임을 알 수 있습니다. train_X와 train_y의 두번째 샘플. 즉, 인덱스가 1번인 샘플을 출력하여 데이터의 구성을 확인해봅시다.\\nprint(train_X[1])\", 'print(train_X[1])\\n[43, 33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54]\\nprint(train_y[1])\\n[33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54, 52]', \"마찬가지로 train_y[1]은 train_X[1]에서 오른쪽으로 한 칸 쉬프트 된 문장임을 알 수 있습니다. 이제 train_X와 train_y에 대해서 원-핫 인코딩을 수행합니다. 문자 단위 RNN에서는 입력 시퀀스에 대해서 워드 임베딩을 하지 않습니다. 다시 말해 임베딩층(embedding layer)을 사용하지 않을 것이므로, 입력 시퀀스인 train_X에 대해서도 원-핫 인코딩을 합니다.\\ntrain_X = to_categorical(train_X)\\ntrain_y = to_categorical(train_y)\\nprint('train_X의 크기(shape) : {}'.format(train_X.shape)) # 원-핫 인코딩\\nprint('train_y의 크기(shape) : {}'.format(train_y.shape)) # 원-핫 인코딩\\ntrain_X의 크기(shape) : (2658, 60, 56)\\ntrain_y의 크기(shape) : (2658, 60, 56)\", 'train_X의 크기(shape) : (2658, 60, 56)\\ntrain_y의 크기(shape) : (2658, 60, 56)\\ntrain_X와 train_y의 크기는 2,658 × 60 × 56 입니다.\\n[이미지: ]\\n이는 샘플의 수(No. of samples)가 2,658개, 입력 시퀀스의 길이(input_length)가 60, 각 벡터의 차원(input_dim)이 55임을 의미합니다. 원-핫 벡터의 차원은 문자 집합의 크기인 56이어야 하므로 원-핫 인코딩이 수행되었음을 알 수 있습니다.\\n2) 모델 설계하기', '2) 모델 설계하기\\n하이퍼파라미터인 은닉 상태의 크기는 256입니다. 모델은 다 대 다 구조의 LSTM을 사용하며, LSTM 은닉층은 두 개를 사용합니다. 전결합층(Fully Connected Layer)을 출력층으로 문자 집합 크기만큼의 뉴런을 배치하여 모델을 설계합니다. 해당 모델은 모든 시점에서 모든 가능한 문자 중 하나의 문자를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용하여 80 에포크를 수행합니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, LSTM, TimeDistributed\\nhidden_units = 256\\nmodel = Sequential()', \"hidden_units = 256\\nmodel = Sequential()\\nmodel.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True))\\nmodel.add(LSTM(hidden_units, return_sequences=True))\\nmodel.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.fit(train_X, train_y, epochs=80, verbose=2)\", \"model.fit(train_X, train_y, epochs=80, verbose=2)\\n특정 문자를 주면 다음 문자를 계속해서 생성해내는 sentence_generation 함수를 구현합니다. 인자로 학습한 모델. 그리고 모델로 다음 문자를 몇 번 생성할 것인지 횟수를 전달해주면, 해당 함수는 임의로 시작 문자를 정한 뒤에 정해진 횟수만큼의 다음 문자를 지속적으로 예측하여 문장을 생성해냅니다.\\ndef sentence_generation(model, length):\\n# 문자에 대한 랜덤한 정수 생성\\nix = [np.random.randint(vocab_size)]\\n# 랜덤한 정수로부터 맵핑되는 문자 생성\\ny_char = [index_to_char[ix[-1]]]\\nprint(ix[-1],'번 문자',y_char[-1],'로 예측을 시작!')\\n# (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\", 'print(ix[-1],\\'번 문자\\',y_char[-1],\\'로 예측을 시작!\\')\\n# (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\\nX = np.zeros((1, length, vocab_size))\\nfor i in range(length):\\n# X[0][i][예측한 문자의 인덱스] = 1, 즉, 예측 문자를 다음 입력 시퀀스에 추가\\nX[0][i][ix[-1]] = 1\\nprint(index_to_char[ix[-1]], end=\"\")\\nix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\\ny_char.append(index_to_char[ix[-1]])\\nreturn (\\'\\').join(y_char)\\nresult = sentence_generation(model, 100)\\nprint(result)\\n49 번 문자 u 로 예측을 시작!', \"return ('').join(y_char)\\nresult = sentence_generation(model, 100)\\nprint(result)\\n49 번 문자 u 로 예측을 시작!\\nury-men would have done just as well. the twelve jurors were to say in that dide. he went on in a di'\\n자세히 보면 사실 말이 되지 않는 문장이지만, 언뜻 보기에 그럴듯해 보이는 문장을 생성해냅니다.\"]\n",
      "[\"이번에는 다 대 일 구조의 RNN을 문자 단위로 학습시키고, 텍스트 생성을 해보겠습니다.\\n1) 데이터에 대한 이해와 전처리\\nimport numpy as np\\nfrom tensorflow.keras.utils import to_categorical\\n다음과 같이 제가 임의로 만든 엉터리 노래 가사가 있습니다.\\nraw_text = '''\\nI get on with life as a programmer,\\nI like to contemplate beer.\\nBut when I start to daydream,\\nMy mind turns straight to wine.\\nDo I love wine more than beer?\\nI like to use words about beer.\\nBut when I stop my talking,\\nMy mind turns straight to wine.\\nI hate bugs and errors.\\nBut I just think back to wine,\", \"My mind turns straight to wine.\\nI hate bugs and errors.\\nBut I just think back to wine,\\nAnd I'm happy once again.\\nI like to hang out with programming and deep learning.\\nBut when left alone,\\nMy mind turns straight to wine.\\n'''\\n위의 텍스트에 존재하는 단락 구분을 없애고 하나의 문자열로 재저장하겠습니다.\\ntokens = raw_text.split()\\nraw_text = ' '.join(tokens)\\nprint(raw_text)\", \"raw_text = ' '.join(tokens)\\nprint(raw_text)\\nI get on with life as a programmer, I like to contemplate beer. But when I start to daydream, My mind turns straight to wine. Do I love wine more than beer? I like to use words about beer. But when I stop my talking, My mind turns straight to wine. I hate bugs and errors. But I just think back to wine, And I'm happy once again. I like to hang out with programming and deep learning. But when left alone, My mind turns straight to wine.\", '단락 구분이 없어지고 하나의 문자열로 재저장되었습니다. 이로부터 문자 집합을 만들어보겠습니다. 기존에는 중복을 제거한 단어들의 모음인 단어 집합(vocabulary)을 만들었으나, 이번에 만들 집합은 단어 집합이 아니라 문자 집합입니다.\\n# 중복을 제거한 문자 집합 생성\\nchar_vocab = sorted(list(set(raw_text)))\\nvocab_size = len(char_vocab)\\nprint(\\'문자 집합 :\\',char_vocab)\\nprint (\\'문자 집합의 크기 : {}\\'.format(vocab_size))\\n문자 집합 : [\\' \\', \"\\'\", \\',\\', \\'.\\', \\'?\\', \\'A\\', \\'B\\', \\'D\\', \\'I\\', \\'M\\', \\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\', \\'f\\', \\'g\\', \\'h\\', \\'i\\', \\'j\\', \\'k\\', \\'l\\', \\'m\\', \\'n\\', \\'o\\', \\'p\\', \\'r\\', \\'s\\', \\'t\\', \\'u\\', \\'v\\', \\'w\\', \\'y\\']\\n문자 집합의 크기 : 33', '문자 집합의 크기 : 33\\n알파벳 또는 구두점 등의 단위의 집합인 문자 집합이 생성되었습니다. 문자 집합의 크기는 33입니다.\\nchar_to_index = dict((char, index) for index, char in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\\nprint(char_to_index)\\n{\\' \\': 0, \"\\'\": 1, \\',\\': 2, \\'.\\': 3, \\'?\\': 4, \\'A\\': 5, \\'B\\': 6, \\'D\\': 7, \\'I\\': 8, \\'M\\': 9, \\'a\\': 10, \\'b\\': 11, \\'c\\': 12, \\'d\\': 13, \\'e\\': 14, \\'f\\': 15, \\'g\\': 16, \\'h\\': 17, \\'i\\': 18, \\'j\\': 19, \\'k\\': 20, \\'l\\': 21, \\'m\\': 22, \\'n\\': 23, \\'o\\': 24, \\'p\\': 25, \\'r\\': 26, \\'s\\': 27, \\'t\\': 28, \\'u\\': 29, \\'v\\': 30, \\'w\\': 31, \\'y\\': 32}', '이번 실습의 문자 집합의 경우 훈련 데이터에 등장한 알파벳의 대, 소문자를 구분하고 구두점과 공백을 포함하였습니다. 이제 훈련에 사용할 문장 샘플들을 만들어보겠습니다. 여기서는 RNN을 이용한 생성한 텍스트 챕터와 유사하게 데이터를 구성합니다. 다만, 단위가 문자 단위라는 점이 다릅니다. 예를 들어 훈련 데이터에 student라는 단어가 있고, 입력 시퀀스의 길이를 5라고 한다면 입력 시퀀스와 예측해야하는 문자는 다음과 같이 구성됩니다. 5개의 입력 문자 시퀀스로부터 다음 문자 시퀀스를 예측하는 것입니다. 즉, RNN의 시점(timesteps)은 5번입니다.\\nstude -> n\\ntuden -> t\\n여기서는 입력 시퀀스의 길이가 10가 되도록 데이터를 구성하겠습니다. 예측 대상인 문자도 필요하므로 길이가 11이 되도록 데이터를 구성합니다.\\nlength = 11\\nsequences = []\\nfor i in range(length, len(raw_text)):', \"length = 11\\nsequences = []\\nfor i in range(length, len(raw_text)):\\nseq = raw_text[i-length:i] # 길이 11의 문자열을 지속적으로 만든다.\\nsequences.append(seq)\\nprint('총 훈련 샘플의 수: %d' % len(sequences))\\n총 훈련 샘플의 수: 426\\n총 샘플의 수는 426개가 완성되었습니다. 이 중 10개만 출력해보겠습니다.\\nsequences[:10]\\n['I get on wi',\\n' get on wit',\\n'get on with',\\n'et on with ',\\n't on with l',\\n' on with li',\\n'on with lif',\\n'n with life',\\n' with life ',\\n'with life a']\", \"'t on with l',\\n' on with li',\\n'on with lif',\\n'n with life',\\n' with life ',\\n'with life a']\\n첫번째 문장이었던 'I get on with life as a programmer,'가 10개의 샘플로 분리된 것을 확인할 수 있습니다. 다른 문장들에 대해서도 sequences에 모두 저장되어져 있습니다. 원한다면, sequences[30:45] 등과 같이 인덱스 범위를 변경하여 출력해보시기 바랍니다. 이제 앞서 만든 char_to_index를 사용하여 전체 데이터에 대해서 정수 인코딩을 수행합니다.\\nencoded_sequences = []\\nfor sequence in sequences: # 전체 데이터에서 문장 샘플을 1개씩 꺼낸다.\\nencoded_sequence = [char_to_index[char] for char in sequence] # 문장 샘플에서 각 문자에 대해서 정수 인코딩을 수행.\", 'encoded_sequence = [char_to_index[char] for char in sequence] # 문장 샘플에서 각 문자에 대해서 정수 인코딩을 수행.\\nencoded_sequences.append(encoded_sequence)\\n정수 인코딩 된 결과가 X에 저장되었습니다. 5개만 출력해보겠습니다.\\nencoded_sequences[:5]\\n[8, 0, 16, 14, 28, 0, 24, 23, 0, 31, 18]\\n[0, 16, 14, 28, 0, 24, 23, 0, 31, 18, 28]\\n[16, 14, 28, 0, 24, 23, 0, 31, 18, 28, 17]\\n[14, 28, 0, 24, 23, 0, 31, 18, 28, 17, 0]\\n[28, 0, 24, 23, 0, 31, 18, 28, 17, 0, 21]', '[14, 28, 0, 24, 23, 0, 31, 18, 28, 17, 0]\\n[28, 0, 24, 23, 0, 31, 18, 28, 17, 0, 21]\\n정수 인코딩이 수행되었습니다. 예측 대상인 문자를 분리시켜주는 작업을 해봅시다. 모든 샘플 문장에 대해서 마지막 문자를 분리하여 마지막 문자가 분리된 샘플은 X_data에 저장하고, 마지막 문자는 y_data에 저장합니다.\\nencoded_sequences = np.array(encoded_sequences)\\n# 맨 마지막 위치의 문자를 분리\\nX_data = encoded_sequences[:,:-1]\\n# 맨 마지막 위치의 문자를 저장\\ny_data = encoded_sequences[:,-1]\\n정상적으로 분리가 되었는지 X와 y 둘 다 5개씩 출력해보겠습니다.\\nprint(X_data[:5])\\nprint(y_data[:5])\\n[ 8  0 16 14 28  0 24 23  0 31]\\n[ 0 16 14 28  0 24 23  0 31 18]', 'print(X_data[:5])\\nprint(y_data[:5])\\n[ 8  0 16 14 28  0 24 23  0 31]\\n[ 0 16 14 28  0 24 23  0 31 18]\\n[16 14 28  0 24 23  0 31 18 28]\\n[14 28  0 24 23  0 31 18 28 17]\\n[28  0 24 23  0 31 18 28 17  0]\\n[18 28 17  0 21]\\n앞서 출력한 5개의 샘플에서 각각 맨 뒤의 문자였던 18, 28, 17, 0, 21이 별도로 분리되어 y에 저장되었습니다. 이제 X와 y에 대해서 원-핫 인코딩을 수행해보겠습니다.\\n# 원-핫 인코딩\\nX_data_one_hot = [to_categorical(encoded, num_classes=vocab_size) for encoded in X_data]\\nX_data_one_hot = np.array(X_data_one_hot)', 'X_data_one_hot = np.array(X_data_one_hot)\\ny_data_one_hot = to_categorical(y_data, num_classes=vocab_size)\\n원-핫 인코딩이 수행되었는지 확인하기 위해 수행한 후의 X의 크기(shape)를 보겠습니다.\\nprint(X_data_one_hot.shape)\\n(426, 10, 33)\\n현재 X의 크기는 426 × 10 × 33 입니다.\\n[이미지: ]\\n이는 샘플의 수(No. of samples)가 426개, 입력 시퀀스의 길이(input_length)가 10, 각 벡터의 차원(input_dim)이 33임을 의미합니다. 원-핫 벡터의 차원은 문자 집합의 크기인 33이어야 하므로 X에 대해서 원-핫 인코딩이 수행되었음을 알 수 있습니다.\\n2) 모델 설계하기', '2) 모델 설계하기\\n하이퍼파라미터인 은닉 상태의 크기는 64입니다. 모델은 다 대 일 구조의 LSTM을 사용합니다. 전결합층(Fully Connected Layer)을 출력층으로 문자 집합 크기만큼의 뉴런을 배치하여 모델을 설계합니다. 해당 모델은 마지막 시점에서 모든 가능한 문자 중 하나의 문자를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용하여 100 에포크를 수행합니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, LSTM\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nhidden_units = 64', \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\nhidden_units = 64\\nmodel = Sequential()\\nmodel.add(LSTM(hidden_units, input_shape=(X_data_one_hot.shape[1], X_data_one_hot.shape[2])))\\nmodel.add(Dense(vocab_size, activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.fit(X_data_one_hot, y_data_one_hot, epochs=100, verbose=2)\", \"model.fit(X_data_one_hot, y_data_one_hot, epochs=100, verbose=2)\\n문장을 생성하는 함수 sentence_generation을 만들어서 문장을 생성해봅시다. 해당 함수는 문자열을 입력하면, 해당 문자열로부터 다음 문자를 예측하는 것을 반복하여 최종적으로 문장을 완성합니다.\\ndef sentence_generation(model, char_to_index, seq_length, seed_text, n):\\n# 초기 시퀀스\\ninit_text = seed_text\\nsentence = ''\\n# 다음 문자 예측은 총 n번만 반복.\\nfor _ in range(n):\\nencoded = [char_to_index[char] for char in seed_text] # 현재 시퀀스에 대한 정수 인코딩\\nencoded = pad_sequences([encoded], maxlen=seq_length, padding='pre') # 데이터에 대한 패딩\", \"encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre') # 데이터에 대한 패딩\\nencoded = to_categorical(encoded, num_classes=len(char_to_index))\\n# 입력한 X(현재 시퀀스)에 대해서 y를 예측하고 y(예측한 문자)를 result에 저장.\\nresult = model.predict(encoded, verbose=0)\\nresult = np.argmax(result, axis=1)\\nfor char, index in char_to_index.items():\\nif index == result:\\nbreak\\n# 현재 시퀀스 + 예측 문자를 현재 시퀀스로 변경\\nseed_text = seed_text + char\\n# 예측 문자를 문장에 저장\\nsentence = sentence + char\\n# n번의 다음 문자 예측이 끝나면 최종 완성된 문장을 리턴.\", \"# 예측 문자를 문장에 저장\\nsentence = sentence + char\\n# n번의 다음 문자 예측이 끝나면 최종 완성된 문장을 리턴.\\nsentence = init_text + sentence\\nreturn sentence\\nprint(sentence_generation(model, char_to_index, 10, 'I get on w', 80))\\nI get on with life as a programmer, I like to hang out with programming and deep learning.\\n두 개의 문장이 출력되었습니다. 이 두 문장은 훈련 데이터에서는 연속적으로 나온 적이 없는 두 문장임에도 모델이 임의로 생성해냈습니다.\\n==================================================\\n--- 09. 워드 임베딩(Word Embedding) ---\\n마지막 편집일시 : 2022년 1월 2일 2:49 오후\", '--- 09. 워드 임베딩(Word Embedding) ---\\n마지막 편집일시 : 2022년 1월 2일 2:49 오후\\n==================================================\\n--- 09-01 워드 임베딩(Word Embedding) ---\\n워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현으로 변환합니다.  희소 표현, 밀집 표현, 그리고 워드 임베딩에 대한 개념을 학습합니다.']\n",
      "['앞서 원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법이었습니다. 이렇게 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 합니다. 원-핫 벡터는 희소 벡터(sparse vector)입니다.\\n이러한 희소 벡터의 문제점은 단어의 개수가 늘어나면 벡터의 차원이 한없이 커진다는 점입니다. 원-핫 벡터로 표현할 때는 갖고 있는 코퍼스에 단어가 10,000개였다면 벡터의 차원은 10,000이어야만 했습니다. 심지어 그 중에서 단어의 인덱스에 해당되는 부분만 1이고 나머지는 0의 값을 가져야만 했습니다. 단어 집합이 클수록 고차원의 벡터가 됩니다. 예를 들어 단어가 10,000개 있고 인덱스가 0부터 시작하면서 강아지란 단어의 인덱스는 4였다면 원 핫 벡터는 이렇게 표현되어야 했습니다.', 'Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이때 1 뒤의 0의 수는 9995개.\\n이러한 벡터 표현은 공간적 낭비를 불러일으킵니다. 공간적 낭비를 일으키는 것은 원-핫 벡터뿐만은 아닙니다. 희소 표현의 일종인 DTM과 같은 경우에도 특정 문서에 여러 단어가 다수 등장하였으나, 다른 많은 문서에서는 해당 특정 문서에 등장했던 단어들이 전부 등장하지 않는다면 역시나 행렬의 많은 값이 0이 되면서 공간적 낭비를 일으킵니다. 이러한 관점에서 DTM은 희소 행렬입니다. 원-핫 벡터와 같은 희소 벡터의 문제점은 단어의 의미를 표현하지 못한다는 점입니다.']\n",
      "['희소 표현과 반대되는 표현으로 밀집 표현(dense representation)이 있습니다. 밀집 표현은 벡터의 차원을 단어 집합의 크기로 상정하지 않습니다. 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춥니다. 또한, 이 과정에서 더 이상 0과 1만 가진 값이 아니라 실수값을 가지게 됩니다. 다시 희소 표현의 예를 가져와봅시다.\\nEx) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이때 1 뒤의 0의 수는 9995개. 차원은 10,000\\n예를 들어 10,000개의 단어가 있을 때 강아지란 단어를 표현하기 위해서는 위와 같은 표현을 사용했습니다. 하지만 밀집 표현을 사용하고, 사용자가 밀집 표현의 차원을 128로 설정한다면, 모든 단어의 벡터 표현의 차원은 128로 바뀌면서 모든 값이 실수가 됩니다.\\nEx) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128', 'Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128\\n이 경우 벡터의 차원이 조밀해졌다고 하여 밀집 벡터(dense vector)라고 합니다.']\n",
      "['단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)이라고 합니다. 그리고 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)라고도 합니다.\\n워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등이 있습니다. 케라스에서 제공하는 도구인 Embedding()는 앞서 언급한 방법들을 사용하지는 않지만, 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤에, 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습하는 방법을 사용합니다. 아래의 표는 앞서 배운 원-핫 벡터와 지금 배우고 있는 임베딩 벡터의 차이를 보여줍니다.\\n원-핫 벡터\\n임베딩 벡터\\n차원\\n고차원(단어 집합의 크기)\\n저차원\\n다른 표현\\n희소 벡터의 일종\\n밀집 벡터의 일종\\n표현 방법\\n수동\\n훈련 데이터로부터 학습함\\n값의 타입\\n1과 0\\n실수', '원-핫 벡터\\n임베딩 벡터\\n차원\\n고차원(단어 집합의 크기)\\n저차원\\n다른 표현\\n희소 벡터의 일종\\n밀집 벡터의 일종\\n표현 방법\\n수동\\n훈련 데이터로부터 학습함\\n값의 타입\\n1과 0\\n실수\\nEmbedding()을 사용하는 것과 Word2Vec, Glove 등의 방법을 사용하는 것에 대한 비교는 사전 훈련된 워드 임베딩 실습에서 다룹니다.\\n==================================================\\n--- 09-02 워드투벡터(Word2Vec) ---\\n앞서 원-핫 벡터는 단어 벡터 간 유의미한 유사도를 계산할 수 없다는 단점이 있음을 언급한 적이 있습니다. 그래서 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 할 수 있는 방법이 필요합니다. 이를 위해서 사용되는 대표적인 방법이 워드투벡터(Word2Vec)입니다. Word2Vec의 개념을 설명하기 앞서 Word2Vec가 어떤 일을 할 수 있는지 확인해보겠습니다.\\n[이미지: ]', '[이미지: ]\\nhttp://w.elnn.kr/search/\\n위 사이트는 한국어 단어에 대해서 벡터 연산을 해볼 수 있는 사이트입니다. 위 사이트에서는 단어들(실제로는 Word2Vec 벡터)로 더하기, 빼기 연산을 할 수 있습니다. 예를 들어 아래의 식에서 좌변을 집어 넣으면, 우변의 답들이 나옵니다.\\n한국 - 서울 + 도쿄 = 일본\\n박찬호 - 야구 + 축구 = 호나우두\\n신기하게도 단어가 가지고 있는 의미들을 가지고 연산을 하고 있는 것처럼 보입니다. 이런 연산이 가능한 이유는 각 단어 벡터가 단어 벡터 간 유사도를 반영한 값을 가지고 있기 때문입니다. 어떻게 가능한 것일까요?']\n",
      "['앞서 원-핫 인코딩을 통해서 얻은 원-핫 벡터는 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법이었습니다. 이와 같이 벡터 또는 행렬의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 합니다.\\n하지만 이러한 표현 방법은 각 단어 벡터간 유의미한 유사성을 표현할 수 없다는 단점이 있었고, 대안으로 단어의 의미를 다차원 공간에 벡터화하는 방법을 사용하는데 이러한 표현을 분산 표현(distributed representation) 이라고 합니다. 그리고 분산 표현을 이용하여 단어 간 의미적 유사성을 벡터화하는 작업을 워드 임베딩(embedding)이라 부르며 이렇게 표현된 벡터를 임베딩 벡터(embedding vector)라고 합니다.']\n",
      "[\"분산 표현(distributed representation) 방법은 기본적으로 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법입니다. 이 가정은 '비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다' 라는 가정입니다. 강아지란 단어는 귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장하는데 분포 가설에 따라서 해당 내용을 가진 텍스트의 단어들을 벡터화한다면 해당 단어 벡터들은 유사한 벡터값을  가집니다. 분산 표현은 분포 가설을 이용하여 텍스트를 학습하고, 단어의 의미를 벡터의 여러 차원에 분산하여 표현합니다.\", '이렇게 표현된 벡터들은 원-핫 벡터처럼 벡터의 차원이 단어 집합(vocabulary)의 크기일 필요가 없으므로, 벡터의 차원이 상대적으로 저차원으로 줄어듭니다. 예를 들어 갖고 있는 텍스트 데이터에 단어가 10,000개 있고 인덱스는 0부터 시작하며 강아지란 단어의 인덱스는 4였다면 강아지란 단어를 표현하는 원-핫 벡터는 다음과 같습니다.\\nEx) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0]\\n1이란 값 뒤에 9,995개의 0의 값을 가지는 벡터가 됩니다. 하지만 Word2Vec으로 임베딩 된 벡터는 굳이 벡터 차원이 단어 집합의 크기가 될 필요가 없습니다. 강아지란 단어를 표현하기 위해 사용자가 설정한 차원의 수를 가지는 벡터가 되며 각 차원의 값은 실수값을 가집니다.\\nEx) 강아지 = [0.2 0.3 0.5 0.7 0.2 ... 중략 ... 0.2]', 'Ex) 강아지 = [0.2 0.3 0.5 0.7 0.2 ... 중략 ... 0.2]\\n요약하면 희소 표현이 고차원에 각 차원이 분리된 표현 방법이었다면, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산 하여 표현합니다. 이런 표현 방법을 사용하면 단어 벡터 간 유의미한 유사도를 계산할 수 있습니다. 이를 위한 대표적인 학습 방법이 Word2Vec입니다.']\n",
      "['Word2Vec의 학습 방식에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있습니다. CBOW는 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법입니다. 반대로, Skip-Gram은 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법입니다. 메커니즘 자체는 거의 동일합니다. 먼저 CBOW에 대해서 알아보겠습니다. 이해를 위해 매우 간소화 된 예시로 설명합니다.\\n예문 : \"The fat cat sat on the mat\"\\n예를 들어서 갖고 있는 코퍼스에 위와 같은 예문이 있다고 합시다. [\\'The\\', \\'fat\\', \\'cat\\', \\'on\\', \\'the\\', \\'mat\\']으로부터 sat을 예측하는 것은 CBOW가 하는 일입니다. 이때 예측해야하는 단어 sat을 중심 단어(center word)라고 하고, 예측에 사용되는 단어들을 주변 단어(context word)라고 합니다.', '중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 결정해야 하는데 이 범위를 윈도우(window)라고 합니다. 예를 들어 윈도우 크기가 2이고, 예측하고자 하는 중심 단어가 sat이라고 한다면 앞의 두 단어인 fat와 cat, 그리고 뒤의 두 단어인 on, the를 입력으로 사용합니다. 윈도우 크기가 n이라고 한다면, 실제 중심 단어를 예측하기 위해 참고하려고 하는 주변 단어의 개수는 2n입니다.\\n[이미지: ]\\n윈도우 크기가 정해지면 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만드는데 이 방법을 슬라이딩 윈도우(sliding window)라고 합니다.', '위 그림에서 좌측의 중심 단어와 주변 단어의 변화는 윈도우 크기가 2일때, 슬라이딩 윈도우가 어떤 식으로 이루어지면서 데이터 셋을 만드는지 보여줍니다. Word2Vec에서 입력은 모두 원-핫 벡터가 되어야 하는데, 우측 그림은 중심 단어와 주변 단어를 어떻게 선택했을 때에 따라서 각각 어떤 원-핫 벡터가 되는지를 보여줍니다. 위 그림은 결국 CBOW를 위한 전체 데이터 셋을 보여주는 것입니다.\\n[이미지: ]\\nCBOW의 인공 신경망을 간단히 도식화하면 위와 같습니다. 입력층(Input layer)의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어가게 되고, 출력층(Output layer)에서 예측하고자 하는 중간 단어의 원-핫 벡터가 레이블로서 필요합니다.', '위 그림에서 알 수 있는 사실은 Word2Vec은 은닉층이 다수인 딥 러닝(deep learning) 모델이 아니라 은닉층이 1개인 얕은 신경망(shallow neural network)이라는 점입니다. 또한 Word2Vec의 은닉층은 일반적인 은닉층과는 달리 활성화 함수가 존재하지 않으며 룩업 테이블이라는 연산을 담당하는 층으로 투사층(projection layer)이라고 부르기도 합니다.\\n[이미지: ]\\nCBOW의 인공 신경망을 좀 더 확대하여, 동작 메커니즘에 대해서 상세하게 알아보겠습니다. 이 그림에서 주목해야할 것은 두 가지 입니다. 하나는 투사층의 크기가 M이라는 점입니다. CBOW에서 투사층의 크기 M은 임베딩하고 난 벡터의 차원이 됩니다. 위의 그림에서 투사층의 크기는 M=5이므로 CBOW를 수행하고나서 얻는 각 단어의 임베딩 벡터의 차원은 5가 될 것입니다.', \"두번째는 입력층과 투사층 사이의 가중치 W는 V × M 행렬이며, 투사층에서 출력층사이의 가중치 W'는 M × V 행렬이라는 점입니다. 여기서 V는 단어 집합의 크기를 의미합니다. 즉, 위의 그림처럼 원-핫 벡터의 차원이 7이고, M은 5라면 가중치 W는 7 × 5 행렬이고, W'는 5 × 7 행렬이 될 것입니다. 주의할 점은 이 두 행렬은 동일한 행렬을 전치(transpose)한 것이 아니라, 서로 다른 행렬이라는 점입니다. 인공 신경망의 훈련 전에 이 가중치 행렬 W와 W'는 랜덤 값을 가지게 됩니다. CBOW는 주변 단어로 중심 단어를 더 정확히 맞추기 위해 계속해서 이 W와 W'를 학습해가는 구조입니다.\\n[이미지: ]\", \"[이미지: ]\\n입력으로 들어오는 주변 단어의 원-핫 벡터와 가중치 W행렬의 곱이 어떻게 이루어지는지 보겠습니다. 위 그림에서는 각 주변 단어의 원-핫 벡터를 $x$로 표기하였습니다. 입력 벡터는 원-핫 벡터입니다. i번째 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 입력 벡터와 가중치 W 행렬의 곱은 사실 W행렬의 i번째 행을 그대로 읽어오는 것과(lookup) 동일합니다. 이 작업을 룩업 테이블(lookup table)이라고 합니다. 앞서 CBOW의 목적은 W와 W'를 잘 훈련시키는 것이라고 언급한 적이 있는데, 그 이유가 여기서 lookup해온 W의 각 행벡터가 Word2Vec 학습 후에는 각 단어의 M차원의 임베딩 벡터로 간주되기 때문입니다.\\n[이미지: ]\", \"[이미지: ]\\n이렇게 주변 단어의 원-핫 벡터에 대해서 가중치 W가 곱해서 생겨진 결과 벡터들은 투사층에서 만나 이 벡터들의 평균인 벡터를 구하게 됩니다. 만약 윈도우 크기 n=2라면, 입력 벡터의 총 개수는 2n이므로 중간 단어를 예측하기 위해서는 총 4개가 입력 벡터로 사용됩니다. 그렇기 때문에 평균을 구할 때는 4개의 결과 벡터에 대해서 평균을 구하게 됩니다. 투사층에서 벡터의 평균을 구하는 부분은 CBOW가 Skip-Gram과 다른 차이점이기도 합니다. 뒤에서 보게되겠지만, Skip-Gram은 입력이 중심 단어 하나이기때문에 투사층에서 벡터의 평균을 구하지 않습니다.\\n[이미지: ]\\n이렇게 구해진 평균 벡터는 두번째 가중치 행렬 W'와 곱해집니다. 곱셈의 결과로는 원-핫 벡터들과 차원이 V로 동일한 벡터가 나옵니다. 만약 입력 벡터의 차원이 7이었다면 여기서 나오는 벡터도 마찬가지입니다.\", '이 벡터에 CBOW는 소프트맥스(softmax) 함수를 지나면서 벡터의 각 원소들의 값은 0과 1사이의 실수로, 총 합은 1이 됩니다. 다중 클래스 분류 문제를 위한 일종의 스코어 벡터(score vector)입니다. 스코어 벡터의 j번째 인덱스가 가진 0과 1사이의 값은 j번째 단어가 중심 단어일 확률을 나타냅니다. 그리고 이 스코어 벡터의 값은 레이블에 해당하는 벡터인 중심 단어 원-핫 벡터의 값에 가까워져야 합니다. 스코어 벡터를 $\\\\hat{𝑦}$라고 하겠습니다. 중심 단어의 원-핫 벡터를 $y$로 했을 때, 이 두 벡터값의 오차를 줄이기위해 CBOW는 손실 함수(loss function)로 크로스 엔트로피(cross-entropy) 함수를 사용합니다. 크로스 엔트로피 함수에 중심 단어인 원-핫 벡터와 스코어 벡터를 입력값으로 넣고, 이를 식으로 표현하면 다음과 같습니다. 아래의 식에서 V는 단어 집합의 크기입니다.', \"$$cost(\\\\hat{𝑦}, y) = -\\\\sum_{j=1}^{V}y_{j}\\\\ log(\\\\hat{𝑦_{j}})$$\\n역전파(Back Propagation)를 수행하면 W와 W'가 학습이 되는데, 학습이 다 되었다면 M차원의 크기를 갖는 W의 행렬의 행을 각 단어의 임베딩 벡터로 사용하거나 W와 W' 행렬 두 가지 모두를 가지고 임베딩 벡터를 사용하기도 합니다.\"]\n",
      "['CBOW에서는 주변 단어를 통해 중심 단어를 예측했다면, Skip-gram은 중심 단어에서 주변 단어를 예측합니다. 앞서 언급한 예문에 대해서 동일하게 윈도우 크기가 2일 때, 데이터셋은 다음과 같이 구성됩니다.\\n[이미지: ]\\n인공 신경망을 도식화해보면 아래와 같습니다.\\n[이미지: ]\\n중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없습니다. 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있습니다.']\n",
      "['[이미지: ]\\n워드 임베딩의 개념 자체는 피드 포워드 신경망 언어 모델(NNLM)을 설명하며 이미 학습한 적이 있습니다. NNLM은 단어 벡터 간 유사도를 구할 수 있도록 워드 임베딩의 개념을 도입하였고, 워드 임베딩 자체에 집중하여 NNLM의 느린 학습 속도와 정확도를 개선하여 탄생한 것이 Word2Vec입니다.\\nNNLM과 Word2Vec의 차이를 비교해봅시다. 우선 예측하는 대상이 달라졌습니다. NNLM은 다음 단어를 예측하는 언어 모델이 목적이므로 다음 단어를 예측하지만, Word2Vec(CBOW)은 워드 임베딩 자체가 목적이므로 다음 단어가 아닌 중심 단어를 예측하게 하여 학습합니다. 중심 단어를 예측하므로 NNLM이 예측 단어의 이전 단어들만을 참고하였던 것과는 달리, Word2Vec은 예측 단어의 전, 후 단어들을 모두 참고합니다.', \"구조도 달라졌습니다. 위의 그림은 n을 학습에 사용하는 단어의 수, m을 임베딩 벡터의 차원, h를 은닉층의 크기, V를 단어 집합의 크기라고 하였을 때 NNLM과 Word2Vec의 차이를 보여줍니다. Word2Vec은 우선 NNLM에 존재하던 활성화 함수가 있는 은닉층을 제거하였습니다. 이에 따라 투사층 다음에 바로 출력층으로 연결되는 구조입니다.\\nWord2Vec이 NNLM보다 학습 속도에서 강점을 가지는 이유는 은닉층을 제거한 것뿐만 아니라 추가적으로 사용되는 기법들 덕분이기도 합니다. 대표적인 기법으로 계층적 소프트맥스(hierarchical softmax)와 네거티브 샘플링(negative sampling)이 있는데 이 책에서는 네거티브 샘플링에 대해서 설명합니다. 이는 '네거티브 샘플링을 이용한 Word2Vec 구현' 실습을 참고바랍니다. Word2Vec과 NNLM의 연산량을 비교하여 학습 속도가 왜 차이나는지 이해해봅시다.\", '우선 입력층에서 투사층, 투사층에서 은닉층, 은닉층에서 출력층으로 향하며 발생하는 NNLM의 연산량을 보겠습니다.\\nNNLM : $(n × m) + (n × m × h) + (h × V)$\\n추가적인 기법들까지 사용하였을 때 Word2Vec은 출력층에서의 연산에서 $V$를 $log(V)$로 바꿀 수 있는데 이에 따라 Word2Vec의 연산량은 아래와 같으며 이는 NNLM보다 훨씬 빠른 학습 속도를 가집니다.\\nWord2Vec : $(n × m) + (m × log(V))$\\n==================================================\\n--- 09-03 영어/한국어 Word2Vec 실습 ---\\n```\\n[ 0.11279297 -0.02612305 -0.04492188  0.06982422  0.140625    0.03039551\\n-0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703', '-0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703\\n... 300개의 값이 출력되는 관계로 중략 ...\\n0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\\n```gensim 패키지에서 제공하는 이미 구현된 Word2Vec을 사용하여 영어와 한국어 데이터를 학습합니다.']\n",
      "['파이썬의 gensim 패키지에는 Word2Vec을 지원하고 있어, gensim 패키지를 이용하면 손쉽게 단어를 임베딩 벡터로 변환시킬 수 있습니다. 영어로 된 코퍼스를 다운받아 전처리를 수행하고, 전처리한 데이터를 바탕으로 Word2Vec 작업을 진행하겠습니다.\\nimport re\\nimport urllib.request\\nimport zipfile\\nfrom lxml import etree\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\n1) 훈련 데이터 이해하기\\n훈련 데이터를 다운로드 합니다.\\n# 데이터 다운로드', 'from nltk.tokenize import word_tokenize, sent_tokenize\\n1) 훈련 데이터 이해하기\\n훈련 데이터를 다운로드 합니다.\\n# 데이터 다운로드\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")', '훈련 데이터 파일은 xml 문법으로 작성되어 있어 자연어를 얻기 위해서는 전처리가 필요합니다. 얻고자 하는 실질적 데이터는 영어문장으로만 구성된 내용을 담고 있는 <content>와 </content> 사이의 내용입니다. 전처리 작업을 통해 xml 문법들은 제거하고, 해당 데이터만 가져와야 합니다. 뿐만 아니라, <content>와 </content> 사이의 내용 중에는 (Laughter)나 (Applause)와 같은 배경음을 나타내는 단어도 등장하는데 이 또한 제거해야 합니다.\\n<file id=\"1\">\\n<head>\\n<url>http://www.ted.com/talks/knut_haanaes_two_reasons_companies_fail_and_how_to_avoid_them</url>\\n<pagesize>72832</pagesize>\\n... xml 문법 중략 ...\\n<content>', \"<pagesize>72832</pagesize>\\n... xml 문법 중략 ...\\n<content>\\nHere are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities:\\n... content 내용 중략 ...\\nTo me, the irony about the Facit story is hearing about the Facit engineers, who had bought cheap, small electronic calculators in Japan that they used to double-check their calculators.\\n(Laughter)\\n... content 내용 중략 ...\", '(Laughter)\\n... content 내용 중략 ...\\n(Applause)\\n</content>\\n</file>\\n<file id=\"2\">\\n<head>\\n<url>http://www.ted.com/talks/lisa_nip_how_humans_could_evolve_to_survive_in_space<url>\\n... 이하 중략 ...\\n2) 훈련 데이터 전처리하기\\n위 데이터를 위한 전처리 코드는 아래와 같습니다.\\ntargetXML = open(\\'ted_en-20160408.xml\\', \\'r\\', encoding=\\'UTF8\\')\\ntarget_text = etree.parse(targetXML)\\n# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\\nparse_text = \\'\\\\n\\'.join(target_text.xpath(\\'//content/text()\\'))', 'parse_text = \\'\\\\n\\'.join(target_text.xpath(\\'//content/text()\\'))\\n# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\\n# 해당 코드는 괄호로 구성된 내용을 제거.\\ncontent_text = re.sub(r\\'\\\\([^)]*\\\\)\\', \\'\\', parse_text)\\n# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\\nsent_text = sent_tokenize(content_text)\\n# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\\nnormalized_text = []\\nfor string in sent_text:\\ntokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\\nnormalized_text.append(tokens)\\n# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.', \"normalized_text.append(tokens)\\n# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\\nresult = [word_tokenize(sentence) for sentence in normalized_text]\\nprint('총 샘플의 개수 : {}'.format(len(result)))\\n총 샘플의 개수 : 273424\\n총 샘플의 개수는 약 27만 3천개입니다.\\n# 샘플 3개만 출력\\nfor line in result[:3]:\\nprint(line)\\n['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\", \"['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\\n['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\\n상위 3개 문장만 출력해보았는데 토큰화가 수행되었음을 볼 수 있습니다. Word2Vec 모델에 텍스트 데이터를 훈련시킵니다.\\n3) Word2Vec 훈련시키기\\nfrom gensim.models import Word2Vec\\nfrom gensim.models import KeyedVectors\", '3) Word2Vec 훈련시키기\\nfrom gensim.models import Word2Vec\\nfrom gensim.models import KeyedVectors\\nmodel = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)\\nWord2Vec의 하이퍼파라미터값은 다음과 같습니다.\\nvector_size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\\nwindow = 컨텍스트 윈도우 크기\\nmin_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\\nworkers = 학습을 위한 프로세스 수\\nsg = 0은 CBOW, 1은 Skip-gram.', 'workers = 학습을 위한 프로세스 수\\nsg = 0은 CBOW, 1은 Skip-gram.\\nWord2Vec에 대해서 학습을 진행하였습니다. Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 model.wv.most_similar을 지원합니다. man과 가장 유사한 단어들은 어떤 단어들일까요?\\nmodel_result = model.wv.most_similar(\"man\")\\nprint(model_result)', 'model_result = model.wv.most_similar(\"man\")\\nprint(model_result)\\n[(\\'woman\\', 0.842622697353363), (\\'guy\\', 0.8178728818893433), (\\'boy\\', 0.7774451375007629), (\\'lady\\', 0.7767927646636963), (\\'girl\\', 0.7583760023117065), (\\'gentleman\\', 0.7437191009521484), (\\'soldier\\', 0.7413754463195801), (\\'poet\\', 0.7060446739196777), (\\'kid\\', 0.6925194263458252), (\\'friend\\', 0.6572611331939697)]\\nman과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid 등을 출력하는 것을 볼 수 있습니다. Word2Vec를 통해 단어의 유사도를 계산할 수 있게 되었습니다.', '4) Word2Vec 모델 저장하고 로드하기\\n공들여 학습한 모델을 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장하고 다시 로드해보겠습니다. 이 모델을 가지고 향후 시각화를 진행할 예정이므로 꼭 저장해주세요.\\nmodel.wv.save_word2vec_format(\\'eng_w2v\\') # 모델 저장\\nloaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드\\n로드한 모델에 대해서 다시 man과 유사한 단어를 출력해보겠습니다.\\nmodel_result = loaded_model.most_similar(\"man\")\\nprint(model_result)', 'model_result = loaded_model.most_similar(\"man\")\\nprint(model_result)\\n[(\\'woman\\', 0.842622697353363), (\\'guy\\', 0.8178728818893433), (\\'boy\\', 0.7774451375007629), (\\'lady\\', 0.7767927646636963), (\\'girl\\', 0.7583760023117065), (\\'gentleman\\', 0.7437191009521484), (\\'soldier\\', 0.7413754463195801), (\\'poet\\', 0.7060446739196777), (\\'kid\\', 0.6925194263458252), (\\'friend\\', 0.6572611331939697)]']\n",
      "['네이버 영화 리뷰 데이터로 한국어 Word2Vec을 만들어봅시다.\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport urllib.request\\nfrom gensim.models.word2vec import Word2Vec\\nfrom konlpy.tag import Okt\\n네이버 영화 리뷰 데이터를 다운로드합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\\n네이버 영화 리뷰 데이터를 데이터프레임으로 로드하고 상위 5개의 행을 출력해봅시다.\\ntrain_data = pd.read_table(\\'ratings.txt\\')\\ntrain_data[:5] # 상위 5개 출력\\n[이미지: ]\\n총 리뷰 개수를 확인해보겠습니다.\\nprint(len(train_data)) # 리뷰 개수 출력', \"train_data[:5] # 상위 5개 출력\\n[이미지: ]\\n총 리뷰 개수를 확인해보겠습니다.\\nprint(len(train_data)) # 리뷰 개수 출력\\n200000\\n총 20만개의 샘플이 존재하는데, 결측값 유무를 확인합니다.\\n# NULL 값 존재 유무\\nprint(train_data.isnull().values.any())\\nTrue\\n결측값이 존재하므로 결측값이 존재하는 행을 제거합니다.\\ntrain_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\\nprint(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\\nFalse\\n결측값이 삭제된 후의 리뷰 개수를 확인합니다.\\nprint(len(train_data)) # 리뷰 개수 출력\\n199992\\n총 199,992개의 리뷰가 존재합니다. 정규 표현식을 통해 한글이 아닌 경우 제거하는 전처리를 진행합니다.\\n# 정규 표현식을 통한 한글 외 문자 제거\", '199992\\n총 199,992개의 리뷰가 존재합니다. 정규 표현식을 통해 한글이 아닌 경우 제거하는 전처리를 진행합니다.\\n# 정규 표현식을 통한 한글 외 문자 제거\\ntrain_data[\\'document\\'] = train_data[\\'document\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\\ntrain_data[:5] # 상위 5개 출력\\n[이미지: ]\\n학습 시에 사용하고 싶지 않은 단어들인 불용어를 제거하겠습니다. 형태소 분석기 Okt를 사용하여 각 문장에 대해서 일종의 단어 내지는 형태소 단위로 나누는 토큰화를 수행합니다. 다소 시간이 소요될 수 있습니다.\\n# 불용어 정의\\nstopwords = [\\'의\\',\\'가\\',\\'이\\',\\'은\\',\\'들\\',\\'는\\',\\'좀\\',\\'잘\\',\\'걍\\',\\'과\\',\\'도\\',\\'를\\',\\'으로\\',\\'자\\',\\'에\\',\\'와\\',\\'한\\',\\'하다\\']\\n# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\\nokt = Okt()', \"# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\\nokt = Okt()\\ntokenized_data = []\\nfor sentence in tqdm(train_data['document']):\\ntokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\\nstopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\\ntokenized_data.append(stopwords_removed_sentence)\\n토큰화가 된 상태에서는 각 리뷰의 길이 분포 또한 확인이 가능합니다.\\n# 리뷰 길이 분포 확인\\nprint('리뷰의 최대 길이 :',max(len(review) for review in tokenized_data))\", \"# 리뷰 길이 분포 확인\\nprint('리뷰의 최대 길이 :',max(len(review) for review in tokenized_data))\\nprint('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\\nplt.hist([len(review) for review in tokenized_data], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n리뷰의 최대 길이 : 72\\n리뷰의 평균 길이 : 10.716703668146726\\n[이미지: ]\\nWord2Vec으로 토큰화 된 네이버 영화 리뷰 데이터를 학습합니다.\\nfrom gensim.models import Word2Vec\", '[이미지: ]\\nWord2Vec으로 토큰화 된 네이버 영화 리뷰 데이터를 학습합니다.\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences = tokenized_data, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\\n학습이 다 되었다면 Word2Vec 임베딩 행렬의 크기를 확인합니다.\\n# 완성된 임베딩 매트릭스의 크기 확인\\nmodel.wv.vectors.shape\\n(16477, 100)\\n총 16,477개의 단어가 존재하며 각 단어는 100차원으로 구성되어져 있습니다. \\'최민식\\'과 유사한 단어들을 뽑아봅시다.\\nprint(model.wv.most_similar(\"최민식\"))', 'print(model.wv.most_similar(\"최민식\"))\\n[(\\'한석규\\', 0.8789200782775879), (\\'안성기\\', 0.8757420778274536), (\\'김수현\\', 0.855679452419281), (\\'이민호\\', 0.854516863822937), (\\'김명민\\', 0.8525030612945557), (\\'최민수\\', 0.8492398262023926), (\\'이성재\\', 0.8478372097015381), (\\'윤제문\\', 0.8470626473426819), (\\'김창완\\', 0.8456774950027466), (\\'이주승\\', 0.8442063927650452)]\\n\\'히어로\\'와 유사한 단어들을 뽑아봅시다.\\nprint(model.wv.most_similar(\"히어로\"))', '\\'히어로\\'와 유사한 단어들을 뽑아봅시다.\\nprint(model.wv.most_similar(\"히어로\"))\\n[(\\'슬래셔\\', 0.8747539520263672), (\\'느와르\\', 0.8666149377822876), (\\'무협\\', 0.8423701524734497), (\\'호러\\', 0.8372749090194702), (\\'물의\\', 0.8365858793258667), (\\'무비\\', 0.8260530233383179), (\\'물\\', 0.8197994232177734), (\\'홍콩\\', 0.8120777606964111), (\\'블록버스터\\', 0.8021541833877563), (\\'블랙\\', 0.7880141139030457)]']\n",
      "['자연어 처리 작업을 할때, 케라스의 Embedding()를 사용하여 갖고 있는 훈련 데이터로부터 처음부터 임베딩 벡터를 훈련시키기도 하지만, 위키피디아 등의 방대한 데이터로 사전에 훈련된 워드 임베딩(pre-trained word embedding vector)를 가지고 와서 해당 벡터들의 값을 원하는 작업에 사용 할 수도 있습니다.\\n예를 들어서 감성 분류 작업을 하는데 훈련 데이터의 양이 부족한 상황이라면, 다른 방대한 데이터를 Word2Vec이나 GloVe 등으로 사전에 학습시켜놓은 임베딩 벡터들을 가지고 와서 모델의 입력으로 사용하는 것이 때로는 더 좋은 성능을 얻을 수 있습니다. 사전 훈련된 워드 임베딩을 가져와서 간단히 단어들의 유사도를 구해보는 실습을 해보겠습니다. 실제로 모델에 적용해보는 실습은 향후에 진행합니다.', '구글이 제공하는 사전 훈련된(미리 학습되어져 있는) Word2Vec 모델을 사용하는 방법에 대해서 알아보겠습니다. 구글은 사전 훈련된 3백만 개의 Word2Vec 단어 벡터들을 제공합니다. 각 임베딩 벡터의 차원은 300입니다. gensim을 통해서 이 모델을 불러오는 건 매우 간단합니다. 이 모델을 다운로드하고 파일 경로를 기재하면 됩니다.\\n모델 다운로드 경로 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\\n압축 파일의 용량은 약 1.5GB이지만, 파일의 압축을 풀면 약 3.3GB의 파일이 나옵니다.\\nimport gensim\\nimport urllib.request\\n# 구글의 사전 훈련된 Word2Vec 모델을 로드.', 'import gensim\\nimport urllib.request\\n# 구글의 사전 훈련된 Word2Vec 모델을 로드.\\nurllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\\\\nfilename=\"GoogleNews-vectors-negative300.bin.gz\")\\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\\'GoogleNews-vectors-negative300.bin.gz\\', binary=True)\\n모델의 크기(shape)를 확인해봅시다.\\nprint(word2vec_model.vectors.shape)\\n(3000000, 300)', \"모델의 크기(shape)를 확인해봅시다.\\nprint(word2vec_model.vectors.shape)\\n(3000000, 300)\\n모델의 크기는 3,000,000 x 300입니다. 즉, 3백만 개의 단어와 각 단어의 차원은 300입니다. 파일의 크기가 3기가가 넘는 이유를 계산해보면 아래와 같습니다.\\n3 million words * 300 features * 4bytes/feature = ~3.35GB\\n사전 훈련된 임베딩을 사용하여 두 단어의 유사도를 계산해봅시다.\\nprint(word2vec_model.similarity('this', 'is'))\\nprint(word2vec_model.similarity('post', 'book'))\\n0.407970363878\\n0.0572043891977\\n단어 'book'의 벡터를 출력해봅시다.\\nprint(word2vec_model['book'])\", \"0.407970363878\\n0.0572043891977\\n단어 'book'의 벡터를 출력해봅시다.\\nprint(word2vec_model['book'])\\n[ 0.11279297 -0.02612305 -0.04492188  0.06982422  0.140625    0.03039551\\n-0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703\\n... 300개의 값이 출력되는 관계로 중략 ...\\n0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\", \"0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\\n참고 : Word2vec 모델은 자연어 처리에서 단어를 밀집 벡터로 만들어주는 단어 임베딩 방법론이지만 최근에 들어서는 자연어 처리를 넘어서 추천 시스템에도 사용되고 있는 모델입니다. 적당하게 데이터를 나열해주면 Word2vec은 위치가 근접한 데이터를 유사도가 높은 벡터를 만들어준다는 점에서 착안된 아이디어입니다. 관심있는 분들은 구글에 'item2vec'을 검색해보세요.\\n==================================================\\n--- 09-04 네거티브 샘플링을 이용한 Word2Vec 구현(Skip-Gram with Negative Sampling, SGNS) ---\\n```\\n[('brakes', 0.7013274431228638),\\n('cylinder', 0.6680346727371216),\", \"```\\n[('brakes', 0.7013274431228638),\\n('cylinder', 0.6680346727371216),\\n('brake', 0.6459399461746216),\\n('seat', 0.6365581154823303),\\n('gasoline', 0.6263373494148254),\\n('honda', 0.611443281173706),\\n('mounted', 0.6093355417251587),\\n('ventilator', 0.5999234318733215),\\n('adjustable', 0.5938659310340881),\\n('propellants', 0.5935063362121582)]\\n```네거티브 샘플링(Negative Sampling)을 사용하는 Word2Vec을 직접 케라스(Keras)를 통해 구현해봅시다.\"]\n",
      "[\"Word2Vec의 출력층에서는 소프트맥스 함수를 지난 단어 집합 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이로부터 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업데이트합니다. 만약 단어 집합의 크기가 수만 이상에 달한다면 이 작업은 굉장히 무거운 작업이므로, Word2Vec은 꽤나 학습하기에 무거운 모델이 됩니다.\\nWord2Vec은 역전파 과정에서 모든 단어의 임베딩 벡터값의 업데이트를 수행하지만, 만약 현재 집중하고 있는 중심 단어와 주변 단어가 '강아지'와 '고양이', '귀여운'과 같은 단어라면, 사실 이 단어들과 별 연관 관계가 없는 '돈가스'나 '컴퓨터'와 같은 수많은 단어의 임베딩 벡터값까지 업데이트하는 것은 비효율적입니다.\", \"네거티브 샘플링은 Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법입니다. 가령, 현재 집중하고 있는 주변 단어가 '고양이', '귀여운'이라고 해봅시다. 여기에 '돈가스', '컴퓨터', '회의실'과 같은 단어 집합에서 무작위로 선택된 주변 단어가 아닌 단어들을 일부 가져옵니다. 이렇게 하나의 중심 단어에 대해서 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 변환합니다. 주변 단어들을 긍정(positive), 랜덤으로 샘플링 된 단어들을 부정(negative)으로 레이블링한다면 이진 분류 문제를 위한 데이터셋이 됩니다. 이는 기존의 단어 집합의 크기만큼의 선택지를 두고 다중 클래스 분류 문제를 풀던 Word2Vec보다 훨씬 연산량에서 효율적입니다.\"]\n",
      "['앞서 배운 Skip-gram을 상기해봅시다.\\n[이미지: ]\\nSkip-gram은 중심 단어로부터 주변 단어를 예측하는 모델이었습니다. 위와 같은 문장이 있다고 한다면, Skip-gram은 중심 단어 cat으로부터 주변 단어 The, fat, sat, on을 예측합니다. 기존의 Skip-gram 모델을 일종의 주황 박스로 생각해본다면, 아래의 그림과 같이 입력은 중심 단어, 모델의 예측은 주변 단어인 구조입니다.\\n[이미지: ]\\n하지만 네거티브 샘플링을 사용하는 Skip-gram(Skip-Gram with Negative Sampling, SGNS) 이하 SGNS는 이와는 다른 접근 방식을 취합니다. SGNS는 다음과 같이 중심 단어와 주변 단어가 모두 입력이 되고, 이 두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계인지 그 확률을 예측합니다.\\n[이미지: ]\\n기존의 Skip-gram 데이터셋을 SGNS의 데이터셋으로 바꾸는 과정을 봅시다.\\n[이미지: ]', '[이미지: ]\\n기존의 Skip-gram 데이터셋을 SGNS의 데이터셋으로 바꾸는 과정을 봅시다.\\n[이미지: ]\\n위의 그림에서 좌측의 테이블은 기존의 Skip-gram을 학습하기 위한 데이터셋입니다. Skip-gram은 기본적으로 중심 단어를 입력, 주변 단어를 레이블로 합니다. 하지만 SGNS를 학습하고 싶다면, 이 데이터셋을 우측의 테이블과 같이 수정할 필요가 있습니다. 우선, 기존의 Skip-gram 데이터셋에서 중심 단어와 주변 단어를 각각 입력1, 입력2로 둡니다. 이 둘은 실제로 윈도우 크기 내에서 이웃 관계였므로 레이블은 1로 합니다. 이제 레이블이 0인 샘플들을 준비할 차례입니다.\\n[이미지: ]', '[이미지: ]\\n실제로는 입력1(중심 단어)와 주변 단어 관계가 아닌 단어들을 입력2로 삼기 위해서 단어 집합에서 랜덤으로 선택한 단어들을 입력2로 하고, 레이블을 0으로 합니다. 이제 이 데이터셋은 입력1과 입력2가 실제로 윈도우 크기 내에서 이웃 관계인 경우에는 레이블이 1, 아닌 경우에는 레이블이 0인 데이터셋이 됩니다. 그리고 이제 두 개의 임베딩 테이블을 준비합니다. 두 임베딩 테이블은 훈련 데이터의 단어 집합의 크기를 가지므로 크기가 같습니다.\\n[이미지: ]\\n두 테이블 중 하나는 입력 1인 중심 단어의 테이블 룩업을 위한 임베딩 테이블이고, 하나는 입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블입니다. 각 단어는 각 임베딩 테이블을 테이블 룩업하여 임베딩 벡터로 변환됩니다.\\n[이미지: ]\\n각 임베딩 테이블을 통해 테이블 룩업하여 임베딩 벡터로 변환되었다면 그 후의 연산은 매우 간단합니다.\\n[이미지: ]', '[이미지: ]\\n각 임베딩 테이블을 통해 테이블 룩업하여 임베딩 벡터로 변환되었다면 그 후의 연산은 매우 간단합니다.\\n[이미지: ]\\n중심 단어와 주변 단어의 내적값을 이 모델의 예측값으로 하고, 레이블과의 오차로부터 역전파하여 중심 단어와 주변 단어의 임베딩 벡터값을 업데이트합니다. 학습 후에는 좌측의 임베딩 행렬을 임베딩 벡터로 사용할 수도 있고, 두 행렬을 더한 후 사용하거나 두 행렬을 연결(concatenate)해서 사용할 수도 있습니다. 아래의 실습에서는 좌측의 행렬을 사용하는 방식을 택했습니다.']\n",
      "[\"import pandas as pd\\nimport numpy as np\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\n20뉴스그룹 데이터를 사용합니다. 이번 실습에서는 하나의 샘플에 최소 단어 2개는 있어야 합니다. 그래야만 중심 단어, 주변 단어의 관계가 성립하며 그렇지 않으면 샘플을 구성할 수 없어 에러가 발생합니다. 전처리 과정에서 지속적으로 이를 만족하지 않는 샘플들을 제거하겠습니다.\\ndataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\\ndocuments = dataset.data\", 'documents = dataset.data\\nprint(\\'총 샘플 수 :\\',len(documents))\\n총 샘플 수 : 11314\\n총 샘플 수는 11,314개입니다. 전처리를 진행해봅시다. 불필요한 토큰을 제거하고, 소문자화를 통해 정규화를 진행합니다.\\nnews_df = pd.DataFrame({\\'document\\':documents})\\n# 특수 문자 제거\\nnews_df[\\'clean_doc\\'] = news_df[\\'document\\'].str.replace(\"[^a-zA-Z]\", \" \", regex=True)\\n# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\\nnews_df[\\'clean_doc\\'] = news_df[\\'clean_doc\\'].apply(lambda x: \\' \\'.join([w for w in x.split() if len(w)>3]))\\n# 전체 단어에 대한 소문자 변환', '# 전체 단어에 대한 소문자 변환\\nnews_df[\\'clean_doc\\'] = news_df[\\'clean_doc\\'].apply(lambda x: x.lower())\\n현재 데이터프레임에 Null 값이 있는지 확인합니다.\\nnews_df.isnull().values.any()\\nFalse\\nNull 값이 없지만, 빈 값(empy) 유무도 확인해야 합니다. 모든 빈 값을 Null 값으로 변환하고, 다시 Null 값이 있는지 확인합니다.\\nnews_df.replace(\"\", float(\"NaN\"), inplace=True)\\nnews_df.isnull().values.any()\\nTrue\\nNull 값이 있음을 확인했습니다. Null 값을 제거합니다.\\nnews_df.dropna(inplace=True)\\nprint(\\'총 샘플 수 :\\',len(news_df))\\n총 샘플 수 : 10995\\n샘플 수가 일부 줄어든 것을 확인할 수 있습니다. NLTK에서 정의한 불용어 리스트를 사용하여 불용어를 제거합니다.', \"총 샘플 수 : 10995\\n샘플 수가 일부 줄어든 것을 확인할 수 있습니다. NLTK에서 정의한 불용어 리스트를 사용하여 불용어를 제거합니다.\\n# 불용어를 제거\\nstop_words = stopwords.words('english')\\ntokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\\ntokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\\ntokenized_doc = tokenized_doc.to_list()\\n불용어를 제거하였으므로 단어의 수가 줄어들었습니다. 모든 샘플 중 단어가 1개 이하인 경우를 모두 찾아 제거하겠습니다.\\n# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\", \"# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\\ndrop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\\ntokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\\nprint('총 샘플 수 :',len(tokenized_doc))\\n총 샘플 수 : 10940\\n샘플 수가 다시 줄어들었습니다. 단어 집합을 생성하고, 정수 인코딩을 진행합니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(tokenized_doc)\\nword2idx = tokenizer.word_index\\nidx2word = {value : key for key, value in word2idx.items()}\", 'word2idx = tokenizer.word_index\\nidx2word = {value : key for key, value in word2idx.items()}\\nencoded = tokenizer.texts_to_sequences(tokenized_doc)\\n상위 2개의 샘플을 출력해봅시다.\\nprint(encoded[:2])', 'encoded = tokenizer.texts_to_sequences(tokenized_doc)\\n상위 2개의 샘플을 출력해봅시다.\\nprint(encoded[:2])\\n[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295],', \"[1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\\n단어 집합의 크기를 확인합니다.\\nvocab_size = len(word2idx) + 1\\nprint('단어 집합의 크기 :', vocab_size)\\n단어 집합의 크기 : 64277\\n총 64,277개의 단어가 존재합니다.\"]\n",
      "['토큰화, 정제, 정규화, 불용어 제거, 정수 인코딩까지 일반적인 전처리 과정을 거쳤습니다. 네거티브 샘플링을 통한 데이터셋을 구성할 차례입니다. 이를 위해서는 네거티브 샘플링을 위해서 케라스에서 제공하는 전처리 도구인 skipgrams를 사용합니다. 어떤 전처리가 수행되는지 그 결과를 확인하기 위해서 (꽤 시간이 소요되는 작업이므로) 상위 10개의 뉴스그룹 샘플에 대해서만 수행해봅시다.\\nfrom tensorflow.keras.preprocessing.sequence import skipgrams\\n# 네거티브 샘플링\\nskip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]\\n결과를 확인합니다. 10개의 뉴스그룹 샘플에 대해서 모두 수행되었지만, 첫번째 뉴스그룹 샘플에 대해서만 확인해보겠습니다.', '결과를 확인합니다. 10개의 뉴스그룹 샘플에 대해서 모두 수행되었지만, 첫번째 뉴스그룹 샘플에 대해서만 확인해보겠습니다.\\n# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\\npairs, labels = skip_grams[0][0], skip_grams[0][1]\\nfor i in range(5):\\nprint(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\\nidx2word[pairs[i][0]], pairs[i][0],\\nidx2word[pairs[i][1]], pairs[i][1],\\nlabels[i]))\\n(commited (7837), badar (34572)) -> 0\\n(whole (217), realize (1036)) -> 1\\n(reason (149), commited (7837)) -> 1\\n(letter (705), rediculous (15227)) -> 1', \"(reason (149), commited (7837)) -> 1\\n(letter (705), rediculous (15227)) -> 1\\n(reputation (5533), midonrnax (47527)) -> 0\\n윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블을 갖도록 하고, 그렇지 않은 경우는 0의 레이블을 가지도록 하여 데이터셋을 구성합니다. 이 과정은 각각의 뉴스그룹 샘플에 대해서 동일한 프로세스로 수행됩니다.\\nprint('전체 샘플 수 :',len(skip_grams))\\n전체 샘플 수 : 10\\nencoded 중 상위 10개의 뉴스그룹 샘플에 대해서만 수행하였으므로 10이 출력됩니다. 그리고 10개의 뉴스그룹 샘플 각각은 수많은 중심 단어, 주변 단어의 쌍으로 된 샘플들을 갖고 있습니다. 첫번째 뉴스그룹 샘플이 가지고 있는 pairs와 labels의 개수를 출력해봅시다.\\n# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\", '# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\\nprint(len(pairs))\\nprint(len(labels))\\n2220\\n2220\\n이 작업을 모든 뉴스그룹 샘플에 대해서 수행합니다.\\nskip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]']\n",
      "[\"from tensorflow.keras.models import Sequential, Model\\nfrom tensorflow.keras.layers import Embedding, Reshape, Activation, Input\\nfrom tensorflow.keras.layers import Dot\\nfrom tensorflow.keras.utils import plot_model\\nfrom IPython.display import SVG\\n하이퍼파라미터인 임베딩 벡터의 차원은 100으로 정하고, 두 개의 임베딩 층을 추가합니다.\\nembedding_dim = 100\\n# 중심 단어를 위한 임베딩 테이블\\nw_inputs = Input(shape=(1, ), dtype='int32')\\nword_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\\n# 주변 단어를 위한 임베딩 테이블\", \"word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\\n# 주변 단어를 위한 임베딩 테이블\\nc_inputs = Input(shape=(1, ), dtype='int32')\\ncontext_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)\\n각 임베딩 테이블은 중심 단어와 주변 단어 각각을 위한 임베딩 테이블이며 각 단어는 임베딩 테이블을 거쳐서 내적을 수행하고, 내적의 결과는 1 또는 0을 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값을 얻습니다.\\ndot_product = Dot(axes=2)([word_embedding, context_embedding])\\ndot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\\noutput = Activation('sigmoid')(dot_product)\", \"output = Activation('sigmoid')(dot_product)\\nmodel = Model(inputs=[w_inputs, c_inputs], outputs=output)\\nmodel.summary()\\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\\nplot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')\\n모델의 학습은 5에포크 수행하겠습니다.\\nfor epoch in range(1, 6):\\nloss = 0\\nfor _, elem in enumerate(skip_grams):\\nfirst_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\\nsecond_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\", \"second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\\nlabels = np.array(elem[1], dtype='int32')\\nX = [first_elem, second_elem]\\nY = labels\\nloss += model.train_on_batch(X,Y)\\nprint('Epoch :',epoch, 'Loss :',loss)\\nEpoch: 1 Loss: 4339.997158139944\\nEpoch: 2 Loss: 3549.69356325455\\nEpoch: 3 Loss: 3295.072506020777\\nEpoch: 4 Loss: 3038.1063768607564\\nEpoch: 5 Loss: 2790.9479411702487\"]\n",
      "[\"학습된 모델의 결과를 확인해보겠습니다. 학습된 임베딩 벡터들을 vector.txt에 저장합니다. 그 후 이를 gensim의 models.KeyedVectors.load_word2vec_format()으로 로드하면 쉽게 단어 벡터 간 유사도를 구할 수 있습니다.\\nimport gensim\\nf = open('vectors.txt' ,'w')\\nf.write('{} {}\\\\n'.format(vocab_size-1, embed_size))\\nvectors = model.get_weights()[0]\\nfor word, i in tokenizer.word_index.items():\\nf.write('{} {}\\\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\\nf.close()\\n# 모델 로드\\nw2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\", \"# 모델 로드\\nw2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\\nw2v.most_similar(positive=['soldiers'])\\n[('lebanese', 0.7539176940917969),\\n('troops', 0.7515299916267395),\\n('occupying', 0.7322258949279785),\\n('attacking', 0.7247686386108398),\\n('villagers', 0.7217503786087036),\\n('israeli', 0.7071422338485718),\\n('villages', 0.7000206708908081),\\n('wounded', 0.6976917386054993),\\n('lebanon', 0.6933401823043823),\\n('arab', 0.692956268787384)]\", \"('wounded', 0.6976917386054993),\\n('lebanon', 0.6933401823043823),\\n('arab', 0.692956268787384)]\\nw2v.most_similar(positive=['doctor'])\\n[('nerve', 0.6576169729232788),\\n('migraine', 0.6502577066421509),\\n('patient', 0.6377599835395813),\\n('disease', 0.6300654411315918),\\n('quack', 0.6101700663566589),\\n('cardiac', 0.606243371963501),\\n('infection', 0.6030253171920776),\\n('medication', 0.6001783013343811),\\n('suffering', 0.593578040599823),\\n('hurt', 0.5818471908569336)]\", \"('medication', 0.6001783013343811),\\n('suffering', 0.593578040599823),\\n('hurt', 0.5818471908569336)]\\nw2v.most_similar(positive=['police'])\\n[('prohibit', 0.6182408332824707),\\n('provisions', 0.5706381797790527),\\n('cops', 0.565453290939331),\\n('army', 0.563193142414093),\\n('possess', 0.5538119673728943),\\n('armed', 0.5535427331924438),\\n('rkba', 0.5533647537231445),\\n('ksanti', 0.5518242716789246),\\n('courts', 0.5495947599411011),\\n('officers', 0.5477950572967529)]\\nw2v.most_similar(positive=['knife'])\", \"('officers', 0.5477950572967529)]\\nw2v.most_similar(positive=['knife'])\\n[('knives', 0.7748741507530212),\\n('caucasus', 0.7227305769920349),\\n('defence', 0.7217429280281067),\\n('males', 0.7207540273666382),\\n('heretics', 0.7145630717277527),\\n('azerbaijanis', 0.7136125564575195),\\n('advocate', 0.7055186629295349),\\n('officers', 0.7020978927612305),\\n('punished', 0.7012225389480591),\\n('taxation', 0.7001351118087769)]\\nw2v.most_similar(positive=['engine'])\\n[('brakes', 0.7013274431228638),\", \"w2v.most_similar(positive=['engine'])\\n[('brakes', 0.7013274431228638),\\n('cylinder', 0.6680346727371216),\\n('brake', 0.6459399461746216),\\n('seat', 0.6365581154823303),\\n('gasoline', 0.6263373494148254),\\n('honda', 0.611443281173706),\\n('mounted', 0.6093355417251587),\\n('ventilator', 0.5999234318733215),\\n('adjustable', 0.5938659310340881),\\n('propellants', 0.5935063362121582)]\\n==================================================\\n--- 09-05) 글로브(GloVe) ---\\n```\", \"==================================================\\n--- 09-05) 글로브(GloVe) ---\\n```\\n[('water', 0.8264213732980569), ('fresh', 0.7850091074483321), ('wind', 0.7711854196846724), ('heat', 0.7646505765422197)]\", '```글로브(Global Vectors for Word Representation, GloVe)는 카운트 기반과 예측 기반을 모두 사용하는 방법론으로 2014년에 미국 스탠포드대학에서 개발한 단어 임베딩 방법론입니다. 앞서 학습하였던 기존의 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반의 Word2Vec의 단점을 지적하며 이를 보완한다는 목적으로 나왔고, 실제로도 Word2Vec만큼 뛰어난 성능을 보여줍니다. 현재까지의 연구에 따르면 단정적으로 Word2Vec와 GloVe 중에서 어떤 것이 더 뛰어나다고 말할 수는 없고, 이 두 가지 전부를 사용해보고 성능이 더 좋은 것을 사용하는 것이 바람직합니다.']\n",
      "['기존의 방법론을 언급해보겠습니다. LSA는 DTM이나 TF-IDF 행렬과 같이 각 문서에서의 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소(Truncated SVD)하여 잠재된 의미를 끌어내는 방법론이었습니다. 반면, Word2Vec는 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반의 방법론이었습니다. 서로 다른 방법을 사용하는 이 두 방법론은 각각 장, 단점이 있습니다.', 'LSA는 카운트 기반으로 코퍼스의 전체적인 통계 정보를 고려하기는 하지만, 왕:남자 = 여왕:? (정답은 여자)와 같은 단어 의미의 유추 작업(Analogy task)에는 성능이 떨어집니다. Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만, 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못합니다. GloVe는 이러한 기존 방법론들의 각각의 한계를 지적하며, LSA의 메커니즘이었던 카운트 기반의 방법과 Word2Vec의 메커니즘이었던 예측 기반의 방법론 두 가지를 모두 사용합니다.']\n",
      "['단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬을 말합니다. 예제를 보면 어렵지 않습니다. 아래와 같은 3개 문서로 구성된 텍스트 데이터가 있다고 해봅시다.\\nI like deep learning\\nI like NLP\\nI enjoy flying\\n윈도우 크기가 N일 때는 좌, 우에 존재하는 N개의 단어만 참고하게 됩니다. 윈도우 크기가 1일 때, 위의 텍스트를 가지고 구성한 동시 등장 행렬은 다음과 같습니다.\\n카운트\\nI\\nlike\\nenjoy\\ndeep\\nlearning\\nNLP\\nflying\\nI\\n0\\n2\\n1\\n0\\n0\\n0\\n0\\nlike\\n2\\n0\\n0\\n1\\n0\\n1\\n0\\nenjoy\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\ndeep\\n0\\n1\\n0\\n0\\n1\\n0\\n0\\nlearning\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\nNLP\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\nflying\\n0\\n0\\n1\\n0\\n0\\n0\\n0', '1\\n0\\n0\\n0\\n0\\n0\\n1\\ndeep\\n0\\n1\\n0\\n0\\n1\\n0\\n0\\nlearning\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\nNLP\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\nflying\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n위 행렬은 행렬을 전치(Transpose)해도 동일한 행렬이 된다는 특징이 있습니다. 그 이유는 i 단어의 윈도우 크기 내에서 k 단어가 등장한 빈도는 반대로 k 단어의 윈도우 크기 내에서 i 단어가 등장한 빈도와 동일하기 때문입니다.\\n위의 테이블은 스탠포드 대학교의 자연어 처리 강의를 참고하였습니다.\\n링크 : http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf']\n",
      "['동시 등장 행렬에 대해서 이해했으니, 동시 등장 확률에 대해서 이해해봅시다. 아래의 표는 어떤 동시 등장 행렬을 가지고 정리한 동시 등장 확률(Co-occurrence Probability)을 보여줍니다. 그렇다면, 동시 등장 확률이란 무엇일까요?\\n동시 등장 확률 $P(k\\\\ |\\\\ i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률입니다.\\n$P(k\\\\ |\\\\ i)$에서 i를 중심 단어(Center Word), k를 주변 단어(Context Word)라고 했을 때, 위에서 배운 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모로 하고 i행 k열의 값을 분자로 한 값이라고 볼 수 있겠습니다. 다음은 GloVe의 제안 논문에서 가져온 동시 등장 확률을 표로 정리한 하나의 예입니다.\\n동시 등장 확률과 크기 관계 비(ratio)\\nk=solid\\nk=gas\\nk=water', \"동시 등장 확률과 크기 관계 비(ratio)\\nk=solid\\nk=gas\\nk=water\\nk=fasion\\nP(k l ice)\\n0.00019\\n0.000066\\n0.003\\n0.000017\\nP(k l steam)\\n0.000022\\n0.00078\\n0.0022\\n0.000018\\nP(k l ice) / P(k l steam)\\n8.9\\n0.085\\n1.36\\n0.96\\n위의 표를 통해 알 수 있는 사실은 ice가 등장했을 때 solid가 등장할 확률 0.00019은 steam이 등장했을 때 solid가 등장할 확률인 0.000022보다 약 8.9배 크다는 겁니다. 그도 그럴 것이 solid는 '단단한'이라는 의미를 가졌으니까 '증기'라는 의미를 가지는 steam보다는 당연히 '얼음'이라는 의미를 가지는 ice라는 단어와 더 자주 등장할 겁니다.\", '수식적으로 다시 정리하여 언급하면 k가 solid일 때, P(solid l ice) / P(solid l steam)를 계산한 값은 8.9가 나옵니다. 이 값은 1보다는 매우 큰 값입니다. 왜냐면 P(solid | ice)의 값은 크고, P(solid | steam)의 값은 작기 때문입니다.\\n그런데 k를 solid가 아니라 gas로 바꾸면 얘기는 완전히 달라집니다. gas는 ice보다는 steam과 더 자주 등장하므로, P(gas l ice) / P(gas l steam)를 계산한 값은 1보다 훨씬 작은 값인 0.085가 나옵니다. 반면, k가 water인 경우에는 solid와 steam 두 단어 모두와 동시 등장하는 경우가 많으므로 1에 가까운 값이 나오고, k가 fasion인 경우에는 solid와 steam 두 단어 모두와 동시 등장하는 경우가 적으므로 1에 가까운 값이 나옵니다. 보기 쉽도록 조금 단순화해서 표현한 표는 다음과 같습니다.', '동시 등장 확률과 크기 관계 비(ratio)\\nk=solid\\nk=gas\\nk=water\\nk=fasion\\nP(k l ice)\\n큰 값\\n작은 값\\n큰 값\\n작은 값\\nP(k l steam)\\n작은 값\\n큰 값\\n큰 값\\n작은 값\\nP(k l ice) / P(k l steam)\\n큰 값\\n작은 값\\n1에 가까움\\n1에 가까움\\n동시 등장 행렬과 동시 등장 확률의 이해를 바탕으로 손실 함수를 설계해보겠습니다.']\n",
      "['우선 손실 함수를 설명하기 전에 각 용어를 정리해보겠습니다.\\n$X$ : 동시 등장 행렬(Co-occurrence Matrix)\\n$X_{ij}$ : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 j가 등장하는 횟수\\n$X_{i} : \\\\sum_j X_{ij}$ : 동시 등장 행렬에서 i행의 값을 모두 더한 값\\n$P_{ik}$ : $P(k\\\\ |\\\\ i)$ = $\\\\frac{X_{ik}}{X_{i}}$ : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률\\nEx) P(solid l ice) = 단어 ice가 등장했을 때 단어 solid가 등장할 확률\\n$\\\\frac{P_{ik}}{P_{jk}}$ : ${P_{ik}}$를 ${P_{jk}}$로 나눠준 값\\nEx) P(solid l ice) / P(solid l steam) = 8.9\\n$w_{i}$ : 중심 단어 i의 임베딩 벡터\\n$\\\\tilde{w_{k}}$ : 주변 단어 k의 임베딩 벡터', \"$w_{i}$ : 중심 단어 i의 임베딩 벡터\\n$\\\\tilde{w_{k}}$ : 주변 단어 k의 임베딩 벡터\\nGloVe의 아이디어를 한 줄로 요약하면 '임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것'입니다. 즉, 이를 만족하도록 임베딩 벡터를 만드는 것이 목표입니다. 이를 식으로 표현하면 다음과 같습니다.\\n$dot\\\\ product(w_{i}\\\\ \\\\tilde{w_{k}}) \\\\approx\\\\ P(k\\\\ |\\\\ i) = P_{ik}$\\n뒤에서 보게되겠지만, 더 정확히는 GloVe는 아래와 같은 관계를 가지도록 임베딩 벡터를 설계합니다.\\n$dot\\\\ product(w_{i}\\\\ \\\\tilde{w_{k}}) \\\\approx\\\\ log\\\\ P(k\\\\ |\\\\ i) = log\\\\ P_{ik}$\", '$dot\\\\ product(w_{i}\\\\ \\\\tilde{w_{k}}) \\\\approx\\\\ log\\\\ P(k\\\\ |\\\\ i) = log\\\\ P_{ik}$\\n임베딩 벡터들을 만들기 위한 손실 함수를 처음부터 차근차근 설계해보겠습니다. 가장 중요한 것은 단어 간의 관계를 잘 표현하는 함수여야 한다는 겁니다. 이를 위해 앞서 배운 개념인 $P_{ik} / P_{jk}$를 식에 사용합니다. GloVe의 연구진들은 벡터 $w_{i}, w_{j}, \\\\tilde{w_{k}}$를 가지고 어떤 함수 $F$를 수행하면, $P_{ik} / P_{jk}$가 나온다는 초기 식으로부터 전개를 시작합니다.\\n$$F(w_{i},\\\\ w_{j},\\\\ \\\\tilde{w_{k}}) = \\\\frac{P_{ik}}{P_{jk}}$$', '$$F(w_{i},\\\\ w_{j},\\\\ \\\\tilde{w_{k}}) = \\\\frac{P_{ik}}{P_{jk}}$$\\n아직 이 함수 $F$가 어떤 식을 가지고 있는지는 정해진 게 없습니다. 위의 목적에 맞게 근사할 수 있는 함수식은 무수히 많겠으나 최적의 식에 다가가기 위해서 차근, 차근 디테일을 추가해보겠습니다. 함수 $F$는 두 단어 사이의 동시 등장 확률의 크기 관계 비(ratio) 정보를 벡터 공간에 인코딩하는 것이 목적입니다. 이를 위해 GloVe 연구진들은 $w_{i}$와 $w_{j}$라는 두 벡터의 차이를 함수 $F$의 입력으로 사용하는 것을 제안합니다.\\n$$F(w_{i} -\\\\ w_{j},\\\\ \\\\tilde{w_{k}}) = \\\\frac{P_{ik}}{P_{jk}}$$\\n그런데 우변은 스칼라값이고 좌변은 벡터값입니다. 이를 성립하기 해주기 위해서 함수 $F$의 두 입력에 내적(Dot product)을 수행합니다.', '그런데 우변은 스칼라값이고 좌변은 벡터값입니다. 이를 성립하기 해주기 위해서 함수 $F$의 두 입력에 내적(Dot product)을 수행합니다.\\n$$F((w_{i} -\\\\ w_{j})^{T} \\\\tilde{w_{k}}) = \\\\frac{P_{ik}}{P_{jk}}$$\\n정리하면, 선형 공간(Linear space)에서 단어의 의미 관계를 표현하기 위해 뺄셈과 내적을 택했습니다.', '정리하면, 선형 공간(Linear space)에서 단어의 의미 관계를 표현하기 위해 뺄셈과 내적을 택했습니다.\\n여기서 함수 $F$가 만족해야 할 필수 조건이 있습니다. 중심 단어 $w$와 주변 단어 $\\\\tilde{w}$라는 선택 기준은 실제로는 무작위 선택이므로 이 둘의 관계는 자유롭게 교환될 수 있도록 해야합니다. 이것이 성립되게 하기 위해서 GloVe 연구진은 함수 $F$가 실수의 덥셈과 양수의 곱셈에 대해서 준동형(Homomorphism)을 만족하도록 합니다. 생소한 용어라서 말이 어려워보이는데, 정리하면 $a$와 $b$에 대해서 함수 $F$가 $F(a + b)$가 $F(a)F(b)$와 같도록 만족시켜야 한다는 의미입니다.\\n식으로 나타내면 아래와 같습니다.\\n$F(a+b) = F(a)F(b),\\\\ \\\\forall a,\\\\ b\\\\in \\\\mathbb{R}$', '식으로 나타내면 아래와 같습니다.\\n$F(a+b) = F(a)F(b),\\\\ \\\\forall a,\\\\ b\\\\in \\\\mathbb{R}$\\n이 준동형식을 현재 전개하던 GloVe 식에 적용할 수 있도록 조금씩 바꿔볼 겁니다. 전개하던 GloVe 식에 따르면, 함수 $F$는 결과값으로 스칼라 값($\\\\frac{P_{ik}}{P_{jk}}$)이 나와야 합니다. 준동형식에서 $a$와 $b$가 각각 벡터값이라면 함수 $F$의 결과값으로는 스칼라 값이 나올 수 없지만, $a$와 $b$가 각각 사실 두 벡터의 내적값이라고 하면 결과값으로 스칼라 값이 나올 수 있습니다. 그러므로 위의 준동형식을 아래와 같이 바꿔보겠습니다. 여기서 $v_{1},\\\\ v_{2},\\\\ v_{3},\\\\ v_{4}$는 각각 벡터값입니다. 아래의 $V$는 벡터를 의미합니다.', '$F(v_{1}^{T}v_{2} + v_{3}^{T}v_{4}) = F(v_{1}^{T}v_{2})F(v_{3}^{T}v_{4}),\\\\ \\\\forall v_{1},\\\\ v_{2},\\\\ v_{3},\\\\ v_{4}\\\\in V$\\n그런데 앞서 작성한 GloVe 식에서는 $w_{i}$와 $w_{j}$라는 두 벡터의 차이를 함수 $F$의 입력으로 받았습니다. GloVe 식에 바로 적용을 위해 준동형 식을 이를 뺄셈에 대한 준동형식으로 변경합니다. 그렇게 되면 곱셈도 나눗셈으로 바뀝니다.\\n$$F(v_{1}^{T}v_{2} - v_{3}^{T}v_{4}) = \\\\frac{F(v_{1}^{T}v_{2})}{F(v_{3}^{T}v_{4})},\\\\ \\\\forall v_{1},\\\\ v_{2},\\\\ v_{3},\\\\ v_{4}\\\\in V$$\\n이 준동형 식을 GloVe 식에 적용해보겠습니다. 우선, 함수 $F$의 우변은 다음과 같이 바뀌어야 합니다.', '이 준동형 식을 GloVe 식에 적용해보겠습니다. 우선, 함수 $F$의 우변은 다음과 같이 바뀌어야 합니다.\\n$$F((w_{i} -\\\\ w_{j})^{T} \\\\tilde{w_{k}}) = \\\\frac{F(w_{i}^{T}\\\\tilde{w_{k}})}{F(w_{j}^{T}\\\\tilde{w_{k}})}$$\\n그런데 이전의 식에 따르면 우변은 본래 $\\\\frac{P_{ik}}{P_{jk}}$였으므로, 결과적으로 다음과 같습니다.\\n$$\\\\frac{P_{ik}}{P_{jk}} = \\\\frac{F(w_{i}^{T}\\\\tilde{w_{k}})}{F(w_{j}^{T}\\\\tilde{w_{k}})}$$\\n$$F(w_{i}^{T}\\\\tilde{w_{k}}) = P_{ik} = \\\\frac{X_{ik}}{X_{i}}$$\\n좌변을 풀어쓰면 다음과 같습니다.', '$$F(w_{i}^{T}\\\\tilde{w_{k}}) = P_{ik} = \\\\frac{X_{ik}}{X_{i}}$$\\n좌변을 풀어쓰면 다음과 같습니다.\\n$$F(w_{i}^{T}\\\\tilde{w_{k}}\\\\ -\\\\ w_{j}^{T}\\\\tilde{w_{k}}) = \\\\frac{F(w_{i}^{T}\\\\tilde{w_{k}})}{F(w_{j}^{T}\\\\tilde{w_{k}})}$$\\n이는 뺄셈에 대한 준동형식의 형태와 정확히 일치합니다. 이를 만족하는 함수 $F$를 찾아야 할 때입니다. 그리고 이를 정확하게 만족시키는 함수가 있는데 바로 지수 함수(Exponential function)입니다. $F$를 지수 함수 $exp$라고 해봅시다.\\n$$exp(w_{i}^{T}\\\\tilde{w_{k}}\\\\ -\\\\ w_{j}^{T}\\\\tilde{w_{k}}) = \\\\frac{exp(w_{i}^{T}\\\\tilde{w_{k}})}{exp(w_{j}^{T}\\\\tilde{w_{k}})}$$', '$$exp(w_{i}^{T}\\\\tilde{w_{k}}) = P_{ik} = \\\\frac{X_{ik}}{X_{i}}$$\\n위의 두번째 식으로부터 다음과 같은 식을 얻을 수 있습니다.\\n$$w_{i}^{T}\\\\tilde{w_{k}} = log\\\\ P_{ik} = log\\\\ (\\\\frac{X_{ik}}{X_{i}}) = log\\\\ X_{ik} - log\\\\ X_{i}$$', '$$w_{i}^{T}\\\\tilde{w_{k}} = log\\\\ P_{ik} = log\\\\ (\\\\frac{X_{ik}}{X_{i}}) = log\\\\ X_{ik} - log\\\\ X_{i}$$\\n그런데 여기서 상기해야할 것은 앞서 언급했듯이, 사실 $w_{i}$와 $\\\\tilde{w_{k}}$는 두 값의 위치를 서로 바꾸어도 식이 성립해야 합니다. $X_{ik}$의 정의를 생각해보면 $X_{ki}$와도 같습니다. 그런데 이게 성립되려면 위의 식에서 $log\\\\ X_{i}$항이 걸림돌입니다. 이 부분만 없다면 이를 성립시킬 수 있습니다. 그래서 GloVe 연구팀은 이 $log\\\\ X_{i}$항을 $w_{i}$에 대한 편향 $b_{i}$라는 상수항으로 대체하기로 합니다. 같은 이유로 $\\\\tilde{w_{k}}$에 대한 편향 $\\\\tilde{b_{k}}$를 추가합니다.\\n$$w_{i}^{T}\\\\tilde{w_{k}} + b_{i} + \\\\tilde{b_{k}} = log\\\\ X_{ik}$$', '$$w_{i}^{T}\\\\tilde{w_{k}} + b_{i} + \\\\tilde{b_{k}} = log\\\\ X_{ik}$$\\n이 식이 손실 함수의 핵심이 되는 식입니다. 우변의 값과의 차이를 최소화는 방향으로 좌변의 4개의 항은 학습을 통해 값이 바뀌는 변수들이 됩니다. 즉, 손실 함수는 다음과 같이 일반화될 수 있습니다.\\n$$Loss\\\\ function = \\\\sum_{m, n=1}^{V}\\\\ (w_{m}^{T}\\\\tilde{w_{n}} + b_{m} + \\\\tilde{b_{n}} - logX_{mn})^{2}$$\\n여기서 $V$는 단어 집합의 크기를 의미합니다. 그런데 아직 최적의 손실 함수라기에는 부족합니다. GloVe 연구진은 $log\\\\ X_{ik}$에서 $X_{ik}$값이 0이 될 수 있음을 지적합니다. 대안 중 하나는 $log\\\\ X_{ik}$항을 $log\\\\ (1 + X_{ik})$로 변경하는 것입니다. 하지만 이렇게 해도 여전히 해결되지 않는 문제가 있습니다.', '바로 동시 등장 행렬 $X$는 마치 DTM처럼 희소 행렬(Sparse Matrix)일 가능성이 다분하다는 점입니다. 동시 등장 행렬 $X$에는 많은 값이 0이거나, 동시 등장 빈도가 적어서 많은 값이 작은 수치를 가지는 경우가 많습니다. 이에 대한 가중치를 주는 고민을 하게 되는데 GloVe 연구팀이 선택한 것은 바로 $X_{ik}$의 값에 영향을 받는 가중치 함수(Weighting function) $f(X_{ik})$를 손실 함수에 도입하는 것입니다.\\n[이미지: ]\\nGloVe의 손실 함수에서 사용하는 가중치 함수는 동시 출현 빈도가 높은 단어 쌍에 낮은 가중치를 부여하고, 동시 출현 빈도가 낮은 단어 쌍에 높은 가중치를 부여합니다. 이 이유는 주로 두 가지입니다.', \"$X_{ik}$의 값이 작으면 상대적으로 함수의 값은 작도록 하고, 값이 크면 함수의 값은 상대적으로 크도록 합니다. 하지만 $X_{ik}$가 지나치게 높다고해서 지나친 가중치를 주지 않기위해서 또한 함수의 최대값이 정해져 있습니다. (최대값은 1) 예를 들어 'It is'와 같은 불용어의 동시 등장 빈도수가 높다고해서 지나친 가중을 받아서는 안 됩니다. 이 함수의 값을 손실 함수에 곱해주면 가중치의 역할을 할 수 있습니다.\\n이 함수 $f(x)$의 식은 다음과 같이 정의됩니다.\\n$f(x) = min(1,\\\\ (x/x_{max})^{3/4})$\\n최종적으로 다음과 같은 일반화 된 손실 함수를 얻어낼 수 있습니다.\\n$$Loss\\\\ function = \\\\sum_{m, n=1}^{V}\\\\ f(X_{mn})(w_{m}^{T}\\\\tilde{w_{n}} + b_{m} + \\\\tilde{b_{n}} - logX_{mn})^{2}$$\\nGloVe 패키지를 설치 및 실습하고 훈련 결과를 확인해보겠습니다.\"]\n",
      "[\"글로브 패키지는 현재 관리되고 있지 않아서 최신 파이썬 버전에서는 실행되지 않고 있습니다.\\n실습을 위해 프롬프트에서 아래 커맨드로 GloVe 패키지를 설치합니다.\\npip install glove_python_binary\\nGloVe의 입력이 되는 훈련 데이터는 '영어와 한국어 Word2Vec 학습하기' 챕터에서 사용한 영어 데이터를 재사용합니다. 모든 동일한 전처리를 마치고 이전과 동일하게 result에 결과가 저장되어있다고 가정합니다.\\nfrom glove import Corpus, Glove\\ncorpus = Corpus()\\n# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\\ncorpus.fit(result, window=5)\\nglove = Glove(no_components=100, learning_rate=0.05)\\n# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\", 'glove = Glove(no_components=100, learning_rate=0.05)\\n# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\\nglove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\\nglove.add_dictionary(corpus.dictionary)\\n학습이 완료되었습니다. glove.most_similar()는 입력 단어의 가장 유사한 단어들의 리스트를 리턴합니다.\\nprint(glove.most_similar(\"man\"))\\n[(\\'woman\\', 0.9621753707315267), (\\'guy\\', 0.8860281455579162), (\\'girl\\', 0.8609057388487154), (\\'kid\\', 0.8383640509911114)]\\nprint(glove.most_similar(\"boy\"))', 'print(glove.most_similar(\"boy\"))\\n[(\\'girl\\', 0.9436601252235809), (\\'kid\\', 0.8400949618225224), (\\'woman\\', 0.8397250531245034), (\\'man\\', 0.8303093585541573)]\\nprint(glove.most_similar(\"university\"))\\n[(\\'harvard\\', 0.8690162017225468), (\\'cambridge\\', 0.8373272000675909), (\\'mit\\', 0.8288055170365777), (\\'stanford\\', 0.8212712738131419)]\\nprint(glove.most_similar(\"water\"))\\n[(\\'air\\', 0.838286550826724), (\\'clean\\', 0.8326093688298345), (\\'fresh\\', 0.8232884971285377), (\\'electricity\\', 0.8097066570385377)]', 'print(glove.most_similar(\"physics\"))\\n[(\\'chemistry\\', 0.8379143027061764), (\\'biology\\', 0.827856517644139), (\\'economics\\', 0.775563255616767), (\\'finance\\', 0.7736692309034663)]\\nprint(glove.most_similar(\"muscle\"))\\n[(\\'skeletal\\', 0.7977490484723809), (\\'tissue\\', 0.7714119298512192), (\\'nerve\\', 0.7477850181231441), (\\'stem\\', 0.7222964725687838)]\\nprint(glove.most_similar(\"clean\"))\\n[(\\'water\\', 0.8264213732980569), (\\'fresh\\', 0.7850091074483321), (\\'wind\\', 0.7711854196846724), (\\'heat\\', 0.7646505765422197)]', '사전 훈련된 GloVe를 사용하는 방법은 다음 챕터에서 다룹니다.\\n참고 자료\\nhttps://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\\n==================================================\\n--- 09-06 패스트텍스트(FastText) ---\\n```\\n< ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... 중략>', '--- 09-06 패스트텍스트(FastText) ---\\n```\\n< ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... 중략>\\n```단어를 벡터로 만드는 또 다른 방법으로는 페이스북에서 개발한 FastText가 있습니다. Word2Vec 이후에 나온 것이기 때문에, 메커니즘 자체는 Word2Vec의 확장이라고 볼 수 있습니다. Word2Vec와 FastText와의 가장 큰 차이점이라면 Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주합니다. 내부 단어. 즉, 서브워드(subword)를 고려하여 학습합니다.']\n",
      "['FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급합니다. n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정됩니다. 예를 들어서 n을 3으로 잡은 트라이그램(tri-gram)의 경우, apple은 app, ppl, ple로 분리하고 이들을 벡터로 만듭니다. 더 정확히는 시작과 끝을 의미하는 <, >를 도입하여 아래의 5개 내부 단어(subword) 토큰을 벡터로 만듭니다.\\n# n = 3인 경우\\n<ap, app, ppl, ple, le>\\n그리고 여기에 추가적으로 하나를 더 벡터화하는데, 기존 단어에 <, 와 >를 붙인 토큰입니다.\\n# 특별 토큰\\n<apple>\\n다시 말해 n = 3인 경우, FastText는 단어 apple에 대해서 다음의 6개의 토큰을 벡터화하는 것입니다.\\n# n = 3인 경우\\n<ap, app, ppl, ple, le>, <apple>', '# n = 3인 경우\\n<ap, app, ppl, ple, le>, <apple>\\n그런데 실제 사용할 때는 n의 최소값과 최대값으로 범위를 설정할 수 있는데, 기본값으로는 각각 3과 6으로 설정되어져 있습니다. 다시 말해 최소값 = 3, 최대값 = 6인 경우라면, 단어 apple에 대해서 FastText는 아래 내부 단어들을 벡터화합니다.\\n# n = 3 ~ 6인 경우\\n<ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>\\n여기서 내부 단어들을 벡터화한다는 의미는 저 단어들에 대해서 Word2Vec을 수행한다는 의미입니다. 위와 같이 내부 단어들의 벡터값을 얻었다면, 단어 apple의 벡터값은 저 위 벡터값들의 총 합으로 구성합니다.\\napple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>', 'apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>\\n그리고 이런 방법은 Word2Vec에서는 얻을 수 없었던 강점을 가집니다.']\n",
      "['FastText의 인공 신경망을 학습한 후에는 데이터 셋의 모든 단어의 각 n-gram에 대해서 워드 임베딩이 됩니다. 이렇게 되면 장점은 데이터 셋만 충분한다면 위와 같은 내부 단어(Subword)를 통해 모르는 단어(Out Of Vocabulary, OOV)에 대해서도 다른 단어와의 유사도를 계산할 수 있다는 점입니다.\\n가령, FastText에서 birthplace(출생지)란 단어를 학습하지 않은 상태라고 해봅시다. 하지만 다른 단어에서 birth와 place라는 내부 단어가 있었다면, FastText는 birthplace의 벡터를 얻을 수 있습니다. 이는 모르는 단어에 제대로 대처할 수 없는 Word2Vec, GloVe와는 다른 점입니다.']\n",
      "['Word2Vec의 경우에는 등장 빈도 수가 적은 단어(rare word)에 대해서는 임베딩의 정확도가 높지 않다는 단점이 있었습니다. 참고할 수 있는 경우의 수가 적다보니 정확하게 임베딩이 되지 않는 경우입니다.\\n하지만 FastText의 경우, 만약 단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, Word2Vec과 비교하여 비교적 높은 임베딩 벡터값을 얻습니다.\\nFastText가 노이즈가 많은 코퍼스에서 강점을 가진 것 또한 이와 같은 이유입니다. 모든 훈련 코퍼스에 오타(Typo)나 맞춤법이 틀린 단어가 없으면 이상적이겠지만, 실제 많은 비정형 데이터에는 오타가 섞여있습니다. 그리고 오타가 섞인 단어는 당연히 등장 빈도수가 매우 적으므로 일종의 희귀 단어가 됩니다. 즉, Word2Vec에서는 오타가 섞인 단어는 임베딩이 제대로 되지 않지만 FastText는 이에 대해서도 일정 수준의 성능을 보입니다.', '예를 들어 단어 apple과 오타로 p를 한 번 더 입력한 appple의 경우에는 실제로 많은 개수의 동일한 n-gram을 가질 것입니다.']\n",
      "['간단한 실습을 통해 Word2Vec와 FastText의 차이를 비교해보도록 하겠습니다. 단, 사용하는 코드는 Word2Vec를 실습하기위해 사용했던 이전 챕터의 동일한 코드를 사용합니다.\\n1) Word2Vec\\n우선, 이전 Word2Vec의 실습( https://wikidocs.net/50739 )의 전처리 코드와 Word2Vec 학습 코드를 그대로 수행했음을 가정하겠습니다. 입력 단어에 대해서 유사한 단어를 찾아내는 코드에 이번에는 electrofishing이라는 단어를 넣어보겠습니다.\\nmodel.wv.most_similar(\"electrofishing\")\\n해당 코드는 정상 작동하지 않고 에러를 발생시킵니다.\\nKeyError: \"word \\'electrofishing\\' not in vocabulary\"', '해당 코드는 정상 작동하지 않고 에러를 발생시킵니다.\\nKeyError: \"word \\'electrofishing\\' not in vocabulary\"\\n에러 메시지는 단어 집합(Vocabulary)에 electrofishing이 존재하지 않는다고 합니다. 이처럼 Word2Vec는 학습 데이터에 존재하지 않는 단어. 즉, 모르는 단어에 대해서는 임베딩 벡터가 존재하지 않기 때문에 단어의 유사도를 계산할 수 없습니다.\\n2) FastText\\n이번에는 전처리 코드는 그대로 사용하고 Word2Vec 학습 코드만 FastText 학습 코드로 변경하여 실행해봅시다.\\nfrom gensim.models import FastText\\nmodel = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)\\nelectrofishing에 대해서 유사 단어를 찾아보도록 하겠습니다.\\nmodel.wv.most_similar(\"electrofishing\")', 'electrofishing에 대해서 유사 단어를 찾아보도록 하겠습니다.\\nmodel.wv.most_similar(\"electrofishing\")\\n[(\\'electrolux\\', 0.7934642434120178), (\\'electrolyte\\', 0.78279709815979), (\\'electro\\', 0.779127836227417), (\\'electric\\', 0.7753111720085144), (\\'airbus\\', 0.7648627758026123), (\\'fukushima\\', 0.7612422704696655), (\\'electrochemical\\', 0.7611693143844604), (\\'gastric\\', 0.7483425140380859), (\\'electroshock\\', 0.7477173805236816), (\\'overfishing\\', 0.7435552477836609)]', 'Word2Vec는 학습하지 않은 단어에 대해서 유사한 단어를 찾아내지 못 했지만, FastText는 유사한 단어를 계산해서 출력하고 있음을 볼 수 있습니다.']\n",
      "['한국어의 경우에도 OOV 문제를 해결하기 위해 FastText를 적용하고자 하는 시도들이 있었습니다.\\n(1) 음절 단위\\n예를 들어서 음절 단위의 임베딩의 경우에 n=3일때 ‘자연어처리’라는 단어에 대해 n-gram을 만들어보면 다음과 같습니다.\\n<자연, 자연어, 연어처, 어처리, 처리>\\n(2) 자모 단위\\n이제 더 나아가 자모 단위(초성, 중성, 종성 단위)로 임베딩하는 시도 또한 있었습니다. 음절 단위가 아니라, 자모 단위로 가게 되면 오타나 노이즈 측면에서 더 강한 임베딩을 기대해볼 수 있습니다. 예를 들어 ‘자연어처리’라는 단어에 대해서 초성, 중성, 종성을 분리하고, 만약, 종성이 존재하지 않는다면 ‘_’라는 토큰을 사용한다고 가정한다면 ‘자연어처리’라는 단어는 아래와 같이 분리가 가능합니다.\\n분리된 결과 : ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _\\n그리고 분리된 결과에 대해서 n=3일 때, n-gram을 적용하여, 임베딩을 한다면 다음과 같습니다.', '분리된 결과 : ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _\\n그리고 분리된 결과에 대해서 n=3일 때, n-gram을 적용하여, 임베딩을 한다면 다음과 같습니다.\\n< ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... 중략>\\n이어서 자모 단위 FastText를 실습해해봅시다.\\n==================================================\\n--- 09-07 자모 단위 한국어 FastText 학습하기 ---\\n마지막 편집일시 : 2022년 11월 14일 3:12 오후\\n==================================================\\n--- 09-08 사전 훈련된 워드 임베딩(Pre-trained Word Embedding) ---\\n```\\nfrom tensorflow.keras.models import Sequential', \"```\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten, Input\\nmodel = Sequential()\\nmodel.add(Input(shape=(max_len,), dtype='int32'))\\ne = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\\nmodel.add(e)\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\\nmodel.fit(X_train, y_train, epochs=100, verbose=2)\", 'model.fit(X_train, y_train, epochs=100, verbose=2)\\n```이번에는 케라스의 임베딩 층(embedding layer) 과 사전 훈련된 워드 임베딩(pre-trained word embedding) 을 가져와서 사용하는 것을 비교해봅니다. 자연어 처리를 하려고 할 때 갖고 있는 훈련 데이터의 단어들을 임베딩 층(embedding layer)을 구현하여 임베딩 벡터로 학습하는 경우가 있습니다. 케라스에서는 이를 Embedding()이라는 도구를 사용하여 구현합니다.\\n위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2vec, FastText, GloVe 등을 통해서 미리 훈련된 임베딩 벡터를 불러오는 방법을 사용하는 경우도 있습니다. 이는 현재 갖고 있는 훈련 데이터를 임베딩 층으로 처음부터 학습을 하는 방법과는 대조됩니다.']\n",
      "['케라스는 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 Embedding()을 제공합니다. Embedding()은 인공 신경망 구조 관점에서 임베딩 층(embedding layer)을 구현합니다.\\n1) 임베딩 층은 룩업 테이블이다.\\n임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 합니다.\\n어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터\\n임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다. 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트 됩니다. 그리고 이 밀집 벡터를 임베딩 벡터라고 부릅니다.', '정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것은 어떤 의미일까요? 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있습니다. 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가집니다.\\n[이미지: ]\\n위의 그림은 단어 great이 정수 인코딩 된 후 테이블로부터 해당 인덱스에 위치한 임베딩 벡터를 꺼내오는 모습을 보여줍니다. 위의 그림에서는 임베딩 벡터의 차원이 4로 설정되어져 있습니다. 그리고 단어 great은 정수 인코딩 과정에서 1,918의 정수로 인코딩이 되었고 그에 따라 단어 집합의 크기만큼의 행을 가지는 테이블에서 인덱스 1,918번에 위치한 행을 단어 great의 임베딩 벡터로 사용합니다. 이 임베딩 벡터는 모델의 입력이 되고, 역전파 과정에서 단어 great의 임베딩 벡터값이 학습됩니다.', '룩업 테이블의 개념을 이론적으로 우선 접하고, 처음 케라스를 배울 때 어떤 분들은 임베딩 층의 입력이 원-핫 벡터가 아니어도 동작한다는 점에 헷갈려 합니다. 앞서 NNLM이나 Word2Vec을 설명할 때 룩업 테이블을 언급하면서 입력을 원-핫 벡터로 가정하고 설명드렸기 때문인데, 케라스는 단어를 정수 인덱스로 바꾸고 원-핫 벡터로 변환 후 임베딩 층의 입력으로 사용하는 것이 아니라, 단어를 정수 인코딩까지만 진행 후 임베딩 층의 입력으로 사용하여 룩업 테이블 결과인 임베딩 벡터를 리턴합니다.\\n케라스의 임베딩 층 구현 코드를 봅시다.\\nvocab_size = 20000\\noutput_dim = 128\\ninput_length = 500\\nv = Embedding(vocab_size, output_dim, input_length=input_length)\\n임베딩 층은 다음과 같은 세 개의 인자를 받습니다.\\nvocab_size = 텍스트 데이터의 전체 단어 집합의 크기입니다.', '임베딩 층은 다음과 같은 세 개의 인자를 받습니다.\\nvocab_size = 텍스트 데이터의 전체 단어 집합의 크기입니다.\\noutput_dim = 워드 임베딩 후의 임베딩 벡터의 차원입니다.\\ninput_length = 입력 시퀀스의 길이입니다. 만약 갖고있는 각 샘플의 길이가 500개이라면 이 값은 500입니다.\\nEmbedding()은 (number of samples, input_length)인 2D 정수 텐서를 입력받습니다. 이때 각 sample은 정수 인코딩이 된 결과로 정수 시퀀스입니다. Embedding()은 워드 임베딩 작업을 수행하고 (number of samples, input_length, embedding word dimentionality)인 3D 실수 텐서를 리턴합니다. 케라스의 임베딩 층(embedding layer)을 사용하는 실습을 진행해보겠습니다.\\n2) 임베딩 층 사용하기\\n문장의 긍, 부정을 판단하는 감성 분류 모델을 만들어봅시다.', \"2) 임베딩 층 사용하기\\n문장의 긍, 부정을 판단하는 감성 분류 모델을 만들어봅시다.\\n문장과 레이블 데이터를 만들었습니다. 긍정인 문장은 레이블 1, 부정인 문장은 레이블이 0입니다.\\nimport numpy as np\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nsentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\\ny_train = [1, 0, 0, 1, 1, 0, 1]\\n케라스의 토크나이저를 사용하여 단어 집합을 만들고 그 크기를 확인합니다.\\ntokenizer = Tokenizer()\", \"y_train = [1, 0, 0, 1, 1, 0, 1]\\n케라스의 토크나이저를 사용하여 단어 집합을 만들고 그 크기를 확인합니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(sentences)\\nvocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\\nprint('단어 집합 :',vocab_size)\\n단어 집합 : 16\\n각 문장에 대해서 정수 인코딩을 수행합니다.\\nX_encoded = tokenizer.texts_to_sequences(sentences)\\nprint('정수 인코딩 결과 :',X_encoded)\\n정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\\n가장 길이가 긴 문장의 길이를 구합니다.\\nmax_len = max(len(l) for l in X_encoded)\", \"가장 길이가 긴 문장의 길이를 구합니다.\\nmax_len = max(len(l) for l in X_encoded)\\nprint('최대 길이 :',max_len)\\n최대 길이 : 4\\n최대 길이로 모든 샘플에 대해서 패딩을 진행합니다.\\nX_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\\ny_train = np.array(y_train)\\nprint('패딩 결과 :')\\nprint(X_train)\\n패딩 결과 :\\n[[ 1  2  3  4]\\n[ 5  6  0  0]\\n[ 7  8  0  0]\\n[ 9 10  0  0]\\n[11 12  0  0]\\n[13  0  0  0]\\n[14 15  0  0]]\", '[[ 1  2  3  4]\\n[ 5  6  0  0]\\n[ 7  8  0  0]\\n[ 9 10  0  0]\\n[11 12  0  0]\\n[13  0  0  0]\\n[14 15  0  0]]\\n훈련 데이터에 대한 전처리가 끝났습니다. 전형적인 이진 분류 모델을 설계합니다. 출력층에 1개의 뉴런을 배치하고 활성화 함수로는 시그모이드 함수를, 그리고 손실 함수로 binary_crossentropy를 사용합니다. 그 후 100 에포크 학습합니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten\\nembedding_dim = 4\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\\nmodel.add(Flatten())', \"model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\\nmodel.fit(X_train, y_train, epochs=100, verbose=2)\\n학습 과정에서 현재 각 단어들의 임베딩 벡터들의 값은 출력층의 가중치와 함께 학습됩니다.\"]\n",
      "['케라스의 Embedding()을 사용하여 처음부터 임베딩 벡터값을 학습하기도 하지만, 때로는 이미 훈련되어져 있는 워드 임베딩을 가져와서 이를 임베딩 벡터로 사용하기도 합니다. 훈련 데이터가 적은 상황이라면 케라스의 Embedding()으로 해당 문제를 풀기에 최적화 된 임베딩 벡터값을 얻는 것이 쉽지 않습니다. 이 경우 해당 문제에 특화된 것은 아니지만 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있습니다.\\n사전 훈련된 GloVe와 Word2Vec 임베딩을 사용해서 모델을 훈련시키는 실습을 진행해봅시다.\\nGloVe 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip\\nWord2Vec 다운로드 링크 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM', 'Word2Vec 다운로드 링크 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\\n훈련 데이터는 앞서 사용했던 데이터에 동일한 전처리까지 진행된 상태라고 가정하겠습니다.\\nprint(X_train)\\n[[ 1  2  3  4]\\n[ 5  6  0  0]\\n[ 7  8  0  0]\\n[ 9 10  0  0]\\n[11 12  0  0]\\n[13  0  0  0]\\n[14 15  0  0]]\\nprint(y_train)\\n[1, 0, 0, 1, 1, 0, 1]\\n1) 사전 훈련된 GloVe 사용하기\\nglove.6B.zip를 다운로드하고 압축을 풀면 다수의 파일이 존재하는데 여기서는 glove.6B.100d.txt 파일을 사용합니다.\\nfrom urllib.request import urlretrieve, urlopen\\nimport gzip\\nimport zipfile', 'from urllib.request import urlretrieve, urlopen\\nimport gzip\\nimport zipfile\\nurlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\\nzf = zipfile.ZipFile(\\'glove.6B.zip\\')\\nzf.extractall()\\nzf.close()\\nglove.6B.100d.txt에 있는 모든 임베딩 벡터들을 불러옵니다. 파이썬의 자료구조 딕셔너리(dictionary)를 사용하며, 로드한 임베딩 벡터의 개수를 확인합니다.\\nembedding_dict = dict()\\nf = open(\\'glove.6B.100d.txt\\', encoding=\"utf8\")\\nfor line in f:\\nword_vector = line.split()\\nword = word_vector[0]\\n# 100개의 값을 가지는 array로 변환', \"for line in f:\\nword_vector = line.split()\\nword = word_vector[0]\\n# 100개의 값을 가지는 array로 변환\\nword_vector_arr = np.asarray(word_vector[1:], dtype='float32')\\nembedding_dict[word] = word_vector_arr\\nf.close()\\nprint('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))\\n400000개의 Embedding vector가 있습니다.\\n총 40만개의 임베딩 벡터가 존재합니다. 임의의 단어 'respectable'의 임베딩 벡터값과 크기를 출력해봅니다.\\nprint(embedding_dict['respectable'])\\nprint('벡터의 차원 수 :',len(embedding_dict['respectable']))\", \"print(embedding_dict['respectable'])\\nprint('벡터의 차원 수 :',len(embedding_dict['respectable']))\\n[-0.049773   0.19903    0.10585 ... 중략 ... -0.032502   0.38025  ]\\n벡터의 차원 수 : 100\\n벡터값이 출력되며 벡터의 차원 수는 100입니다. 풀고자 하는 문제의 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성합니다. 이 행렬의 값은 전부 0으로 채웁니다. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것입니다.\\nembedding_matrix = np.zeros((vocab_size, 100))\\nprint('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix)\\n임베딩 행렬의 크기(shape) : (16, 100)\\n기존 데이터의 각 단어와 맵핑된 정수값을 확인해봅시다.\\nprint(tokenizer.word_index.items())\", \"임베딩 행렬의 크기(shape) : (16, 100)\\n기존 데이터의 각 단어와 맵핑된 정수값을 확인해봅시다.\\nprint(tokenizer.word_index.items())\\ndict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\\n단어 'great'의 맵핑된 정수는 2입니다.\\nprint('단어 great의 맵핑된 정수 :',tokenizer.word_index['great']\\n단어 great의 맵핑된 정수 : 2\\n사전 훈련된 GloVe에서 'great'의 벡터값을 확인합니다.\", \"단어 great의 맵핑된 정수 : 2\\n사전 훈련된 GloVe에서 'great'의 벡터값을 확인합니다.\\nprint(embedding_dict['great'])\\n[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\\n.. 중략 ...\\n-0.69183   -1.0426     0.28855    0.63056  ]\\n단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑한 후 'great'의 벡터값이 의도한 인덱스의 위치에 삽입되었는지 확인해보겠습니다.\\nfor word, index in tokenizer.word_index.items():\\n# 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\\nvector_value = embedding_dict.get(word)\\nif vector_value is not None:\\nembedding_matrix[index] = vector_value\", 'if vector_value is not None:\\nembedding_matrix[index] = vector_value\\nembedding_matrix의 인덱스 2에서의 값을 확인합니다.\\nembedding_matrix[2]\\narray([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\\n... 중략 ...\\n-0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])', \"... 중략 ...\\n-0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])\\n이전에 확인한 사전에 훈련된 GloVe에서의 'great'의 벡터값과 일치합니다. 이제 Embedding layer에  embedding_matrix를 초기값으로 설정합니다. 현재 실습에서 사전 훈련된 워드 임베딩을 100차원의 값인 것으로 사용하고 있기 때문에 임베딩 층의 output_dim의 인자값으로 100을 주어야 합니다. 그리고 사전 훈련된 워드 임베딩을 그대로 사용할 경우 추가 훈련을 하지 않는다는 의미에서 trainable의 인자값을 False로 선택할 수 있습니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten\\noutput_dim = 100\\nmodel = Sequential()\", \"from tensorflow.keras.layers import Dense, Embedding, Flatten\\noutput_dim = 100\\nmodel = Sequential()\\ne = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\\nmodel.add(e)\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\\nmodel.fit(X_train, y_train, epochs=100, verbose=2)\\n사전 훈련된 GloVe 임베딩에 대한 예제는 아래의 케라스 블로그 링크에도 기재되어 있습니다.\", \"사전 훈련된 GloVe 임베딩에 대한 예제는 아래의 케라스 블로그 링크에도 기재되어 있습니다.\\n링크 : https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\\n2) 사전 훈련된 Word2Vec 사용하기\\n구글의 사전 훈련된 Word2Vec 모델을 로드하여 word2vec_model에 저장 후 크기를 확인합니다.\\nimport gensim\\n!pip install gdown\\n!gdown https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j\\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\", \"print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인\\n모델의 크기(shape) : (3000000, 300)\\n300의 차원을 가진 Word2Vec 벡터가 3,000,000개 있습니다. 모든 값이 0으로 채워진 임베딩 행렬을 만들어줍니다. 풀고자 하는 문제의 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성합니다. 이 행렬의 값은 전부 0으로 채웁니다. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것입니다.\\nembedding_matrix = np.zeros((vocab_size, 300))\\nprint('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix)\\n임베딩 행렬의 크기(shape) : (16, 300)\", \"print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix)\\n임베딩 행렬의 크기(shape) : (16, 300)\\nword2vec_model에서 특정 단어를 입력하면 해당 단어의 임베딩 벡터를 리턴받을텐데, 만약 word2vec_model에 특정 단어의 임베딩 벡터가 없다면 None을 리턴하도록 하는 함수 get_vector()를 구현합니다.\\ndef get_vector(word):\\nif word in word2vec_model:\\nreturn word2vec_model[word]\\nelse:\\nreturn None\\n단어 집합으로부터 단어를 1개씩 호출하여 word2vec_model에 해당 단어의 임베딩 벡터값이 존재하는지 확인합니다. 만약 None이 아니라면 존재한다는 의미이므로 임베딩 행렬에 해당 단어의 인덱스 위치의 행에 임베딩 벡터의 값을 저장합니다.\\nfor word, index in tokenizer.word_index.items():\", \"for word, index in tokenizer.word_index.items():\\n# 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\\nvector_value = get_vector(word)\\nif vector_value is not None:\\nembedding_matrix[index] = vector_value\\n현재 풀고자하는 문제의 16개의 단어와 맵핑되는 임베딩 행렬이 완성됩니다. 제대로 맵핑이 됐는지 확인해볼까요? 기존에 word2vec_model에 저장되어 있던 단어 'nice'의 임베딩 벡터값을 확인해봅시다.\\nprint(word2vec_model['nice'])\\n[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\\n0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\\n... 중략 ...\", \"0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\\n... 중략 ...\\n-0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\\n-0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\\n단어 'nice'의 맵핑된 정수를 확인합니다.\\nprint('단어 nice의 맵핑된 정수 :', tokenizer.word_index['nice'])\\n단어 nice의 맵핑된 정수 : 1\\n1의 값을 가지므로 embedding_matirx의 1번 인덱스에는 단어 'nice'의 임베딩 벡터값이 있어야 합니다.\\nprint(embedding_matrix[1])\\n[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\", 'print(embedding_matrix[1])\\n[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\\n0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\\n... 중략 ...\\n-0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\\n-0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\\n값이 word2vec_model에서 확인했던 것과 동일한 것을 확인할 수 있습니다. 단어 집합에 있는 다른 단어들에 대해서도 확인해보세요. 이제 Embedding에 사전 훈련된 embedding_matrix를 입력으로 넣어주고 모델을 학습시켜보겠습니다.', \"from tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten, Input\\nmodel = Sequential()\\nmodel.add(Input(shape=(max_len,), dtype='int32'))\\ne = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\\nmodel.add(e)\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\\nmodel.fit(X_train, y_train, epochs=100, verbose=2)\", \"model.fit(X_train, y_train, epochs=100, verbose=2)\\n사전 훈련된 워드 임베딩을 이용한 텍스트 분류는 'NLP를 이용한 합성곱 신경망' 챕터의 의도 분류 실습( https://wikidocs.net/86083 )을 참고하세요.\\n==================================================\\n--- 09-09 엘모(Embeddings from Language Model, ELMo) ---\\n```\\n1115/1115 [==============================] - 381s 342ms/step\\n테스트 정확도: 0.9803\\n```[이미지: ]\\n논문 링크 : https://aclweb.org/anthology/N18-1202\", \"테스트 정확도: 0.9803\\n```[이미지: ]\\n논문 링크 : https://aclweb.org/anthology/N18-1202\\nELMo(Embeddings from Language Model)는 2018년에 제안된 새로운 워드 임베딩 방법론입니다. ELMo라는 이름은 세서미 스트리트라는 미국 인형극의 케릭터 이름이기도 한데, 뒤에서 배우게 되는 BERT나 최근 마이크로소프트가 사용한 Big Bird라는 NLP 모델 또한 ELMo에 이어 세서미 스트리트의 케릭터의 이름을 사용했습니다. ELMo는 Embeddings from Language Model의 약자입니다. 해석하면 '언어 모델로 하는 임베딩'입니다. ELMo의 가장 큰 특징은 사전 훈련된 언어 모델(Pre-trained language model)을 사용한다는 점입니다. 이는 ELMo의 이름에 LM이 들어간 이유입니다.\", '현재 텐서플로우 2.0에서는 TF-Hub의 ELMo를 사용할 수 없습니다. 사용하려면 텐서플로우 버전을 1버전으로 낮추어야 합니다. Colab에서 실습하시는 것을 권장드립니다. Colab에서는 손쉽게 텐서플로우 버전을 1버전으로 설정할 수 있습니다. 아래 실습 내용을 참고하세요.']\n",
      "['Bank라는 단어를 생각해봅시다. Bank Account(은행 계좌)와 River Bank(강둑)에서의 Bank는 전혀 다른 의미를 가지는데, Word2Vec이나 GloVe 등으로 표현된 임베딩 벡터들은 이를 제대로 반영하지 못한다는 단점이 있습니다. 예를 들어서 Word2Vec이나 GloVe 등의 임베딩 방법론으로 Bank란 단어를 [0.2 0.8 -1.2]라는 임베딩 벡터로 임베딩하였다고 하면, 이 단어는 Bank Account(은행 계좌)와 River Bank(강둑)에서의 Bank는 전혀 다른 의미임에도 불구하고 두 가지 상황 모두에서 [0.2 0.8 -1.2]의 벡터가 사용됩니다.\\n같은 표기의 단어라도 문맥에 따라서 다르게 워드 임베딩을 할 수 있으면 자연어 처리의 성능을 올릴 수 있을 것입니다. 워드 임베딩 시 문맥을 고려해서 임베딩을 하겠다는 아이디어가 문맥을 반영한 워드 임베딩(Contextualized Word Embedding) 입니다.']\n",
      "['다음 단어를 예측하는 작업인 언어 모델링을 상기해봅시다. 아래의 그림은 은닉층이 2개인 일반적인 단방향 RNN 언어 모델의 언어 모델링을 보여줍니다.\\n[이미지: ]\\nRNN 언어 모델은 문장으로부터 단어 단위로 입력을 받는데, RNN 내부의 은닉 상태 $h_{t}$는 시점(time step)이 지날수록 점점 업데이트되갑니다. 이는 결과적으로 RNN의 $h_{t}$의 값이 문장의 문맥 정보를 점차적으로 반영한다고 말할 수 있습니다. 그런데 ELMo는 위의 그림의 순방향 RNN 뿐만 아니라, 위의 그림과는 반대 방향으로 문장을 스캔하는 역방향 RNN 또한 활용합니다. ELMo는 양쪽 방향의 언어 모델을 둘 다 학습하여 활용한다고하여 이 언어 모델을 biLM(Bidirectional Language Model) 이라고 합니다.', \"ELMo에서 말하는 biLM은 기본적으로 다층 구조(Multi-layer)를 전제로 합니다. 은닉층이 최소 2개 이상이라는 의미입니다. 아래의 그림은 은닉층이 2개인 순방향 언어 모델과 역방향 언어 모델의 모습을 보여줍니다.\\n[이미지: ]\\n이때 biLM의 각 시점의 입력이 되는 단어 벡터는 이번 챕터에서 설명한 임베딩 층(embedding layer)을 사용해서 얻은 것이 아니라 합성곱 신경망을 이용한 문자 임베딩(character embedding)을 통해 얻은 단어 벡터입니다. 문자 임베딩에 대한 설명은 'NLP를 위한 합성곱 신경망' 챕터에서 다루는 내용으로 여기서는 임베딩층, Word2Vec 등 외에 단어 벡터를 얻는 또 다른 방식도 있다고만 알아둡시다. 문자 임베딩은 마치 서브단어(subword)의 정보를 참고하는 것처럼 문맥과 상관없이 dog란 단어와 doggy란 단어의 연관성을 찾아낼 수 있습니다. 또한 이 방법은 OOV에도 견고한다는 장점이 있습니다.\", '주의할 점은 앞서 설명한 양방향 RNN과 ELMo에서의 biLM은 다릅니다. 양방향 RNN은 순방향 RNN의 은닉 상태와 역방향의 RNN의 은닉 상태를 연결(concatenate)하여 다음층의 입력으로 사용합니다. 반면, biLM의 순방향 언어모델과 역방향 언어모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습합니다.']\n",
      "['biLM이 언어 모델링을 통해 학습된 후 ELMo가 사전 훈련된 biLM을 통해 입력 문장으로부터 단어를 임베딩하기 위한 과정을 보겠습니다.\\n[이미지: ]\\n이 예제에서는 play란 단어가 임베딩이 되고 있다는 가정 하에 ELMo를 설명합니다. play라는 단어를 임베딩 하기위해서 ELMo는 위의 점선의 사각형 내부의 각 층의 결과값을 재료로 사용합니다. 다시 말해 해당 시점(time step)의 BiLM의 각 층의 출력값을 가져옵니다. 그리고 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결(concatenate)하고 추가 작업을 진행합니다.\\n여기서 각 층의 출력값이란 첫번째는 임베딩 층을 말하며, 나머지 층은 각 층의 은닉 상태를 말합니다. ELMo의 직관적인 아이디어는 각 층의 출력값이 가진 정보는 전부 서로 다른 종류의 정보를 갖고 있을 것이므로, 이들을 모두 활용한다는 점에 있습니다. 아래는 ELMo가 임베딩 벡터를 얻는 과정을 보여줍니다.', '1) 각 층의 출력값을 연결(concatenate)한다.\\n[이미지: ]\\n2) 각 층의 출력값 별로 가중치를 준다.\\n[이미지: ]\\n이 가중치를 여기서는 $s_{1}$, $s_{2}$, $s_{3}$라고 합시다.\\n3) 각 층의 출력값을 모두 더한다.\\n[이미지: ]\\n2)번과 3)번의 단계를 요약하여 가중합(Weighted Sum)을 한다고 할 수 있습니다.\\n4) 벡터의 크기를 결정하는 스칼라 매개변수를 곱한다.\\n[이미지: ]\\n이 스칼라 매개변수를 여기서는 $γ$이라고 합시다.\\n이렇게 완성된 벡터를 ELMo 표현(representation)이라고 합니다. 지금까지는 ELMo 표현을 얻기 위한 과정이고 이제 ELMo를 입력으로 사용하고 수행하고 싶은 텍스트 분류, 질의 응답 시스템 등의 자연어 처리 작업이 있을 것입니다. 예를 들어 텍스트 분류 작업을 하고 싶다고 가정합시다. 그렇다면 ELMo 표현을 어떻게 텍스트 분류 작업에 사용할 수 있을까요?', 'ELMo 표현을 기존의 임베딩 벡터와 함께 사용할 수 있습니다. 우선 텍스트 분류 작업을 위해서 GloVe와 같은 기존의 방법론을 사용한 임베딩 벡터를 준비했다고 합시다. 이때, GloVe를 사용한 임베딩 벡터만 텍스트 분류 작업에 사용하는 것이 아니라 이렇게 준비된 ELMo 표현을 GloVe 임베딩 벡터와 연결(concatenate)해서 입력으로 사용할 수 있습니다. 그리고 이때 biLM의 가중치는 고정시키고, 위에서 사용한  $s_{1}$, $s_{2}$, $s_{3}$와 $γ$는 훈련 과정에서 학습됩니다.\\n[이미지: ]\\n위의 그림은 ELMo 표현이 기존의 GloVe 등과 같은 임베딩 벡터와 함께 NLP 태스크의 입력이 되는 것을 보여줍니다.']\n",
      "['이번 예제의 실습은 Colab에서 수행한다고 가정합니다. 우선 시작 전에 텐서플로우 버전을 1버전으로 설정하겠습니다.\\n%tensorflow_version 1.x\\n텐서플로우 허브로부터 다양한 사전 훈련된 모델(Pre-tained Model)들을 사용할 수 있습니다. 여기서는 사전 훈련된 모델로부터 ELMo 표현을 사용해보는 정도로 예제를 진행해보겠습니다. 시작 전에 텐서플로우 허브를 인스톨 해야합니다. 윈도우의 명령 프롬프트나 UNIX의 터미널에서 아래의 명령을 수행합니다.\\npip install tensorflow-hub\\n설치가 끝났다면 이제 텐서플로우 허브를 임포트할 수 있습니다. 필요한 도구들을 임포트합니다.\\nimport tensorflow_hub as hub\\nimport tensorflow as tf\\nfrom keras import backend as K\\nimport urllib.request\\nimport pandas as pd\\nimport numpy as np', 'from keras import backend as K\\nimport urllib.request\\nimport pandas as pd\\nimport numpy as np\\n텐서플로우 허브로부터 ELMo를 다운로드하겠습니다.\\nelmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\\n# 텐서플로우 허브로부터 ELMo를 다운로드\\nsess = tf.Session()\\nK.set_session(sess)\\nsess.run(tf.global_variables_initializer())\\nsess.run(tf.tables_initializer())\\n기본적으로 필요한 것들을 임포트하였습니다. 이제 데이터를 불러오고, 5개만 출력해보겠습니다.\\n파일 원본 출처 : https://www.kaggle.com/uciml/sms-spam-collection-dataset\\n스팸 메일 분류하기 데이터를 다운로드하겠습니다.', '파일 원본 출처 : https://www.kaggle.com/uciml/sms-spam-collection-dataset\\n스팸 메일 분류하기 데이터를 다운로드하겠습니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\\ndata = pd.read_csv(\\'spam.csv\\', encoding=\\'latin-1\\')\\ndata[:5]\\n[이미지: ]\\n여기서 필요한 건 v2열과 v1열입니다. v1열은 숫자 레이블로 바꿔야 할 필요가 있습니다. 이를 각각 X_data와 y_data로 저장하겠습니다.\\ndata[\\'v1\\'] = data[\\'v1\\'].replace([\\'ham\\',\\'spam\\'],[0,1])\\ny_data = list(data[\\'v1\\'])', \"data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\\ny_data = list(data['v1'])\\nX_data = list(data['v2'])\\nv2열을 X_data에 저장합니다. v1열에 있는 ham과 spam 레이블을 각각 숫자 0과 1로 바꾸고 y_data에 저장합니다. 정상적으로 저장되었는지 이를 각각 5개만 출력해보겠습니다.\\nX_data[:5]\\n['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\\n'Ok lar... Joking wif u oni...',\", '\\'Ok lar... Joking wif u oni...\\',\\n\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C\\'s apply 08452810075over18\\'s\",\\n\\'U dun say so early hor... U c already then say...\\',\\n\"Nah I don\\'t think he goes to usf, he lives around here though\"]\\nprint(y_data[:5])\\n[0, 0, 1, 0, 0]\\n훈련 데이터와 테스트 데이터를 8:2 비율로 분할해보겠습니다. 그런데 그 전에 이를 위해 전체 데이터 개수의 80%와 20%는 각각 몇 개인지 확인합니다.\\nprint(len(X_data))\\nn_of_train = int(len(X_data) * 0.8)', 'print(len(X_data))\\nn_of_train = int(len(X_data) * 0.8)\\nn_of_test = int(len(X_data) - n_of_train)\\nprint(n_of_train)\\nprint(n_of_test)\\n5572\\n4457\\n1115\\n전체 데이터는 5,572개이며 8:2로 비율하면 각각 4,457과 1,115가 됩니다. 이를 각각 훈련 데이터와 테스트 데이터의 양으로 하여 데이터를 분할하겠습니다.\\nX_train = np.asarray(X_data[:n_of_train]) #X_data 데이터 중에서 앞의 4457개의 데이터만 저장\\ny_train = np.asarray(y_data[:n_of_train]) #y_data 데이터 중에서 앞의 4457개의 데이터만 저장\\nX_test = np.asarray(X_data[n_of_train:]) #X_data 데이터 중에서 뒤의 1115개의 데이터만 저장', 'X_test = np.asarray(X_data[n_of_train:]) #X_data 데이터 중에서 뒤의 1115개의 데이터만 저장\\ny_test = np.asarray(y_data[n_of_train:]) #y_data 데이터 중에서 뒤의 1115개의 데이터만 저장\\n이제 훈련을 위한 데이터 준비는 끝났습니다. 이제 ELMo와 설계한 모델을 연결하는 작업들을 진행해보겠습니다. ELMo는 텐서플로우 허브로부터 가져온 것이기 때문에 케라스에서 사용하기 위해서는 케라스에서 사용할 수 있도록 변환해주는 작업들이 필요합니다.\\ndef ELMoEmbedding(x):\\nreturn elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature=\"default\")[\"default\"]\\n# 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수\\n이제 모델을 설계합니다.\\nfrom keras.models import Model', \"# 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수\\n이제 모델을 설계합니다.\\nfrom keras.models import Model\\nfrom keras.layers import Dense, Lambda, Input\\ninput_text = Input(shape=(1,), dtype=tf.string)\\nembedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\\nhidden_layer = Dense(256, activation='relu')(embedding_layer)\\noutput_layer = Dense(1, activation='sigmoid')(hidden_layer)\\nmodel = Model(inputs=[input_text], outputs=output_layer)\", \"model = Model(inputs=[input_text], outputs=output_layer)\\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\\n모델은 ELMo를 이용한 임베딩 층을 거쳐서 256개의 뉴런이 있는 은닉층을 거친 후 마지막 1개의 뉴런을 통해 이진 분류를 수행합니다. 이진 분류를 위한 마지막 뉴런의 활성화 함수는 시그모이드 함수이며, 모델의 손실 함수는 binary_crossentropy입니다.\\nhistory = model.fit(X_train, y_train, epochs=1, batch_size=60)\\nEpoch 1/1\\n4457/4457 [==============================] - 1508s 338ms/step - loss: 0.1129 - acc: 0.9619\", '4457/4457 [==============================] - 1508s 338ms/step - loss: 0.1129 - acc: 0.9619\\n훈련 데이터에서는 정확도 96%를 얻었습니다. 이제 테스트 데이터에 대해서 평가해보겠습니다.\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\\n1115/1115 [==============================] - 381s 342ms/step\\n테스트 정확도: 0.9803\\n1번의 에포크에서 테스트 데이터 정확도 98%를 얻어냅니다.\\n==================================================\\n--- 09-10 임베딩 벡터의 시각화(Embedding Visualization) ---\\n```\\n!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v', '```\\n!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v\\n```구글은 임베딩 프로젝터(embedding projector)라는 데이터 시각화 도구를 지원합니다. 학습한 임베딩 벡터들을 시각화해봅시다.\\n임베딩 프로젝터 논문 : https://arxiv.org/pdf/1611.05469v1.pdf']\n",
      "[\"학습한 임베딩 벡터들을 시각화해보겠습니다. 시각화를 위해서는 이미 모델을 학습하고, 파일로 저장되어져 있어야 합니다. 모델이 저장되어져 있다면 아래 커맨드를 통해 시각화에 필요한 파일들을 생성할 수 있습니다.\\n!python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름\\n여기서는 편의를 위해 이전 챕터에서 학습하고 저장하는 실습까지 진행했던 영어 Word2Vec 모델인 'eng_w2v'를 재사용합니다. eng_w2v라는 Word2Vec 모델이 이미 존재한다는 가정 하에 주피터 노트북에서 아래 커맨드를 수행합니다.\\n!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v\\n커맨드를 수행하면 주피터 노트북이 시작되는 경로에 기존에 있던 eng_w2v 외에도 두 개의 파일이 생깁니다.\\n[이미지: ]\", \"커맨드를 수행하면 주피터 노트북이 시작되는 경로에 기존에 있던 eng_w2v 외에도 두 개의 파일이 생깁니다.\\n[이미지: ]\\n새로 생긴 eng_w2v_metadata.tsv와 eng_w2v_tensor.tsv 이 두 개 파일이 임베딩 벡터 시각화를 위해 사용할 파일입니다. 만약 eng_w2v 모델 파일이 아니라 다른 모델 파일 이름으로 실습을 진행하고 있다면, '모델 이름_metadata.tsv'와 '모델 이름_tensor.tsv'라는 파일이 생성됩니다.\"]\n",
      "['구글의 임베딩 프로젝터를 사용해서 워드 임베딩 모델을 시각화해보겠습니다. 아래의 링크에 접속합니다.\\n링크 : https://projector.tensorflow.org/\\n사이트에 접속해서 좌측 상단을 보면 Load라는 버튼이 있습니다.\\n[이미지: ]\\nLoad라는 버튼을 누르면 아래와 같은 창이 뜨는데 총 두 개의 Choose file 버튼이 있습니다.\\n[이미지: ]\\n위에 있는 Choose file 버튼을 누르고 eng_w2v_tensor.tsv 파일을 업로드하고, 아래에 있는 Choose file 버튼을 누르고 eng_w2v_metadata.tsv 파일을 업로드합니다. 두 파일을 업로드하면 임베딩 프로젝터에 학습했던 워드 임베딩 모델이 시각화됩니다.\\n[이미지: ]', '[이미지: ]\\n그 후에는 임베딩 프로젝터의 다양한 기능을 사용할 수 있습니다. 예를 들어 임베딩 프로젝터는 복잡한 데이터를 차원을 축소하여 시각화 할 수 있도록 도와주는 PCA, t-SNE 등을 제공합니다. 위의 그림은 \\'man\\' 이라는 단어를 선택하고, 코사인 유사도를 기준으로 가장 유사한 상위 10개 벡터들을 표시해봤습니다.\\n==================================================\\n--- 09-11 문서 벡터를 이용한 추천 시스템(Recommendation System using Document Embedding) ---\\n```\\nrecommendations(\"The Murder of Roger Ackroyd\")', '```\\nrecommendations(\"The Murder of Roger Ackroyd\")\\n```문서들을 고정된 길이의 벡터로 변환한다면 벡터 간 비교로 문서들을 서로 비교할 수 있습니다. 각 문서를 문서 벡터로 변환하는 방법은 이미 구현된 패키지인 Doc2Vec이나 Sent2Vec 등을 사용하여 학습하는 방법도 존재하지만, 단어 벡터를 얻은 뒤 문서에 존재하는 단어 벡터들의 평균을 문서 벡터로 간주할 수 있습니다. 이번에는 문서 내 각 단어들을 Word2Vec을 통해 단어 벡터로 변환하고, 이들의 평균으로 문서 벡터를 얻어 선호하는 도서와 유사한 도서를 찾아주는 도서 추천 시스템을 만들어보겠습니다.\\ngensim 4.3.2 버전에서 진행된 실습입니다.\\nimport gensim\\nprint(gensim.__version__)\\n4.3.2']\n",
      "['이번 실습에서 사용할 데이터는 책의 이미지와 책의 줄거리를 크롤링한 데이터로 아래의 링크에서 다운로드 할 수 있습니다.\\n데이터 다운로드 링크 : https://drive.google.com/file/d/15Q7DZ7xrJsI2Hji-WbkU9j1mwnODBd5A/view?usp=sharing\\n# 현재 코드가 gensim 4.3.2 버전에서 동작합니다.\\nimport urllib.request\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport requests\\nimport re\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom nltk.tokenize import RegexpTokenizer\\nimport nltk\\nfrom gensim.models import Word2Vec\\nfrom gensim.models import KeyedVectors', 'import nltk\\nfrom gensim.models import Word2Vec\\nfrom gensim.models import KeyedVectors\\nfrom nltk.corpus import stopwords\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n데이터를 데이터프레임으로 로드하고 전체 문서의 수를 출력해봅시다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/data.csv\", filename=\"data.csv\")\\ndf = pd.read_csv(\"data.csv\")\\nprint(\\'전체 문서의 수 :\\',len(df))\\n전체 문서의 수 : 2382\\n상위 5개의 행만 출력해봅시다.\\ndf[:5]\\n[이미지: ]', 'print(\\'전체 문서의 수 :\\',len(df))\\n전체 문서의 수 : 2382\\n상위 5개의 행만 출력해봅시다.\\ndf[:5]\\n[이미지: ]\\n불필요한 열들이 존재하는데, 여기서는 줄거리에 해당하는 열인 \\'Desc 열\\'이 중요합니다. 해당 열에 있는 데이터에 대해서 Word2Vec을 학습하기 위해 전처리를 진행합니다. 해당 열에 대해서 전처리를 수행하고 \\'cleaned\\'라는 열에 저장해봅시다.\\ndef _removeNonAscii(s):\\nreturn \"\".join(i for i in s if  ord(i)<128)\\ndef make_lower_case(text):\\nreturn text.lower()\\ndef remove_stop_words(text):\\ntext = text.split()\\nstops = set(stopwords.words(\"english\"))\\ntext = [w for w in text if not w in stops]\\ntext = \" \".join(text)\\nreturn text', 'text = [w for w in text if not w in stops]\\ntext = \" \".join(text)\\nreturn text\\ndef remove_html(text):\\nhtml_pattern = re.compile(\\'<.*?>\\')\\nreturn html_pattern.sub(r\\'\\', text)\\ndef remove_punctuation(text):\\ntokenizer = RegexpTokenizer(r\\'[a-zA-Z]+\\')\\ntext = tokenizer.tokenize(text)\\ntext = \" \".join(text)\\nreturn text\\ndf[\\'cleaned\\'] = df[\\'Desc\\'].apply(_removeNonAscii)\\ndf[\\'cleaned\\'] = df.cleaned.apply(make_lower_case)\\ndf[\\'cleaned\\'] = df.cleaned.apply(remove_stop_words)', \"df['cleaned'] = df.cleaned.apply(remove_stop_words)\\ndf['cleaned'] = df.cleaned.apply(remove_punctuation)\\ndf['cleaned'] = df.cleaned.apply(remove_html)\\n상위 5개의 행만 출력합니다.\\ndf['cleaned'][:5]\\n0    know power shifting west east north south pres...\\n1    following success accidental billionaires mone...\\n2    tap power social software networks build busin...\\n3    william j bernstein american financial theoris...\\n4    amazing book joined steve jobs many akio morit...\\nName: cleaned, dtype: object\", \"4    amazing book joined steve jobs many akio morit...\\nName: cleaned, dtype: object\\n전처리 과정에서 빈 값이 생긴 행이 있을 수 있습니다. 빈 값이 생긴 행이 있다면, nan 값으로 변환 후에 해당 행을 제거합니다.\\ndf['cleaned'].replace('', np.nan, inplace=True)\\ndf = df[df['cleaned'].notna()]\\nprint('전체 문서의 수 :',len(df))\\n전체 문서의 수 : 2381\\n전체 문서의 수가 1개 줄어들었습니다. 토큰화를 수행하여 corpus라는 리스트에 저장합니다. 해당 리스트 corpus를 통해 Word2Vec을 훈련할 것입니다.\\ncorpus = []\\nfor words in df['cleaned']:\\ncorpus.append(words.split())\"]\n",
      "['구글 드라이브에서 Word2Vec을 다운로드하기 위해서 gdrive 패키지를 설치 후 저자가 업로드 한 파일을 다운로드합니다.\\n!pip install gdown\\n!gdown https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j\\nWord2Vec을 처음부터 학습할 수도 있겠지만, 데이터가 충분하지 않은 상황에서 사전 훈련된 워드 임베딩을 단어 벡터의 초기값으로 사용하여 성능을 높일 수 있습니다. 여기서는 사전 훈련된 Word2Vec을 로드하고 초기 단어 벡터값으로 사용합니다.\\nword2vec_model = Word2Vec(vector_size=300, window=5, min_count=2, workers=-1)\\nword2vec_model.build_vocab(corpus)\\nword2vec_model.wv.vectors_lockf = np.ones(len(word2vec_model.wv), dtype=np.float32)', \"word2vec_model.wv.vectors_lockf = np.ones(len(word2vec_model.wv), dtype=np.float32)\\nword2vec_model.wv.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\\nword2vec_model.train(corpus, total_examples = word2vec_model.corpus_count, epochs = 15)\"]\n",
      "['각 문서에 존재하는 단어들의 벡터값의 평균을 구하여 해당 문서의 벡터값을 연산해봅시다.\\ndef get_document_vectors(document_list):\\ndocument_embedding_list = []\\n# 각 문서에 대해서\\nfor line in document_list:\\ndoc2vec = None\\ncount = 0\\nfor word in line.split():\\nif word in list(word2vec_model.wv.index_to_key):\\ncount += 1\\n# 해당 문서에 있는 모든 단어들의 벡터값을 더한다.\\nif doc2vec is None:\\ndoc2vec = word2vec_model.wv[word]\\nelse:\\ndoc2vec = doc2vec + word2vec_model.wv[word]\\nif doc2vec is not None:\\n# 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.\\ndoc2vec = doc2vec / count', \"if doc2vec is not None:\\n# 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.\\ndoc2vec = doc2vec / count\\ndocument_embedding_list.append(doc2vec)\\n# 각 문서에 대한 문서 벡터 리스트를 리턴\\nreturn document_embedding_list\\ndocument_embedding_list = get_document_vectors(df['cleaned'])\\nprint('문서 벡터의 수 :',len(document_embedding_list))\\n문서 벡터의 수 : 2381\"]\n",
      "[\"각 문서 벡터 간의 코사인 유사도를 구합니다.\\ncosine_similarities = cosine_similarity(document_embedding_list, document_embedding_list)\\nprint('코사인 유사도 매트릭스의 크기 :',cosine_similarities.shape)\\n코사인 유사도 매트릭스의 크기 : (2381, 2381)\\n선택한 책에 대해서 코사인 유사도를 이용하여 가장 줄거리가 유사한 5개의 책을 찾아내는 함수를 만듭니다.\\ndef recommendations(title):\\nbooks = df[['title', 'image_link']]\\n# 책의 제목을 입력하면 해당 제목의 인덱스를 리턴받아 idx에 저장.\\nindices = pd.Series(df.index, index = df['title']).drop_duplicates()\\nidx = indices[title]\", \"indices = pd.Series(df.index, index = df['title']).drop_duplicates()\\nidx = indices[title]\\n# 입력된 책과 줄거리(document embedding)가 유사한 책 5개 선정.\\nsim_scores = list(enumerate(cosine_similarities[idx]))\\nsim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\\nsim_scores = sim_scores[1:6]\\n# 가장 유사한 책 5권의 인덱스\\nbook_indices = [i[0] for i in sim_scores]\\n# 전체 데이터프레임에서 해당 인덱스의 행만 추출. 5개의 행을 가진다.\\nrecommend = books.iloc[book_indices].reset_index(drop=True)\\nfig = plt.figure(figsize=(20, 30))\", 'recommend = books.iloc[book_indices].reset_index(drop=True)\\nfig = plt.figure(figsize=(20, 30))\\n# 데이터프레임으로부터 순차적으로 이미지를 출력\\nfor index, row in recommend.iterrows():\\nresponse = requests.get(row[\\'image_link\\'])\\nimg = Image.open(BytesIO(response.content))\\nfig.add_subplot(1, 5, index + 1)\\nplt.imshow(img)\\nplt.title(row[\\'title\\'])\\n좋아하는 책 제목을 입력으로 넣으면 해당 책의 문서 벡터(줄거리 벡터)와 유사한 문서 벡터값을 가진 책들을 추천해줍니다. 책 제목과 표지를 출력합니다.\\nrecommendations(\"The Da Vinci Code\")\\n[이미지: ]', 'recommendations(\"The Da Vinci Code\")\\n[이미지: ]\\n다빈치 코드는 작가 댄 브라운의 작품입니다. 추천되는 작품들 또한 5개 중 3개가 댄 브라운의 작품들이 추천됨을 확인할 수 있습니다. 이번에는 아가사 크리스티의 애크로이드 살인사건과 유사한 도서를 추천받아 봅시다.\\nrecommendations(\"The Murder of Roger Ackroyd\")\\n[이미지: ]\\n애크로이드 살인사건은 미스터리 스릴러 소설인데 이와 유사한 소설들이 추천되는 것을 확인할 수 있습니다.\\n==================================================\\n--- 09-12 문서 임베딩 : 워드 임베딩의 평균(Average Word Embedding) ---\\n```\\n테스트 정확도: 0.8876', '--- 09-12 문서 임베딩 : 워드 임베딩의 평균(Average Word Embedding) ---\\n```\\n테스트 정확도: 0.8876\\n```앞서 특정 문장 내의 단어들의 임베딩 벡터들의 평균이 그 문장의 벡터가 될 수 있음을 설명했습니다. 이번에는 임베딩이 잘 된 상황에서는 단어 벡터들의 평균만으로 텍스트 분류를 수행할 수 있음을 보이고, 워드 임베딩의 중요성을 이해해보겠습니다.\\n영화 사이트 IMDB 영화 리뷰 데이터는 리뷰 텍스트에 리뷰가 긍정인 경우 1을, 부정인 경우 0으로 레이블링 한 데이터로 25,000개의 훈련 데이터와 테스트 데이터 25,000개로 구성된 데이터입니다. 케라스를 통해서 이 데이터셋을 바로 다운로드 할 수 있습니다. 이 데이터에 대한 상세 설명은 뒤의 RNN을 이용한 텍스트 분류 챕터에서 IMDB 리뷰 데이터를 다룰 때 설명합니다. 여기서는 데이터나 코드에 대한 설명보다는 단어 벡터의 평균으로 얻을 수 있는 성능에 주목해주세요.', '이 자료는 24년 2월, 텐서플로우 2.15 버전에서 동작하는 것으로 확인된 코드입니다.']\n",
      "[\"import numpy as np\\nfrom tensorflow.keras.datasets import imdb\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\n케라스에서는 imdb.data_load()를 통해서 IMDB 리뷰 데이터를 다운로드 할 수 있는데, 데이터를 로드할 때 파라미터로 num_words를 사용하면 이 데이터에서 등장 빈도 순위로 몇 번째에 해당하는 단어까지를 사용할 것인지를 의미합니다. 만약 vocab_size를 20,000으로 지정할 경우 등장 빈도 순위가 20,000등이 넘는 단어들은 데이터를 로드할 때 전부 제거 후 로드합니다.\\nvocab_size = 20000\\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\\nprint('훈련용 리뷰 개수 :',len(X_train))\", \"print('훈련용 리뷰 개수 :',len(X_train))\\nprint('테스트용 리뷰 개수 :',len(X_test))\\n훈련용 리뷰 개수 : 25000\\n테스트용 리뷰 개수 : 25000\\n이 데이터는 이미 정수 인코딩까지의 전처리가 진행되어져 있습니다. 그래서 단어 집합을 만들고, 각 단어를 정수로 인코딩하는 과정을 할 필요가 없습니다. 첫번째 리뷰와 첫번째 리뷰의 레이블을 출력해봅시다.\\nprint('훈련 데이터의 첫번째 샘플 :',X_train[0])\\nprint('훈련 데이터의 첫번째 샘플의 레이블 :',y_train[0])\", '훈련 데이터의 첫번째 샘플 : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619,', '5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334,', '134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]', \"훈련 데이터의 첫번째 샘플의 레이블 : 1\\n정수 1이 출력되는데 이는 첫번째 리뷰가 긍정 리뷰임을 의미합니다. 각 리뷰의 평균 길이를 계산해봅시다.\\nprint('훈련용 리뷰의 평규 길이: {}'.format(np.mean(list(map(len, X_train)), dtype=int)))\\nprint('테스트용 리뷰의 평균 길이: {}'.format(np.mean(list(map(len, X_test)), dtype=int)))\\n훈련용 리뷰의 평규 길이: 238\\n테스트용 리뷰의 평균 길이: 230\\n훈련용 리뷰와 테스트용 리뷰의 평균 길이가 각각 238, 230입니다. 평균보다는 큰 수치인 400으로 패딩합니다.\\nmax_len = 400\\nX_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)\\nprint('X_train의 크기(shape) :', X_train.shape)\", \"X_test = pad_sequences(X_test, maxlen=max_len)\\nprint('X_train의 크기(shape) :', X_train.shape)\\nprint('X_test의 크기(shape) :', X_test.shape)\\nX_train shape : (25000, 400)\\nX_test shape : (25000, 400)\"]\n",
      "['모델의 입력으로 사용하기 위한 모든 전처리를 마쳤습니다. 임베딩 벡터를 평균으로 사용하는 모델을 설계해봅시다. GlobalAveragePooling1D()은 입력으로 들어오는 모든 벡터들의 평균을 구하는 역할을 합니다. Embedding() 다음에 GlobalAveragePooling1D()을 추가하면 해당 문장의 모든 단어 벡터들의 평균 벡터를 구합니다.\\n이진 분류를 수행해야 하므로 그 후에는 시그모이드 함수를 활성화 함수로 사용하는 뉴런 1개를 배치합니다. 훈련 데이터의 20%를 검증 데이터로 사용하고 총 10 에포크 학습합니다.\\nfrom tensorflow.keras.models import Sequential, load_model\\nfrom tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D', \"from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nembedding_dim = 64\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\n# 모든 단어 벡터의 평균을 구한다.\\nmodel.add(GlobalAveragePooling1D())\\nmodel.add(Dense(1, activation='sigmoid'))\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\", \"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('embedding_average_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\\nmodel.fit(X_train, y_train, batch_size=32, epochs=10, callbacks=[es, mc], validation_split=0.2)\\n학습이 끝난 후에 테스트 데이터에 대해서 평가합니다.\\nloaded_model = load_model('embedding_average_model.h5')\", '학습이 끝난 후에 테스트 데이터에 대해서 평가합니다.\\nloaded_model = load_model(\\'embedding_average_model.h5\\')\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\\n테스트 정확도: 0.8876\\n별 다른 신경망을 추가하지 않고 단어 벡터의 평균만으로도 88.76%라는 준수한 정확도를 얻어냅니다.\\n==================================================\\n--- 09-13 Doc2Vec으로 공시 사업보고서 유사도 계산하기 ---\\n```', \"==================================================\\n--- 09-13 Doc2Vec으로 공시 사업보고서 유사도 계산하기 ---\\n```\\n[('네오위즈', 0.5055375099182129), ('NAVER', 0.4846588373184204), ('네오위즈홀딩스', 0.47819197177886963), ('퓨쳐스트림네트웍스', 0.4654642939567566), ('신풍제약우', 0.46335992217063904), ('LG생활건강우', 0.4604458212852478), ('금호석유우', 0.4568769931793213), ('컴투스', 0.4565160274505615), ('코리아써키트2우B', 0.45594915747642517), ('세방우', 0.4553225636482239)]\", '```Word2Vec은 단어를 임베딩하는 워드 임베딩 알고리즘이었습니다. Doc2Vec은 Word2Vec을 변형하여 문서의 임베딩을 얻을 수 있도록 한 알고리즘입니다. 논문 제목과 논문의 링크는 아래와 같습니다.\\n논문 제목 : Distributed Representations of Sentences and Documents\\n논문 링크 : https://arxiv.org/abs/1405.4053\\nWord2Vec과 마찬가지로 파이썬 머신 러닝 패키지인 Gensim을 통해서 쉽게 사용할 수 있습니다. 이번 실습에서는 저자가 수집해놓은 전자공시시스템(Dart)에 올라와있는 각 회사의 사업보고서를 Doc2Vec을 통해서 학습시키고, 특정 회사와 사업 보고서가 유사한 회사들을 찾아보겠습니다.']\n",
      "['해당 실습은 형태소 분석기 Mecab의 원활한 설치를 위해서 구글의 Colab에서 진행했다고 가정합니다. 여러분들의 컴퓨터에 Mecab을 설치하였거나, 다른 형태소 분석기를 사용한다면 Colab에서 하지 않더라도 상관없습니다.\\n!pip install --upgrade --no-cache-dir gdown\\n# dart.csv 파일 다운로드\\n!gdown https://drive.google.com/uc?id=1XS0UlE8gNNTRjnL6e64sMacOhtVERIqL\\n# 형태소 분석기 Mecab 설치\\n!pip install konlpy\\n!pip install mecab-python\\n!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\\nimport pandas as pd\\nfrom konlpy.tag import Mecab', \"import pandas as pd\\nfrom konlpy.tag import Mecab\\nfrom gensim.models.doc2vec import TaggedDocument\\nfrom tqdm import tqdm\\ndart.csv 파일을 데이터프레임으로 로드합니다. 그리고 결측값을 가진 샘플을 제거해줍니다.\\ndf = pd.read_csv('/content/dart.csv',  sep=',')\\ndf = df.dropna()\\ndf\\n[이미지: ]\\n총 2,295개의 행과 4개의 열로 구성되어져 있습니다. 첫번째 열은 종목 번호에 해당하는 code 열, 두번째 열은 해당 종목이 KOSPI인지 KOSDAQ인지를 알려주는 market 열, 세번째 열은 회사명에 해당하는 name 열, 그리고 네번째 business 열은 우리가 학습할 사업 보고서에 해당됩니다.\", \"이제 business 열에 대해서 형태소 분석을 수행해봅시다. 이와 동시에 Doc2Vec 학습을 위해서 필요한 형식으로 데이터의 형식을 변환합니다. Doc2Vec 학습을 위해서는 해당 문서의 '제목'과 단어 토큰화가 된 상태의 해당 문서의 '본문' 두 가지가 필요합니다.\\nTaggedDocument의 tags에 해당 문서의 '제목'을, 그리고 words에 해당 문서의 '본문'에 해당하는 단어 토큰화 결과를 저장하고, 이 결과를 원소로 하는 파이썬 리스트인 tagged_corpus_list를 만듭니다.\\nmecab = Mecab()\\ntagged_corpus_list = []\\nfor index, row in tqdm(df.iterrows(), total=len(df)):\\ntext = row['business']\\ntag = row['name']\\ntagged_corpus_list.append(TaggedDocument(tags=[tag], words=mecab.morphs(text)))\", \"tag = row['name']\\ntagged_corpus_list.append(TaggedDocument(tags=[tag], words=mecab.morphs(text)))\\nprint('문서의 수 :', len(tagged_corpus_list))\\n문서의 수 : 2295\\n첫번째 문서의 전처리 결과를 확인해봅시다.\\ntagged_corpus_list[0]\", 'TaggedDocument(words=[\\'II\\', \\'.\\', \\'사업\\', \\'의\\', \\'내용\\', \\'1\\', \\'.\\', \\'사업\\', \\'의\\', \\'개요\\', \\'가\\', \\'.\\', \\'일반\\', \\'적\\', \\'인\\', \\'사항\\', \\'기업\\', \\'회계\\', \\'기준\\', \\'서\\', \\'제\\', \\'1110\\', \\'호\\', \\'\"\\', \\'연결\\', \\'재무제표\\', \\'\"\\', \\'의\\', \\'의하\\', \\'여\\', \\'2018\\', \\'년\\', \\'12\\', \\'월\\', \\'17\\', \\'일\\', \\'에\\', \\'설립\\', \\'한\\', \\'동화\\', \\'크립톤\\', \\'기업가\\', \\'정신\\', \\'제일\\', \\'호\\', \\'창업\\', \\'벤처\\', \\'전문\\', \\'사모\\', \\'투자\\', \\'합자회사\\', \\'를\\', \\'종속\\', \\'회사\\', \\'에\\', \\'편입\\', \\'하\\', \\'였\\', \\'습니다\\', ... 중략 ... \\'대기\\', \\'관리\\', \\'권\\', \\'역\\', \\'의\\', \\'대기\\', \\'환경\\', \\'개선\\', \\'에\\', \\'관한\\', \\'특별법\\', \\'을\\', \\'준\\', \\'수\\', \\'하\\', \\'고\\', \\'있\\', \\'습니다\\', \\'.\\'], tags=[\\'동화약품\\'])', 'TaggedDocument 안 words에는 토큰화 된 사업 보고서, tags에는 해당 문서의 제목이 저장되어져 있습니다.']\n",
      "[\"이제 모델을 학습합니다. 사업 보고서가 꽤 긴 문서인데다 토큰화 외에 불용어 제거 등 별도 추가 전처리를 진행하지 않았기 때문에 저자가 Colab에서 수행했을 때, 소요 시간이 25분 걸렸습니다.\\nfrom gensim.models import doc2vec\\nmodel = doc2vec.Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.025, workers=8, window=8)\\n# Vocabulary 빌드\\nmodel.build_vocab(tagged_corpus_list)\\n# Doc2Vec 학습\\nmodel.train(tagged_corpus_list, total_examples=model.corpus_count, epochs=20)\\n# 모델 저장\\nmodel.save('dart.doc2vec')\\n코드를 다 수행하고나면 3개의 파일이 생깁니다.\\ndart.doc2vec\\ndart.doc2vec.syn1neg.npy\", \"# 모델 저장\\nmodel.save('dart.doc2vec')\\n코드를 다 수행하고나면 3개의 파일이 생깁니다.\\ndart.doc2vec\\ndart.doc2vec.syn1neg.npy\\ndart.doc2vec.wv.vectors.npy\\n이제 모델을 테스트해봅시다. 회사 동화약품과 사업 보고서가 유사한 회사들은 어디일까요?\\nsimilar_doc = model.dv.most_similar('동화약품')\\nprint(similar_doc)\", \"similar_doc = model.dv.most_similar('동화약품')\\nprint(similar_doc)\\n[('종근당', 0.5310906171798706), ('삼일제약', 0.5263979434967041), ('일양약품', 0.5260423421859741), ('영진약품', 0.5254894495010376), ('제일약품', 0.5089458227157593), ('유한양행', 0.5015101432800293), ('국제약품', 0.4985279440879822), ('삼아제약', 0.49677950143814087), ('동아에스티', 0.49451446533203125), ('대웅제약', 0.48559868335723877)]\\n이 외에 다른 테스트에서도 관련 업종의 회사들이 나오는 것을 확인할 수 있었습니다.\\nsimilar_doc = model.dv.most_similar('하이트진로')\\nprint(similar_doc)\", \"similar_doc = model.dv.most_similar('하이트진로')\\nprint(similar_doc)\\n[('하이트진로홀딩스', 0.8621307611465454), ('무학', 0.5214779376983643), ('보해양조', 0.5100635290145874), ('국순당', 0.48447638750076294), ('롯데칠성', 0.4617755115032196), ('금비', 0.4156178832054138), ('삼양패키징', 0.40965551137924194), ('삼광글라스', 0.40689679980278015), ('경방', 0.40154141187667847), ('오뚜기', 0.3958606719970703)]\\nsimilar_doc = model.dv.most_similar('LG이노텍')\\nprint(similar_doc)\", \"similar_doc = model.dv.most_similar('LG이노텍')\\nprint(similar_doc)\\n[('LG전자', 0.533338725566864), ('LG', 0.523799479007721), ('삼성전기', 0.45796477794647217), ('LG디스플레이', 0.4485859274864197), ('서울반도체', 0.42762115597724915), ('루멘스', 0.42333459854125977), ('삼성SDI', 0.4111291170120239), ('큐엠씨', 0.409035325050354), ('서울바이오시스', 0.4087420105934143), ('삼성공조', 0.4040142595767975)]\\nsimilar_doc = model.dv.most_similar('메리츠화재')\\nprint(similar_doc)\", \"similar_doc = model.dv.most_similar('메리츠화재')\\nprint(similar_doc)\\n[('메리츠금융지주', 0.7080470323562622), ('한화손해보험', 0.69782555103302), ('롯데손해보험', 0.6945951581001282), ('DB손해보험', 0.6699072122573853), ('한화생명', 0.665973961353302), ('흥국화재', 0.6471891403198242), ('현대해상', 0.6267702579498291), ('코리안리', 0.5982924699783325), ('삼성화재', 0.5873793959617615), ('동양생명', 0.5722818970680237)]\\nsimilar_doc = model.dv.most_similar('카카오')\\nprint(similar_doc)\", \"similar_doc = model.dv.most_similar('카카오')\\nprint(similar_doc)\\n[('네오위즈', 0.5055375099182129), ('NAVER', 0.4846588373184204), ('네오위즈홀딩스', 0.47819197177886963), ('퓨쳐스트림네트웍스', 0.4654642939567566), ('신풍제약우', 0.46335992217063904), ('LG생활건강우', 0.4604458212852478), ('금호석유우', 0.4568769931793213), ('컴투스', 0.4565160274505615), ('코리아써키트2우B', 0.45594915747642517), ('세방우', 0.4553225636482239)]\\n==================================================\\n--- 09-14 실전! 한국어 위키피디아로 Word2Vec 학습하기 ---\\n```\", \"==================================================\\n--- 09-14 실전! 한국어 위키피디아로 Word2Vec 학습하기 ---\\n```\\n[('집적회로', 0.7714468836784363), ('연료전지', 0.7699108719825745), ('전자', 0.7606919407844543), ('웨이퍼', 0.745188295841217), ('실리콘', 0.743209958076477), ('트랜지스터', 0.7398351430892944), ('PCB', 0.7275883555412292), ('TSMC', 0.7156406044960022), ('가속기', 0.6962155699729919), ('광전자', 0.6957612037658691)]\\n```아래의 실습은 구글의 Colab을 사용한다고 가정하고 작성되었습니다. 다른 환경에서 실습하셔도 상관은 없지만, 형태소 분석기 Mecab은 다른 방법으로 설치하셔야 합니다.\"]\n",
      "['위키피디아로부터 데이터를 파싱하기 위한 파이썬 패키지인 wikiextractor를 설치합니다.\\npip install wikiextractor\\n위키피디아 데이터를 다운로드 한 후에 전처리에서 사용할 형태소 분석기인 Mecab을 설치합니다.\\n# Colab에 Mecab 설치\\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\\n%cd Mecab-ko-for-Google-Colab\\n!bash install_mecab-ko_on_colab190912.sh\\n위키피디아 덤프(위키피디아 데이터)를 다운로드합니다.\\n!wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2\\n위키익스트랙터를 사용하여 위키피디아 덤프를 파싱합니다.', \"위키익스트랙터를 사용하여 위키피디아 덤프를 파싱합니다.\\n!python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2\\n현재 경로에 있는 디렉토리와 파일들의 리스트를 받아옵니다.\\n%ls\\nimages/                                    LICENSE\\ninstall_mecab-ko_on_colab190912.sh         README.md\\ninstall_mecab-ko_on_colab_light_210108.sh  text/\\nkowiki-latest-pages-articles.xml.bz2\\ntext라는 디렉토리 안에는 또 어떤 디렉토리들이 있는지 파이썬을 사용하여 확인해봅시다.\\nimport os\\nimport re\\nos.listdir('text')\\n['AG', 'AI', 'AH', 'AC', 'AE', 'AB', 'AA', 'AD', 'AF']\", \"import os\\nimport re\\nos.listdir('text')\\n['AG', 'AI', 'AH', 'AC', 'AE', 'AB', 'AA', 'AD', 'AF']\\nAA라는 디렉토리의 파일들을 확인해봅시다.\\n%ls text/AA\\nwiki_00  wiki_12  wiki_24  wiki_36  wiki_48  wiki_60  wiki_72  wiki_84  wiki_96\\nwiki_01  wiki_13  wiki_25  wiki_37  wiki_49  wiki_61  wiki_73  wiki_85  wiki_97\\nwiki_02  wiki_14  wiki_26  wiki_38  wiki_50  wiki_62  wiki_74  wiki_86  wiki_98\\nwiki_03  wiki_15  wiki_27  wiki_39  wiki_51  wiki_63  wiki_75  wiki_87  wiki_99\", 'wiki_03  wiki_15  wiki_27  wiki_39  wiki_51  wiki_63  wiki_75  wiki_87  wiki_99\\nwiki_04  wiki_16  wiki_28  wiki_40  wiki_52  wiki_64  wiki_76  wiki_88\\nwiki_05  wiki_17  wiki_29  wiki_41  wiki_53  wiki_65  wiki_77  wiki_89\\nwiki_06  wiki_18  wiki_30  wiki_42  wiki_54  wiki_66  wiki_78  wiki_90\\nwiki_07  wiki_19  wiki_31  wiki_43  wiki_55  wiki_67  wiki_79  wiki_91\\nwiki_08  wiki_20  wiki_32  wiki_44  wiki_56  wiki_68  wiki_80  wiki_92', \"wiki_08  wiki_20  wiki_32  wiki_44  wiki_56  wiki_68  wiki_80  wiki_92\\nwiki_09  wiki_21  wiki_33  wiki_45  wiki_57  wiki_69  wiki_81  wiki_93\\nwiki_10  wiki_22  wiki_34  wiki_46  wiki_58  wiki_70  wiki_82  wiki_94\\nwiki_11  wiki_23  wiki_35  wiki_47  wiki_59  wiki_71  wiki_83  wiki_95\\n텍스트 파일로 변환된 위키피디아 한국어 덤프는 총 6개의 디렉토리로 구성되어져 있습니다. AA ~ AF의 디렉토리로 각 디렉토리 내에는 'wiki_00 ~ wiki_약 90내외의 숫자'의 파일들이 들어있습니다. 다시 말해 각 디렉토리에는 약 90여개의 파일들이 들어있습니다. 각 파일들을 열어보면 이와 같은 구성이 반복되고 있습니다.\", '<doc id=\"문서 번호\" url=\"실제 위키피디아 문서 주소\" title=\"문서 제목\">\\n내용\\n</doc>\\n예를 들어서 AA 디렉토리의 wiki_00 파일을 읽어보면, 지미 카터에 대한 내용이 나옵니다.\\n<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\\n지미 카터\\n제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령(1977년 ~ 1981년)이다.\\n지미 카터는 조지아 주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대\\n위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다.\\n... 이하 중략...\\n</doc>', '위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다.\\n... 이하 중략...\\n</doc>\\n이제 이 6개 AA ~ AF 디렉토리 안의 wiki 숫자 형태의 수많은 파일들을 하나로 통합하는 과정을 진행해야 합니다. AA ~ AF 디렉토리 안의 모든 파일들의 경로를 파이썬의 리스트 형태로 저장합니다.\\ndef list_wiki(dirname):\\nfilepaths = []\\nfilenames = os.listdir(dirname)\\nfor filename in filenames:\\nfilepath = os.path.join(dirname, filename)\\nif os.path.isdir(filepath):\\n# 재귀 함수\\nfilepaths.extend(list_wiki(filepath))\\nelse:\\nfind = re.findall(r\"wiki_[0-9][0-9]\", filepath)\\nif 0 < len(find):\\nfilepaths.append(filepath)', 'else:\\nfind = re.findall(r\"wiki_[0-9][0-9]\", filepath)\\nif 0 < len(find):\\nfilepaths.append(filepath)\\nreturn sorted(filepaths)\\nfilepaths = list_wiki(\\'text\\')\\n총 파일의 개수를 확인해봅시다.\\nlen(filepaths)\\n850\\n총 파일의 개수는 850개입니다. 이제 output_file.txt라는 파일에 850개의 파일을 전부 하나로 합칩니다.\\nwith open(\"output_file.txt\", \"w\") as outfile:\\nfor filename in filepaths:\\nwith open(filename) as infile:\\ncontents = infile.read()\\noutfile.write(contents)\\n파일을 읽고 10줄만 출력해보겠습니다.\\nf = open(\\'output_file.txt\\', encoding=\"utf8\")\\ni = 0\\nwhile True:', '파일을 읽고 10줄만 출력해보겠습니다.\\nf = open(\\'output_file.txt\\', encoding=\"utf8\")\\ni = 0\\nwhile True:\\nline = f.readline()\\nif line != \\'\\\\n\\':\\ni = i+1\\nprint(\"%d번째 줄 :\"%i + line)\\nif i==10:\\nbreak\\nf.close()\\n1번째 줄 :<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\\n2번째 줄 :지미 카터\\n3번째 줄 :제임스 얼 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39대 대통령 (1977년 ~ 1981년)이다.\\n4번째 줄 :생애.\\n5번째 줄 :어린 시절.\\n6번째 줄 :지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.', '4번째 줄 :생애.\\n5번째 줄 :어린 시절.\\n6번째 줄 :지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\\n7번째 줄 :조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\\n8번째 줄 :정계 입문.\\n9번째 줄 :1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주지사 선거에 낙선하지만, 1970년 조지아 주지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n10번째 줄 :대통령 재임.']\n",
      "['from tqdm import tqdm\\nfrom konlpy.tag import Mecab\\n형태소 분석기 Mecab을 사용하여 토큰화를 진행해보겠습니다.\\nmecab = Mecab()\\n우선 output_file에는 총 몇 줄이 있는지 확인합니다.\\nf = open(\\'output_file.txt\\', encoding=\"utf8\")\\nlines = f.read().splitlines()\\nprint(len(lines))\\n9718793\\n9,718,793개의 줄이 존재합니다. 상위 10개만 출력해봅시다.\\nlines[:10]\\n[\\'<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\\',\\n\\'지미 카터\\',\\n\\'\\',\\n\\'제임스 얼 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39대 대통령 (1977년 ~ 1981년)이다.\\',\\n\\'생애.\\',\\n\\'어린 시절.\\',', '\\'\\',\\n\\'제임스 얼 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39대 대통령 (1977년 ~ 1981년)이다.\\',\\n\\'생애.\\',\\n\\'어린 시절.\\',\\n\\'지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\\',\\n\\'조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\\',\\n\\'정계 입문.\\',\\n\\'1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주지사 선거에 낙선하지만, 1970년 조지아 주지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\']', \"두번째 줄을 보면 아무런 단어도 들어있지 않은 ''와 같은 줄도 존재합니다. 해당 문자열은 형태소 분석에서 제외하도록 하고 형태소 분석을 수행해봅시다.\\nresult = []\\nfor line in tqdm(lines):\\n# 빈 문자열이 아닌 경우에만 수행\\nif line:\\nresult.append(mecab.morphs(line))\\n100%|██████████| 9718793/9718793 [15:27<00:00, 10478.61it/s]\\n빈 문자열은 제외하고 형태소 분석을 진행했습니다. 이제 몇 개의 줄. 즉, 몇 개의 문장이 존재하는지 확인해봅시다.\\nlen(result)\\n6559314\\n6,559,314개로 문장의 수가 줄었습니다.\"]\n",
      "['형태소 분석을 통해서 토큰화가 진행된 상태이므로 Word2Vec을 학습합니다.\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(result, size=100, window=5, min_count=5, workers=4, sg=0)\\nmodel_result1 = model.wv.most_similar(\"대한민국\")\\nprint(model_result1)', 'model_result1 = model.wv.most_similar(\"대한민국\")\\nprint(model_result1)\\n[(\\'한국\\', 0.7382678389549255), (\\'미국\\', 0.6731516122817993), (\\'일본\\', 0.6541135907173157), (\\'부산\\', 0.5798133611679077), (\\'홍콩\\', 0.5752249360084534), (\\'서울\\', 0.5541036128997803), (\\'오스트레일리아\\', 0.5531408786773682), (\\'태국\\', 0.548468828201294), (\\'경상남도\\', 0.5462549924850464), (\\'제주특별자치도\\', 0.5385439395904541)]\\nmodel_result2 = model.wv.most_similar(\"어벤져스\")\\nprint(model_result2)', 'model_result2 = model.wv.most_similar(\"어벤져스\")\\nprint(model_result2)\\n[(\\'스파이더맨\\', 0.80271977186203), (\\'트랜스포머\\', 0.773989200592041), (\\'아이언맨\\', 0.7648921012878418), (\\'스타트렉\\', 0.7645636796951294), (\\'어벤저스\\', 0.7626765966415405), (\\'엑스맨\\', 0.7586475610733032), (\\'《》,\\', 0.7560415267944336), (\\'트와일라잇\\', 0.7518032789230347), (\\'퍼니셔\\', 0.7391209602355957), (\\'테일즈\\', 0.7386105060577393)]\\nmodel_result3 = model.wv.most_similar(\"반도체\")\\nprint(model_result3)', 'model_result3 = model.wv.most_similar(\"반도체\")\\nprint(model_result3)\\n[(\\'집적회로\\', 0.7714468836784363), (\\'연료전지\\', 0.7699108719825745), (\\'전자\\', 0.7606919407844543), (\\'웨이퍼\\', 0.745188295841217), (\\'실리콘\\', 0.743209958076477), (\\'트랜지스터\\', 0.7398351430892944), (\\'PCB\\', 0.7275883555412292), (\\'TSMC\\', 0.7156406044960022), (\\'가속기\\', 0.6962155699729919), (\\'광전자\\', 0.6957612037658691)]\\n==================================================\\n--- 10. RNN을 이용한 텍스트 분류(Text Classification) ---', '==================================================\\n--- 10. RNN을 이용한 텍스트 분류(Text Classification) ---\\n텍스트 분류(Text Classification)는 텍스트를 입력으로 받아 텍스트가 어떤 종류의 범주에 속하는지를 구분하는 작업을 말합니다. 가령, 스팸 메일 분류를 한다고 한다면, 스팸 메일 분류는 일반 메일과 스팸 메일이라는 두 개의 범주를 정해놓고 입력받은 메일 본문을 두 개의 메일 종류 중 하나로 분류하는 작업이 될 것입니다.\\n텍스트 분류에서 분류해야할 범주가 두 가지라면 이진 분류(Binary Classification) 라고 하며, 세 가지 이상이라면 다중 클래스 분류(Multi-Class Classification) 라고 합니다. 일반 메일과 스팸 메일 두 개의 범주를 가진 스팸 메일 분류는 이진 분류에 해당됩니다.', \"스팸 메일 분류 외에도 영화 리뷰와 같은 텍스트를 입력 받아서 이 리뷰가 긍정 리뷰인지 부정 리뷰인지를 분류하는 '감성 분석', 입력 받은 텍스트로부터 사용자의 의도를 질문, 명령, 거절 등과 같은 의도를 분류하는 '의도 분석' 과 같은 분류 문제들이 있습니다.\\n이번 챕터에서는 RNN 계열의 신경망 바닐라 RNN, LSTM, GRU를 사용하여 텍스트 분류를 수행해보고, 딥 러닝 코드에 대한 이해도를 높입니다.\\n==================================================\\n--- 10-01 케라스를 이용한 텍스트 분류 개요(Text Classification using Keras) ---\\n```\\nmodel.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))\\n```이번 챕터에서는 인공 신경망을 이용하여 텍스트 분류를 수행합니다.\"]\n",
      "['앞으로 진행하는 텍스트 분류 실습은 지도 학습(Supervised Learning)에 속합니다. 지도 학습의 훈련 데이터는 레이블이라는 이름의 미리 정답이 적혀있는 데이터로 구성되어 있습니다. 비유하면, 기계는 정답이 적혀져 있는 문제지를 열심히 공부하고, 향후에 정답이 없는 문제에 대해서도 정답을 예측해서 대답하게 되는 메커니즘입니다.\\n예를 들어 스팸 메일 분류기의 훈련 데이터같은 경우에는 메일의 내용과 해당 메일이 정상 메일인지, 스팸 메일인지 적혀있는 레이블로 구성되어져 있습니다. 아래와 같은 형식의 메일 샘플이 약 20,000개 있다고 가정해봅시다.\\n텍스트(메일의 내용)\\n레이블(스팸 여부)\\n당신에게 드리는 마지막 혜택! ...\\n스팸 메일\\n내일 뵐 수 있을지 확인 부탁...\\n정상 메일\\n쉿! 혼자 보세요...\\n스팸 메일\\n언제까지 답장 가능할...\\n정상 메일\\n...\\n...\\n(광고) 멋있어질 수 있는...\\n스팸 메일', '내일 뵐 수 있을지 확인 부탁...\\n정상 메일\\n쉿! 혼자 보세요...\\n스팸 메일\\n언제까지 답장 가능할...\\n정상 메일\\n...\\n...\\n(광고) 멋있어질 수 있는...\\n스팸 메일\\n20,000개의 메일 샘플을 가진 이 데이터는 메일의 내용을 담고 있는 텍스트 데이터와 이 데이터가 스팸 메일인지 아닌지가 적혀있는 레이블. 두 가지 열로 이루어져있습니다. 기계가 이 20,000개의 메일 샘플 데이터를 학습했다고 해봅시다. 만약 데이터에 문제가 없고, 모델 또한 잘 설계되어져 있다면 학습이 다 된 이 모델은 훈련 데이터에 존재하지 않았던 어떤 메일 텍스트가 주어지더라도 정확한 레이블을 예측하게 됩니다.\\n[이미지: ]']\n",
      "['위에서는 20,000개의 메일 샘플을 전부 훈련에 사용한다고 했지만 사실 갖고있는 전체 데이터를 전부 훈련에 사용하는 것 보다는 테스트용은 일부 남겨놓는 것으로 바람직합니다. 예를 들어 20,000개의 샘플 중 18,000개의 샘플은 훈련용으로 사용하고, 2,000개의 샘플은 테스트용으로 보류한 채 훈련 과정에서는 사용하지 않을 수 있습니다. 그 후 훈련이 끝나면, 보류해두었던 2,000개의 테스트용 샘플로 모델에게 레이블은 보여주지 않고, 모델에게 레이블을 맞춰보라고 요구한 뒤, 채점을 통해 정확도(accuracy)를 계산할 수 있습니다.', '이번 챕터에서는 갖고 있는 데이터 중 분류 대상인 텍스트 데이터의 열을 X, 레이블 데이터의 열을 y라고 명명합니다. 그리고 이를 훈련 데이터(X_train, y_train)와 테스트 데이터(X_test, y_test)로 분리합니다. 모델은 X_train과 y_train을 학습하고, X_test에 대해서 레이블을 예측하게 됩니다. 그리고 모델이 예측한 레이블과 y_test를 비교해서 정확도를 계산합니다.']\n",
      "['케라스의 Embedding()은 단어 각각에 대해 정수로 변환된 입력에 대해서 임베딩 작업을 수행합니다.\\n단어 각각에 숫자 맵핑, 정수를 부여하는 방법으로는 정수 인코딩 챕터에서와 같이 단어를 빈도수 순대로 정렬하고 순차적으로 정수를 부여하는 방법이 있습니다. 로이터 뉴스 분류하기와 IMDB 리뷰 감성 분류하기 실습에서도 이 방법을 사용하였으며, 해당 실습에서 사용할 데이터들은 이미 이 작업이 끝난 상태입니다.\\n등장 빈도 순으로 단어를 정렬하여 정수를 부여하였을 때의 장점은 등장 빈도수가 적은 단어의 제거입니다. 예를 들어서 25,000개의 단어가 있다고 가정하고, 해당 단어를 등장 빈도수 순가 높은 순서로 1부터 25,000까지 정수를 부여했다고 해보겠습니다. 이렇게 되면 등장 빈도 순으로 등수가 부여된 것과 다름없으므로 전처리 작업에서 1,000보다 큰 정수로 맵핑된 단어들을 제거한다면 등장 빈도 상위 1,000개의 단어만 남길 수 있습니다.']\n",
      "['model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))\\nRNN 코드의 인자는 다음과 같습니다.\\nhidden_units = RNN의 출력의 크기 = 은닉 상태의 크기.\\ntimesteps = 시점의 수 = 각 문서에서의 단어 수.\\ninput_dim = 입력의 크기 = 임베딩 벡터의 차원.']\n",
      "['[이미지: ]\\n텍스트 분류는 RNN의 다 대 일(many-to-one) 문제에 속합니다. 즉, 텍스트 분류는 모든 시점(time step)에 대해서 입력을 받지만 최종 시점의 RNN 셀만이 은닉 상태를 출력하고, 이것이 출력층으로 가서 활성화 함수를 통해 정답을 고르는 문제가 됩니다.\\n이때 두 개의 선택지 중에서 정답를 고르는 이진 분류(Binary Classification) 문제라고 하며, 세 개 이상의 선택지 중에서 정답을 고르는 다중 클래스 분류(Multi-Class Classification) 문제라고 합니다. 이 두 문제에서는 각각 문제에 맞는 활성화 함수와 손실 함수를 사용해야 합니다.', \"이진 분류의 문제의 경우 출력층의 활성화 함수로 시그모이드 함수를, 손실 함수로 binary_crossentropy를 사용합니다. 반면, 다중 클래스 문제라면 출력층의 활성화 함수로 소프트맥스 함수를, 손실 함수로 categorical_crossentropy를 사용합니다. 또한, 다중 클래스 분류 문제의 경우에는 클래스가 N개라면 출력층에 해당되는 밀집층(dense layer)의 크기는 N으로 합니다. 다르게 표현하면 출력층 뉴런의 수는 N개입니다.\\n이번 챕터에서 스팸 메일 분류하기나 감성 분류하기 실습들이 이진 분류 문제에 해당되며, 로이터 뉴스 분류하기 문제가 다중 클래스 분류 문제에 해당됩니다.\\n==================================================\\n--- 10-02 스팸 메일 분류하기(Spam Detection) ---\\n```\\nepochs = range(1, len(history.history['acc']) + 1)\", \"--- 10-02 스팸 메일 분류하기(Spam Detection) ---\\n```\\nepochs = range(1, len(history.history['acc']) + 1)\\nplt.plot(epochs, history.history['loss'])\\nplt.plot(epochs, history.history['val_loss'])\\nplt.title('model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'val'], loc='upper left')\\nplt.show()\\n```캐글에서 제공하는 스팸 메일 데이터를 학습시켜 스팸 메일 분류기를 구현해보겠습니다.\"]\n",
      "['다운로드 링크 : https://www.kaggle.com/uciml/sms-spam-collection-dataset\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport urllib.request\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\n다운로드 받은 spam.csv 파일을 데이터프레임으로 로드하고 총 샘플의 수를 확인해봅시다.', '다운로드 받은 spam.csv 파일을 데이터프레임으로 로드하고 총 샘플의 수를 확인해봅시다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/spam.csv\", filename=\"spam.csv\")\\ndata = pd.read_csv(\\'spam.csv\\', encoding=\\'latin1\\')\\nprint(\\'총 샘플의 수 :\\',len(data))\\n총 샘플의 수 : 5572\\n총 5,572개의 샘플이 존재합니다. 상위 5개의 샘플만 출력해봅시다.\\ndata[:5]\\n[이미지: ]', \"총 샘플의 수 : 5572\\n총 5,572개의 샘플이 존재합니다. 상위 5개의 샘플만 출력해봅시다.\\ndata[:5]\\n[이미지: ]\\n스팸 메일 데이터 중에서 5개의 행만 출력해보았습니다. 이 데이터에는 총 5개의 열이 있는데, 여기서 Unnamed라는 이름의 3개의 열은 텍스트 분류를 할 때 불필요한 열입니다. v1열은 해당 메일이 스팸인지 아닌지를 나타내는 레이블에 해당되는 열입니다. ham은 정상 메일을 의미하고, spam은 스팸 메일을 의미합니다. v2열은 메일의 본문을 담고있습니다.\\n레이블과 메일 내용이 담긴 v1열과 v2열만 필요하므로, Unnamed: 2, Unnamed: 3, Unnamed: 4 열은 삭제합니다. 또한, v1열에 있는 ham과 spam 레이블을 각각 숫자 0과 1로 바꾸겠습니다. 다시 data에서 5개의 행만 출력해보겠습니다.\\ndel data['Unnamed: 2']\\ndel data['Unnamed: 3']\\ndel data['Unnamed: 4']\", \"del data['Unnamed: 2']\\ndel data['Unnamed: 3']\\ndel data['Unnamed: 4']\\ndata['v1'] = data['v1'].replace(['ham','spam'],[0,1])\\ndata[:5]\\n[이미지: ]\\n불필요한 열이 제거되고 v1열의 값이 숫자로 변환된 것을 확인할 수 있습니다. 해당 데이터프레임의 정보를 확인해보겠습니다.\\ndata.info()\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 5572 entries, 0 to 5571\\nData columns (total 2 columns):\\nv1    5572 non-null int64\\nv2    5572 non-null object\\ndtypes: int64(1), object(1)\\nmemory usage: 87.1+ KB\", \"v2    5572 non-null object\\ndtypes: int64(1), object(1)\\nmemory usage: 87.1+ KB\\nv1열은 정수형, v2열은 문자열 데이터를 갖고있습니다. Null 값을 가진 샘플이 있는지 isnull().values.any()로 확인합니다.\\nprint('결측값 여부 :',data.isnull().values.any())\\n결측값 여부 : False\\nFalse는 별도의 Null 값은 없음을 의미합니다. 초기 데이터에 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'열에는 NaN이 있었는데 해당 상태에서 수행하는 isnull().values.any()는 True를 리턴합니다. Null 값이 없다면 데이터에 중복이 있는지 확인해보겠습니다.\\nprint('v2열의 유니크한 값 :',data['v2'].nunique())\\nv2열의 유니크한 값 : 5169\", \"print('v2열의 유니크한 값 :',data['v2'].nunique())\\nv2열의 유니크한 값 : 5169\\n총 5,572개의 샘플이 존재하는데 v2열에서 중복을 제거한 샘플의 개수가 5,169개라는 것은 403개의 중복 샘플이 존재한다는 의미입니다. 중복 샘플을 제거하고 전체 샘플 수를 확인합니다.\\n# v2 열에서 중복인 내용이 있다면 중복 제거\\ndata.drop_duplicates(subset=['v2'], inplace=True)\\nprint('총 샘플의 수 :',len(data))\\n총 샘플의 수 : 5169\\n총 샘플의 수가 5,572개에서 5,169개로 줄었습니다. 레이블 값의 분포를 시각화합니다.\\ndata['v1'].value_counts().plot(kind='bar')\\n[이미지: ]\\n레이블이 대부분 0에 편중되어있는데, 이는 스팸 메일 데이터의 대부분의 메일이 정상 메일임을 의미합니다. 정확한 수치를 파악해보겠습니다.\\nprint('정상 메일과 스팸 메일의 개수')\", 'print(\\'정상 메일과 스팸 메일의 개수\\')\\nprint(data.groupby(\\'v1\\').size().reset_index(name=\\'count\\'))\\n정상 메일과 스팸 메일의 개수\\nv1  count\\n0   0   4516\\n1   1    653\\n레이블 0은 총 4,516개가 존재하고 1은 653개가 존재합니다. 이를 %로 환산합니다.\\nprint(f\\'정상 메일의 비율 = {round(data[\"v1\"].value_counts()[0]/len(data) * 100,3)}%\\')\\nprint(f\\'스팸 메일의 비율 = {round(data[\"v1\"].value_counts()[1]/len(data) * 100,3)}%\\')\\n정상 메일의 비율 = 87.367%\\n스팸 메일의 비율 = 12.633%\\nv2열과 v1열을 X데이터와 y데이터라는 X_data, y_data로 저장합니다.\\nX_data = data[\\'v2\\']\\ny_data = data[\\'v1\\']', \"v2열과 v1열을 X데이터와 y데이터라는 X_data, y_data로 저장합니다.\\nX_data = data['v2']\\ny_data = data['v1']\\nprint('메일 본문의 개수: {}'.format(len(X_data)))\\nprint('레이블의 개수: {}'.format(len(y_data)))\\n메일 본문의 개수: 5169\\n레이블의 개수: 5169\\n훈련 데이터와 테스트 데이터를 분리합니다. 주의할 점은 현재 레이블이 굉장히 불균형합니다. 다시 말해 정상 메일 샘플(87%, 4516개)에 비해서 스팸 메일 샘플(12%, 653개)이 굉장히 적습니다. 만약, 훈련 데이터와 테스트 데이터를 분리하는 과정에서 우연히 대부분의 스팸 메일 샘플이 테스트 데이터에 포함되고 훈련 데이터에는 대부분 정상 메일 샘플만 포함되게 된다면 어떻게 될까요? 학습 과정에서 모델은 스팸 메일 샘플을 거의 관측하지 못해서 모델의 성능이 저하될 것입니다.\", \"이렇게 레이블이 불균형한 경우에는 데이터를 나눌 때에도 훈련 데이터와 테스트 데이터에 각 레이블의 분포가 고르게 분포되도록 하는 것이 중요할 수 있습니다. 사이킷 런의 train_test_split에 stratify의 인자로서 레이블 데이터를 기재하면 훈련 데이터와 테스트 데이터를 분리할 때 레이블의 분포가 고르게 분포하도록 합니다. test_size에 0.2를 기재하여 훈련 데이터와 테스트 데이터를 8:2 비율로 분리합니다.\\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0, stratify=y_data)\\n훈련 데이터와 테스트 데이터가 분리되었습니다. 레이블이 고르게 분포되었는지 확인합니다.\\nprint('--------훈련 데이터의 비율-----------')\", \"훈련 데이터와 테스트 데이터가 분리되었습니다. 레이블이 고르게 분포되었는지 확인합니다.\\nprint('--------훈련 데이터의 비율-----------')\\nprint(f'정상 메일 = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')\\nprint(f'스팸 메일 = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')\\n--------훈련 데이터의 비율-----------\\n정상 메일 = 87.376%\\n스팸 메일 = 12.624%\\nprint('--------테스트 데이터의 비율-----------')\\nprint(f'정상 메일 = {round(y_test.value_counts()[0]/len(y_test) * 100,3)}%')\\nprint(f'스팸 메일 = {round(y_test.value_counts()[1]/len(y_test) * 100,3)}%')\", \"print(f'스팸 메일 = {round(y_test.value_counts()[1]/len(y_test) * 100,3)}%')\\n--------테스트 데이터의 비율-----------\\n정상 메일 = 87.331%\\n스팸 메일 = 12.669%\\n훈련 데이터와 테스트 데이터 모두 정상 메일은 87%, 스팸 메일은 12%가 존재합니다. 케라스 토크나이저를 통해 훈련 데이터에 대해서 토큰화와 정수 인코딩 과정을 수행합니다. X_train_encoded에는 X_train의 각 단어들이 맵핑되는 정수로 인코딩되어 저장됩니다. 5개의 메일만 출력해서 확인해보겠습니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(X_train)\\nX_train_encoded = tokenizer.texts_to_sequences(X_train)\\nprint(X_train_encoded[:5])\\n[[102, 1, 210, 230, 3, 17, 39],\", 'print(X_train_encoded[:5])\\n[[102, 1, 210, 230, 3, 17, 39],\\n[1, 59, 8, 427, 17, 5, 137, 2, 2326],\\n[157, 180, 12, 13, 98, 93, 47, 9, 40, 3485, 247, 8, 7, 87, 6, 80, 1312, 5, 3486, 7, 2327, 11, 660, 306, 20, 25, 467, 708, 1028, 203, 129, 193, 800, 2328, 23, 1, 144, 71, 2, 111, 78, 43, 2, 130, 11, 800, 186, 122, 1512],\\n[1, 1154, 13, 104, 292],\\n[222, 622, 857, 540, 623, 22, 23, 83, 10, 47, 6, 257, 32, 6, 26, 64, 936, 407]]\\n각 메일이 정수 인코딩이 되었습니다. 각 정수가 어떤 단어에 부여되었는지 확인해봅시다.', '각 메일이 정수 인코딩이 되었습니다. 각 정수가 어떤 단어에 부여되었는지 확인해봅시다.\\nword_to_index = tokenizer.word_index\\nprint(word_to_index)', '{\\'i\\': 1, \\'to\\': 2, \\'you\\': 3, \\'a\\': 4, \\'the\\': 5, \\'u\\': 6, \\'and\\': 7, \\'in\\': 8, \\'is\\': 9, \\'me\\': 10, \\'my\\': 11, \\'for\\': 12, \\'your\\': 13, \\'it\\': 14, \\'of\\': 15, \\'have\\': 16, \\'on\\': 17, \\'call\\': 18, \\'that\\': 19, \\'are\\': 20, \\'2\\': 21, \\'now\\': 22, \\'so\\': 23, \\'but\\': 24, \\'not\\': 25, \\'can\\': 26, \\'or\\': 27, \"i\\'m\": 28, \\'get\\': 29, \\'at\\': 30, \\'do\\': 31, \\'if\\': 32, \\'be\\': 33, \\'will\\': 34, \\'just\\': 35, \\'with\\': 36, \\'we\\': 37, \\'no\\': 38, \\'this\\': 39, \\'ur\\': 40, \\'up\\': 41, \\'4\\': 42, \\'how\\': 43, \\'gt\\': 44, \\'lt\\': 45, \\'go\\': 46, \\'when\\': 47, \\'from\\': 48,', \"39, 'ur': 40, 'up': 41, '4': 42, 'how': 43, 'gt': 44, 'lt': 45, 'go': 46, 'when': 47, 'from': 48, 'what': 49, 'ok': 50, 'out': 51, 'know': 52, - 이하 생략}\", '무수히 많은 단어가 출력되므로 출력 결과는 중간에 생략했습니다. 위에서 부여된 각 정수는 각 단어의 빈도수가 높을 수록 낮은 정수가 부여됩니다. 다시 말해, 단어 i는 현재 전체 훈련 데이터에서 빈도수가 가장 높은 단어입니다.\\n각 단어에 대한 등장 빈도수는 tokenizer.word_counts.items()를 출력해서 확인할 수 있습니다. 이를 응용하여 빈도수가 낮은 단어들이 훈련 데이터에서 얼만큼의 비중을 차지하는지 확인해볼 수 있습니다. 등장 빈도수가 1회 밖에 되지 않는 단어들이 전체 단어 집합에서 얼만큼의 비율을 차지하며, 전체 훈련 데이터에서 등장 빈도로 얼만큼의 비율을 차지하는지 확인해보겠습니다.\\nthreshold = 2\\ntotal_cnt = len(word_to_index) # 단어의 수\\nrare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\\ntotal_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합', 'rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\\ntotal_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\\nrare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\\n# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\\nfor key, value in tokenizer.word_counts.items():\\ntotal_freq = total_freq + value\\n# 단어의 등장 빈도수가 threshold보다 작으면\\nif(value < threshold):\\nrare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint(\\'등장 빈도가 %s번 이하인 희귀 단어의 수: %s\\'%(threshold - 1, rare_cnt))\\nprint(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)', 'print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\\nprint(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\\n등장 빈도가 1번 이하인 희귀 단어의 수: 4337\\n단어 집합(vocabulary)에서 희귀 단어의 비율: 55.45326684567191\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 6.65745644331875', '단어 집합(vocabulary)에서 희귀 단어의 비율: 55.45326684567191\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 6.65745644331875\\n등장 빈도가 threshold 값인 2회 미만. 즉, 1회 밖에 되지 않는 단어들은 단어 집합에서 무려 절반 이상을 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 6%밖에 되지 않습니다. 만약, 이러한 분석을 통해 등장 빈도가 지나치게 낮은 단어들은 자연어 처리에서 제외하고 싶다면 케라스 토크나이저 선언 시에 단어 집합의 크기를 제한할 수 있습니다. 가령, 아래의 코드로 등장 빈도가 1회인 단어들을 제외할 수 있을 겁니다.\\ntokenizer = Tokenizer(num_words = total_cnt - rare_cnt + 1)', \"tokenizer = Tokenizer(num_words = total_cnt - rare_cnt + 1)\\n하지만 이번 실습에서는 별도로 단어 집합의 크기를 제한하진 않겠습니다. 단어 집합의 크기를 vocab_size에 저장하겠습니다. 패딩을 위한 토큰인 0번 단어를 고려하며 +1을 해서 저장합니다.\\nvocab_size = len(word_to_index) + 1\\nprint('단어 집합의 크기: {}'.format((vocab_size)))\\n단어 집합의 크기: 7822\\n단어 집합의 크기는 7,822입니다. 전체 데이터에서 가장 길이가 긴 메일과 전체 메일 데이터의 길이 분포를 확인합니다.\\nprint('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))\\nprint('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\", 'print(\\'메일의 평균 길이 : %f\\' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\\nplt.hist([len(sample) for sample in X_data], bins=50)\\nplt.xlabel(\\'length of samples\\')\\nplt.ylabel(\\'number of samples\\')\\nplt.show()\\n메일의 최대 길이 : 189\\n메일의 평균 길이 : 15.754534\\n[이미지: ]\\n가장 긴 메일의 길이는 189이며, 전체 데이터의 길이 분포는 대체적으로 약 50이하의 길이를 가집니다.\\nmax_len = 189\\nX_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\\nprint(\"훈련 데이터의 크기(shape):\", X_train_padded.shape)', 'print(\"훈련 데이터의 크기(shape):\", X_train_padded.shape)\\nmaxlen에는 가장 긴 메일의 길이였던 189이라는 숫자를 넣었습니다. 이는 4,135개의 X_train_encoded의 길이를 전부 189로 바꿉니다. 189보다 길이가 짧은 메일 샘플은 전부 숫자 0이 패딩되어 189의 길이를 가집니다.\\n훈련 데이터의 크기(shape): (4135, 189)\\nX_train_encoded 데이터는 4,135 × 189의 크기를 갖게됩니다. 모델을 설계해보겠습니다.']\n",
      "['하이퍼파라미터인 임베딩 벡터의 차원은 32, 은닉 상태의 크기는 32입니다. 모델은 다 대 일 구조의 RNN을 사용합니다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델입니다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용하여 4번의 에포크를 수행합니다.\\n하이퍼파라미터인 배치 크기는 64이며, validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\\nfrom tensorflow.keras.layers import SimpleRNN, Embedding, Dense', \"from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\\nfrom tensorflow.keras.models import Sequential\\nembedding_dim = 32\\nhidden_units = 32\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(SimpleRNN(hidden_units))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\nhistory = model.fit(X_train_padded, y_train, epochs=4, batch_size=64, validation_split=0.2)\\n테스트 데이터에 대해서 정확도를 확인해보겠습니다.\", '테스트 데이터에 대해서 정확도를 확인해보겠습니다.\\nX_test_encoded = tokenizer.texts_to_sequences(X_test)\\nX_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))\\n1034/1034 [==============================] - 0s 166us/step\\n테스트 정확도: 0.9821\\n정확도가 98%가 나왔습니다. 이번 실습에서는 훈련 데이터와 검증 데이터에 대해서 같이 정확도를 확인하면서 훈련하였으므로, 이를 비교하여 그래프로 시각화해보겠습니다.\\nepochs = range(1, len(history.history[\\'acc\\']) + 1)\\nplt.plot(epochs, history.history[\\'loss\\'])', \"epochs = range(1, len(history.history['acc']) + 1)\\nplt.plot(epochs, history.history['loss'])\\nplt.plot(epochs, history.history['val_loss'])\\nplt.title('model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'val'], loc='upper left')\\nplt.show()\\n[이미지: ]\\n이번 실습 데이터는 데이터의 양이 적어 과적합이 빠르게 시작되므로, 검증 데이터에 대한 오차가 증가하기 시작하는 시점의 바로 직전인 에포크 3~4 정도가 적당합니다. 이 데이터는 에포크 5를 넘어가기 시작하면 검증 데이터의 오차가 증가하는 경향이 있습니다.\\n==================================================\", \"==================================================\\n--- 10-03 로이터 뉴스 분류하기(Reuters News Classification) ---\\n```\\nepochs = range(1, len(history.history['acc']) + 1)\\nplt.plot(epochs, history.history['loss'])\\nplt.plot(epochs, history.history['val_loss'])\\nplt.title('model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'test'], loc='upper left')\\nplt.show()\\n```케라스에서 제공하는 로이터 뉴스 데이터를 LSTM을 이용하여 텍스트 분류를 진행해보겠습니다. 로이터 뉴스 기사 데이터는 총 11,258개의 뉴스 기사가 46개의 뉴스 카테고리로 분류되는 뉴스 기사 데이터입니다.\"]\n",
      "['import numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.datasets import reuters', '케라스 데이터셋으로부터 로이터 뉴스 기사 데이터를 로드하고, 뉴스 기사 데이터를 훈련용과 테스트용으로 나누겠습니다. reuters.load_data()에서 num_words는 이 데이터에서 등장 빈도 순위로 몇 번째에 해당하는 단어까지만 사용할 것인지 조절합니다. 예를 들어서 100이란 값을 넣으면, 등장 빈도 순위로 상위 1~100 등에 해당하는 단어만 사용하게 됩니다. 모든 단어를 사용하고자 한다면 None으로 설정합니다. 아직 무슨 의미인지 이해가 어려운 분들을 위해서 아래에서 출력 결과를 보여드리면서 다시 설명하겠습니다. test_split은 전체 뉴스 기사 데이터 중 테스트용 뉴스 기사로 몇 퍼센트를 사용할 것인지를 의미합니다. 이번 실습에서는 전체 뉴스 기사 중 20%를 테스트용 뉴스 기사로 사용할 것이므로 0.2로 설정했습니다. 훈련용 뉴스 기사와 테스트용 뉴스 기사가 8:2로 정상적으로 분리되어 로드되는지 확인해봅시다.', \"(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\\nprint('훈련용 뉴스 기사 : {}'.format(len(X_train)))\\nprint('테스트용 뉴스 기사 : {}'.format(len(X_test)))\\nnum_classes = len(set(y_train))\\nprint('카테고리 : {}'.format(num_classes))\\n훈련용 뉴스 기사 : 8982\\n테스트용 뉴스 기사 : 2246\\n카테고리 : 46\\n훈련용 뉴스 기사는 8,982개, 테스트용 뉴스 기사는 2,246개, 카테고리는 46개인것을 확인할 수 있습니다. 훈련용 뉴스 기사 데이터의 구성을 확인하기 위해 첫번째 뉴스 기사를 출력해보았습니다.\\nprint('첫번째 훈련용 뉴스 기사 :',X_train[0])\\nprint('첫번째 훈련용 뉴스 기사의 레이블 :',y_train[0])\", \"print('첫번째 훈련용 뉴스 기사 :',X_train[0])\\nprint('첫번째 훈련용 뉴스 기사의 레이블 :',y_train[0])\\n첫번째 훈련용 뉴스 기사 : [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\\n첫번째 훈련용 뉴스 기사의 레이블 : 3\", '첫번째 훈련용 뉴스 기사의 레이블 : 3\\n위와 같이 훈련용 뉴스 기사 데이터인 X_train 중 첫번째 뉴스 기사인 X_train[0]에는 정수의 나열이 저장되어있습니다. 텍스트가 아니라서 의아할 수 있는데, 현재 이 데이터는 토큰화과 정수 인코딩(각 단어를 정수로 변환)이 끝난 상태입니다.\\n이 데이터는 단어들이 몇 번 등장하는 지의 빈도수에 따라서 정수를 부여했습니다. 1이라는 숫자는 이 단어가 이 데이터에서 등장 빈도가 1등이라는 뜻입니다. 27,595라는 숫자는 이 단어가 데이터에서 27,595번째로 빈도수가 높은 단어라는 뜻입니다. 즉, 실제로는 빈도가 굉장히 낮은 단어입니다. 앞서 num_words에다가 None을 부여했는데, 만약 num_words에 1,000을 넣었다면 빈도수 순위가 1,000 이하의 단어만 가져온다는 의미이므로 데이터에서 1,000을 넘는 정수는 나오지 않습니다.', \"뉴스 기사들의 레이블들을 의미하는 y_train에서 첫번째 뉴스 기사의 레이블인 y_train[0]에는 3이라는 값이 들어있습니다. 이 숫자는 첫번째 훈련용 뉴스 기사가 46개의 카테고리 중 3에 해당하는 카테고리임을 의미합니다. 방금 확인한 X_train[0]과 y_train[0]은 8,982개의 훈련용 뉴스 기사 중 첫번째 뉴스 기사의 본문과 레이블만 확인한 것입니다. 이번에는 8,982개의 훈련용 뉴스 기사의 길이가 대체적으로 어떤 크기를 가지는지 확인해보겠습니다.\\nprint('뉴스 기사의 최대 길이 :{}'.format(max(len(sample) for sample in X_train)))\\nprint('뉴스 기사의 평균 길이 :{}'.format(sum(map(len, X_train))/len(X_train)))\\nplt.hist([len(sample) for sample in X_train], bins=50)\\nplt.xlabel('length of samples')\", \"plt.hist([len(sample) for sample in X_train], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n뉴스 기사의 최대 길이 : 2376\\n뉴스 기사의 평균 길이 : 145.5398574927633\\n[이미지: ]\\n대체적으로 대부분의 뉴스가 100~200 사이의 길이를 가집니다. 각 뉴스의 레이블 값의 분포를 보겠습니다.\\nfig, axe = plt.subplots(ncols=1)\\nfig.set_size_inches(12,5)\\nsns.countplot(y_train)\\n[이미지: ]\\n3, 4가 가장 많은 레이블을 차지하는 것을 확인할 수 있습니다. 각 레이블에 대한 정확한 개수를 알아보겠습니다.\\nunique_elements, counts_elements = np.unique(y_train, return_counts=True)\", 'unique_elements, counts_elements = np.unique(y_train, return_counts=True)\\nprint(\"각 레이블에 대한 빈도수:\")\\nprint(np.asarray((unique_elements, counts_elements)))\\n각 레이블에 대한 빈도수:\\n[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\\n14   15   16   17   18   19   20   21   22   23   24   25   26   27\\n28   29   30   31   32   33   34   35   36   37   38   39   40   41\\n42   43   44   45]\\n[  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172', '42   43   44   45]\\n[  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\\n26   20  444   39   66  549  269  100   15   41   62   92   24   15\\n48   19   45   39   32   11   50   10   49   19   19   24   36   30\\n13   21   12   18]]\\n3번 레이블은 총 3,159개가 존재하고 4번 레이블은 총 1,949개가 존재하는 것을 확인할 수 있습니다. X_train에 들어있는 숫자들이 각자 어떤 단어들을 나타내고 있는지 확인해보겠습니다. reuters.get_word_index는 각 단어와 그 단어에 부여된 인덱스를 리턴합니다.\\nword_to_index = reuters.get_word_index()\\nprint(word_to_index)', \"{'mdbl': 10996, 'fawc': 16260, 'degussa': 12089, 'woods': 8803, 'hanging': 13796, 'localized': 20672, 'sation': 20673, 'chanthaburi': 20675, 'refunding': 10997, 'hermann': 8804, 'passsengers': 20676, 'stipulate': 20677, 'heublein': 8352, 'screaming': 20713, 'tcby': 16261, 'four': 185, 'grains': 1642, 'broiler': 20680, 'wooden': 12090, 'wednesday': 1220, 'highveld': 13797, 'duffour': 7593, '0053': 20681, 'elections': 3914, '270': 2563, '271': 3551, '272': 5113, '273': 3552, '274': 3400,\", \"'0053': 20681, 'elections': 3914, '270': 2563, '271': 3551, '272': 5113, '273': 3552, '274': 3400, 'rudman': 7975, '276': 3401, '277': 3478, '278': 3632, '279': 4309, 'dormancy': 9381, - 이하 중략 -}\", \"많은 단어가 출력되므로 출력 결과는 중략했습니다. 이번에는 정수로부터 단어를 알 수 있도록 해보겠습니다. word_to_index에서 key와 value를 반대로 저장한 index_to_word를 만듭니다. 주의할 점은 reuters.get_word_index()에 저장된 값에 +3을 해야 실제 맵핑되는 정수입니다. 이것은 로이터 뉴스 데이터셋에서 정한 규칙입니다.\\nindex_to_word = {}\\nfor key, value in word_to_index.items():\\nindex_to_word[value+3] = key\\nindex_to_word[ ]에다가 인덱스를 입력하면 단어를 확인할 수 있습니다. +3을 했으므로 빈도수 1등에 해당하는 단어를 알고싶다면 숫자 4를 넣어야 합니다. 보통 불용어로 분류되는 the가 이 데이터에서도 어김없이 등장 빈도수로 1위를 차지했습니다.\\nprint('빈도수 상위 1번 단어 : {}'.format(index_to_word[4]))\", \"print('빈도수 상위 1번 단어 : {}'.format(index_to_word[4]))\\n빈도수 상위 1번 단어 : the\\n이번에는 임의로 빈도수 128등 단어를 알아봅시다.\\nprint('빈도수 상위 128등 단어 : {}'.format(index_to_word[131]))\\n빈도수 상위 128등 단어 : tax\\nindex_to_word에서 숫자 0은 패딩을 의미하는 토큰인 pad, 숫자 1은 문장의 시작을 의미하는 sos, 숫자 2는 OOV를 위한 토큰인 unk라는 특별 토큰에 맵핑되어져야 합니다. 이를 반영하여 index_to_word를 완성해줍니다. 이 또한 로이터 뉴스 데이터셋에서 정한 규칙이므로 납득하고 넘어갑시다. index_to_word를 이용해서 첫번째 훈련용 뉴스 기사인 X_train[0]가 어떤 단어들로 구성되어있는지를 복원해보겠습니다. X_train[0]에 있는 모든 단어들을 하나씩 불러와서 index_to_word의 입력으로 넣고 그 결과를 연결하면 됩니다.\", 'for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\\nindex_to_word[index] = token\\nprint(\\' \\'.join([index_to_word[index] for index in X_train[0]]))', \"index_to_word[index] = token\\nprint(' '.join([index_to_word[index] for index in X_train[0]]))\\n<sos> mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\", '위 결과는 복원된 결과를 보여줍니다. 물론 정수 인코딩을 수행하기 전 어느정도 전처리가 된 상태라서 제대로 된 문장이 나오지는 않습니다. 로이터 뉴스 데이터가 어떤 구성을 갖고있는지에 대해서 알아보았습니다. 텍스트 분류를 수행해보겠습니다.']\n",
      "['학습에서는 등장 빈도 순위 상위 1,000개의 단어들만 사용하겠습니다. 훈련용 뉴스 기사 데이터과 테스트용 뉴스 기사 데이터에 있는 각각의 뉴스의 길이는 서로 다르므로 모든 뉴스 기사의 길이를 100으로 패딩해줍니다. 이후 훈련용, 테스트용 뉴스 기사 데이터의 레이블에 원-핫 인코딩을 합니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint', 'from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nfrom tensorflow.keras.models import load_model\\nvocab_size = 1000\\nmax_len = 100\\n(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=vocab_size, test_split=0.2)\\nX_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)\\ny_train = to_categorical(y_train)\\ny_test = to_categorical(y_test)', 'y_train = to_categorical(y_train)\\ny_test = to_categorical(y_test)\\n하이퍼파라미터인 임베딩 벡터의 차원은 128, 은닉 상태의 크기는 128입니다. 단어 집합의 크기는 앞서 1,000으로 정했습니다. 모델은 다 대 일 구조의 LSTM을 사용합니다. 해당 모델은 마지막 시점에서 46개의 선택지 중 하나의 선택지를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 하이퍼파라미터인 배치 크기는 128이며, 30 에포크를 수행합니다.', \"EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 정해진 에포크에 도달하지 못하여도 학습을 조기 종료(Early Stopping)합니다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다. validation_data로는 X_test와 y_test를 사용합니다. val_loss가 줄어들다가 증가하는 상황이 오면 과적합으로 판단하기 위함입니다.\\nembedding_dim = 128\\nhidden_units = 128\\nnum_classes = 46\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(LSTM(hidden_units))\", \"model = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(LSTM(hidden_units))\\nmodel.add(Dense(num_classes, activation='softmax'))\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\", 'model.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'acc\\'])\\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))\\n저자의 경우 21 에포크에서 훈련이 조기 종료되었습니다. 훈련이 다 되었다면 테스트 데이터에 대해서 정확도를 측정할 차례입니다. 훈련 과정에서 검증 데이터가 가장 높았을 때 저장된 모델인 \\'best_model.h5\\'를 로드하고, 성능을 평가합니다.\\nloaded_model = load_model(\\'best_model.h5\\')\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))', 'print(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\\n2246/2246 [==============================] - 1s 656us/sample - loss: 1.2355 - acc: 0.7124\\n테스트 정확도: 0.7124', '테스트 데이터에 대한 정확도는 71.24%입니다. 케라스의 model.fit()에서 validation_data는 실제 기계가 데이터를 훈련하지는 않고 에포크마다 정확도와 loss를 출력하여 과적합을 판단하기 위한 용도로만 사용됩니다. 그래서 validation_data에서 이미 X_test, y_test를 사용했지만 기계는 이 데이터로 학습한 적이 없습니다. 모델이 학습하지 않은 데이터인 X_test, y_test를 테스트 데이터로서 성능 평가 용도로 model.evaluate()에서도 사용했습니다. 이번 모델은 검증 데이터와 테스트 데이터가 동일한 셈입니다. 사실 데이터가 충분하다면, 검증 데이터와 테스트 데이터는 다르게 사용하는 것이 바람직합니다. 앞서 스팸 메일 분류하기 실습에서는 validation_data 대신 validation_split=0.2를 사용하여 검증 데이터와 테스트 데이터를 다른 데이터를 사용하였음을 상기합시다', '. 에포크마다 변화하는 훈련 데이터와 검증 데이터(테스트 데이터)의 손실을 시각화해보겠습니다.', \"epochs = range(1, len(history.history['acc']) + 1)\\nplt.plot(epochs, history.history['loss'])\\nplt.plot(epochs, history.history['val_loss'])\\nplt.title('model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'test'], loc='upper left')\\nplt.show()\\n[이미지: ]\\n전체적으로는 검증 데이터의 손실이 줄어드는 경향이 있지만 뒤로 갈수록 점차 검증 데이터의 손실이 증가하려고 하는 경향이 보입니다. 이는 과적합의 신호일 수 있습니다.\\n==================================================\\n--- 10-04 IMDB 리뷰 감성 분류하기(IMDB Movie Review Sentiment Analysis) ---\\n```\", '--- 10-04 IMDB 리뷰 감성 분류하기(IMDB Movie Review Sentiment Analysis) ---\\n```\\n98.95% 확률로 긍정 리뷰입니다.\\n```감성 분류를 연습하기 위해 자주 사용하는 영어 데이터로 영화 사이트 IMDB의 리뷰 데이터가 있습니다. 이 데이터는 리뷰에 대한 텍스트와 해당 리뷰가 긍정인 경우 1을 부정인 경우 0으로 표시한 레이블로 구성된 데이터입니다. 스탠포드 대학교에서 2011년에 낸 논문에서 이 데이터를 소개하였으며, 당시 논문에서는 이 데이터를 훈련 데이터와 테스트 데이터를 50:50대 비율로 분할하여 88.89%의 정확도를 얻었다고 소개하고 있습니다.\\n논문 링크 : http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf', '논문 링크 : http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf\\n케라스에서는 해당 IMDB 영화 리뷰 데이터를 imdb.load_data() 함수를 통해 바로 다운로드 할 수 있도록 지원하고 있습니다. 해당 데이터를 학습하여 감성 분류를 수행하는 모델을 만들어보겠습니다.']\n",
      "['import numpy as np\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.datasets import imdb\\n케라스 데이터셋으로부터 imdb.data_load()를 통해 영화 리뷰 데이터를 로드합니다. IMDB 리뷰 데이터는 앞서 배운 로이터 뉴스 데이터에서 훈련 데이터와 테스트 데이터를 우리가 직접 비율을 조절했던 것과는 달리 이미 훈련 데이터와 테스트 데이터를 50:50 비율로 구분해서 제공합니다. 로이터 뉴스 데이터에서 사용했던 test_split과 같은 데이터의 비율을 조절하는 인자는 imdb.load_data에서는 지원하지 않습니다.', \"imdb.data_load()의 인자로 num_words를 사용하면 이 데이터에서 등장 빈도 순위로 몇 등까지의 단어를 사용할 것인지를 의미합니다. 예를 들어서 10,000을 넣으면, 등장 빈도 순위가 1~10,000에 해당하는 단어만 사용하게 됩니다. 즉, 단어 집합의 크기는 10,000이 됩니다. 지금은 별도로 제한하지 않겠습니다. 훈련용 리뷰의 개수, 테스트용 리뷰의 개수, 카테고리의 수를 출력합니다.\\n(X_train, y_train), (X_test, y_test) = imdb.load_data()\\nprint('훈련용 리뷰 개수 : {}'.format(len(X_train)))\\nprint('테스트용 리뷰 개수 : {}'.format(len(X_test)))\\nnum_classes = len(set(y_train))\\nprint('카테고리 : {}'.format(num_classes))\\n훈련용 리뷰 개수 : 25000\\n테스트용 리뷰 개수 : 25000\\n카테고리 : 2\", \"print('카테고리 : {}'.format(num_classes))\\n훈련용 리뷰 개수 : 25000\\n테스트용 리뷰 개수 : 25000\\n카테고리 : 2\\n훈련용 리뷰는 25,000개, 테스트용 리뷰는 25,000개, 카테고리는 2개입니다. 훈련 데이터가 어떻게 구성되어있는지를 확인하기 위해 첫번째 훈련용 리뷰와 레이블을 출력합니다.\\nprint('첫번째 훈련용 리뷰 :',X_train[0])\\nprint('첫번째 훈련용 리뷰의 레이블 :',y_train[0])\", '첫번째 훈련용 리뷰 : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38,', '19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16,', '226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]', \"첫번째 훈련용 리뷰의 레이블 : 1\\n리뷰 본문에 해당하는 X_train[0]에는 숫자들이 들어있습니다. 이 데이터는 토큰화와 정수 인코딩이라는 텍스트 전처리가 끝난 상태입니다. IMDB 리뷰 데이터는 전체 데이터에서 각 단어들의 등장 빈도에 따라서 인덱스를 부여했습니다. 숫자가 낮을수록 이 데이터에서 등장 빈도 순위가 높습니다. 위에서 단어 집합의 크기를 제한하지 않았기 때문에 22,665와 같은 큰 숫자도 보입니다.\\n첫번째 훈련용 리뷰의 레이블에 해당하는 y_train[0]의 값은 1입니다. 이 문제의 경우 감성 정보로서 0 또는 1의 값을 가지는데 긍정은 1의 값을 가집니다. 25,000개의 훈련용 리뷰의 각 길이는 전부 다른데, 리뷰의 길이 분포를 그래프로 시각화해보겠습니다.\\nreviews_length = [len(review) for review in X_train]\\nprint('리뷰의 최대 길이 : {}'.format(np.max(reviews_length)))\", 'print(\\'리뷰의 최대 길이 : {}\\'.format(np.max(reviews_length)))\\nprint(\\'리뷰의 평균 길이 : {}\\'.format(np.mean(reviews_length)))\\nplt.subplot(1,2,1)\\nplt.boxplot(reviews_length)\\nplt.subplot(1,2,2)\\nplt.hist(reviews_length, bins=50)\\nplt.show()\\n리뷰의 최대 길이 : 2494\\n리뷰의 평균 길이 : 238.71364\\n[이미지: ]\\n대체적으로 1,000이하의 길이를 가지며, 특히 100~500길이를 가진 데이터가 많은 것을 확인할 수 있습니다. 반면, 가장 긴 길이를 가진 데이터는 길이가 2,000이 넘는 것도 확인할 수 있습니다. 레이블의 분포를 확인해보겠습니다.\\nunique_elements, counts_elements = np.unique(y_train, return_counts=True)\\nprint(\"각 레이블에 대한 빈도수:\")', 'unique_elements, counts_elements = np.unique(y_train, return_counts=True)\\nprint(\"각 레이블에 대한 빈도수:\")\\nprint(np.asarray((unique_elements, counts_elements)))\\n각 레이블에 대한 빈도수:\\n[[    0     1]\\n[12500 12500]]\\n25,000개의 리뷰가 존재하는데 두 레이블 0과 1은 각각 12,500개로 균등한 분포를 가지고 있습니다. X_train에 들어있는 숫자들이 각각 어떤 단어들을 나타내고 있는지 확인해보겠습니다. imdb.get_word_index()에 각 단어와 맵핑되는 정수가 저장되어져 있습니다. 주의할 점은 imdb.get_word_index()에 저장된 값에 +3을 해야 실제 맵핑되는 정수입니다. 이것은 IMDB 리뷰 데이터셋에서 정한 규칙입니다.\\nword_to_index = imdb.get_word_index()', \"word_to_index = imdb.get_word_index()\\nindex_to_word = {}\\nfor key, value in word_to_index.items():\\nindex_to_word[value+3] = key\\nindex_to_word에 인덱스를 집어넣으면 전처리 전에 어떤 단어였는지 확인할 수 있습니다. IMDB 리뷰 데이터셋에서는 0, 1, 2, 3은 특별 토큰으로 취급하고 있습니다. 그래서 정수 4부터가 실제 IMDB 리뷰 데이터셋에서 빈도수가 가장 높은 실제 영단어입니다.\\nprint('빈도수 상위 1등 단어 : {}'.format(index_to_word[4]))\\n빈도수 상위 1등 단어 : the\\nprint('빈도수 상위 3938등 단어 : {}'.format(index_to_word[3941]))\\n빈도수 상위 3938등 단어 : suited\\n이 데이터에서 빈도가 가장 높은 단어는 the이고, 빈도가 3938번째로 높은 단어는 suited입니다.\", '빈도수 상위 3938등 단어 : suited\\n이 데이터에서 빈도가 가장 높은 단어는 the이고, 빈도가 3938번째로 높은 단어는 suited입니다.\\nfor index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\\nindex_to_word[index] = token\\nprint(\\' \\'.join([index_to_word[index] for index in X_train[0]]))\\n첫번째 훈련용 리뷰의 X_train[0]의 각 단어가 정수로 바뀌기 전에 어떤 단어들이었는지 확인해보겠습니다.', \"<sos> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <unk> is an amazing actor and now the same being director <unk> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <unk> and would\", \"it was just brilliant so much that i bought the film as soon as it was released for <unk> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <unk> to the two little boy's that played the <unk> of norman and paul they were just brilliant children are often left out of the <unk> list i think because the stars that play them all grown up\", \"children are often left out of the <unk> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\"]\n",
      "['단어 집합의 크기를 10,000으로 제한하고, 리뷰 최대 길이는 500으로 제한하여 패딩을 진행합니다.\\nimport re\\nfrom tensorflow.keras.datasets import imdb\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, GRU, Embedding\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nfrom tensorflow.keras.models import load_model\\nvocab_size = 10000\\nmax_len = 500', 'from tensorflow.keras.models import load_model\\nvocab_size = 10000\\nmax_len = 500\\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\\nX_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)', 'X_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)\\n하이퍼파라미터인 임베딩 벡터의 차원은 100, 은닉 상태의 크기는 128입니다. 모델은 다 대 일 구조의 GRU를 사용합니다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델입니다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 하이퍼파라미터인 배치 크기는 64이며, 15 에포크를 수행합니다.', \"EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미입니다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다. validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\\nembedding_dim = 100\\nhidden_units = 128\\nmodel = Sequential()\", \"embedding_dim = 100\\nhidden_units = 128\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(GRU(hidden_units))\\nmodel.add(Dense(1, activation='sigmoid'))\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\", 'model.compile(optimizer=\\'rmsprop\\', loss=\\'binary_crossentropy\\', metrics=[\\'acc\\'])\\nhistory = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\\n저자의 경우, 조기 종료 조건에 따라서 에포크 9에서 조기 종료되었습니다. 훈련이 다 되었다면 테스트 데이터에 대해서 정확도를 평가할 차례입니다. 훈련 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 \\'GRU_model.h5\\'를 로드합니다.\\nloaded_model = load_model(\\'GRU_model.h5\\')\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))', 'print(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\\n25000/25000 [==============================] - 61s 2ms/sample - loss: 0.3440 - acc: 0.8893\\n테스트 정확도: 0.8893\\n테스트 데이터에 대해서 정확도 88.93%를 얻습니다. 임의의 문장에 대해서 리뷰의 긍, 부정을 예측하고자 합니다. 이를 위해서는 모델에 넣기 전에 임의의 문장에 대해서 전처리를 해주어야 합니다. sentiment_predict은 입력된 문장에 대해서 기본적인 전처리와 정수 인코딩, 패딩을 한 후에 모델의 입력으로 사용하여 예측값을 리턴하는 함수입니다.\\ndef sentiment_predict(new_sentence):\\n# 알파벳과 숫자를 제외하고 모두 제거 및 알파벳 소문자화', \"def sentiment_predict(new_sentence):\\n# 알파벳과 숫자를 제외하고 모두 제거 및 알파벳 소문자화\\nnew_sentence = re.sub('[^0-9a-zA-Z ]', '', new_sentence).lower()\\nencoded = []\\n# 띄어쓰기 단위 토큰화 후 정수 인코딩\\nfor word in new_sentence.split():\\ntry :\\n# 단어 집합의 크기를 10,000으로 제한.\\nif word_to_index[word] <= 10000:\\nencoded.append(word_to_index[word]+3)\\nelse:\\n# 10,000 이상의 숫자는 <unk> 토큰으로 변환.\\nencoded.append(2)\\n# 단어 집합에 없는 단어는 <unk> 토큰으로 변환.\\nexcept KeyError:\\nencoded.append(2)\\npad_sequence = pad_sequences([encoded], maxlen=max_len)\", 'except KeyError:\\nencoded.append(2)\\npad_sequence = pad_sequences([encoded], maxlen=max_len)\\nscore = float(loaded_model.predict(pad_sequence)) # 예측\\nif(score > 0.5):\\nprint(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\\nelse:\\nprint(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))\\nIMDB 사이트에 접속해서 영화 블랙팬서의 1점 리뷰를 가져왔습니다. 부정으로 예측하는지 테스트해보겠습니다.', 'IMDB 사이트에 접속해서 영화 블랙팬서의 1점 리뷰를 가져왔습니다. 부정으로 예측하는지 테스트해보겠습니다.\\ntest_input = \"This movie was just way too overrated. The fighting was not professional and in slow motion. I was expecting more from a 200 million budget movie. The little sister of T.Challa was just trying too hard to be funny. The story was really dumb as well. Don\\'t watch this movie if you are going because others say its great unless you are a Black Panther fan or Marvels fan.\"\\nsentiment_predict(test_input)\\n97.43% 확률로 부정 리뷰입니다.', 'sentiment_predict(test_input)\\n97.43% 확률로 부정 리뷰입니다.\\nIMDB 사이트에 접속해서 영화 어벤져스의 10점 리뷰를 가져왔습니다. 긍정으로 예측하는지 테스트해보겠습니다.\\ntest_input = \" I was lucky enough to be included in the group to see the advanced screening in Melbourne on the 15th of April, 2012. And, firstly, I need to say a big thank-you to Disney and Marvel Studios. \\\\', \"Now, the film... how can I even begin to explain how I feel about this film? It is, as the title of this review says a 'comic book triumph'. I went into the film with very, very high expectations and I was not disappointed. \\\\\", 'Seeing Joss Whedon\\'s direction and envisioning of the film come to life on the big screen is perfect. The script is amazingly detailed and laced with sharp wit a humor. The special effects are literally mind-blowing and the action scenes are both hard-hitting and beautifully choreographed.\"\\nsentiment_predict(test_input)\\n98.95% 확률로 긍정 리뷰입니다.\\n==================================================\\n--- 10-05 나이브 베이즈 분류기(Naive Bayes Classifier) ---\\n```\\n정확도: 0.7738980350504514', \"--- 10-05 나이브 베이즈 분류기(Naive Bayes Classifier) ---\\n```\\n정확도: 0.7738980350504514\\n```텍스트 분류를 위해 전통적으로 사용되는 분류기로 나이브 베이즈 분류기가 있습니다. 나이브 베이즈 분류기는 인공 신경망 알고리즘에는 속하지 않지만, 머신 러닝의 주요 알고리즘으로 분류에 있어 준수한 성능을 보여주는 것으로 알려져 있습니다.\\n1) 베이즈의 정리(Bayes' theorem)를 이용한 분류 메커니즘\\n나이브 베이즈 분류기를 이해하기 위해서는 우선 베이즈의 정리(Bayes' theorem)를 이해할 필요가 있습니다. 베이즈 정리는 조건부 확률을 계산하는 방법 중 하나입니다.\", \"나이브 베이즈 분류기를 이해하기 위해서는 우선 베이즈의 정리(Bayes' theorem)를 이해할 필요가 있습니다. 베이즈 정리는 조건부 확률을 계산하는 방법 중 하나입니다.\\n$P(A)$가 A가 일어날 확률, $P(B)$가 B가 일어날 확률, $P(B|A)$가 A가 일어나고나서 B가 일어날 확률, $P(A|B)$가 B가 일어나고나서 A가 일어날 확률이라고 해봅시다. 이때 $P(B|A)$를 쉽게 구할 수 있는 상황이라면, 아래와 같은 식을 통해 $P(A|B)$를 구할 수 있습니다.\\n$$P(A|B)=\\\\frac{P(B|A)P(A)}{P(B)}$$\\n나이브 베이즈 분류기는 이러한 베이즈 정리를 이용하여 텍스트 분류를 수행합니다. 예를 들어서 나이브 베이즈 분류기를 통해서 스팸 메일 필터를 만들어본다고 합시다. 입력 텍스트(메일의 본문)이 주어졌을 때, 입력 텍스트가 정상 메일인지 스팸 메일인지 구분하기 위한 확률을 이와 같이 표현할 수 있습니다.\", 'P(정상 메일 | 입력 텍스트) = 입력 텍스트가 있을 때 정상 메일일 확률\\nP(스팸 메일 | 입력 텍스트) = 입력 텍스트가 있을 때 스팸 메일일 확률\\n이를 베이즈의 정리에 따라서 식을 표현하면 이와 같습니다.\\nP(정상 메일 | 입력 텍스트) = (P(입력 텍스트 | 정상 메일) × P(정상 메일)) / P(입력 텍스트)\\nP(스팸 메일 | 입력 텍스트) = (P(입력 텍스트 | 스팸 메일) × P(스팸 메일)) / P(입력 텍스트)\\n입력 텍스트가 주어졌을 때, P(정상 메일 | 입력 텍스트)가 P(스팸 메일 | 입력 텍스트)보다 크다면 정상 메일이라고 볼 수 있으며, 그 반대라면 스팸 메일이라고 볼 수 있습니다. 그런데 두 확률 모두 식을 보면 P(입력 텍스트)를 분모로 하고 있음을 알 수 있습니다. 그렇기 때문에 분모를 양쪽에서 제거하여 식을 간소화합니다.\\nP(정상 메일 | 입력 텍스트) = P(입력 텍스트 | 정상 메일) × P(정상 메일)', 'P(정상 메일 | 입력 텍스트) = P(입력 텍스트 | 정상 메일) × P(정상 메일)\\nP(스팸 메일 | 입력 텍스트) = P(입력 텍스트 | 스팸 메일) × P(스팸 메일)\\n입력 텍스트는 메일의 본문을 의미한다고 언급했습니다. 그런데 메일의 본문을 어떻게 나이브 베이즈 분류기의 입력으로 사용할 수 있을까요? 메일의 본문을 단어 토큰화하여 이 단어들을 나이브 베이즈의 분류기의 입력으로 사용합니다.\\n만약 메일의 본문에 있는 단어가 3개라고 가정해보겠습니다. 기본적으로 나이브 베이즈 분류기는 모든 단어가 독립적이라고 가정합니다. 메일의 본문에 있는 단어 3개를 $w_{1}$, $w_{2}$, $w_{3}$라고 표현한다면 결국 나이브 베이즈 분류기의 정상 메일일 확률과 스팸 메일일 확률을 구하는 식은 아래와 같습니다.\\nP(정상 메일 | 입력 텍스트) = P($w_{1}$ | 정상 메일) × P($w_{2}$ | 정상 메일) × P($w_{3}$ | 정상 메일) × P(정상 메일)', 'P(정상 메일 | 입력 텍스트) = P($w_{1}$ | 정상 메일) × P($w_{2}$ | 정상 메일) × P($w_{3}$ | 정상 메일) × P(정상 메일)\\nP(스팸 메일 | 입력 텍스트) = P($w_{1}$ | 스팸 메일) × P($w_{2}$ | 스팸 메일) × P($w_{3}$ | 스팸 메일) × P(스팸 메일)\\n식을 보고 눈치채신 분들도 있겠지만, 나이브 베이즈 분류기에서 토큰화 이전의 단어의 순서는 중요하지 않습니다. 즉, BoW와 같이 단어의 순서를 무시하고 오직 빈도수만을 고려합니다. 이제 실제 단어들로 이루어진 예제를 통해서 확률을 구해보겠습니다.\\n2) 스팸 메일 분류기(Spam Detection)\\n앞서 배운 나이브 베이즈 분류식을 가지고, 입력 텍스트로부터 해당 텍스트가 정상 메일인지 스팸 메일인지를 구분하는 작업을 해보겠습니다. 아래와 같은 훈련 데이터가 있다고 가정하겠습니다.\\n메일로부터 토큰화 및 정제 된 단어들\\n분류\\n1\\nme free lottery', '메일로부터 토큰화 및 정제 된 단어들\\n분류\\n1\\nme free lottery\\n스팸 메일\\n2\\nfree get free you\\n스팸 메일\\n3\\nyou free scholarship\\n정상 메일\\n4\\nfree to contact me\\n정상 메일\\n5\\nyou won award\\n정상 메일\\n6\\nyou ticket lottery\\n스팸 메일\\n이때 you free lottery라는 입력 텍스트에 대해서 정상 메일일 확률과 스팸 메일일 확률 각각을 구해보겠습니다.\\nP(정상 메일 | 입력 텍스트) =  P(you | 정상 메일) × P(free | 정상 메일) × P(lottery | 정상 메일) × P(정상 메일)\\nP(스팸 메일 | 입력 텍스트) =  P(you | 스팸 메일) × P(free | 스팸 메일) × P(lottery | 스팸 메일) × P(스팸 메일)\\nP(정상 메일) = P(스팸 메일) = 총 메일 6개 중 3개 = 0.5', 'P(정상 메일) = P(스팸 메일) = 총 메일 6개 중 3개 = 0.5\\n위 예제에서는 P(정상 메일)과 P(스팸 메일)의 값은 같으므로, 두 식에서 두 개의 확률은 생략이 가능합니다.\\nP(정상 메일 | 입력 텍스트) =  P(you | 정상 메일) × P(free | 정상 메일) × P(lottery | 정상 메일)\\nP(스팸 메일 | 입력 텍스트) =  P(you | 스팸 메일) × P(free | 스팸 메일) × P(lottery | 스팸 메일)\\nP(you | 정상 메일)을 구하는 방법은 정상 메일에 등장한 모든 단어의 빈도 수의 총합을 분모로하고, 정상 메일에서 you가 총 등장한 빈도의 수를 분자로 하는 것입니다. 이 경우에는 2/10 = 0.2가 됩니다. 이와 같은 원리로 식을 전개하면 이와 같습니다.\\nP(정상 메일 | 입력 텍스트) =  2/10 × 2/10 × 0/10 = 0\\nP(스팸 메일 | 입력 텍스트) =  2/10 × 3/10 × 2/10 = 0.012', 'P(정상 메일 | 입력 텍스트) =  2/10 × 2/10 × 0/10 = 0\\nP(스팸 메일 | 입력 텍스트) =  2/10 × 3/10 × 2/10 = 0.012\\n결과적으로 P(정상 메일 | 입력 텍스트) < P(스팸 메일 | 입력 텍스트)이므로 입력 텍스트 you free lottery는 스팸 메일로 분류됩니다.\\n그런데 예제를 보니 이상한 점이 보입니다. 물론, 직관적으로 보기에도 you, free, lottery라는 단어가 스팸 메일에서 빈도수가 더 높기때문에 스팸 메일인 확률이 더 높은 것은 확실합니다. 그러나 입력 텍스트에 대해서 단, 하나의 단어라도 훈련 텍스트에 없었다면 확률 전체가 0이 되는 것은 지나친 일반화입니다. 이 경우에는 정상 메일에 lottery가 단 한 번도 등장하지 않았고, 그 이유로 정상 메일일 확률 자체가 0%가 되어버렸습니다.', '이를 방지하기 위해서 나이브 베이즈 분류기에서는 각 단어에 대한 확률의 분모, 분자에 전부 숫자를 더해서 분자가 0이 되는 것을 방지하는 라플라스 스무딩을 사용하기도 합니다.\\n3) 뉴스그룹 데이터 분류하기(Classification of 20 News Group with Naive Bayes Classifier)\\n사이킷 런에서는 Twenty Newsgroups이라고 불리는 20개의 다른 주제를 가진 18,846개의 뉴스그룹 데이터를 제공합니다. (토픽 모델링의 LSA 챕터에서 사용했던 데이터와 동일한 데이터.) 해당 데이터는 이미 훈련 데이터(11,314개)와 테스트 데이터(7,532개)를 미리 분류해놓았기 때문에 별도로 훈련 데이터와 테스트 데이터를 분류할 필요는 없습니다. 훈련 데이터로 훈련을 해서 모델을 만들고, 테스트 데이터를 예측했을 때의 정확도를 확인해보겠습니다.\\n(1) 뉴스그룹 데이터에 대한 이해', \"(1) 뉴스그룹 데이터에 대한 이해\\n해당 데이터는 총 6개의 속성을 갖고 있는데, 그 중에서 우리가 사용할 것은 해당 데이터의 본문을 갖고 있는 'data' 속성과 해당 데이터가 어떤 카테고리에 속하는지 0부터 19까지의 라벨이 붙어있는 'target' 속성이 되겠습니다. 그럼 코드를 보면서 데이터의 구성을 더 구체적으로 알아보도록 하겠습니다.\\n우선 훈련 데이터를 다운로드 받아보겠습니다.\\nfrom sklearn.datasets import fetch_20newsgroups\\nnewsdata=fetch_20newsgroups(subset='train')\\nprint(newsdata.keys())\", \"newsdata=fetch_20newsgroups(subset='train')\\nprint(newsdata.keys())\\n위의 코드 부분에 subset 부분에 'all'을 넣으면 18,846개의 전체 데이터 다운로드할 수 있으며, 'train'을 넣으면 훈련 데이터를, 'test'를 넣으면 테스트 데이터를 다운로드할 수 있습니다. newsdata.keys()를 출력하여 해당 데이터가 어떤 속성으로 구성되어져 있는지 출력해보았습니다.\\ndict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])\\n해당 데이터는 data, filenames, target_names, target, DESCR, description이라는 6개 속성의 데이터를 갖고 있습니다.\", '해당 데이터는 data, filenames, target_names, target, DESCR, description이라는 6개 속성의 데이터를 갖고 있습니다.\\nprint (len(newsdata.data), len(newsdata.filenames), len(newsdata.target_names), len(newsdata.target))\\n훈련용 샘플의 개수를 확인해보겠습니다.\\n11314 11314 20 11314\\n훈련용 샘플은 총 11,314개로 구성되어 있습니다. newsdata.target_names는 이 데이터의 20개의 카테고리의 이름을 담고있습니다. 어떤 카테고리들로 구성되어있는지 확인해보겠습니다.\\nprint(newsdata.target_names)', \"print(newsdata.target_names)\\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\\ntarget에는 총 0부터 19까지의 숫자가 들어가있는데 첫번째 샘플의 경우에는 몇 번 카테고리인지 확인해보겠습니다.\", \"target에는 총 0부터 19까지의 숫자가 들어가있는데 첫번째 샘플의 경우에는 몇 번 카테고리인지 확인해보겠습니다.\\nprint(newsdata.target[0])\\n7\\n첫번째 샘플은 카테고리 7번에 속한다고 라벨이 붙어있습니다.\\nprint(newsdata.target_names[7])\\nrec.autos\\n7번 카테고리의 카테고리 제목은 rec.autos입니다. 즉, 첫번째 샘플은 rec.autos 카테고리에 속합니다. 그렇다면 첫번째 샘플이 어떤 내용을 갖고 있는지 확인해보겠습니다.\\nprint(newsdata.data[0])\\nFrom: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\", 'Nntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\nI was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is\\nall I know. If anyone can tellme a model name, engine specs, years', 'all I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\nThanks,\\n- IL\\n---- brought to you by your neighborhood Lerxst ----\\n메일의 내용을 보니 스포츠 카에 대한 글로 보입니다. 즉, 이 스포츠 카에 대한 글은 총 0부터 19까지의 카테고리 중 7번 레이블에 속하는 글이고, 7번은 rec.autos 카테고리를 의미한다는 것을 알 수 있습니다.\\n(2) 나이브 베이즈 분류', '(2) 나이브 베이즈 분류\\n이제 다운로드 받은 훈련 데이터에 대한 전처리를 진행해보겠습니다. 사용할 데이터는 newsdata.data와 그에 대한 카테고리 레이블이 되어있는 newsdata.target이라고 언급한 바 있습니다. 여기서 전처리를 해야하는 데이터는 newsdata.data입니다. 위에서 봤듯이 해당 데이터는 토큰화가 전혀 되어있지 않습니다. 나이브 베이즈 분류를 위해서는 데이터를 BoW로 만들어줄 필요가 있습니다.\\n여기서는 입력한 텍스트를 자동으로 BoW로 만드는 CountVectorizer를 사용합니다.\\n(BoW 챕터 및 DTM 챕터 참고)\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfTransformer\\nfrom sklearn.naive_bayes import MultinomialNB # 다항분포 나이브 베이즈 모델', 'from sklearn.naive_bayes import MultinomialNB # 다항분포 나이브 베이즈 모델\\nfrom sklearn.metrics import accuracy_score #정확도 계산\\ndtmvector = CountVectorizer()\\nX_train_dtm = dtmvector.fit_transform(newsdata.data)\\nprint(X_train_dtm.shape)\\n(11314, 130107)\\n이제 자동으로 DTM이 완성되었습니다. 11,314는 훈련용 샘플의 개수이고 DTM 관점에서는 문서의 수가 되겠습니다. 130,107은 전체 훈련 데이터에 등장한 단어의 수를 의미합니다.', '물론, DTM을 그대로 나이브 베이즈 분류기에 사용할 수도 있겠지만 DTM 행렬 대신 TF-IDF 가중치를 적용한 TF-IDF 행렬을 입력으로 텍스트 분류를 수행하면, 성능의 개선을 얻을 수도 있습니다. (DTM 챕터 참고) 주의할 점은 TF-IDF 행렬이 항상 DTM으로 수행했을 때보다 성능이 뛰어나지는 않습니다. 사이킷런은 TF-IDF를 자동 계산해주는 TfidVectorizer 클래스를 제공하므로 이를 사용해보겠습니다.\\ntfidf_transformer = TfidfTransformer()\\ntfidfv = tfidf_transformer.fit_transform(X_train_dtm)\\nprint(tfidfv.shape)\\n(11314, 130107)\\n이제 TF-IDF 행렬이 만들어졌습니다. 이제 본격적으로 나이브 베이즈 분류를 수행해보겠습니다. 사이킷런은 나이브 베이즈 모델을 지원하므로, 이를 그대로 갖고와서 사용하겠습니다.\\nmod = MultinomialNB()', \"mod = MultinomialNB()\\nmod.fit(tfidfv, newsdata.target)\\n모델의 입력으로 TF-IDF 행렬과 11,314개의 훈련 데이터에 대한 레이블이 적혀있는 newsdata.target이 들어갑니다. 이는 앞서 배운 분류 예제들을 상기해보면, 각각 X_train과 y_train에 해당되는 데이터들입니다.\\nMultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\\n여기서 alpha=1.0은 라플라스 스무딩이 적용되었음을 의미합니다.\\nnewsdata_test = fetch_20newsgroups(subset='test', shuffle=True) #테스트 데이터 갖고오기\\nX_test_dtm = dtmvector.transform(newsdata_test.data) #테스트 데이터를 DTM으로 변환\", 'X_test_dtm = dtmvector.transform(newsdata_test.data) #테스트 데이터를 DTM으로 변환\\ntfidfv_test = tfidf_transformer.transform(X_test_dtm) #DTM을 TF-IDF 행렬로 변환\\npredicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\\nprint(\"정확도:\", accuracy_score(newsdata_test.target, predicted)) #예측값과 실제값 비교\\n정확도: 0.7738980350504514\\n77%의 정확도를 얻었습니다. 여기서는 하지 않았지만, 잠재 의미 분석 챕터에서 진행했던 전처리를 모두 진행하고 다시 나이브 베이즈 분류기를 돌려보세요. 80% 이상의 정확도를 얻을 수 있습니다.\\n==================================================', '==================================================\\n--- 10-06 네이버 영화 리뷰 감성 분류하기(Naver Movie Review Sentiment Analysis) ---\\n```\\n80.77% 확률로 긍정 리뷰입니다.\\n```\\nColab 링크: https://colab.research.google.com/drive/1GH-VebLl1lktLJOvJ4a86R5l8O3dByqt?usp=sharing\\n이번에 사용할 데이터는 네이버 영화 리뷰 데이터입니다. 총 200,000개 리뷰로 구성된 데이터로 영화 리뷰에 대한 텍스트와 해당 리뷰가 긍정인 경우 1, 부정인 경우 0을 표시한 레이블로 구성되어져 있습니다. 해당 데이터를 다운로드 받아 감성 분류를 수행하는 모델을 만들어보겠습니다.']\n",
      "['데이터 다운로드 링크 : https://github.com/e9t/nsmc/\\nimport pickle\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport re\\nimport urllib.request\\nfrom konlpy.tag import Okt\\nfrom tqdm import tqdm\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\n1) 데이터 로드하기\\n위 링크로부터 훈련 데이터에 해당하는 ratings_train.txt와 테스트 데이터에 해당하는 ratings_test.txt를 다운로드합니다.', '1) 데이터 로드하기\\n위 링크로부터 훈련 데이터에 해당하는 ratings_train.txt와 테스트 데이터에 해당하는 ratings_test.txt를 다운로드합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\\npandas를 이용하여 훈련 데이터는 train_data에 테스트 데이터는 test_data에 저장합니다.\\ntrain_data = pd.read_table(\\'ratings_train.txt\\')', \"train_data = pd.read_table('ratings_train.txt')\\ntest_data = pd.read_table('ratings_test.txt')\\ntrain_data에 존재하는 영화 리뷰의 개수를 확인해봅시다.\\nprint('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\\n훈련용 리뷰 개수 : 150000\\ntrain_data는 총 150,000개의 리뷰가 존재합니다. 상위 5개의 샘플을 출력해봅시다.\\ntrain_data[:5] # 상위 5개 출력\\n[이미지: ]\\n해당 데이터는 id, document, label 총 3개의 열로 구성되어져 있습니다. id는 감성 분류를 수행하는데 도움이 되지 않으므로 앞으로 무시합니다. 결국 이 모델은 리뷰 내용을 담고있는 document와 해당 리뷰가 긍정(1), 부정(0)인지를 나타내는 label 두 개의 열을 학습하는 모델이 되어야 합니다.\", \"또한 단지 상위 5개의 샘플만 출력해보았지만 한국어 데이터와 영어 데이터의 차이를 확인할 수 있습니다. 예를 들어, 인덱스 2번 샘플은 띄어쓰기를 하지 않아도 글을 쉽게 이해할 수 있는 한국어의 특성으로 인해 띄어쓰기가 되어있지 않습니다. test_data의 리뷰 개수와 상위 5개의 샘플을 확인해봅시다.\\nprint('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력\\n테스트용 리뷰 개수 : 50000\\ntest_data는 총 50,000개의 영화 리뷰가 존재합니다. 상위 5개의 샘플을 출력해봅시다.\\ntest_data[:5]\\n[이미지: ]\\ntest_data도 train_data와 동일한 형식으로 id, document, label 3개의 열로 구성되어져 있습니다.\\n2) 데이터 정제하기\\ntrain_data의 데이터 중복 유무를 확인합니다.\\n# document 열과 label 열의 중복을 제외한 값의 개수\", \"2) 데이터 정제하기\\ntrain_data의 데이터 중복 유무를 확인합니다.\\n# document 열과 label 열의 중복을 제외한 값의 개수\\ntrain_data['document'].nunique(), train_data['label'].nunique()\\n(146182, 2)\\n총 150,000개의 샘플이 존재하는데 document열에서 중복을 제거한 샘플의 개수가 146,182개라는 것은 약 4,000개의 중복 샘플이 존재한다는 의미입니다. label 열은 0 또는 1의 두 가지 값만을 가지므로 2가 출력됩니다. 중복 샘플을 제거합니다.\\n# document 열의 중복 제거\\ntrain_data.drop_duplicates(subset=['document'], inplace=True)\\n중복 샘플을 제거하였습니다. 중복이 제거되었는지 전체 샘플 수를 확인합니다.\\nprint('총 샘플의 수 :',len(train_data))\\n총 샘플의 수 : 146183\", \"중복 샘플을 제거하였습니다. 중복이 제거되었는지 전체 샘플 수를 확인합니다.\\nprint('총 샘플의 수 :',len(train_data))\\n총 샘플의 수 : 146183\\n중복 샘플이 제거되었습니다. train_data에서 해당 리뷰의 긍, 부정 유무가 기재되어있는 레이블(label) 값의 분포를 보겠습니다.\\ntrain_data['label'].value_counts().plot(kind = 'bar')\\n[이미지: ]\\n앞서 확인하였듯이 약 146,000개의 영화 리뷰 샘플이 존재하는데 그래프 상으로 긍정과 부정 둘 다 약 72,000개의 샘플이 존재하여 레이블의 분포가 균일한 것처럼 보입니다. 정확하게 몇 개인지 확인해봅시다.\\nprint(train_data.groupby('label').size().reset_index(name = 'count'))\\nlabel  count\\n0      0  73342\\n1      1  72841\", 'label  count\\n0      0  73342\\n1      1  72841\\n레이블이 0인 리뷰가 근소하게 많습니다. 리뷰 중에 Null 값을 가진 샘플이 있는지 확인합니다.\\nprint(train_data.isnull().values.any())\\nTrue\\nTrue가 나왔다면 데이터 중에 Null 값을 가진 샘플이 존재한다는 의미입니다. 어떤 열에 존재하는지 확인해봅시다.\\nprint(train_data.isnull().sum())\\nid          0\\ndocument    1\\nlabel       0\\ndtype: int64\\n리뷰가 적혀있는 document 열에서 Null 값을 가진 샘플이 총 1개가 존재한다고 합니다. 그렇다면 document 열에서 Null 값이 존재한다는 것을 조건으로 Null 값을 가진 샘플이 어느 인덱스의 위치에 존재하는지 한 번 출력해봅시다.\\ntrain_data.loc[train_data.document.isnull()]\\n[이미지: ]', \"train_data.loc[train_data.document.isnull()]\\n[이미지: ]\\n출력 결과는 위와 같습니다. Null 값을 가진 샘플을 제거하겠습니다.\\ntrain_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\\nprint(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\\nFalse\\nNull 값을 가진 샘플이 제거되었습니다. 다시 샘플의 개수를 출력하여 1개의 샘플이 제거되었는지 확인해봅시다.\\nprint(len(train_data))\\n146182\\n데이터의 전처리를 수행해보겠습니다. 위의 train_data와 test_data에서 온점(.)이나 ?와 같은 각종 특수문자가 사용된 것을 확인했습니다. train_data로부터 한글만 남기고 제거하기 위해서 정규 표현식을 사용해보겠습니다.\", \"우선 영어를 예시로 정규 표현식을 설명해보겠습니다. 영어의 알파벳들을 나타내는 정규 표현식은 [a-zA-Z]입니다. 이 정규 표현식은 영어의 소문자와 대문자들을 모두 포함하고 있는 정규 표현식으로 이를 응용하면 영어에 속하지 않는 구두점이나 특수문자를 제거할 수 있습니다. 예를 들어 알파벳과 공백을 제외하고 모두 제거하는 전처리를 수행하는 예제는 다음과 같습니다.\\n#알파벳과 공백을 제외하고 모두 제거\\neng_text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\\nprint(re.sub(r'[^a-zA-Z ]', '', eng_text))\\n'do you expect people to read the FAQ etc and actually accept hard atheism'\", \"'do you expect people to read the FAQ etc and actually accept hard atheism'\\n위와 같은 원리를 한국어 데이터에 적용하고 싶다면, 우선 한글을 범위 지정할 수 있는 정규 표현식을 찾아내면 되겠습니다. 우선 자음과 모음에 대한 범위를 지정해보겠습니다. 일반적으로 자음의 범위는 ㄱ ~ ㅎ, 모음의 범위는 ㅏ ~ ㅣ와 같이 지정할 수 있습니다. 해당 범위 내에 어떤 자음과 모음이 속하는지 알고 싶다면 아래의 링크를 참고하시기 바랍니다.\\n링크 : https://www.unicode.org/charts/PDF/U3130.pdf\\nㄱ ~ ㅎ: 3131 ~ 314E\\nㅏ ~ ㅣ: 314F ~ 3163\\n완성형 한글의 범위는 가 ~ 힣과 같이 사용합니다. 해당 범위 내에 포함된 음절들은 아래의 링크에서 확인할 수 있습니다.\\n링크 : https://www.unicode.org/charts/PDF/UAC00.pdf\", '링크 : https://www.unicode.org/charts/PDF/UAC00.pdf\\n위 범위 지정을 모두 반영하여 train_data에 한글과 공백을 제외하고 모두 제거하는 정규 표현식을 수행해봅시다.\\n# 한글과 공백을 제외하고 모두 제거\\ntrain_data[\\'document\\'] = train_data[\\'document\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\\ntrain_data[:5]\\n[이미지: ]', 'train_data[:5]\\n[이미지: ]\\n상위 5개의 샘플을 다시 출력해보았는데, 정규 표현식을 수행하자 기존의 공백. 즉, 띄어쓰기는 유지되면서 온점과 같은 구두점 등은 제거되었습니다. 사실 네이버 영화 리뷰는 한글이 아니더라도 영어, 숫자, 특수문자로도 리뷰를 업로드할 수 있습니다. 다시 말해 기존에 한글이 없는 리뷰였다면 더 이상 아무런 값도 없는 빈(empty) 값이 되었을 것입니다. train_data에 공백(whitespace)만 있거나 빈 값을 가진 행이 있다면 Null 값으로 변경하도록 하고, Null 값이 존재하는지 확인해보겠습니다.\\ntrain_data[\\'document\\'] = train_data[\\'document\\'].str.replace(\\'^ +\\', \"\", regex=True) # white space 데이터를 empty value로 변경\\ntrain_data[\\'document\\'].replace(\\'\\', np.nan, inplace=True)', \"train_data['document'].replace('', np.nan, inplace=True)\\nprint(train_data.isnull().sum())\\nid            0\\ndocument    789\\nlabel         0\\ndtype: int64\\nNull 값이 789개나 새로 생겼습니다. Null 값이 있는 행을 5개만 출력해볼까요?\\ntrain_data.loc[train_data.document.isnull()][:5]\\n[이미지: ]\\nNull 샘플들은 레이블이 긍정일 수도 있고, 부정일 수도 있습니다. 아무런 의미도 없는 데이터므로 제거해줍니다.\\ntrain_data = train_data.dropna(how = 'any')\\nprint(len(train_data))\\n145393\\n샘플 개수가 또 다시 줄어서 145,393개가 남았습니다. 테스트 데이터에 앞서 진행한 전처리 과정을 동일하게 진행합니다.\", 'print(len(train_data))\\n145393\\n샘플 개수가 또 다시 줄어서 145,393개가 남았습니다. 테스트 데이터에 앞서 진행한 전처리 과정을 동일하게 진행합니다.\\ntest_data.drop_duplicates(subset = [\\'document\\'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\\ntest_data[\\'document\\'] = test_data[\\'document\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\\ntest_data[\\'document\\'] = test_data[\\'document\\'].str.replace(\\'^ +\\', \"\", regex=True) # 공백은 empty 값으로 변경\\ntest_data[\\'document\\'].replace(\\'\\', np.nan, inplace=True) # 공백은 Null 값으로 변경', \"test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\\ntest_data = test_data.dropna(how='any') # Null 값 제거\\nprint('전처리 후 테스트용 샘플의 개수 :',len(test_data))\\n전처리 후 테스트용 샘플의 개수 : 48852\\n3) 토큰화\\n토큰화를 진행해봅시다. 토큰화 과정에서 불용어를 제거하겠습니다. 불용어는 정의하기 나름인데, 한국어의 조사, 접속사 등의 보편적인 불용어를 사용할 수도 있겠지만 결국 풀고자 하는 문제의 데이터를 지속 검토하면서 계속해서 추가하는 경우 또한 많습니다. 실제 현업인 상황이라면 일반적으로 아래의 불용어보다 더 많은 불용어를 사용할 수 있습니다.\\nstopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\", \"stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\\n여기서는 위 정도로만 불용어를 정의하고, 토큰화를 위한 형태소 분석기는 KoNLPy의 Okt를 사용합니다. Okt를 복습해봅시다.\\nokt = Okt()\\nokt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)\\n['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']\", \"['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']\\nOkt는 위와 같이 KoNLPy에서 제공하는 형태소 분석기입니다. 한국어을 토큰화할 때는 영어처럼 띄어쓰기 기준으로 토큰화를 하는 것이 아니라, 주로 형태소 분석기를 사용한다고 언급한 바 있습니다. stem = True를 사용하면 일정 수준의 정규화를 수행해주는데, 예를 들어 위의 예제의 결과를 보면 '이런'이 '이렇다'로 변환되었고 '만드는'이 '만들다'로 변환된 것을 알 수 있습니다. train_data에 형태소 분석기를 사용하여 토큰화를 하면서 불용어를 제거하여 X_train에 저장합니다.\\nX_train = []\\nfor sentence in tqdm(train_data['document']):\\ntokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\", \"tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\\nstopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\\nX_train.append(stopwords_removed_sentence)\\n상위 3개의 샘플만 출력하여 결과를 확인해봅시다.\\nprint(X_train[:3])\\n[['아', '더빙', '진짜', '짜증나다', '목소리'], ['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍다', '않다'], ['너', '무재', '밓었', '다그', '래서', '보다', '추천', '다']]\\n형태소 토큰화가 진행된 것을 볼 수 있습니다. 테스트 데이터에 대해서도 동일하게 토큰화를 해줍니다.\\nX_test = []\", \"형태소 토큰화가 진행된 것을 볼 수 있습니다. 테스트 데이터에 대해서도 동일하게 토큰화를 해줍니다.\\nX_test = []\\nfor sentence in tqdm(test_data['document']):\\ntokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\\nstopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\\nX_test.append(stopwords_removed_sentence)\\n지금까지 훈련 데이터와 테스트 데이터에 대해서 텍스트 전처리를 진행해보았습니다.\\n4) 정수 인코딩\\n기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. 우선, 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다.\\ntokenizer = Tokenizer()\", \"tokenizer = Tokenizer()\\ntokenizer.fit_on_texts(X_train)\\n단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. 이는 tokenizer.word_index를 출력하여 확인 가능합니다.\\nprint(tokenizer.word_index)\\n{'영화': 1, '보다': 2, '을': 3, '없다': 4, '이다': 5, '있다': 6, '좋다': 7, ... 중략 ... '디케이드': 43751, '수간': 43752}\\n단어가 43,000개가 넘게 존재합니다. 각 정수는 전체 훈련 데이터에서 등장 빈도수가 높은 순서대로 부여되었기 때문에, 높은 정수가 부여된 단어들은 등장 빈도수가 매우 낮다는 것을 의미합니다. 여기서는 빈도수가 낮은 단어들은 자연어 처리에서 배제하고자 합니다. 등장 빈도수가 3회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다.\\nthreshold = 3\", \"threshold = 3\\ntotal_cnt = len(tokenizer.word_index) # 단어의 수\\nrare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\\ntotal_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\\nrare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\\n# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\\nfor key, value in tokenizer.word_counts.items():\\ntotal_freq = total_freq + value\\n# 단어의 등장 빈도수가 threshold보다 작으면\\nif(value < threshold):\\nrare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint('단어 집합(vocabulary)의 크기 :',total_cnt)\", 'rare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint(\\'단어 집합(vocabulary)의 크기 :\\',total_cnt)\\nprint(\\'등장 빈도가 %s번 이하인 희귀 단어의 수: %s\\'%(threshold - 1, rare_cnt))\\nprint(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\\nprint(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\\n단어 집합(vocabulary)의 크기 : 43752\\n등장 빈도가 2번 이하인 희귀 단어의 수: 24337\\n단어 집합에서 희귀 단어의 비율: 55.62488571950996\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.8715872104872904', \"단어 집합에서 희귀 단어의 비율: 55.62488571950996\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.8715872104872904\\n등장 빈도가 threshold 값인 3회 미만. 즉, 2회 이하인 단어들은 단어 집합에서 무려 절반 이상을 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 1.87%밖에 되지 않습니다. 아무래도 등장 빈도가 2회 이하인 단어들은 자연어 처리에서 별로 중요하지 않을 듯 합니다. 그래서 이 단어들은 정수 인코딩 과정에서 배제시키겠습니다.\\n등장 빈도수가 2이하인 단어들의 수를 제외한 단어의 개수를 단어 집합의 최대 크기로 제한하겠습니다.\\n# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\\n# 0번 패딩 토큰을 고려하여 + 1\\nvocab_size = total_cnt - rare_cnt + 1\\nprint('단어 집합의 크기 :',vocab_size)\\n단어 집합의 크기 : 19416\", \"vocab_size = total_cnt - rare_cnt + 1\\nprint('단어 집합의 크기 :',vocab_size)\\n단어 집합의 크기 : 19416\\n단어 집합의 크기는 19,416개입니다. 이를 케라스 토크나이저의 인자로 넘겨주고 텍스트 시퀀스를 정수 시퀀스로 변환합니다.\\ntokenizer = Tokenizer(vocab_size)\\ntokenizer.fit_on_texts(X_train)\\nX_train = tokenizer.texts_to_sequences(X_train)\\nX_test = tokenizer.texts_to_sequences(X_test)\\n정수 인코딩이 진행되었는지 확인하고자 X_train에 대해서 상위 3개의 샘플만 출력합니다.\\nprint(X_train[:3])\", \"정수 인코딩이 진행되었는지 확인하고자 X_train에 대해서 상위 3개의 샘플만 출력합니다.\\nprint(X_train[:3])\\n[[50, 454, 16, 260, 659], [933, 457, 41, 602, 1, 214, 1449, 24, 961, 675, 19], [386, 2444, 2315, 5671, 2, 222, 9]]\\n각 샘플 내의 단어들은 각 단어에 대한 정수로 변환된 것을 확인할 수 있습니다. 단어의 개수는 19,416개로 제한되었으므로 0번 단어 ~ 19,415번 단어까지만 사용 중입니다. 0번 단어는 패딩을 위한 토큰임을 상기합시다. train_data에서 y_train과 y_test를 별도로 저장해줍니다\\n.\\ny_train = np.array(train_data['label'])\\ny_test = np.array(test_data['label'])\\n5) 빈 샘플(empty samples) 제거\", \"y_test = np.array(test_data['label'])\\n5) 빈 샘플(empty samples) 제거\\n전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 빈(empty) 샘플이 되었다는 것을 의미합니다. 빈 샘플들은 어떤 레이블이 붙어있던 의미가 없으므로 빈 샘플들을 제거해주는 작업을 하겠습니다. 각 샘플들의 길이를 확인해서 길이가 0인 샘플들의 인덱스를 받아오겠습니다.\\ndrop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\\ndrop_train에는 X_train으로부터 얻은 빈 샘플들의 인덱스가 저장됩니다. 앞서 훈련 데이터(X_train, y_train)의 샘플 개수는 145,791개임을 확인했었습니다. 그렇다면 빈 샘플들을 제거한 후의 샘플 개수는 몇 개일까요?\\n# 빈 샘플들을 제거\", \"# 빈 샘플들을 제거\\nX_train = np.delete(X_train, drop_train, axis=0)\\ny_train = np.delete(y_train, drop_train, axis=0)\\nprint(len(X_train))\\nprint(len(y_train))\\n145162\\n145162\\n145,162개로 샘플의 수가 줄어든 것을 확인할 수 있습니다.\\n6) 패딩\\n서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행해보겠습니다. 전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포를 알아보겠습니다.\\nprint('리뷰의 최대 길이 :',max(len(review) for review in X_train))\\nprint('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\\nplt.hist([len(review) for review in X_train], bins=50)\", \"plt.hist([len(review) for review in X_train], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n리뷰의 최대 길이 : 69\\n리뷰의 평균 길이 : 10.812485361182679\\n[이미지: ]\\n가장 긴 리뷰의 길이는 69이며, 그래프를 봤을 때 전체 데이터의 길이 분포는 대체적으로 약 11내외의 길이를 가지는 것을 볼 수 있습니다. 모델이 처리할 수 있도록 X_train과 X_test의 모든 샘플의 길이를 특정 길이로 동일하게 맞춰줄 필요가 있습니다. 특정 길이 변수를 max_len으로 정합니다. 대부분의 리뷰가 내용이 잘리지 않도록 할 수 있는 최적의 max_len의 값은 몇일까요? 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수를 만듭니다.\", \"def below_threshold_len(max_len, nested_list):\\ncount = 0\\nfor sentence in nested_list:\\nif(len(sentence) <= max_len):\\ncount = count + 1\\nprint('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\\n위의 분포 그래프를 봤을 때, max_len = 30이 적당할 것 같습니다. 이 값이 얼마나 많은 리뷰 길이를 커버하는지 확인해봅시다.\\nmax_len = 30\\nbelow_threshold_len(max_len, X_train)\\n전체 샘플 중 길이가 30 이하인 샘플의 비율: 94.31944999380003\\n전체 훈련 데이터 중 약 94%의 리뷰가 30이하의 길이를 가지는 것을 확인했습니다. 모든 샘플의 길이를 30으로 맞추겠습니다.\", '전체 훈련 데이터 중 약 94%의 리뷰가 30이하의 길이를 가지는 것을 확인했습니다. 모든 샘플의 길이를 30으로 맞추겠습니다.\\nX_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)']\n",
      "['하이퍼파라미터인 임베딩 벡터의 차원은 100, 은닉 상태의 크기는 128입니다. 모델은 다 대 일 구조의 LSTM을 사용합니다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델입니다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 하이퍼파라미터인 배치 크기는 64이며, 15 에포크를 수행합니다.', \"EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미입니다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다. validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM\", \"from tensorflow.keras.layers import Embedding, Dense, LSTM\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.models import load_model\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nembedding_dim = 100\\nhidden_units = 128\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(LSTM(hidden_units))\\nmodel.add(Dense(1, activation='sigmoid'))\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\", \"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\nhistory = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\", '저자의 경우 조기 종료 조건에 따라서 9 에포크에서 훈련이 멈췄습니다. 훈련이 다 되었다면 테스트 데이터에 대해서 정확도를 측정할 차례입니다. 훈련 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 \\'best_model.h5\\'를 로드합니다.\\nloaded_model = load_model(\\'best_model.h5\\')\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\\n테스트 정확도: 0.8544\\n테스트 데이터에서 85.44%의 정확도를 얻습니다. 위 코드는 뒤에서 이어질 네이버 쇼핑 리뷰 분류 실습과 한국어 스팀 리뷰 감성 분류 실습에서도 거의 동일하게 사용될 코드입니다.\\n모델과 마찬가지로 토크나이저도 다음과 같이 파일로 저장 후 다시 로드할 수 있습니다.\\nwith open(\\'tokenizer.pickle\\', \\'wb\\') as handle:\\npickle.dump(tokenizer, handle)', \"with open('tokenizer.pickle', 'wb') as handle:\\npickle.dump(tokenizer, handle)\\nwith open('tokenizer.pickle', 'rb') as handle:\\ntokenizer = pickle.load(handle)\"]\n",
      "[\"임의의 리뷰에 대해서 예측하는 함수를 만들어보겠습니다. 기본적으로 현재 학습한 model에 새로운 입력에 대해서 예측값을 얻는 것은 model.predict()를 사용합니다. 그리고 model.fit()을 할 때와 마찬가지로 새로운 입력에 대해서도 동일한 전처리를 수행 후에 model.predict()의 입력으로 사용해야 합니다.\\ndef sentiment_predict(new_sentence):\\nnew_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\\nnew_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\\nnew_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\\nencoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\", 'encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\\npad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\\nscore = float(loaded_model.predict(pad_new)) # 예측\\nif(score > 0.5):\\nprint(\"{:.2f}% 확률로 긍정 리뷰입니다.\\\\n\".format(score * 100))\\nelse:\\nprint(\"{:.2f}% 확률로 부정 리뷰입니다.\\\\n\".format((1 - score) * 100))\\nsentiment_predict(\\'이 영화 개꿀잼 ㅋㅋㅋ\\')\\n97.76% 확률로 긍정 리뷰입니다.\\nsentiment_predict(\\'이 영화 핵노잼 ㅠㅠ\\')\\n98.55% 확률로 부정 리뷰입니다.\\nsentiment_predict(\\'이딴게 영화냐 ㅉㅉ\\')\\n99.91% 확률로 부정 리뷰입니다.\\nsentiment_predict(\\'감독 뭐하는 놈이냐?\\')', \"sentiment_predict('이딴게 영화냐 ㅉㅉ')\\n99.91% 확률로 부정 리뷰입니다.\\nsentiment_predict('감독 뭐하는 놈이냐?')\\n98.21% 확률로 부정 리뷰입니다.\\nsentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')\\n80.77% 확률로 긍정 리뷰입니다.\\n참고하세요!\\nㄱ-ㅎ와 ㅏ-ㅣ 사이에 어떤 글자들이 포함되어져 있는지는 아래의 링크에서 확인할 수 있습니다.\\nhttps://www.unicode.org/charts/PDF/U3130.pdf\\n가-힣 사이에 어떤 글자들이 포함되어져 있는지는 아래의 링크에서 확인할 수 있습니다.\\nhttps://www.unicode.org/charts/PDF/UAC00.pdf\\n==================================================\\n--- 10-07 네이버 쇼핑 리뷰 감성 분류하기(Naver Shopping Review Sentiment Analysis) ---\\n```\", '--- 10-07 네이버 쇼핑 리뷰 감성 분류하기(Naver Shopping Review Sentiment Analysis) ---\\n```\\n91.69% 확률로 부정 리뷰입니다.\\n```1. Colab에 Mecab 설치\\n여기서는 형태소 분석기 Mecab을 사용합니다. 저자의 경우 Mecab을 편하게 사용하기 위해서 구글의 Colab을 사용하였습니다. 참고로 Colab에서 실습하는 경우가 아니라면 아래의 방법으로 Mecab이 설치되지 않습니다. 이 경우 해당 환경에 맞게 Mecab을 설치하시거나 다른 형태소 분석기를 사용하시기 바랍니다.\\n# Colab에 Mecab 설치\\n!pip install konlpy\\n!pip install mecab-python\\n!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)']\n",
      "['다운로드 링크 : https://github.com/bab2min/corpus/tree/master/sentiment\\nimport re\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport urllib.request\\nfrom collections import Counter\\nfrom konlpy.tag import Mecab\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\n1) 데이터 로드하기\\n위의 링크로부터 전체 데이터에 해당하는 ratings_total.txt를 다운로드합니다.', '1) 데이터 로드하기\\n위의 링크로부터 전체 데이터에 해당하는 ratings_total.txt를 다운로드합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")\\n해당 데이터에는 열제목이 별도로 없습니다. 그래서 임의로 두 개의 열제목인 \\'ratings\\'와 \\'reviews\\'를 추가해주겠습니다.\\ntotal_data = pd.read_table(\\'ratings_total.txt\\', names=[\\'ratings\\', \\'reviews\\'])\\nprint(\\'전체 리뷰 개수 :\\',len(total_data)) # 전체 리뷰 개수 출력\\n전체 리뷰 개수 : 200000\\n총 20만개의 샘플이 존재합니다. 상위 5개의 샘플만 출력해봅시다.\\ntotal_data[:5]\\n[이미지: ]', \"전체 리뷰 개수 : 200000\\n총 20만개의 샘플이 존재합니다. 상위 5개의 샘플만 출력해봅시다.\\ntotal_data[:5]\\n[이미지: ]\\n2) 훈련 데이터와 테스트 데이터 분리하기\\n현재 갖고 있는 데이터는 레이블을 별도로 갖고있지 않습니다. 평점이 4, 5인 리뷰에는 레이블 1을, 평점이 1, 2인 리뷰에는 레이블 0을 부여합니다. 부여된 레이블은 새로 생성한 label이라는 열에 저장합니다.\\ntotal_data['label'] = np.select([total_data.ratings > 3], [1], default=0)\\ntotal_data[:5]\\n[이미지: ]\\n각 열에 대해서 중복을 제외한 샘플의 수를 카운트합니다.\\ntotal_data['ratings'].nunique(), total_data['reviews'].nunique(), total_data['label'].nunique()\\n(4, 199908, 2)\", \"(4, 199908, 2)\\nratings열의 경우 1, 2, 4, 5라는 네 가지 값을 가지고 있습니다. reviews열에서 중복을 제외한 경우 199,908개입니다. 현재 20만개의 리뷰가 존재하므로 이는 현재 갖고 있는 데이터에 중복인 샘플들이 있다는 의미입니다. 중복인 샘플들을 제거해줍니다.\\ntotal_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거\\nprint('총 샘플의 수 :',len(total_data))\\n총 샘플의 수 : 199908\\nNULL 값 유무를 확인합니다.\\nprint(total_data.isnull().values.any())\\nFalse\\n훈련 데이터와 테스트 데이터를 3:1 비율로 분리합니다.\\ntrain_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\", \"train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\\nprint('훈련용 리뷰의 개수 :', len(train_data))\\nprint('테스트용 리뷰의 개수 :', len(test_data))\\n훈련용 리뷰의 개수 : 149931\\n테스트용 리뷰의 개수 : 49977\\n훈련용 리뷰의 경우 약 14만 9,900개. 테스트용 리뷰의 경우 약 4만 9,900개가 존재합니다.\\n3) 레이블의 분포 확인\\n훈련 데이터의 레이블의 분포를 확인해봅시다.\\ntrain_data['label'].value_counts().plot(kind = 'bar')\\n[이미지: ]\\nprint(train_data.groupby('label').size().reset_index(name = 'count'))\\nlabel  count\\n0      0  74918\\n1      1  75013\", 'label  count\\n0      0  74918\\n1      1  75013\\n두 레이블 모두 약 7만 5천개로 50:50 비율을 가지고 있습니다.\\n4) 데이터 정제하기\\n정규 표현식을 사용하여 한글을 제외하고 모두 제거해줍니다. 또한 혹시 이 과정에서 빈 샘플이 생기지는 않는지 확인합니다.\\n# 한글과 공백을 제외하고 모두 제거\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\\ntrain_data[\\'reviews\\'].replace(\\'\\', np.nan, inplace=True)\\nprint(train_data.isnull().sum())\\nratings    0\\nreviews    0\\nlabel      0\\ndtype: int64\\n테스트 데이터에 대해서도 같은 과정을 거칩니다.', 'ratings    0\\nreviews    0\\nlabel      0\\ndtype: int64\\n테스트 데이터에 대해서도 같은 과정을 거칩니다.\\ntest_data.drop_duplicates(subset = [\\'reviews\\'], inplace=True) # 중복 제거\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\\ntest_data[\\'reviews\\'].replace(\\'\\', np.nan, inplace=True) # 공백은 Null 값으로 변경\\ntest_data = test_data.dropna(how=\\'any\\') # Null 값 제거\\nprint(\\'전처리 후 테스트용 샘플의 개수 :\\',len(test_data))\\n전처리 후 테스트용 샘플의 개수 : 49977\\n5) 토큰화', \"print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\\n전처리 후 테스트용 샘플의 개수 : 49977\\n5) 토큰화\\n형태소 분석기 Mecab을 사용하여 토큰화 작업을 수행합니다. 다음은 임의의 문장에 대해서 테스트한 토큰화 결과입니다.\\nmecab = Mecab()\\nprint(mecab.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔'))\\n['와', '이런', '것', '도', '상품', '이', '라고', '차라리', '내', '가', '만드', '는', '게', '나을', '뻔']\\n불용어를 지정하여 필요없는 토큰들은 제거하도록 합니다.\\nstopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\\n훈련 데이터와 테스트 데이터에 대해서 동일한 과정을 거칩니다.\", \"훈련 데이터와 테스트 데이터에 대해서 동일한 과정을 거칩니다.\\ntrain_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\\ntrain_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\\ntest_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\\ntest_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\\n6) 단어와 길이 분포 확인하기\", \"6) 단어와 길이 분포 확인하기\\n긍정 리뷰에는 주로 어떤 단어들이 많이 등장하고, 부정 리뷰에는 주로 어떤 단어들이 등장하는지 두 가지 경우에 대해서 각 단어의 빈도수를 계산해보겠습니다. 각 레이블에 따라서 별도로 단어들의 리스트를 저장해줍니다.\\nnegative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\\npositive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)\\nCounter()를 사용하여 각 단어에 대한 빈도수를 카운트합니다. 우선 부정 리뷰에 대해서 빈도수가 높은 상위 20개 단어들을 출력합니다.\\nnegative_word_count = Counter(negative_words)\\nprint(negative_word_count.most_common(20))\", \"negative_word_count = Counter(negative_words)\\nprint(negative_word_count.most_common(20))\\n[('네요', 31799), ('는데', 20295), ('안', 19718), ('어요', 14849), ('있', 13200), ('너무', 13058), ('했', 11783), ('좋', 9812), ('배송', 9677), ('같', 8997), ('구매', 8876), ('어', 8869), ('거', 8854), ('없', 8670), ('아요', 8642), ('습니다', 8436), ('그냥', 8355), ('되', 8345), ('잘', 8029), ('않', 7984)]\\n'네요', '는데', '안', '않', '너무', '없' 등과 같은 단어들이 부정 리뷰에서 주로 등장합니다. 긍정 리뷰에 대해서도 출력해봅시다.\\npositive_word_count = Counter(positive_words)\", \"positive_word_count = Counter(positive_words)\\nprint(positive_word_count.most_common(20))\\n[('좋', 39488), ('아요', 21184), ('네요', 19895), ('어요', 18686), ('잘', 18602), ('구매', 16171), ('습니다', 13320), ('있', 12391), ('배송', 12275), ('는데', 11670), ('했', 9818), ('합니다', 9801), ('먹', 9635), ('재', 9273), ('너무', 8397), ('같', 7868), ('만족', 7261), ('거', 6482), ('어', 6294), ('쓰', 6292)]\\n'좋', '아요', '네요', '잘', '너무', '만족' 등과 같은 단어들이 주로 많이 등장합니다. 두 가지 경우에 대해서 각각 길이 분포를 확인해봅시다.\", \"'좋', '아요', '네요', '잘', '너무', '만족' 등과 같은 단어들이 주로 많이 등장합니다. 두 가지 경우에 대해서 각각 길이 분포를 확인해봅시다.\\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\\ntext_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\\nax1.hist(text_len, color='red')\\nax1.set_title('Positive Reviews')\\nax1.set_xlabel('length of samples')\\nax1.set_ylabel('number of samples')\\nprint('긍정 리뷰의 평균 길이 :', np.mean(text_len))\\ntext_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\", \"text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\\nax2.hist(text_len, color='blue')\\nax2.set_title('Negative Reviews')\\nfig.suptitle('Words in texts')\\nax2.set_xlabel('length of samples')\\nax2.set_ylabel('number of samples')\\nprint('부정 리뷰의 평균 길이 :', np.mean(text_len))\\nplt.show()\\n긍정 리뷰의 평균 길이 : 13.587751456414221\\n부정 리뷰의 평균 길이 : 17.02953896259911\\n[이미지: ]\\n긍정 리뷰보다는 부정 리뷰가 좀 더 길게 작성된 경향이 있는 것 같습니다.\\nX_train = train_data['tokenized'].values\\ny_train = train_data['label'].values\", \"X_train = train_data['tokenized'].values\\ny_train = train_data['label'].values\\nX_test= test_data['tokenized'].values\\ny_test = test_data['label'].values\\n7) 정수 인코딩\\n기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(X_train)\\n단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. 이는 tokenizer.word_index를 출력하여 확인 가능합니다. 등장 횟수가 1회인 단어들은 자연어 처리에서 배제하고자 합니다. 이 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다.\\nthreshold = 2\", \"threshold = 2\\ntotal_cnt = len(tokenizer.word_index) # 단어의 수\\nrare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\\ntotal_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\\nrare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\\n# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\\nfor key, value in tokenizer.word_counts.items():\\ntotal_freq = total_freq + value\\n# 단어의 등장 빈도수가 threshold보다 작으면\\nif(value < threshold):\\nrare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint('단어 집합(vocabulary)의 크기 :',total_cnt)\", 'rare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint(\\'단어 집합(vocabulary)의 크기 :\\',total_cnt)\\nprint(\\'등장 빈도가 %s번 이하인 희귀 단어의 수: %s\\'%(threshold - 1, rare_cnt))\\nprint(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\\nprint(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\\n단어 집합(vocabulary)의 크기 : 39997\\n등장 빈도가 1번 이하인 희귀 단어의 수: 18212\\n단어 집합에서 희귀 단어의 비율: 45.53341500612546\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.7935245745567578', \"단어 집합에서 희귀 단어의 비율: 45.53341500612546\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.7935245745567578\\n단어가 약 40,000개가 존재합니다. 등장 빈도가 threshold 값인 2회 미만. 즉, 1회인 단어들은 단어 집합에서 약 45%를 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 매우 적은 수치인 약 0.8%밖에 되지 않습니다. 아무래도 등장 빈도가 1회인 단어들은 자연어 처리에서 중요하지 않을 것으로 저자는 판단했습니다. 이 단어들은 정수 인코딩 과정에서 배제시키겠습니다. 등장 빈도수가 1인 단어들의 수를 제외한 단어의 개수를 단어 집합의 최대 크기로 제한합니다.\\n# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\\n# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\\nvocab_size = total_cnt - rare_cnt + 2\\nprint('단어 집합의 크기 :',vocab_size)\", \"vocab_size = total_cnt - rare_cnt + 2\\nprint('단어 집합의 크기 :',vocab_size)\\n단어 집합의 크기 : 21787\\n단어 집합의 크기는 21,787개입니다. 이를 토크나이저의 인자로 넘겨주고, 텍스트 시퀀스를 정수 시퀀스로 변환합니다. 정수 인코딩 과정에서 이보다 큰 숫자가 부여된 단어들은 OOV로 변환하겠습니다.\\ntokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\\ntokenizer.fit_on_texts(X_train)\\nX_train = tokenizer.texts_to_sequences(X_train)\\nX_test = tokenizer.texts_to_sequences(X_test)\\nX_train과 X_test에 대해서 상위 3개의 샘플만 출력합니다.\\nprint(X_train[:3])\", 'X_train과 X_test에 대해서 상위 3개의 샘플만 출력합니다.\\nprint(X_train[:3])\\n[[67, 2060, 299, 14260, 263, 73, 6, 236, 168, 137, 805, 2951, 625, 2, 77, 62, 207, 40, 1343, 155, 3, 6], [482, 409, 52, 8530, 2561, 2517, 339, 2918, 250, 2357, 38, 473, 2], [46, 24, 825, 105, 35, 2372, 160, 7, 10, 8061, 4, 1319, 29, 140, 322, 41, 59, 160, 140, 7, 1916, 2, 113, 162, 1379, 323, 119, 136]]\\nprint(X_test[:3])', \"print(X_test[:3])\\n[[14, 704, 767, 116, 186, 252, 12], [339, 3904, 62, 3816, 1651], [11, 69, 2, 49, 164, 3, 27, 15, 6, 513, 289, 17, 92, 110, 564, 59, 7, 2]]\\n8) 패딩\\n서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행해보겠습니다. 전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포를 알아보겠습니다.\\nprint('리뷰의 최대 길이 :',max(len(review) for review in X_train))\\nprint('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\\nplt.hist([len(review) for review in X_train], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\", \"plt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n리뷰의 최대 길이 : 85\\n리뷰의 평균 길이 : 15.307554808545264\\n[이미지: ]\\n리뷰의 최대 길이는 85, 평균 길이는 약 15입니다. 그래프로 봤을 때, 전체적으로는 60이하의 길이를 가지는 것으로 보입니다.\\ndef below_threshold_len(max_len, nested_list):\\ncount = 0\\nfor sentence in nested_list:\\nif(len(sentence) <= max_len):\\ncount = count + 1\\nprint('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\\n최대 길이가 85이므로 만약 80으로 패딩할 경우, 몇 개의 샘플들을 온전히 보전할 수 있는지 확인해봅시다.\\nmax_len = 80\", '최대 길이가 85이므로 만약 80으로 패딩할 경우, 몇 개의 샘플들을 온전히 보전할 수 있는지 확인해봅시다.\\nmax_len = 80\\nbelow_threshold_len(max_len, X_train)\\n전체 샘플 중 길이가 80 이하인 샘플의 비율: 99.99933302652553\\n훈련용 리뷰의 99.99%가 80이하의 길이를 가집니다. 훈련용 리뷰를 길이 80으로 패딩하겠습니다.\\nX_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)']\n",
      "['하이퍼파라미터인 임베딩 벡터의 차원은 100, 은닉 상태의 크기는 128입니다. 모델은 다 대 일 구조의 LSTM를 사용합니다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델입니다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 하이퍼파라미터인 배치 크기는 64이며, 15 에포크를 수행합니다.', \"EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미입니다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다. validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\\nfrom tensorflow.keras.layers import Embedding, Dense, GRU\", \"from tensorflow.keras.layers import Embedding, Dense, GRU\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.models import load_model\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nembedding_dim = 100\\nhidden_units = 128\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(GRU(hidden_units))\\nmodel.add(Dense(1, activation='sigmoid'))\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\", \"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\nhistory = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\\n저자의 경우 에포크 10에서 조기 종료가 발생했습니다.\\nloaded_model = load_model('best_model.h5')\", '저자의 경우 에포크 10에서 조기 종료가 발생했습니다.\\nloaded_model = load_model(\\'best_model.h5\\')\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\\n1562/1562 [==============================] - 5s 3ms/step - loss: 0.2108 - acc: 0.9237\\n테스트 정확도: 0.9237']\n",
      "[\"임의의 문장에 대한 예측을 위해서는 학습하기 전 전처리를 동일하게 적용해줍니다. 전처리의 순서는 정규 표현식을 통한 한국어 외 문자 제거, 토큰화, 불용어 제거, 정수 인코딩, 패딩 순입니다.\\ndef sentiment_predict(new_sentence):\\nnew_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\\nnew_sentence = mecab.morphs(new_sentence)\\nnew_sentence = [word for word in new_sentence if not word in stopwords]\\nencoded = tokenizer.texts_to_sequences([new_sentence])\\npad_new = pad_sequences(encoded, maxlen = max_len)\\nscore = float(loaded_model.predict(pad_new))\\nif(score > 0.5):\", 'score = float(loaded_model.predict(pad_new))\\nif(score > 0.5):\\nprint(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\\nelse:\\nprint(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))\\nsentiment_predict(\\'이 상품 진짜 좋아요... 저는 강추합니다. 대박\\')\\n98.88% 확률로 긍정 리뷰입니다.\\nsentiment_predict(\\'진짜 배송도 늦고 개짜증나네요. 뭐 이런 걸 상품이라고 만듬?\\')\\n99.31% 확률로 부정 리뷰입니다.\\nsentiment_predict(\\'판매자님... 너무 짱이에요.. 대박나삼\\')\\n98.36% 확률로 긍정 리뷰입니다.\\nsentiment_predict(\\'ㅁㄴㅇㄻㄴㅇㄻㄴㅇ리뷰쓰기도 귀찮아\\')\\n91.69% 확률로 부정 리뷰입니다.\\n==================================================', '91.69% 확률로 부정 리뷰입니다.\\n==================================================\\n--- 10-08 BiLSTM으로 한국어 스팀 리뷰 감성 분류하기 ---\\n```\\n92.49% 확률로 긍정 리뷰입니다.\\n```게임 플랫폼 스팀에 등록된 한국어 리뷰에 대해서 감성 분석을 진행해보겠습니다. 이 데이터는 긍정인 리뷰에는 레이블 1이, 부정인 리뷰에는 레이블 0이 부여되어져 있습니다. 앞서 진행했던 감성 분류 문제들과 마찬가지로 이진 분류 문제를 풉니다.']\n",
      "['[이미지: ]\\n양방향 LSTM은 두 개의 독립적인 LSTM 아키텍처를 함께 사용하는 구조입니다. 위 그림에서 주황색 LSTM 셀은 순차적으로 입력을 받습니다. 자연어 처리라고 한다면, 마치 사람처럼 문장을 왼쪽 단어부터 순차적으로 읽는 셈입니다. 양방향 LSTM은 뒤의 문맥까지 고려하기 위해서 문장을 오른쪽에서 반대로 읽는 역방향의 LSTM 셀(위 그림에서 초록색)을 함께 사용합니다. 이 두 가지 정보를 고려하여 출력층에서 예측 시에 두 가지 정보를 모두 사용합니다.\\n위 그림은 다 대 다(many-to-many) 문제를 푸는 경우의 양방향 LSTM을 보여주고 있습니다. 그런데 양방향 LSTM을 다 대 일(many-to-one) 문제인 텍스트 분류 문제에 사용한다고 하면, 한 가지 의문이 생깁니다.\\n[이미지: ]', '[이미지: ]\\n일반적으로 순방향 LSTM은 마지막 시점의 은닉 상태를 출력층으로 보내서 텍스트 분류를 수행합니다. 그렇다면 역방향 LSTM도 순방향과 같은 시점의 은닉 상태를 사용하면 될까요? 위 그림과 같이 텍스트 분류를 진행하는 경우, 역방향 LSTM은 $x_{4}$만 본 상태입니다. 이 경우 역방향 LSTM이 텍스트 분류를 위한 유용한 정보를 갖고 있다고 기대하기 어렵습니다.\\n[이미지: ]\\n그래서 케라스에서는 양방향 LSTM을 사용하면서 return_sequences=False를 택할 경우에는 위의 그림과 같이 동작하고 있습니다. 순방향 LSTM의 경우에는 마지막 시점의 은닉 상태를 반환하고, 역방향 LSTM의 경우에는 첫번째 시점의 은닉 상태를 반환합니다. 위 구조를 통해서 양방향 LSTM으로 텍스트 분류를 수행할 수 있습니다.']\n",
      "['여기서는 형태소 분석기 Mecab을 사용합니다. 저자의 경우 Mecab을 편하게 사용하기 위해서 구글의 Colab을 사용하였습니다. 참고로 Colab에서 실습하는 경우가 아니라면 아래의 방법으로 Mecab이 설치되지 않습니다. 이 경우 해당 환경에 맞게 Mecab을 설치하시거나 다른 형태소 분석기를 사용하시기 바랍니다.\\n# Colab에 Mecab 설치\\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\\n%cd Mecab-ko-for-Google-Colab\\n!bash install_mecab-ko_on_colab190912.sh']\n",
      "['다운로드 링크 : https://github.com/bab2min/corpus/tree/master/sentiment\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport urllib.request\\nfrom collections import Counter\\nfrom konlpy.tag import Mecab\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\n1) 데이터 로드하기', 'from tensorflow.keras.preprocessing.sequence import pad_sequences\\n1) 데이터 로드하기\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/steam.txt\", filename=\"steam.txt\")\\n해당 데이터에는 열제목이 별도로 없습니다. 그래서 임의로 두 개의 열제목인 \\'label\\'과 \\'reviews\\'를 추가해주겠습니다.\\ntotal_data = pd.read_table(\\'steam.txt\\', names=[\\'label\\', \\'reviews\\'])\\nprint(\\'전체 리뷰 개수 :\\',len(total_data)) # 전체 리뷰 개수 출력\\n전체 리뷰 개수 : 100000\\n총 10만개의 샘플이 존재합니다. 상위 5개의 샘플만 출력해봅시다.\\ntotal_data[:5]\\n[이미지: ]', \"전체 리뷰 개수 : 100000\\n총 10만개의 샘플이 존재합니다. 상위 5개의 샘플만 출력해봅시다.\\ntotal_data[:5]\\n[이미지: ]\\n각 열에 대해서 중복을 제외한 샘플의 수를 카운트합니다.\\ntotal_data['reviews'].nunique(), total_data['label'].nunique()\\n(99892, 2)\\nreviews열에서 중복을 제외한 경우 99,892개입니다. 현재 10만개의 리뷰가 존재하므로 이는 현재 갖고 있는 데이터에 중복인 샘플들이 있다는 의미입니다. 중복인 샘플들을 제거해줍니다.\\ntotal_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거\\nprint('총 샘플의 수 :',len(total_data))\\n총 샘플의 수 : 99892\\nNULL 값 유무를 확인합니다.\\nprint(total_data.isnull().values.any())\\nFalse\", \"총 샘플의 수 : 99892\\nNULL 값 유무를 확인합니다.\\nprint(total_data.isnull().values.any())\\nFalse\\n2) 훈련 데이터와 테스트 데이터 분리하기\\n훈련 데이터와 테스트 데이터를 3:1 비율로 분리합니다.\\ntrain_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\\nprint('훈련용 리뷰의 개수 :', len(train_data))\\nprint('테스트용 리뷰의 개수 :', len(test_data))\\n훈련용 리뷰의 개수 : 74919\\n테스트용 리뷰의 개수 : 24973\\n훈련용 리뷰의 경우 약 7만 5,000개. 테스트용 리뷰의 경우 약 2만 5,000개가 존재합니다.\\n3) 레이블의 분포 확인\\n훈련 데이터의 레이블의 분포를 확인해봅시다.\\ntrain_data['label'].value_counts().plot(kind = 'bar')\\n[이미지: ]\", '훈련 데이터의 레이블의 분포를 확인해봅시다.\\ntrain_data[\\'label\\'].value_counts().plot(kind = \\'bar\\')\\n[이미지: ]\\nprint(train_data.groupby(\\'label\\').size().reset_index(name = \\'count\\'))\\nlabel  count\\n0      0  37376\\n1      1  37543\\n두 레이블 모두 약 3만 7천개로 50:50 비율을 가지고 있습니다.\\n4) 데이터 정제하기\\n정규 표현식을 사용하여 한글을 제외하고 모두 제거해줍니다. 또한 혹시 이 과정에서 빈 샘플이 생기지는 않는지 확인합니다.\\n# 한글과 공백을 제외하고 모두 제거\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\\ntrain_data[\\'reviews\\'].replace(\\'\\', np.nan, inplace=True)', 'train_data[\\'reviews\\'].replace(\\'\\', np.nan, inplace=True)\\nprint(train_data.isnull().sum())\\nlabel      0\\nreviews    0\\ndtype: int64\\n테스트 데이터에 대해서도 같은 과정을 거칩니다.\\ntest_data.drop_duplicates(subset = [\\'reviews\\'], inplace=True) # 중복 제거\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\\ntest_data[\\'reviews\\'].replace(\\'\\', np.nan, inplace=True) # 공백은 Null 값으로 변경\\ntest_data = test_data.dropna(how=\\'any\\') # Null 값 제거\\nprint(\\'전처리 후 테스트용 샘플의 개수 :\\',len(test_data))', \"test_data = test_data.dropna(how='any') # Null 값 제거\\nprint('전처리 후 테스트용 샘플의 개수 :',len(test_data))\\n불용어를 정의해줍니다.\\nstopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']\\n5) 토큰화\\n형태소 분석기 Mecab을 사용하여 토큰화 작업을 수행합니다.\\nmecab = Mecab()\\ntrain_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\", \"mecab = Mecab()\\ntrain_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\\ntrain_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\\ntest_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\\ntest_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\\n6) 단어와 길이 분포 확인하기\", \"6) 단어와 길이 분포 확인하기\\n긍정 리뷰에는 주로 어떤 단어들이 많이 등장하고, 부정 리뷰에는 주로 어떤 단어들이 등장하는지 두 가지 경우에 대해서 각 단어의 빈도수를 계산해보겠습니다. 각 레이블에 따라서 별도로 단어들의 리스트를 저장해줍니다.\\nnegative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\\npositive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)\\nCounter()를 사용하여 각 단어에 대한 빈도수를 카운트합니다. 우선 부정 리뷰에 대해서 빈도수가 높은 상위 20개 단어들을 출력합니다.\\nnegative_word_count = Counter(negative_words)\\nprint(negative_word_count.most_common(20))\", \"negative_word_count = Counter(negative_words)\\nprint(negative_word_count.most_common(20))\\n[('안', 8129), ('없', 7141), ('는데', 5786), ('있', 5692), ('같', 4247), ('로', 4083), ('할', 3920), ('거', 3902), ('나', 3805), ('해', 3653), ('너무', 3522), ('으로', 3351), ('기', 3348), ('했', 3265), ('어', 3143), ('보', 2987), ('습니다', 2962), ('것', 2935), ('지만', 2911), ('좋', 2899)]\\n긍정 리뷰에 대해서도 동일하게 출력해봅시다.\\npositive_word_count = Counter(positive_words)\\nprint(positive_word_count.most_common(20))\", \"positive_word_count = Counter(positive_words)\\nprint(positive_word_count.most_common(20))\\n[('있', 9987), ('좋', 6542), ('습니다', 5179), ('재밌', 4997), ('할', 4838), ('지만', 4809), ('해', 4354), ('없', 4145), ('보', 3907), ('으로', 3900), ('로', 3879), ('수', 3835), ('는데', 3825), ('기', 3592), ('안', 3368), ('것', 3362), ('같', 3356), ('네요', 3189), ('어', 3112), ('나', 3055)]\\n두 가지 경우에 대해서 각각 길이 분포를 확인해봅시다.\\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\", \"두 가지 경우에 대해서 각각 길이 분포를 확인해봅시다.\\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\\ntext_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\\nax1.hist(text_len, color='red')\\nax1.set_title('Positive Reviews')\\nax1.set_xlabel('length of samples')\\nax1.set_ylabel('number of samples')\\nprint('긍정 리뷰의 평균 길이 :', np.mean(text_len))\\ntext_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\\nax2.hist(text_len, color='blue')\\nax2.set_title('Negative Reviews')\", \"ax2.hist(text_len, color='blue')\\nax2.set_title('Negative Reviews')\\nfig.suptitle('Words in texts')\\nax2.set_xlabel('length of samples')\\nax2.set_ylabel('number of samples')\\nprint('부정 리뷰의 평균 길이 :', np.mean(text_len))\\nplt.show()\\n긍정 리뷰의 평균 길이 : 14.948459100231734\\n부정 리뷰의 평균 길이 : 15.284193065068493\\n[이미지: ]\\n유의미한 차이가 있는 것 같지는 않습니다.\\nX_train = train_data['tokenized'].values\\ny_train = train_data['label'].values\\nX_test= test_data['tokenized'].values\\ny_test = test_data['label'].values\\n7) 정수 인코딩\", \"X_test= test_data['tokenized'].values\\ny_test = test_data['label'].values\\n7) 정수 인코딩\\n기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. 우선, 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(X_train)\\n단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. 이는 tokenizer.word_index를 출력하여 확인 가능합니다. 등장 횟수가 1회인 단어들은 자연어 처리에서 배제하고자 합니다. 이 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다.\\nthreshold = 2\\ntotal_cnt = len(tokenizer.word_index) # 단어의 수\", \"threshold = 2\\ntotal_cnt = len(tokenizer.word_index) # 단어의 수\\nrare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\\ntotal_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\\nrare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\\n# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\\nfor key, value in tokenizer.word_counts.items():\\ntotal_freq = total_freq + value\\n# 단어의 등장 빈도수가 threshold보다 작으면\\nif(value < threshold):\\nrare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint('단어 집합(vocabulary)의 크기 :',total_cnt)\", 'rare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint(\\'단어 집합(vocabulary)의 크기 :\\',total_cnt)\\nprint(\\'등장 빈도가 %s번 이하인 희귀 단어의 수: %s\\'%(threshold - 1, rare_cnt))\\nprint(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\\nprint(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\\n단어 집합(vocabulary)의 크기 : 32817\\n등장 빈도가 1번 이하인 희귀 단어의 수: 13878\\n단어 집합에서 희귀 단어의 비율: 42.28905750068562\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.2254607619437832', \"단어 집합에서 희귀 단어의 비율: 42.28905750068562\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.2254607619437832\\n단어가 약 32,000개가 존재합니다. 등장 빈도가 threshold 값인 2회 미만. 즉, 1회인 단어들은 단어 집합에서 약 42%를 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 매우 적은 수치인 약 1.2%밖에 되지 않습니다. 아무래도 등장 빈도가 1회인 단어들은 자연어 처리에서 별로 중요하지 않을 듯 합니다. 그래서 이 단어들은 정수 인코딩 과정에서 배제시키겠습니다. 등장 빈도수가 1인 단어들의 수를 제외한 단어의 개수를 단어 집합의 최대 크기로 제한하겠습니다.\\n# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\\n# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\\nvocab_size = total_cnt - rare_cnt + 2\\nprint('단어 집합의 크기 :',vocab_size)\", \"vocab_size = total_cnt - rare_cnt + 2\\nprint('단어 집합의 크기 :',vocab_size)\\n단어 집합의 크기 : 18941\\n단어 집합의 크기는 18,941개입니다. 이를 토크나이저의 인자로 넘겨주면, 토크나이저는 텍스트 시퀀스를 숫자 시퀀스로 변환합니다. 이러한 정수 인코딩 과정에서 이보다 큰 숫자가 부여된 단어들은 OOV로 변환하겠습니다.\\ntokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\\ntokenizer.fit_on_texts(X_train)\\nX_train = tokenizer.texts_to_sequences(X_train)\\nX_test = tokenizer.texts_to_sequences(X_test)\\n정수 인코딩이 진행되었는지 확인하고자 X_train과 X_test에 대해서 상위 3개의 샘플만 출력합니다.\\nprint(X_train[:3])\", '정수 인코딩이 진행되었는지 확인하고자 X_train과 X_test에 대해서 상위 3개의 샘플만 출력합니다.\\nprint(X_train[:3])\\n[[495, 7, 35, 87, 149, 2429, 599, 26, 8, 70, 47, 235, 111, 38, 44, 52], [161, 300, 18, 20, 63, 3582, 985, 6, 56], [7, 17, 1476, 4]]\\nprint(X_test[:3])\\n[[728, 34, 16, 431, 52, 106, 132, 99, 6461, 453], [4527, 687, 835, 712, 792, 108, 4, 1779, 95, 370, 3518, 81, 558, 1904, 4189, 262, 169, 61, 25, 363, 35, 87, 974, 19, 6294, 6422], [1792, 806, 685, 49, 23, 349]]\\n8) 패딩', \"8) 패딩\\n서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행해보겠습니다. 전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포를 알아보겠습니다.\\nprint('리뷰의 최대 길이 :',max(len(review) for review in X_train))\\nprint('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\\nplt.hist([len(review) for review in X_train], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n리뷰의 최대 길이 : 64\\n리뷰의 평균 길이 : 15.115951894712957\\n리뷰의 최대 길이는 64, 평균 길이는 약 15입니다. 그래프로 봤을 때, 전체적으로는 60이하의 길이를 가지는 것으로 보입니다.\", \"리뷰의 최대 길이는 64, 평균 길이는 약 15입니다. 그래프로 봤을 때, 전체적으로는 60이하의 길이를 가지는 것으로 보입니다.\\ndef below_threshold_len(max_len, nested_list):\\ncount = 0\\nfor sentence in nested_list:\\nif(len(sentence) <= max_len):\\ncount = count + 1\\nprint('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\\n최대 길이가 64이므로 만약 60으로 패딩할 경우, 몇 개의 샘플들을 온전히 보전할 수 있는지 확인합니다.\\nmax_len = 60\\nbelow_threshold_len(max_len, X_train)\\n전체 샘플 중 길이가 60 이하인 샘플의 비율: 99.99599567532935\", 'max_len = 60\\nbelow_threshold_len(max_len, X_train)\\n전체 샘플 중 길이가 60 이하인 샘플의 비율: 99.99599567532935\\n훈련용 리뷰의 99.99%가 60이하의 길이를 가집니다. 훈련용 리뷰를 길이 60으로 패딩하겠습니다.\\nX_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)']\n",
      "['하이퍼파라미터, EarlyStopping과 ModelCheckpoint와 같은 모델에 대한 상세 코드는 네이버 영화 리뷰 분류와 네이버 쇼핑 리뷰 분류 때와 크게 다르지 않습니다. 하지만 이번에는 양방향 LSTM을 사용했다는 점에서 앞선 실습들과 차이를 보입니다. LSTM이 Bidirectional( ) 안에 기재되었다는 사실에 주목합시다.\\nimport re\\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.models import load_model\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nembedding_dim = 100\\nhidden_units = 128\\nmodel = Sequential()', \"embedding_dim = 100\\nhidden_units = 128\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(Bidirectional(LSTM(hidden_units))) # Bidirectional LSTM을 사용\\nmodel.add(Dense(1, activation='sigmoid'))\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\", 'model.compile(optimizer=\\'rmsprop\\', loss=\\'binary_crossentropy\\', metrics=[\\'acc\\'])\\nhistory = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)\\n저자의 경우 에포크 7에서 조기 종료가 발생했습니다.\\nloaded_model = load_model(\\'best_model.h5\\')\\nprint(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\\n781/781 [==============================] - 3s 4ms/step - loss: 0.4534 - acc: 0.7893\\n테스트 정확도: 0.7893']\n",
      "[\"임의의 문장에 대한 예측을 위해서는 학습하기 전 전처리를 동일하게 적용해줍니다.\\ndef sentiment_predict(new_sentence):\\nnew_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\\nnew_sentence = mecab.morphs(new_sentence) # 토큰화\\nnew_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\\nencoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\\npad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\\nscore = float(loaded_model.predict(pad_new)) # 예측\\nif(score > 0.5):\", 'score = float(loaded_model.predict(pad_new)) # 예측\\nif(score > 0.5):\\nprint(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\\nelse:\\nprint(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))\\nsentiment_predict(\\'노잼 ..완전 재미 없음 ㅉㅉ\\')\\n93.66% 확률로 부정 리뷰입니다.\\nsentiment_predict(\\'조금 어렵지만 재밌음ㅋㅋ\\')\\n97.43% 확률로 긍정 리뷰입니다.\\nsentiment_predict(\\'케릭터가 예뻐서 좋아요\\')\\n92.49% 확률로 긍정 리뷰입니다.\\n==================================================\\n--- 11. NLP를 위한 합성곱 신경망(Convolution Neural Network) ---\\n마지막 편집일시 : 2022년 1월 2일 2:51 오후', '--- 11. NLP를 위한 합성곱 신경망(Convolution Neural Network) ---\\n마지막 편집일시 : 2022년 1월 2일 2:51 오후\\n==================================================\\n--- 11-01 합성곱 신경망(Convolution Neural Network) ---\\n합성곱 신경망(Convolutional Neural Network)은 이미지 처리에 탁월한 성능을 보이는 신경망입니다. 하지만 합성곱 신경망으로 텍스트 처리를 하기 위한 시도들이 있었고, 이번 챕터는 합성곱 신경망으로 어떻게 텍스트를 처리 하는지에 대해서 설명하기 위해 편성한 챕터입니다. 하지만 합성곱 신경망을 좀 더 정확하게 이해하기 위해 합성곱 신경망의 주요 목적인 이미지 처리에서의 합성곱 신경망부터 먼저 설명하겠습니다.', '합성곱 신경망은 크게 합성곱층과(Convolution layer)와 풀링층(Pooling layer)으로 구성됩니다. 아래의 그림은 합성곱 신경망의 일반적인 예를 보여줍니다.\\n[이미지: ]\\n(http://cs231n.github.io/convolutional-networks)\\n위 그림에서 CONV는 합성곱 연산을 의미하고, 합성곱 연산의 결과가 활성화 함수 ReLU를 지납니다. 이 두 과정을 합성곱층이라고 합니다. 그 후에 POOL이라는 구간을 지나는데 이는 풀링 연산을 의미하며 풀링층이라고 합니다. 이제부터 합성곱 연산과 풀링 연산의 의미에 대해서 학습해봅시다.']\n",
      "['합성곱 신경망은 이미지 처리에 탁월한 성능을 보이는 신경망입니다. 이미지 처리를 하기 위해서 앞서 배운 다층 퍼셉트론을 사용할 수는 있지만 한계가 있었습니다. 예를 들어, 알파벳 손글씨를 분류하는 어떤 문제가 있다고 해봅시다. 아래의 그림은 알파벳 Y를 비교적 정자로 쓴 손글씨와 다소 휘갈겨 쓴 손글씨 두 개를 2차원 텐서인 행렬로 표현한 것입니다.\\n[이미지: ]\\n사람이 보기에는 두 그림 모두 알파벳 Y로 손쉽게 판단이 가능하지만, 기계가 보기에는 각 픽셀마다 가진 값이 대부분 상이하므로 사실상 다른 값을 가진 입력입니다. 그런데 이미지라는 것은 위와 같이 같은 대상이라도 휘어지거나, 이동되었거나, 방향이 뒤틀렸거나 등 다양한 변형이 존재합니다. 다층 퍼셉트론은 몇 가지 픽셀만 값이 달라져도 민감하게 예측에 영향을 받는다는 단점이 있습니다.', '좀 더 구체적으로 보겠습니다. 위 손글씨를 다층 퍼셉트론으로 분류한다고 하면, 이미지를 1차원 텐서인 벡터로 변환하고 다층 퍼셉트론의 입력층으로 사용해야 합니다. 두번째 손글씨를 다층 퍼셉트론으로 분류하기 위해서 벡터로 바꾸면 다음과 같습니다.\\n[이미지: ]\\n1차원으로 변환된 결과는 사람이 보기에도 이게 원래 어떤 이미지였는지 알아보기가 어렵습니다. 이는 기계도 마찬가지 입니다. 위와 같이 결과는 변환 전에 가지고 있던 공간적인 구조(spatial structure) 정보가 유실된 상태입니다. 여기서 공간적인 구조 정보라는 것은 거리가 가까운 어떤 픽셀들끼리는 어떤 연관이 있고, 어떤 픽셀들끼리는 값이 비슷하거나 등을 포함하고 있습니다. 결국 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고, 이를 위해 합성곱 신경망을 사용합니다.']\n",
      "['이미지 처리의 기본적인 용어인 채널에 대해서 간단히 정의하겠습니다.\\n기계는 글자나 이미지보다 숫자. 다시 말해, 텐서를 더 잘 처리할 수 있습니다. 이미지는 (높이, 너비, 채널)이라는 3차원 텐서입니다. 여기서 높이는 이미지의 세로 방향 픽셀 수, 너비는 이미지의 가로 방향 픽셀 수, 채널은 색 성분을 의미합니다. 흑백 이미지는 채널 수가 1이며, 각 픽셀은 0부터 255 사이의 값을 가집니다. 아래는 28 × 28 픽셀의 손글씨 데이터를 보여줍니다.\\n[이미지: ]\\n위 손글씨 데이터는 흑백 이미지므로 채널 수가 1임을 고려하면 (28 × 28 × 1)의 크기를 가지는 3차원 텐서입니다. 그렇다면 흑백이 아니라 우리가 통상적으로 접하게 되는 컬러 이미지는 어떨까요? 컬러 이미지는 적색(Red), 녹색(Green), 청색(Blue) 채널 수가 3개입니다.\\n[이미지: ]', '[이미지: ]\\n하나의 픽셀은 세 가지 색깔, 삼원색의 조합으로 이루어집니다. 만약, 높이가 28, 너비가 28인 컬러 이미지가 있다면 이 이미지의 텐서는 (28 × 28 × 3)의 크기를 가지는 3차원 텐서입니다. 채널은 때로는 깊이(depth)라고도 합니다. 이 경우 이미지는 (높이, 너비, 깊이)라는 3차원 텐서로 표현된다고 말할 수 있을 겁니다.']\n",
      "['합성곱층은 합성곱 연산을 통해서 이미지의 특징을 추출하는 역할을 합니다. 우선, 합성곱 연산에 대해서 이해해봅시다. 합성곱은 영어로 컨볼루션이라고도 불리는데, 커널(kernel) 또는 필터(filter) 라는 $n × m$ 크기의 행렬로 $\\\\text{높이}(height) × \\\\text{너비}(width)$ 크기의 이미지를 처음부터 끝까지 겹치며 훑으면서 $n × m$ 크기의 겹쳐지는 부분의 각 이미지와 커널의 원소의 값을 곱해서 모두 더한 값을 출력으로 하는 것을 말합니다. 이때, 이미지의 가장 왼쪽 위부터 가장 오른쪽 아래까지 순차적으로 훑습니다.\\n커널(kernel)은 일반적으로 3 × 3 또는 5 × 5를 사용합니다.\\n예를 통해 이해해봅시다. 아래는 $3 × 3$ 크기의 커널로 $5 × 5$의 이미지 행렬에 합성곱 연산을 수행하는 과정을 보여줍니다. 한 번의 연산을 1 스텝(step)이라고 하였을 때, 합성곱 연산의 네번째 스텝까지 이미지와 식으로 표현해봤습니다.']\n",
      "['[이미지: ]\\n(1×1) + (2×0) + (3×1) + (2×1) + (1×0) + (0×1) + (3×0) + (0×1) + (1×0) = 6']\n",
      "['[이미지: ]\\n(2×1) + (3×0) + (4×1) + (1×1) + (0×0) + (1×1) + (0×0) + (1×1) + (1×0) = 9']\n",
      "['[이미지: ]\\n(3×1) + (4×0) + (5×1) + (0×1) + (1×0) + (2×1) + (1×0) + (1×1) + (0×0) = 11']\n",
      "['[이미지: ]\\n(2×1) + (1×0) + (0×1) + (3×1) + (0×0) + (1×1) + (1×0) + (4×1) + (1×0) = 10\\n위 연산을 총 9번의 스텝까지 마쳤다고 가정하였을 때, 최종 결과는 아래와 같습니다.\\n[이미지: ]\\n위와 같이 입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과를 특성 맵(feature map)이라고 합니다.\\n위의 예제에서는 커널의 크기가 3 × 3이었지만, 커널의 크기는 사용자가 정할 수 있습니다. 또한 커널의 이동 범위가 위의 예제에서는 한 칸이었지만, 이 또한 사용자가 정할 수 있습니다. 이러한 이동 범위를 스트라이드(stride)라고 합니다.\\n아래의 예제는 스트라이드가 2일 경우에 5 × 5 이미지에 합성곱 연산을 수행하는 3 × 3 커널의 움직임을 보여줍니다. 최종적으로 2 × 2의 크기의 특성 맵을 얻습니다.\\n[이미지: ]']\n",
      "['위의 예에서 5 × 5 이미지에 3 × 3의 커널로 합성곱 연산을 하였을 때, 스트라이드가 1일 경우에는 3 × 3의 특성 맵을 얻었습니다. 이와 같이 합성곱 연산의 결과로 얻은 특성 맵은 입력보다 크기가 작아진다는 특징이 있습니다. 만약, 합성곱 층을 여러개 쌓았다면 최종적으로 얻은 특성 맵은 초기 입력보다 매우 작아진 상태가 되버립니다. 합성곱 연산 이후에도 특성 맵의 크기가 입력의 크기와 동일하게 유지되도록 하고 싶다면 패딩(padding)을 사용하면 됩니다.\\n[이미지: ]\\n패딩은 (합성곱 연산을 하기 전에) 입력의 가장자리에 지정된 개수의 폭만큼 행과 열을 추가해주는 것을 말합니다. 좀 더 쉽게 설명하면 지정된 개수의 폭만큼 테두리를 추가합니다. 주로 값을 0으로 채우는 제로 패딩(zero padding)을 사용합니다. 위의 그림은 5 × 5 이미지에 1폭짜리 제로 패딩을 사용하여 위, 아래에 하나의 행을 좌, 우에 하나의 열을 추가한 모습을 보여줍니다.', '커널은 주로 3 × 3 또는 5 × 5를 사용한다고 언급한 바 있습니다. 만약 스트라이드가 1이라고 하였을 때, 3 × 3 크기의 커널을 사용한다면 1폭짜리 제로 패딩을 사용하고, 5 × 5 크기의 커널을 사용한다면 2폭 짜리 제로 패딩을 사용하면 입력과 특성 맵의 크기를 보존할 수 있습니다. 예를 들어 5 × 5 크기의 이미지에 1폭짜리 제로 패딩을 하면 7 × 7 이미지가 되는데, 여기에 3 × 3의 커널을 사용하여 1 스트라이드로 합성곱을 한 후의 특성 맵은 기존의 입력 이미지의 크기와 같은 5 × 5가 됩니다.']\n",
      "['합성곱 신경망에서의 가중치와 편향을 이해하기 위해서 우선 다층 퍼셉트론을 복습해보겠습니다.']\n",
      "['다층 퍼셉트론으로 3 × 3 이미지를 처리한다고 가정해보겠습니다. 우선 이미지를 1차원 텐서인 벡터로 만들면, 3 × 3 = 9가 되므로 입력층은 9개의 뉴런을 가집니다. 그리고 4개의 뉴런을 가지는 은닉층을 추가한다고 해보겠습니다. 이는 아래의 그림과 같습니다.\\n[이미지: ]\\n위에서 각 연결선은 가중치를 의미하므로, 위의 그림에서는 9 × 4 = 36개의 가중치를 가집니다. 비교를 위해 합성곱 신경망으로 3 × 3 이미지를 처리한다고 해보겠습니다. 2 × 2 커널을 사용하고, 스트라이드는 1로 합니다. (*는 합성곱 연산을 의미합니다.)\\n[이미지: ]\\n사실 합성곱 신경망에서 가중치는 커널 행렬의 원소들입니다. 이를 인공 신경망의 형태로 표현한다면 다음과 같이 표현할 수 있습니다.\\n[이미지: ]', '[이미지: ]\\n사실 합성곱 신경망에서 가중치는 커널 행렬의 원소들입니다. 이를 인공 신경망의 형태로 표현한다면 다음과 같이 표현할 수 있습니다.\\n[이미지: ]\\n최종적으로 특성 맵을 얻기 위해서는 동일한 커널로 이미지 전체를 훑으며 합성곱 연산을 진행합니다. 결국 이미지 전체를 훑으면서 사용되는 가중치는 $w_{0}$, $w_{1}$, $w_{2}$, $w_{3}$ 4개 뿐입니다. 그리고 각 합성곱 연산마다 이미지의 모든 픽셀을 사용하는 것이 아니라, 커널과 맵핑되는 픽셀만을 입력으로 사용하는 것을 볼 수 있습니다. 결국 합성곱 신경망은 다층 퍼셉트론을 사용할 때보다 훨씬 적은 수의 가중치를 사용하며 공간적 구조 정보를 보존한다는 특징이 있습니다.', '다층 퍼셉트론의 은닉층에서는 가중치 연산 후에 비선형성을 추가하기 위해서 활성화 함수를 통과시켰습니다. 합성곱 신경망의 은닉층에서도 마찬가지입니다. 합성곱 연산을 통해 얻은 특성 맵은 다층 퍼셉트론때와 마찬가지로 비선형성 추가를 위해서 활성화 함수를 지나게 됩니다. 이때 렐루 함수나 렐루 함수의 변형들이 주로 사용됩니다. 이와 같이 합성곱 연산을 통해서 특성 맵을 얻고, 활성화 함수를 지나는 연산을 하는 합성곱 신경망의 층을 합성곱 신경망에서는 합성곱 층(convolution layer)이라고 합니다.']\n",
      "['[이미지: ]\\n합성곱 신경망에도 편향(bias)를 당연히 추가할 수 있습니다. 만약, 편향을 사용한다면 커널을 적용한 뒤에 더해집니다. 편향은 하나의 값만 존재하며, 커널이 적용된 결과의 모든 원소에 더해집니다.']\n",
      "['입력의 크기와 커널의 크기, 그리고 스트라이드의 값만 알면 합성곱 연산의 결과인 특성 맵의 크기를 계산할 수 있습니다.\\n$I_{h}$ : 입력의 높이\\n$I_{w}$ : 입력의 너비\\n$K_{h}$ : 커널의 높이\\n$K_{w}$ : 커널의 너비\\n$S$ : 스트라이드\\n$O_{h}$ : 특성 맵의 높이\\n$O_{w}$ : 특성 맵의 너비\\n이에 따라 특성 맵의 높이와 너비는 다음과 같습니다.\\n$$O_{h} = floor(\\\\frac{I_{h} - K_{h}}{S}+1)$$\\n$$O_{w} = floor(\\\\frac{I_{w} - K_{w}}{S}+1)$$', '$$O_{h} = floor(\\\\frac{I_{h} - K_{h}}{S}+1)$$\\n$$O_{w} = floor(\\\\frac{I_{w} - K_{w}}{S}+1)$$\\n여기서 $floor$ 함수는 소수점 발생 시 소수점 이하를 버리는 역할을 합니다. 예를 들어 위의 첫번째 예제의 경우 5 × 5 크기의 이미지에 3 × 3 커널을 사용하고 스트라이드 1로 합성곱 연산을 했습니다. 이 경우 특성 맵의 크기는  (5 - 3 + 1 ) × (5 - 3 + 1) = 3 × 3임을 알 수 있습니다. 이는 또한 총 9번의 스텝이 필요함을 의미하기도 합니다.\\n패딩의 폭을 $P$라고 하고, 패딩까지 고려한 식은 다음과 같습니다.\\n$$O_{h} = floor(\\\\frac{I_{h} - K_{h} + 2P}{S}+1)$$\\n$$O_{w} = floor(\\\\frac{I_{w} - K_{w} + 2P}{S}+1)$$']\n",
      "[\"지금까지는 채널(channel) 또는 깊이(depth)를 고려하지 않고, 2차원 텐서를 가정하고 설명했습니다. 하지만 실제로 합성곱 연산의 입력은 '다수의 채널을 가진' 이미지 또는 이전 연산의 결과로 나온 특성 맵일 수 있습니다. 만약, 다수의 채널을 가진 입력 데이터를 가지고 합성곱 연산을 한다고 하면 커널의 채널 수도 입력의 채널 수만큼 존재해야 합니다. 다시 말해 입력 데이터의 채널 수와 커널의 채널 수는 같아야 합니다. 채널 수가 같으므로 합성곱 연산을 채널마다 수행합니다. 그리고 그 결과를 모두 더하여 최종 특성 맵을 얻습니다.\\n[이미지: ]\", '[이미지: ]\\n위 그림은 3개의 채널을 가진 입력 데이터와 3개의 채널을 가진 커널의 합성곱 연산을 보여줍니다. 커널의 각 채널끼리의 크기는 같아야 합니다. 각 채널 간 합성곱 연산을 마치고, 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵을 만듭니다. 주의할 점은 위의 연산에서 사용되는 커널은 3개의 커널이 아니라 3개의 채널을 가진 1개의 커널이라는 점입니다.\\n위 그림은 높이 3, 너비 3, 채널 3의 입력이 높이 2, 너비 2, 채널 3의 커널과 합성곱 연산을 하여 높이 2, 너비 2, 채널 1의 특성 맵을 얻는다는 의미입니다. 합성곱 연산의 결과로 얻은 특성 맵의 채널 차원은 RGB 채널 등과 같은 컬러의 의미를 담고 있지는 않습니다. 이 연산에서 각 차원을 변수로 두고 좀 더 일반화시켜보겠습니다.']\n",
      "['일반화를 위해 사용하는 각 변수가 의미하는 바는 다음과 같습니다.\\n$I_{h}$ : 입력의 높이\\n$I_{w}$ : 입력의 너비\\n$K_{h}$ : 커널의 높이\\n$K_{w}$ : 커널의 너비\\n$O_{h}$ : 특성 맵의 높이\\n$O_{w}$ : 특성 맵의 너비\\n$C_{i}$ : 입력 데이터의 채널\\n다음은 3차원 텐서의 합성곱 연산을 보여줍니다.\\n[이미지: ]\\n높이 $I_{h}$, 너비 $I_{w}$, 채널 $C_{i}$의 입력 데이터는 동일한 채널 수 $C_{i}$를 가지는 높이 $K_{h}$, 너비 $K_{w}$의 커널과 합성곱 연산을 하여 높이 $O_{h}$, 너비 $O_{w}$, 채널 1의 특성 맵을 얻습니다. 그런데 하나의 입력에 여러 개의 커널을 사용하는 합성곱 연산을 할 수도 있습니다.\\n합성곱 연산에서 다수의 커널을 사용할 경우, 특성 맵의 크기가 어떻게 바뀌는지 봅시다. 다음은 $C_{o}$를 합성곱 연산에 사용하는 커널의 수라고 하였을 때의 합성곱 연산 과정을 보여줍니다.', '[이미지: ]\\n합성곱 연산에서 다수의 커널을 사용할 경우, 사용한 커널 수는 합성곱 연산의 결과로 나오는 특성 맵의 채널 수가 됩니다.\\n이를 이해했다면 커널의 크기와 입력 데이터의 채널 수 $C_{i}$와 특성 맵(출력 데이터)의 채널 수 $C_{o}$가 주어졌을 때, 가중치 매개변수의 총 개수를 구할 수 있습니다. 가중치는 커널의 원소들이므로 하나의 커널의 하나의 채널은 $K_{i}$ × $K_{o}$개의 매개변수를 가지고 있습니다. 그런데 합성곱 연산을 하려면 커널은 입력 데이터의 채널 수와 동일한 채널 수를 가져야 합니다. 이에 따라 하나의 커널이 가지는 매개변수의 수는  $K_{i}$ × $K_{o}$ × $C_{i}$입니다. 그런데 이러한 커널이 총 $C_{o}$개가 있어야 하므로 가중치 매개변수의 총 수는 다음과 같습니다.\\n가중치 매개변수의 총 수 : $K_{i}$ × $K_{o}$ × $C_{i}$ × $C_{o}$']\n",
      "['일반적으로 합성곱 층(합성곱 연산 + 활성화 함수) 다음에는 풀링 층을 추가하는 것이 일반적입니다. 풀링 층에서는 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄이는 풀링 연산이 이루어집니다. 풀링 연산에는 일반적으로 최대 풀링(max pooling)과 평균 풀링(average pooling)이 사용됩니다. 우선 최대 풀링을 통해서 풀링 연산을 이해해봅시다.\\n[이미지: ]\\n풀링 연산에서도 합성곱 연산과 마찬가지로 커널과 스트라이드의 개념을 가집니다. 위의 그림은 스트라이드가 2일 때, 2 × 2 크기 커널로 맥스 풀링 연산을 했을 때 특성맵이 절반의 크기로 다운샘플링되는 것을 보여줍니다. 맥스 풀링은 커널과 겹치는 영역 안에서 최대값을 추출하는 방식으로 다운샘플링합니다.', \"다른 풀링 기법인 평균 풀링은 최대값을 추출하는 것이 아니라 평균값을 추출하는 연산이 됩니다. 풀링 연산은 커널과 스트라이드 개념이 존재한다는 점에서 합성곱 연산과 유사하지만, 합성곱 연산과의 차이점은 학습해야 할 가중치가 없으며 연산 후에 채널 수가 변하지 않는다는 점입니다.\\n==================================================\\n--- 11-02 자연어 처리를 위한 1D CNN(1D Convolutional Neural Networks) ---\\n```\\nmodel = Sequential()\\nmodel.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\\nmodel.add(GlobalMaxPooling1D())\\n```합성곱 신경망을 자연어 처리에서 사용하기 위한 1D CNN을 이해해보겠습니다.\"]\n",
      "['앞서 합성곱 신경망을 설명하며 합성곱 연산을 다음과 같이 정의했습니다.\\n합성곱 연산이란 커널(kernel) 또는 필터(filter) 라는 n × m 크기의 행렬로 높이(height) × 너비(width) 크기의 이미지를 처음부터 끝까지 겹치며 훑으면서 n × m 크기의 겹쳐지는 부분의 각 이미지와 커널의 원소의 값을 곱해서 모두 더한 값을 출력으로 하는 것을 말합니다. 이때, 이미지의 가장 왼쪽 위부터 가장 오른쪽 아래까지 순차적으로 훑습니다.\\n위와 같은 이미지 처리에서의 합성곱 연산을 2D 합성곱 연산이라고 부릅니다.']\n",
      "[\"자연어 처리에 사용되는 1D 합성곱 연산을 정리해봅시다. LSTM을 이용한 여러 실습을 상기해보면, 각 문장은 임베딩 층(embedding layer)을 지나서 각 단어가 임베딩 벡터가 된 상태로 LSTM의 입력이 되었습니다. 이는 1D 합성곱 연산에서도 마찬가지입니다. 1D 합성곱 연산에서도 입력이 되는 것은 각 단어가 벡터로 변환된 문장 행렬로 LSTM과 입력을 받는 형태는 동일합니다.\\n'wait for the video and don't rent it'이라는 문장이 있을 때, 이 문장이 토큰화, 패딩, 임베딩 층(Embedding layer)을 거친다면 다음과 같은 문장 형태의 행렬로 변환될 것입니다. 아래 그림에서 $n$은 문장의 길이, $k$는 임베딩 벡터의 차원입니다.\\n[이미지: ]\", '[이미지: ]\\n그리고 이 행렬이 만약 LSTM의 입력으로 주어진다면, LSTM은 첫번째 시점에는 첫번째 행을 입력으로 받고, 두번째 시점에는 두번째 행을 입력으로 받으며 순차적으로 단어를 처리합니다. 그렇다면 1D 합성곱 연산의 경우에는 저 행렬을 어떻게 처리할까요?\\n1D 합성곱 연산에서 커널의 너비는 문장 행렬에서의 임베딩 벡터의 차원과 동일하게 설정됩니다. 그렇기 때문에 1D 합성곱 연산에서는 커널의 높이만으로 해당 커널의 크기라고 간주합니다. 가령, 커널의 크기가 2인 경우에는 아래의 그림과 같이 높이가 2, 너비가 임베딩 벡터의 차원인 커널이 사용됩니다.\\n[이미지: ]', \"[이미지: ]\\n커널의 너비가 임베딩 벡터의 차원이라는 의미는 커널이 2D 합성곱 연산때와는 달리 너비 방향으로는 더 이상 움직일 곳이 없다는 것을 의미합니다. 그래서 1D 합성곱 연산에서는 커널이 문장 행렬의 높이 방향으로만 움직이게 되어있습니다. 쉽게 설명하면, 위 그림에서 커널은 2D 합성곱 연산때와는 달리 오른쪽으로는 움직일 공간이 없으므로, 아래쪽으로만 이동해야 합니다.\\n한 번의 연산을 1 스텝(step)이라고 하였을 때, 합성곱 연산의 네번째 스텝까지 표현한 이미지는 다음과 같습니다. 크기가 2인 커널은 처음에는 'wait for'에 대해서 합성곱 연산을 하고, 두번째 스텝에는 'for the'에 대해서 연산을, 세번째 스텝에는 'the video'에 대해서 연산을, 네번째 스텝에서는 'video and'에 대해서 연산을 하게 됩니다.\\n[이미지: ]\", '[이미지: ]\\n이렇게 여덟번째 스텝까지 반복하였을 때, 결과적으로는 우측의 8차원 벡터를 1D 합성곱 연산의 결과로서 얻게될 것입니다. 그런데 커널의 크기가 꼭 2일 필요가 있을까요? 2D 합성곱 연산에서 커널의 크기가 3 × 3 또는 5 × 5 또는 등등의 여러 크기의 커널을 자유자재로 사용할 수 있었듯이, 1D 합성곱 연산에서도 커널의 크기는 사용자가 변경할 수 있습니다. 가령, 커널의 크기를 3으로 한다면, 네번째 스텝에서의 연산은 아래의 그림과 같을 것입니다.\\n[이미지: ]', '[이미지: ]\\n커널의 크기가 달라진다는 것은 어떤 의미가 있을까요? CNN에서의 커널은 신경망 관점에서는 가중치 행렬이므로 커널의 크기에 따라 학습하게 되는 파라미터의 수는 달라집니다. 1D 합성곱 연산과 자연어 처리 관점에서는 커널의 크기에 따라서 참고하는 단어의 묶음의 크기가 달라집니다. 이는 참고하는 n-gram이 달라진다고 볼 수 있습니다. 커널의 크기가 2라면 각 연산의 스텝에서 참고하는 것은 bigram입니다. 커널의 크기가 3이라면 각 연산의 스텝에서 참고하는 것은 trigram입니다.']\n",
      "['이미지 처리에서의 CNN에서 그랬듯이, 일반적으로 1D 합성곱 연산을 사용하는 1D CNN에서도 합성곱 층(합성곱 연산 + 활성화 함수) 다음에는 풀링 층을 추가하게됩니다. 그 중 대표적으로 사용되는 것이 맥스 풀링(Max-pooling)입니다. 맥스 풀링은 각 합성곱 연산으로부터 얻은 결과 벡터에서 가장 큰 값을 가진 스칼라 값을 빼내는 연산입니다.\\n아래의 그림은 크기가 2인 커널과 크기가 3인 커널 두 개의 커널로부터 각각 결과 벡터를 얻고, 각 벡터에서 가장 큰 값을 꺼내오는 맥스 풀링 연산을 보여줍니다.\\n[이미지: ]']\n",
      "['지금까지 배운 개념들을 가지고 텍스트 분류를 위한 CNN을 설계해봅시다. 우선, 설계하고자 하는 신경망은 이진 분류를 위한 신경망입니다. 단, 시그모이드 함수가 아니라 소프트맥스 함수를 사용할 것이므로 출력층에서 뉴런의 개수가 2인 신경망을 설계합니다.\\n[이미지: ]\\n커널은 크기가 4인 커널 2개, 3인 커널 2개, 2인 커널 2개를 사용합니다. 문장의 길이가 9인 경우, 합성곱 연산을 한 후에는 각각 6차원 벡터 2개, 7차원 벡터 2개, 8차원 벡터 2개를 얻습니다. 벡터가 6개므로 맥스 풀링을 한 후에는 6개의 스칼라 값을 얻는데, 일반적으로 이렇게 얻은 스칼라값들은 전부 연결(concatenate)하여 하나의 벡터로 만들어줍니다. 이렇게 얻은 벡터는 1D CNN을 통해서 문장으로부터 얻은 벡터입니다. 이를 뉴런이 2개인 출력층에 완전 연결시키므로서(Dense layer를 사용) 텍스트 분류를 수행합니다.']\n",
      "[\"케라스로 1D 합성곱 층을 추가하는 코드는 다음과 같습니다.\\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\\nmodel = Sequential()\\nmodel.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\\n각 인자에 대한 설명은 다음과 같습니다.\\nnum_filters = 커널의 개수.\\nkernel_size = 커널의 크기.\\npadding = 패딩 방법.\\n- valid : 패딩 없음. 제로 패딩없이 유효한(valid) 값만을 사용한다는 의미에서 valid.\\n- same : 합성곱 연산 후에 출력이 입력과 동일한 차원을 가지도록 왼쪽과 오른쪽(또는 위, 아래)에  제로 패딩을 추가.\\nactivation = 활성화 함수.\\n만약 위에서 설명한 맥스 풀링을 추가하고자 한다면 다음과 같이 코드를 작성할 수 있습니다.\", \"activation = 활성화 함수.\\n만약 위에서 설명한 맥스 풀링을 추가하고자 한다면 다음과 같이 코드를 작성할 수 있습니다.\\nmodel = Sequential()\\nmodel.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\\nmodel.add(GlobalMaxPooling1D())\\n==================================================\\n--- 11-03 1D CNN으로 IMDB 리뷰 분류하기 ---\\n```\\n25000/25000 [==============================] - 3s 3ms/step - loss: 0.5373 - acc: 0.8873\\n테스트 정확도: 0.8873\\n```1D CNN을 이용하여 IMDB 리뷰를 분류해보겠습니다. 이전에 다룬 데이터이므로 데이터에 대한 설명은 생략합니다.\"]\n",
      "['from tensorflow.keras import datasets\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\n최대 10,000개의 단어만을 허용하여 데이터를 로드합니다.\\nvocab_size = 10000\\n(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=vocab_size)\\nX_train을 상위 5개만 출력해봅시다.\\nprint(X_train[:5])\\n[list([1, 14, 22, 16, 43, 530, 973, 1622, ... 중략 ... 32])\\nlist([1, 194, 1153, 194, 8255, 78, ... 중략 ... 95])\\nlist([1, 14, 47, 8, 30, 31, ... 중략 ... 113])\\nlist([1, 4, 2, 2, 33, 2804, ... 중략 ... 2574])', \"list([1, 14, 47, 8, 30, 31, ... 중략 ... 113])\\nlist([1, 4, 2, 2, 33, 2804, ... 중략 ... 2574])\\nlist([1, 249, 1323, 7, 61, 113, ... 중략 ... 113])]\\n각 샘플의 길이가 긴 관계로 출력 시 중간 내용은 중략하였습니다. 각 샘플은 이미 정수 인코딩까지 전처리가 된 상태입니다. 하지만 각 샘플들의 길이는 서로 다르죠? 패딩을 진행하여 모든 샘플들의 길이를 200으로 맞춥니다.\\nmax_len = 200\\nX_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)\\n패딩이 되었는지 크기(shape)를 확인해봅시다.\\nprint('X_train의 크기(shape) :',X_train.shape)\\nprint('X_test의 크기(shape) :',X_test.shape)\", \"print('X_train의 크기(shape) :',X_train.shape)\\nprint('X_test의 크기(shape) :',X_test.shape)\\nX_train의 크기(shape) : (25000, 200)\\nX_test의 크기(shape) : (25000, 200)\\n훈련 데이터, 테스트 데이터 각 25,000 샘플이 전부 길이 200을 가지는 것을 확인할 수 있습니다. y_train도 출력해봅시다.\\nprint(y_train[:5])\\n[1 0 0 1 0]\\n1과 0으로 구성된 것을 확인하였습니다. 이진 분류를 수행할 것이므로 레이블에는 더 이상 전처리를 할 것이 없습니다.\"]\n",
      "['IMDB 리뷰 분류를 위한 1D CNN 모델을 설계해봅시다. 하이퍼파라미터인 임베딩 벡터의 차원은 256, 드롭 아웃 비율은 0.3, 커널의 크기는 3이며 해당 커널은 총 256개 사용합니다. 합성곱 층과 맥스풀링 연산 후 전결합층(Fully Connected Layer)을 은닉층을 추가로 사용했는데, 은닉층의 뉴런 수는 128입니다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델입니다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 20 에포크를 수행합니다.', \"EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 3회 증가하면 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미입니다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다. validation_data로는 X_test와 y_test를 사용합니다. val_loss가 줄어들다가 증가하는 상황이 오면 과적합으로 판단하기 위함입니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\", 'from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nfrom tensorflow.keras.models import load_model\\nembedding_dim = 256 # 임베딩 벡터의 차원\\ndropout_ratio = 0.3 # 드롭아웃 비율\\nnum_filters = 256 # 커널의 수\\nkernel_size = 3 # 커널의 크기\\nhidden_units = 128 # 뉴런의 수\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(Dropout(dropout_ratio))', \"model.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(Dropout(dropout_ratio))\\nmodel.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\\nmodel.add(GlobalMaxPooling1D())\\nmodel.add(Dense(hidden_units, activation='relu'))\\nmodel.add(Dropout(dropout_ratio))\\nmodel.add(Dense(1, activation='sigmoid'))\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\", 'model.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\', metrics=[\\'acc\\'])\\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])\\n저자의 경우 에포크 4에서 조기 종료되었습니다. 저장된 모델을 로드하여 테스트 정확도를 확인합니다.\\nloaded_model = load_model(\\'best_model.h5\\')\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\\n25000/25000 [==============================] - 3s 3ms/step - loss: 0.5373 - acc: 0.8873\\n테스트 정확도: 0.8873\\n테스트 데이터에서 88.73%의 정확도를 얻습니다.', '테스트 정확도: 0.8873\\n테스트 데이터에서 88.73%의 정확도를 얻습니다.\\n==================================================\\n--- 11-04 1D CNN으로 스팸 메일 분류하기 ---\\n```\\n테스트 정확도: 0.9797\\n```1D CNN을 이용하여 스팸 메일을 분류해보겠습니다. 이전에 다룬 데이터이므로 데이터에 대한 설명은 생략합니다.']\n",
      "['모든 전처리는 11-2. RNN을 이용한 텍스트 분류 챕터의 스팸 메일 분류하기(Spam Detection)( https://wikidocs.net/22894 )와 동일하게 수행하였다고 가정합니다.']\n",
      "['하이퍼파라미터인 임베딩 벡터의 차원은 32, 드롭 아웃 비율은 0.3, 커널의 크기는 5이며 해당 커널은 총 32개 사용합니다. 합성곱 층과 맥스풀링 연산 후 출력층으로 연결됩니다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델입니다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 하이퍼파라미터인 배치 크기는 64이며, 10 에포크를 수행합니다.', \"EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 3회 증가하면 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미입니다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다. validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\", \"from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nembedding_dim = 32\\ndropout_ratio = 0.3\\nnum_filters = 32\\nkernel_size = 5\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim))\\nmodel.add(Dropout(dropout_ratio))\\nmodel.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\", \"model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\\nmodel.add(GlobalMaxPooling1D())\\nmodel.add(Dropout(dropout_ratio))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\\nmc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode='max', verbose=1, save_best_only=True)\", 'history = model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[es, mc])\\n테스트 데이터에 대해서 정확도를 확인해보겠습니다.\\nX_test_encoded = tokenizer.texts_to_sequences(X_test)\\nX_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))\\n테스트 정확도: 0.9797\\n==================================================\\n--- 11-05 Multi-Kernel 1D CNN으로 네이버 영화 리뷰 분류하기 ---\\n```\\n85.93% 확률로 긍정 리뷰입니다.', '--- 11-05 Multi-Kernel 1D CNN으로 네이버 영화 리뷰 분류하기 ---\\n```\\n85.93% 확률로 긍정 리뷰입니다.\\n```다양한 크기의 커널들을 혼합하여 네이버 영화 리뷰를 분류해보겠습니다.']\n",
      "['모든 전처리는 RNN을 이용한 텍스트 분류 챕터의 네이버 영화 리뷰 분류하기( https://wikidocs.net/44249 )와 동일하게 수행하였다고 가정합니다.']\n",
      "['케라스에서 다수의 커널을 사용할 경우에는 Funtional API를 사용하여 구현합니다. 우선 필요한 도구들을 임포트합니다. 하이퍼파라미터인 임베딩 벡터의 차원은 128, 드롭아웃 비율은 0.5와 0.8 두 가지를 사용합니다. 각 커널의 개수는 128개를 사용하고, 전결합층(Fully Connected Layer)을 은닉층을 추가로 사용했는데, 은닉층의 뉴런 수는 128입니다.\\nfrom tensorflow.keras.models import Sequential, Model\\nfrom tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint', 'from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nfrom tensorflow.keras.models import load_model\\nembedding_dim = 128\\ndropout_ratio = (0.5, 0.8)\\nnum_filters = 128\\nhidden_units = 128\\n입력 층과 임베딩 층을 정의합니다. 임베딩 층 이후에는 드롭아웃의 인자값이 0.5. 즉, 50% 드롭아웃을 해주었습니다.\\nmodel_input = Input(shape = (max_len,))\\nz = Embedding(vocab_size, embedding_dim, input_length = max_len, name=\"embedding\")(model_input)\\nz = Dropout(dropout_ratio[0])(z)\\n3, 4, 5의 크기를 가지는 커널을 각각 128개 사용합니다. 그리고 이들을 맥스풀링합니다.', 'z = Dropout(dropout_ratio[0])(z)\\n3, 4, 5의 크기를 가지는 커널을 각각 128개 사용합니다. 그리고 이들을 맥스풀링합니다.\\nconv_blocks = []\\nfor sz in [3, 4, 5]:\\nconv = Conv1D(filters = num_filters,\\nkernel_size = sz,\\npadding = \"valid\",\\nactivation = \"relu\",\\nstrides = 1)(z)\\nconv = GlobalMaxPooling1D()(conv)\\nconv_blocks.append(conv)', 'activation = \"relu\",\\nstrides = 1)(z)\\nconv = GlobalMaxPooling1D()(conv)\\nconv_blocks.append(conv)\\n각 맥스풀링한 결과를 연결(concatenate)합니다. 그리고 이를 전결합층(Fully Connected Layer)을 사용한 은닉층으로 전달합니다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델입니다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 하이퍼파라미터인 배치 크기는 64이며, 10 에포크를 수행합니다.', \"EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미입니다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다. validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\\nz = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\", 'z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\\nz = Dropout(dropout_ratio[1])(z)\\nz = Dense(hidden_units, activation=\"relu\")(z)\\nmodel_output = Dense(1, activation=\"sigmoid\")(z)\\nmodel = Model(model_input, model_output)\\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\\nes = EarlyStopping(monitor=\\'val_loss\\', mode=\\'min\\', verbose=1, patience=4)\\nmc = ModelCheckpoint(\\'CNN_model.h5\\', monitor=\\'val_acc\\', mode=\\'max\\', verbose=1, save_best_only=True)', 'mc = ModelCheckpoint(\\'CNN_model.h5\\', monitor=\\'val_acc\\', mode=\\'max\\', verbose=1, save_best_only=True)\\nmodel.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.2, verbose=2, callbacks=[es, mc])\\n학습 후 저장한 모델을 로드하여 테스트 데이터에 대해서 평가합니다.\\nloaded_model = load_model(\\'CNN_model.h5\\')\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\\n테스트 정확도: 0.8430\\n84%의 정확도를 얻었습니다.']\n",
      "[\"RNN을 이용한 텍스트 분류 챕터의 네이버 영화 리뷰 분류하기 실습( https://wikidocs.net/44249 )에서 사용한 동일한 예측 함수 sentiment_predict를 사용했습니다.\\nsentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')\\n93.73% 확률로 긍정 리뷰입니다.\\nsentiment_predict('이 영화 핵노잼 ㅠㅠ')\\n97.03% 확률로 부정 리뷰입니다.\\nsentiment_predict('이딴게 영화냐 ㅉㅉ')\\n97.18% 확률로 부정 리뷰입니다.\\nsentiment_predict('감독 뭐하는 놈이냐?')\\n82.89% 확률로 부정 리뷰입니다.\\nsentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')\\n85.93% 확률로 긍정 리뷰입니다.\\n==================================================\", '85.93% 확률로 긍정 리뷰입니다.\\n==================================================\\n--- 11-06 사전 훈련된 워드 임베딩을 이용한 의도 분류(Intent Classification using Pre-trained Word Embedding) ---\\n```\\n정확도(Accuracy) : 0.99\\n```의도 분류(Intent Classification)는 개체명 인식(Named Entity Recognition)과 더불어 챗봇(Chatbot)의 중요 모듈로서 사용될 수 있습니다. 사전 훈련된 워드 임베딩(Pre-traiend word embedding)을 입력으로 의도 분류를 수행해봅시다. 의도 분류 실습은 결국 텍스트 분류입니다.']\n",
      "['import os\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport urllib.request\\nfrom sklearn import preprocessing\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\nfrom sklearn.metrics import classification_report\\n깃허브로부터 의도 데이터를 다운로드하여 데이터프레임에 로드합니다.', 'from sklearn.metrics import classification_report\\n깃허브로부터 의도 데이터를 다운로드하여 데이터프레임에 로드합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/11.%201D%20CNN%20Text%20Classification/dataset/intent_train_data.csv\", filename=\"intent_train_data.csv\")\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/11.%201D%20CNN%20Text%20Classification/dataset/intent_test_data.csv\", filename=\"intent_test_data.csv\")', \"train_data = pd.read_csv('intent_train_data.csv')\\ntest_data = pd.read_csv('intent_test_data.csv')\\n훈련 데이터를 로드한 데이터프레임을 출력해봅시다.\\ntrain_data\\n[이미지: ]\\n테스트 데이터를 로드한 데이터프레임을 출력해봅시다.\\ntest_data\\n[이미지: ]\\n훈련 데이터와 테스트 데이터 그리고 레이블을 리스트로 저장하고 샘플 수를 출력해봅시다.\\nintent_train = train_data['intent'].tolist()\\nlabel_train = train_data['label'].tolist()\\nintent_test = test_data['intent'].tolist()\\nlabel_test = test_data['label'].tolist()\\nprint('훈련용 문장의 수 :', len(intent_train))\\nprint('훈련용 레이블의 수 :', len(label_train))\", \"print('훈련용 문장의 수 :', len(intent_train))\\nprint('훈련용 레이블의 수 :', len(label_train))\\nprint('테스트용 문장의 수 :', len(intent_test))\\nprint('테스트용 레이블의 수 :', len(label_test))\\n훈련용 문장의 수 : 11784\\n훈련용 레이블의 수 : 11784\\n테스트용 문장의 수 : 600\\n테스트용 레이블의 수 : 600\\n훈련 데이터의 상위 5개 샘플과 레이블을 출력해봅시다.\\nprint(intent_train[:5])\\nprint(label_train[:5])\", \"테스트용 레이블의 수 : 600\\n훈련 데이터의 상위 5개 샘플과 레이블을 출력해봅시다.\\nprint(intent_train[:5])\\nprint(label_train[:5])\\n['add another song to the cita rom ntica playlist', 'add clem burke in my playlist pre party r b jams', 'add live from aragon ballroom to trapeo', 'add unite and win to my night out', 'add track to my digster future hits']\\n['AddToPlaylist', 'AddToPlaylist', 'AddToPlaylist', 'AddToPlaylist', 'AddToPlaylist']\", \"['AddToPlaylist', 'AddToPlaylist', 'AddToPlaylist', 'AddToPlaylist', 'AddToPlaylist']\\n첫번째 샘플을 보면 'add another song to the cita rom ntica playlist'라는 문장의 레이블은 'AddToPlaylist'입니다. 이 문장의 의도는 이 곡을 플레이리스트에 추가해줘라는 의도입니다. 그 외에도 어떤 종류의 범주가 있는지 보려면 정확하게 인덱스를 2000씩 +하면서 출력해보면 됩니다.\\nprint(intent_train[2000:2002])\\nprint(label_train[2000:2002])\\n['please book reservations for 3 people at a restaurant in alderwood manor', 'book a table in mt for 3 for now at a pub that serves south indian']\", '[\\'BookRestaurant\\', \\'BookRestaurant\\']\\nprint(intent_train[4000:4002])\\nprint(label_train[4000:4002])\\n[\\'what will the weather be like on feb 8 , 2034 in cedar mountain wilderness\\', \"tell me the forecast in the same area here on robert e lee \\'s birthday\"]\\n[\\'GetWeather\\', \\'GetWeather\\']\\nprint(intent_train[6000:6002])\\nprint(label_train[6000:6002])\\n[\\'rate the current album one points\\', \\'i give a zero rating for this essay\\']\\n[\\'RateBook\\', \\'RateBook\\']\\nprint(intent_train[8000:8002])', '[\\'RateBook\\', \\'RateBook\\']\\nprint(intent_train[8000:8002])\\nprint(label_train[8000:8002])\\n[\"i\\'m trying to find the show chant ii\", \\'find spirit of the bush\\']\\n[\\'SearchCreativeWork\\', \\'SearchCreativeWork\\']\\nprint(intent_train[10000:10002])\\nprint(label_train[10000:10002])\\n[\\'when is blood and ice cream trilogie playing at the nearest movie theatre \\\\\\\\?\\', \\'show movie schedules\\']\\n[\\'SearchScreeningEvent\\', \\'SearchScreeningEvent\\']', \"['SearchScreeningEvent', 'SearchScreeningEvent']\\n이를 통해 눈치채셨겠지만, 사실 이 데이터는 일정한 순서로 배치되어져 있습니다. 그래서 뒤에서는 이 데이터를 랜덤으로 섞어주는 작업을 해줍니다. 훈련 데이터에 존재하는 레이블의 분포를 그래프로 시각화하여 보겠습니다.\\ntrain_data['label'].value_counts().plot(kind = 'bar')\\n[이미지: ]\\n훈련 데이터에는 6개의 카테고리가 존재합니다.\\nAddToPlaylist, BookRestaurant, GetWeather , RateBook , SearchCreativeWork, SearchScreeningEvent\", 'AddToPlaylist, BookRestaurant, GetWeather , RateBook , SearchCreativeWork, SearchScreeningEvent\\n각각의 데이터는 약 2,000개씩 존재합니다. label_train과 label_test에 존재하는 6개의 카테고리들을 고유한 정수로 인코딩해봅시다. 이런 경우에는 사이킷런(sklearn)의 preprocessing.LabelEncoder()가 유용합니다. label_idx에는 어떤 레이블이 어떤 정수에 맵핑되었는지 저장되어져 있습니다.\\n# 레이블 인코딩. 레이블에 고유한 정수를 부여\\nidx_encode = preprocessing.LabelEncoder()\\nidx_encode.fit(label_train)\\nlabel_train = idx_encode.transform(label_train) # 주어진 고유한 정수로 변환', \"idx_encode.fit(label_train)\\nlabel_train = idx_encode.transform(label_train) # 주어진 고유한 정수로 변환\\nlabel_test = idx_encode.transform(label_test) # 고유한 정수로 변환\\nlabel_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\\nprint('레이블과 정수의 맵핑 관계 :',label_idx)\\n레이블과 정수의 맵핑 관계 : {'AddToPlaylist': 0, 'BookRestaurant': 1, 'GetWeather': 2, 'RateBook': 3, 'SearchCreativeWork': 4, 'SearchScreeningEvent': 5}\\n훈련 데이터의 레이블과 테스트 데이터의 레이블 데이터를 상위 5개씩만 출력해봅시다.\\nprint(intent_train[:5])\", \"훈련 데이터의 레이블과 테스트 데이터의 레이블 데이터를 상위 5개씩만 출력해봅시다.\\nprint(intent_train[:5])\\nprint(label_train[:5])\\n['add another song to the cita rom ntica playlist', 'add clem burke in my playlist pre party r b jams', 'add live from aragon ballroom to trapeo', 'add unite and win to my night out', 'add track to my digster future hits']\\n[0 0 0 0 0]\\nprint(intent_test[:5])\\nprint(label_test[:5])\", '[0 0 0 0 0]\\nprint(intent_test[:5])\\nprint(label_test[:5])\\n[\"i \\'d like to have this track onto my classical relaxations playlist\", \\'add the album to my flow espa ol playlist\\', \\'add digging now to my young at heart playlist\\', \\'add this song by too poetic to my piano ballads playlist\\', \\'add this album to old school death metal\\']\\n[0 0 0 0 0]', \"[0 0 0 0 0]\\n이전에 'AddToPlaylist'라는 문자열로 저장되어져 있었던 레이블이 정수 0으로 변환되었습니다. 의도 문장에 대해서도 정수 인코딩을 진행해봅시다. 우선 의도 문장에 대해서 토큰화를 수행하고 단어 집합(vocabulary)을 만듭니다. 이어서 정수 인코딩을 수행하여 텍스트 시퀀스를 정수 시퀀스로 변환합니다.\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(intent_train)\\nsequences = tokenizer.texts_to_sequences(intent_train)\\nsequences[:5] # 상위 5개 샘플 출력\\n[[11, 191, 61, 4, 1, 4013, 1141, 1572, 15],\\n[11, 2624, 1573, 3, 14, 15, 939, 82, 256, 188, 548],\\n[11, 187, 42, 2625, 4014, 4, 1968],\", \"[11, 2624, 1573, 3, 14, 15, 939, 82, 256, 188, 548],\\n[11, 187, 42, 2625, 4014, 4, 1968],\\n[11, 2626, 22, 2627, 4, 14, 192, 27],\\n[11, 92, 4, 14, 651, 520, 195]]\\n단어 집합의 크기를 확인해보겠습니다.\\nword_index = tokenizer.word_index\\nvocab_size = len(word_index) + 1\\nprint('단어 집합(Vocabulary)의 크기 :',vocab_size)\\n단어 집합(Vocabulary)의 크기 : 9870\\n패딩을 위해서 훈련 데이터의 길이 분포를 확인해보겠습니다.\\nprint('문장의 최대 길이 :',max(len(l) for l in sequences))\\nprint('문장의 평균 길이 :',sum(map(len, sequences))/len(sequences))\", \"print('문장의 평균 길이 :',sum(map(len, sequences))/len(sequences))\\nplt.hist([len(s) for s in sequences], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n문장의 최대 길이 : 35\\n문장의 평균 길이 : 9.364392396469789\\n[이미지: ]\\n문장의 최대 길이는 35이므로, 최대 길이 35로 모든 훈련 데이터를 패딩하겠습니다. 레이블의 경우에는 다중 클래스 분류를 수행하기 위해서 원-핫 인코딩을 수행합니다.\\nmax_len = 35\\nintent_train = pad_sequences(sequences, maxlen = max_len)\\nlabel_train = to_categorical(np.asarray(label_train))\", \"label_train = to_categorical(np.asarray(label_train))\\nprint('훈련 데이터의 크기(shape):', intent_train.shape)\\nprint('훈련 데이터 레이블의 크기(shape):', label_train.shape)\\n훈련 데이터의 크기(shape) : (11784, 35)\\n훈련 데이터 레이블의 크기(shape) : (11784, 6)\\nprint('훈련 데이터의 첫번째 샘플 :',intent_train[0])\\nprint('훈련 데이터의 첫번째 샘플의 레이블 :',label_train[0])\\n첫번째 샘플 : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0   11  191\\n61    4    1 4013 1141 1572   15]\", \"61    4    1 4013 1141 1572   15]\\n첫번째 샘플의 레이블 : [1. 0. 0. 0. 0. 0.]\\n훈련 데이터로부터 검증 데이터를 분리하겠습니다. 앞서 언급했듯이 훈련 데이터가 일정한 순서로 배치되어져 있으므로 훈련 데이터에서 앞의 10%나 중간 10%나 뒤의 10%를 검증 데이터로 분리했다가는 운이 나쁘면 특정 레이블의 데이터들만을 분리할 수도 있습니다. 가령, 검증 데이터에 0번 레이블의 데이터만 있다면 제대로 된 검증이 안 될 것입니다. 그래서 검증 데이터로 분리하기 전에 훈련 데이터의 순서를 랜덤으로 섞어주겠습니다. 이를 위해 순서가 뒤죽박죽의 정수 시퀀스를 만들어줍니다.\\nindices = np.arange(intent_train.shape[0])\\nnp.random.shuffle(indices)\\nprint('랜덤 시퀀스 :',indices)\\n랜덤 시퀀스 : [1147 9504 9615 ... 4685  227 8774]\", \"np.random.shuffle(indices)\\nprint('랜덤 시퀀스 :',indices)\\n랜덤 시퀀스 : [1147 9504 9615 ... 4685  227 8774]\\n이 정수의 순서를 각 샘플의 순서가 되도록 훈련 데이터를 섞어줍니다.\\nintent_train = intent_train[indices]\\nlabel_train = label_train[indices]\\n검증 데이터는 훈련 데이터 중 10%만을 사용합니다. 훈련 데이터의 개수에 0.1를 곱하면 몇 일까요?\\nn_of_val = int(0.1 * intent_train.shape[0])\\nprint('검증 데이터의 개수 :',n_of_val)\\n검증 데이터의 개수 : 1178\\n1,178이네요. 검증 데이터는 1,178개만 사용하도록 훈련 데이터에서 분리해줍니다.\\nX_train = intent_train[:-n_of_val]\\ny_train = label_train[:-n_of_val]\", \"X_train = intent_train[:-n_of_val]\\ny_train = label_train[:-n_of_val]\\nX_val = intent_train[-n_of_val:]\\ny_val = label_train[-n_of_val:]\\nX_test = intent_test\\ny_test = label_test\\nprint('훈련 데이터의 크기(shape):', X_train.shape)\\nprint('검증 데이터의 크기(shape):', X_val.shape)\\nprint('훈련 데이터 레이블의 크기(shape):', y_train.shape)\\nprint('검증 데이터 레이블의 크기(shape):', y_val.shape)\\nprint('테스트 데이터의 개수 :', len(X_test))\\nprint('테스트 데이터 레이블의 개수 :', len(y_test))\\n훈련 데이터의 크기(shape): (10606, 35)\\n검증 데이터의 크기(shape): (1178, 35)\", '훈련 데이터의 크기(shape): (10606, 35)\\n검증 데이터의 크기(shape): (1178, 35)\\n훈련 데이터 레이블의 크기(shape): (10606, 6)\\n검증 데이터 레이블의 크기(shape): (1178, 6)\\n테스트 데이터의 개수 : 600\\n테스트 데이터 레이블의 개수 : 600']\n",
      "[\"스탠포드 대학교에서 제공하는 사전 훈련된 GloVe 임베딩을 사용합니다.\\n!wget http://nlp.stanford.edu/data/glove.6B.zip\\n!unzip glove*.zip\\n아래 코드의 상세 내용은 사전 훈련된 워드 임베딩 실습을 참고하세요. 임베딩 벡터를 로드합니다.\\nembedding_dict = dict()\\nf = open(os.path.join('glove.6B.100d.txt'), encoding='utf-8')\\nfor line in f:\\nword_vector = line.split()\\nword = word_vector[0]\\nword_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\\nembedding_dict[word] = word_vector_arr\\nf.close()\", \"embedding_dict[word] = word_vector_arr\\nf.close()\\nprint('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))\\n400000개의 Embedding vector가 있습니다.\\n총 40만개의 임베딩 벡터가 존재합니다. 임의로 사전 훈련된 임베딩에서 단어 'respectable' 임베딩 벡터값과 벡터의 차원을 출력합니다.\\nprint(embedding_dict['respectable'])\\nprint(len(embedding_dict['respectable']))\\n[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\\n0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\\n0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\", '0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\\n0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\\n-0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\\n-0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\\n-0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\\n-0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\\n-0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488', '-0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\\n-0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\\n-0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\\n0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\\n0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\\n0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\\n-0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694', \"-0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\\n-0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\\n-0.21616   -0.19187   -0.032502   0.38025  ]\\n100\\n임베딩 벡터의 차원은 100차원입니다. 이번 실습에서 사용할 임베딩 테이블을 구축해야합니다. 사전 훈련된 임베딩의 벡터의 차원이 100이므로 임베딩 테이블의 열도 100차원이어야 합니다. vocab_size를 행의 크기로, 열의 크기는 100인 테이블을 만듭니다.\\nembedding_dim = 100\\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\\nprint('임베딩 테이블의 크기(shape) :',np.shape(embedding_matrix))\\n임베딩 테이블의 크기(shape) : (9870, 100)\", \"print('임베딩 테이블의 크기(shape) :',np.shape(embedding_matrix))\\n임베딩 테이블의 크기(shape) : (9870, 100)\\n훈련 데이터에 있는 단어와 사전 훈련된 워드 임베딩 벡터의 값을 맵핑하여 임베딩 테이블에 저장합니다.\\nfor word, i in word_index.items():\\nembedding_vector = embedding_dict.get(word)\\nif embedding_vector is not None:\\nembedding_matrix[i] = embedding_vector\"]\n",
      "['모델은 네이버 영화 리뷰 분류하기에서 사용했던 Multi-Kernel 1D CNN 구조를 하이퍼파라미터를 바꿔서 사용합니다.\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate\\nkernel_sizes = [2, 3, 5]\\nnum_filters = 512\\ndropout_ratio = 0.5\\nmodel_input = Input(shape=(max_len,))\\noutput = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix],\\ninput_length=max_len, trainable=False)(model_input)\\nconv_blocks = []\\nfor size in kernel_sizes:', 'input_length=max_len, trainable=False)(model_input)\\nconv_blocks = []\\nfor size in kernel_sizes:\\nconv = Conv1D(filters=num_filters,\\nkernel_size=size,\\npadding=\"valid\",\\nactivation=\"relu\",\\nstrides=1)(output)\\nconv = GlobalMaxPooling1D()(conv)\\nconv_blocks.append(conv)\\noutput = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\\noutput = Dropout(dropout_ratio)(output)\\nmodel_output = Dense(len(label_idx), activation=\\'softmax\\')(output)\\nmodel = Model(model_input, model_output)', \"model = Model(model_input, model_output)\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\\nmodel.summary()\\n앞서 분리해두었던 검증 데이터로 성능을 점검하면서 훈련을 시작합니다.\\nhistory = model.fit(X_train, y_train,\\nbatch_size=64,\\nepochs=10,\\nvalidation_data=(X_val, y_val))\\n훈련 데이터에서 99%, 검증 데이터에서 98%의 정확도를 얻었습니다. 정확도와 loss의 변화를 그래프로 시각화해봅시다.\\nepochs = range(1, len(history.history['acc']) + 1)\\nplt.plot(epochs, history.history['acc'])\\nplt.plot(epochs, history.history['val_acc'])\", \"plt.plot(epochs, history.history['acc'])\\nplt.plot(epochs, history.history['val_acc'])\\nplt.title('model accuracy')\\nplt.ylabel('accuracy')\\nplt.xlabel('epochs')\\nplt.legend(['train', 'test'], loc='lower right')\\nplt.show()\\nepochs = range(1, len(history.history['loss']) + 1)\\nplt.plot(epochs, history.history['loss'])\\nplt.plot(epochs, history.history['val_loss'])\\nplt.title('model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epochs')\\nplt.legend(['train', 'test'], loc='upper right')\\nplt.show()\\n[이미지: ]\", \"plt.xlabel('epochs')\\nplt.legend(['train', 'test'], loc='upper right')\\nplt.show()\\n[이미지: ]\\n테스트 데이터에 대해서 평가해봅시다.\\nX_test = tokenizer.texts_to_sequences(X_test)\\nX_test = pad_sequences(X_test, maxlen=max_len)\\ny_predicted = model.predict(X_test)\\ny_predicted = y_predicted.argmax(axis=-1) # 예측을 정수 시퀀스로 변환\\nprint('정확도(Accuracy) : ', sum(y_predicted == y_test) / len(y_test))\\n정확도(Accuracy) : 0.99\\n99%의 정확도를 얻었습니다.\\n==================================================\\n--- 11-07 문자 임베딩(Character Embedding) ---\", '==================================================\\n--- 11-07 문자 임베딩(Character Embedding) ---\\n워드 임베딩과는 다른 방법으로 단어의 벡터 표현 방법을 얻는 문자 임베딩(Character Embedding)에 대해서 알아보겠습니다.', \"'understand'라는 단어는 '이해하다'라는 뜻을 가진 영단어입니다. 그런데 여기에 mis-를 앞에 붙여주게 되면, 'misunderstand'라는 '오해하다'라는 뜻의 다른 의미의 영단어가 됩니다. 비슷한 예시를 들어보겠습니다. 'underestimate'라는 단어는 '과소평가하다'라는 단어입니다. 그렇다면 'misunderestimate'는 무슨 뜻일까요? 사실 이 단어는 실존하는 단어가 아님에도 이 단어의 뜻을 추측할 수 있습니다. 영어권 언어에서 mis-라는 접두사는 '잘못판단하는' 이라는 의미의 'mistaken'의 의미를 담고있으므로 '과소평가하다' 라는 단어 앞에 mis-라는 접두사가 붙었다면 'misunderestimate'는 '잘못 과소평가하다'라는 추측이 가능합니다. 문자 임베딩은 사람의 이러한 이해 능력을 흉내내는 알고리즘입니다. 여기서는 CNN과 RNN을 이용한 두 가지 방법을 제시합니다.\"]\n",
      "['1D CNN은 전체 시퀀스 입력 안의 더 작은 시퀀스에 집중하여 정보를 얻어내는 동작을 하는 알고리즘입니다. FastText가 문자의 N-gram의 조합을 이용하여 OOV 문제를 해결하였듯이, 1D CNN을 문자 임베딩에 사용할 경우에는 문자의 N-gram으로부터 정보를 얻어내게 됩니다. 앞서 학습했던 1D CNN을 이용한 텍스트 분류를 이해했다면 문자 임베딩을 이해하는 것은 매우 간단합니다. 기본적으로 단어를 문자 단위로 쪼개고나서 입력으로 사용하는 것 외에는 달라진 것이 없습니다.\\n[이미지: ]', \"[이미지: ]\\n위의 그림은 임의의 단어 'have'에 대해서 1D CNN을 통해서 단어 표현 벡터를 얻는 과정을 보여줍니다. 우선, 단어 'have'를 'h', 'a', 'v', 'e'와 같이 문자 단위로 분리합니다. 그리고 임베딩 층(Embedding layer)을 이용한 임베딩을 단어에 대해서 하는 것이 아니라 문자에 대해서 하게 됩니다. 다시 말해 문자를 임베딩합니다. (문장의 길이를 맞추기 위해 패딩을 했던 지난 실습 예시들과 마찬가지로 여기서도 패딩은 가능합니다.) 그 후 1D CNN을 적용하게 되는데, 위의 그림은 커널의 사이즈가 4인 커널 2개, 3인 커널 2개, 2인 커널 2개를 사용할 때를 보여줍니다. 벡터가 6개므로 맥스 풀링을 한 후에는 6개의 스칼라 값을 얻는데, 이렇게 얻은 스칼라값들은 전부 연결(concatenate)하여 하나의 벡터로 만들어줍니다.\", \"최종적으로 이렇게 얻은 벡터를 단어 'have'의 벡터로 사용합니다. 그림에서는 문자 레벨 표현(Character-level representation)이라고 기재된 벡터에 해당됩니다. 이렇게 단어 벡터를 얻을 경우, 어떤 단어이든 기본적으로 문자 레벨로 쪼개므로 기존 워드 임베딩의 접근에서 OOV라고 하더라도 벡터를 얻을 수 있습니다. 가령, 'docker'라는 영단어가 훈련 데이터에 없었으나 테스트 데이터에 존재하는 단어였다고 해봅시다. Word2Vec이나 GloVe의 경우에는 OOV 문제가 발생하게 되겠지만, 1D CNN을 이용하는 경우에는 'd', 'o', 'c', 'k', 'e', 'r'로 전부 분리되어 각 문자로 임베딩이 되고나서 1D CNN을 거친 후에 'docker'의 벡터를 얻게됩니다.\"]\n",
      "['문자 임베딩을 얻는 또 다른 방법으로는 BiLSTM을 이용한 방법이 있습니다. 1D CNN때와 마찬가지로 기본적으로 단어를 문자로 쪼갠 후, 임베딩 층을 사용하여 문자 임베딩을 입력으로 사용하는 것은 같습니다. 이번 챕터도 BiLSTM을 이용한 텍스트 분류를 이해했다면 쉽습니다. BiLSTM의 다 대 일(many-to-one) 구조를 이해하기 어렵다면 BiLSTM을 이용한 한국어 스팀 리뷰 감성 분류하기 실습을 참고하시기 바랍니다.\\n[이미지: ]', \"[이미지: ]\\n위 그림은 임의의 단어 'have'에 대해서 BiLSTM을 통해서 단어 표현 벡터를 얻는 과정을 보여줍니다. 우선, 단어 'have'를 'h', 'a', 'v', 'e'와 같이 문자 단위로 분리합니다. 그리고 임베딩 층(Embedding layer)을 이용한 임베딩을 단어에 대해서 하는 것이 아니라 문자에 대해서 하게 됩니다. 다시 말해 문자를 임베딩합니다. 그리고나서 순방향 LSTM은 단어 순방향으로 순차적으로 문자 임베딩 벡터를 읽습니다. 반면, 역방향 LSTM은 단어의 역방향으로 순차적으로 문자 임베딩 벡터를 읽습니다. 그리고 순방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태를 연결(concatenate)합니다. 최종적으로 이렇게 얻은 벡터를 단어 'have'의 벡터로 사용합니다. 그림에서는 문자 레벨 표현(Character-level representation)이라고 기재된 벡터에 해당됩니다.\", '문자 임베딩을 워드 임베딩의 대체재로서 쓰거나, 문자 임베딩을 워드 임베딩과 연결(concatenate)하여 신경망의 입력으로 사용하기도 합니다.\\n==================================================\\n--- 12. 태깅 작업(Tagging Task) ---\\n마지막 편집일시 : 2022년 1월 2일 2:51 오후\\n==================================================\\n--- 12-01 케라스를 이용한 태깅 작업 개요(Tagging Task using Keras) ---\\n```\\nmodel.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))', '```\\nmodel.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))\\n```이번 챕터에서는 케라스(Keras)로 인공 신경망을 이용하여 태깅 작업을 하는 모델을 만듭니다. 개체명 인식기와 품사 태거를 만드는데, 이러한 두 작업의 공통점은 RNN의 다-대-다(Many-to-Many) 작업이면서 또한 앞, 뒤 시점의 입력을 모두 참고하는 양방향 RNN(Bidirectional RNN)을 사용한다는 점입니다. 전체적으로 실습의 진행 방향을 정리해보겠습니다. 정확한 이해를 위해 텍스트 분류 개요 챕터와 비교하며 같이 읽기를 권합니다.']\n",
      "[\"태깅 작업은 앞서 배운 텍스트 분류 작업과 동일하게 지도 학습(Supervised Learning)에 속합니다. 이 챕터에서는 태깅을 해야하는 단어 데이터를 X, 레이블에 해당되는 태깅 정보 데이터는 y라고 이름을 붙였습니다. X에 대한 훈련 데이터는 X_train, 테스트 데이터는 X_test라고 명명하고 y에 대한 훈련 데이터는 y_train, 테스트 데이터는 y_test라고 명명합니다.\\n이번 챕터에서 X와 y데이터의 쌍(pair)은 병렬 구조를 가진다는 특징을 가집니다. X와 y의 각 데이터의 길이는 같습니다. 예를 들어 개체명 인식 데이터의 상위 4개 샘플만 출력해본다고 가정해보겠습니다. 데이터는 다음과 같은 구조를 가질 수 있습니다.\\n#\\nX_train\\ny_train\\n길이\\n0\\n['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']\", \"#\\nX_train\\ny_train\\n길이\\n0\\n['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']\\n['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']\\n8\\n1\\n['peter', 'blackburn']\\n['B-PER', 'I-PER']\\n2\\n2\\n['brussels', '1996-08-22' ]\\n['B-LOC', 'O']\\n2\\n3\\n['The', 'European', 'Commission']\\n['O', 'B-ORG', 'I-ORG']\\n3\", \"['B-LOC', 'O']\\n2\\n3\\n['The', 'European', 'Commission']\\n['O', 'B-ORG', 'I-ORG']\\n3\\n가령, X_train[3]의 'The'와 y_train[3]의 'O'는 하나의 쌍(pair)입니다. 또한, X_train[3]의 'European'과 y_train[3]의 'B-ORG'는 쌍의 관계를 가지며, X_train[3]의 'Commision'과 y_train[3]의 'I-ORG'는 쌍의 관계를 가집니다. 이렇게 병렬 관계를 가지는 토큰화가 이루어진 데이터는 정수 인코딩 과정을 거친 후, 모든 데이터의 길이를 동일하게 맞춰주기위한 패딩(Padding) 작업을 거친 후에 딥 러닝 모델의 입력으로 사용됩니다.\"]\n",
      "['위와 같이 입력 시퀀스 X = [$x_{1}$, $x_{2}$, $x_{3}$, ..., $x_{n}$]에 대하여 레이블 시퀀스 y = [$y_{1}$, $y_{2}$, $y_{3}$, ..., $y_{n}$]를 각각 부여하는 작업을 시퀀스 레이블링 작업(Sequence Labeling Task)이라고 합니다. 태깅 작업은 대표적인 시퀀스 레이블링 작업입니다.']\n",
      "['model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))\\n이번 챕터에는 양방향 LSTM을 사용합니다. 이전 시점의 단어 정보 뿐만 아니라, 다음 시점의 단어 정보도 참고하기 위함입니다. 양방향은 기존의 단방향 LSTM()을 Bidirectional() 안에 넣으면 됩니다. LSTM의 인자값은 단방향 LSTM을 사용할 때와 동일합니다. 인자값을 하나를 줄 경우에는 이는 은닉 상태의 차원을 의미하며, 위 코드 상으로는 hidden_units로 기재되었습니다.']\n",
      "['[이미지: ]\\nRNN의 은닉층은 모든 시점에 대해서 은닉 상태의 값을 출력할 수도, 마지막 시점에 대해서만 은닉 상태의 값을 출력할 수도 있습니다. 인자로 return_sequences=True를 넣을 것인지, 넣지 않을 것인지로(넣지 않으면 기본값은 False이므로 return_sequences=False로 인식.) 설정할 수 있는데 태깅 작업의 경우에는 다 대 다(many-to-many) 문제로 return_sequences=True를 설정하여 출력층에 모든 은닉 상태의 값을 보냅니다.\\nRNN이 어떻게 설계되는지 확인해보겠습니다. 예를 들어 위에서 설명한 데이터 중 첫번째 데이터에 해당되는 X_train[0]를 가지고 4번의 시점(time steps)까지 RNN을 진행하였을 때의 그림은 다음과 같습니다.\\n[이미지: ]\\n이번 실습에서는 양방향 RNN을 사용할 것이므로 아래의 그림과 같습니다.\\n[이미지: ]', '[이미지: ]\\n이번 실습에서는 양방향 RNN을 사용할 것이므로 아래의 그림과 같습니다.\\n[이미지: ]\\n==================================================\\n--- 12-02 양방향 LSTM를 이용한 품사 태깅(Part-of-speech Tagging using Bi-LSTM) ---\\n```\\n단어             |실제값  |예측값\\n-----------------------------------\\nin               : IN      IN\\naddition         : NN      NN\\n,                : ,       ,\\nbuick            : NNP     NNP\\nis               : VBZ     VBZ\\na                : DT      DT\\nrelatively       : RB      RB\\nrespected        : VBN     VBN', 'a                : DT      DT\\nrelatively       : RB      RB\\nrespected        : VBN     VBN\\nnameplate        : NN      NN\\namong            : IN      IN\\namerican         : NNP     NNP\\nexpress          : NNP     NNP\\ncard             : NN      NN\\nholders          : NNS     NNS\\n,                : ,       ,\\nsays             : VBZ     VBZ\\n0                : -NONE-  -NONE-\\n*t*-1            : -NONE-  -NONE-\\nan               : DT      DT\\namerican         : NNP     NNP\\nexpress          : NNP     NNP', 'an               : DT      DT\\namerican         : NNP     NNP\\nexpress          : NNP     NNP\\nspokeswoman      : NN      NN\\n.                : .       .\\n```품사 태깅에 대해서는 이미 텍스트 전처리 챕터에서 토큰화와 함께 언급한 바 있습니다. 그 당시에는 NLTK와 KoNLPy를 이용해서 품사 태깅을 수행하였지만, 여기서는 직접 양방향 LSTM을 이용한 품사 태깅을 수행하는 모델을 만들어봅니다.']\n",
      "['양방향 LSTM을 이용해서 품사 태깅을 하는 모델을 만들어보겠습니다.\\nimport nltk\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\nfrom sklearn.model_selection import train_test_split\\nNLTK를 이용하면 영어 코퍼스에 토큰화와 품사 태깅 전처리를 진행한 문장 데이터를 받아올 수 있습니다. 해당 데이터를 훈련시켜 품사 태깅을 수행하는 모델을 만들어보겠습니다. 전체 문장 샘플의 개수를 확인합니다.\\n# 토큰화에 품사 태깅이 된 데이터 받아오기', '# 토큰화에 품사 태깅이 된 데이터 받아오기\\ntagged_sentences = nltk.corpus.treebank.tagged_sents()\\nprint(\"품사 태깅이 된 문장 개수: \", len(tagged_sentences))\\n품사 태깅이 된 문장 개수:  3914\\n첫번째 샘플만 출력해보겠습니다.\\nprint(tagged_sentences[0])\\n[(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\'), (\\',\\', \\',\\'), (\\'61\\', \\'CD\\'), (\\'years\\', \\'NNS\\'), (\\'old\\', \\'JJ\\'), (\\',\\', \\',\\'), (\\'will\\', \\'MD\\'), (\\'join\\', \\'VB\\'), (\\'the\\', \\'DT\\'), (\\'board\\', \\'NN\\'), (\\'as\\', \\'IN\\'), (\\'a\\', \\'DT\\'), (\\'nonexecutive\\', \\'JJ\\'), (\\'director\\', \\'NN\\'), (\\'Nov.\\', \\'NNP\\'), (\\'29\\', \\'CD\\'), (\\'.\\', \\'.\\')]', \"품사 태깅 전처리가 수행된 첫번째 문장이 출력된 것을 볼 수 있습니다. 이러한 문장 샘플이 총 3,914개가 있습니다. 그런데 훈련을 시키려면 훈련 데이터에서 단어에 해당되는 부분과 품사 태깅 정보에 해당되는 부분을 분리시켜야 합니다. 즉, [('Pierre', 'NNP'), ('Vinken', 'NNP')]와 같은 문장 샘플이 있다면 Pierre과 Vinken을 같이 저장하고, NNP와 NNP를 같이 저장할 필요가 있습니다.\\n이런 경우 파이썬 함수 중에서 zip()함수가 유용한 역할을 합니다. zip()함수는 동일한 개수를 가지는 시퀀스 자료형에서 동일한 순서에 등장하는 원소들끼리 묶어주는 역할을 합니다.\\nsentences, pos_tags = [], []\\nfor tagged_sentence in tagged_sentences: # 3,914개의 문장 샘플을 1개씩 불러온다.\", \"sentences, pos_tags = [], []\\nfor tagged_sentence in tagged_sentences: # 3,914개의 문장 샘플을 1개씩 불러온다.\\nsentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 품사 태깅 정보들은 tag_info에 저장한다.\\nsentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\\npos_tags.append(list(tag_info)) # 각 샘플에서 품사 태깅 정보만 저장한다.\\n각 문장 샘플에 대해서 단어는 sentences에 태깅 정보는 pos_tags에 저장하였습니다. 첫번째 문장 샘플을 출력해보겠습니다.\\nprint(sentences[0])\\nprint(pos_tags[0])\\n['Pierre' 'Vinken' ',' '61' 'years' 'old' ',' 'will' 'join' 'the' 'board'\", \"print(pos_tags[0])\\n['Pierre' 'Vinken' ',' '61' 'years' 'old' ',' 'will' 'join' 'the' 'board'\\n'as' 'a' 'nonexecutive' 'director' 'Nov.' '29' '.']\\n['NNP' 'NNP' ',' 'CD' 'NNS' 'JJ' ',' 'MD' 'VB' 'DT' 'NN' 'IN' 'DT' 'JJ'\\n'NN' 'NNP' 'CD' '.']\\n첫번째 샘플에 대해서 단어에 대해서 sentences[0]에, 품사에 대해서만 pos_tags[0]에 저장된 것을 볼 수 있습니다. 뒤에서 보겠지만, sentences는 예측을 위한 X에 해당되며 pos_tags는 예측 대상인 y에 해당됩니다. 임의로 8번 인덱스 샘플에 대해서도 확인해보겠습니다.\\nprint(sentences[8])\\nprint(pos_tags[8])\", 'print(sentences[8])\\nprint(pos_tags[8])\\n[\\'We\\', \"\\'re\", \\'talking\\', \\'about\\', \\'years\\', \\'ago\\', \\'before\\', \\'anyone\\', \\'heard\\', \\'of\\', \\'asbestos\\', \\'having\\', \\'any\\', \\'questionable\\', \\'properties\\', \\'.\\']\\n[\\'PRP\\', \\'VBP\\', \\'VBG\\', \\'IN\\', \\'NNS\\', \\'IN\\', \\'IN\\', \\'NN\\', \\'VBD\\', \\'IN\\', \\'NN\\', \\'VBG\\', \\'DT\\', \\'JJ\\', \\'NNS\\', \\'.\\']\\n단어에 대해서만 sentences[8]에, 또한 품사에 대해서만 pos_tags[8]에 저장된 것을 확인할 수 있습니다. 또한 첫번째 샘플과 길이가 다른 것을 볼 수 있습니다. 사실 3,914개의 문장 샘플의 길이는 전부 제각각입니다. 전체 데이터의 길이 분포를 확인해봅시다.', \"print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\\nprint('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\\nplt.hist([len(s) for s in sentences], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n샘플의 최대 길이 : 271\\n샘플의 평균 길이 : 25.722024\\n[이미지: ]\\n위의 그래프는 대부분의 샘플의 길이가 150 이내며 대부분 0~50의 길이를 가지는 것을 보여줍니다. 이제 케라스 토크나이저를 통해서 정수 인코딩을 진행합니다. 우선 케라스 토크나이저를 다음과 같이 함수로 구현합니다.\\ndef tokenize(samples):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(samples)\", \"def tokenize(samples):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(samples)\\nreturn tokenizer\\n문장 데이터에 대해서는 src_tokenizer를, 레이블에 해당되는 품사 태깅 정보에 대해서는 tar_tokenizer를 사용합니다.\\nsrc_tokenizer = tokenize(sentences)\\ntar_tokenizer = tokenize(pos_tags)\\n단어 집합과 품사 태깅 정보 집합의 크기를 확인해보겠습니다.\\nvocab_size = len(src_tokenizer.word_index) + 1\\ntag_size = len(tar_tokenizer.word_index) + 1\\nprint('단어 집합의 크기 : {}'.format(vocab_size))\\nprint('태깅 정보 집합의 크기 : {}'.format(tag_size))\\n단어 집합의 크기 : 11388\\n태깅 정보 집합의 크기 : 47\", \"print('태깅 정보 집합의 크기 : {}'.format(tag_size))\\n단어 집합의 크기 : 11388\\n태깅 정보 집합의 크기 : 47\\n정수 인코딩을 수행합니다.\\nX_train = src_tokenizer.texts_to_sequences(sentences)\\ny_train = tar_tokenizer.texts_to_sequences(pos_tags)\\n문장 데이터에 대해서 정수 인코딩이 수행된 결과는 X_train, 품사 태깅 데이터에 대해서 정수 인코딩이 수행된 결과는 y_train에 저장되었습니다. 정수 인코딩이 되었는지 확인을 위해 임의로 2번 인덱스 샘플을 출력해보겠습니다.\\nprint(X_train[:2])\\nprint(y_train[:2])\", 'print(X_train[:2])\\nprint(y_train[:2])\\n[[5601, 3746, 1, 2024, 86, 331, 1, 46, 2405, 2, 131, 27, 6, 2025, 332, 459, 2026, 3], [31, 3746, 20, 177, 4, 5602, 2915, 1, 2, 2916, 637, 147, 3]]\\n[[3, 3, 8, 10, 6, 7, 8, 21, 13, 4, 1, 2, 4, 7, 1, 3, 10, 9], [3, 3, 17, 1, 2, 3, 3, 8, 4, 3, 19, 1, 9]]\\n앞서 본 그래프에 따르면, 대부분의 샘플은 길이가 150 이내입니다. X에 해당되는 데이터 X_train의 샘플들과 y에 해당되는 데이터 y_train 샘플들의 모든 길이를 임의로 150정도로 맞추어 보겠습니다. 케라스의 pad_sequences()를 사용합니다.\\nmax_len = 150', \"max_len = 150\\nX_train = pad_sequences(X_train, padding='post', maxlen=max_len)\\ny_train = pad_sequences(y_train, padding='post', maxlen=max_len)\\n모든 샘플의 길이가 150이 되었습니다. 훈련 데이터와 테스트 데이터를 8:2의 비율로 분리합니다.\\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)\\n각 데이터에 대한 크기(shape)를 확인해보겠습니다.\\nprint('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\\nprint('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\\nprint('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\", \"print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\\nprint('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\\n훈련 샘플 문장의 크기 : (3131, 150)\\n훈련 샘플 레이블의 크기 : (3131, 150)\\n테스트 샘플 문장의 크기 : (783, 150)\\n테스트 샘플 레이블의 크기 : (783, 150)\"]\n",
      "['임베딩 벡터의 차원과 LSTM의 은닉 상태의 차원은 128로 지정했습니다. 다대다 문제이므로 LSTM의 return_sequences의 값은 True로 지정하였으며, 양방향 사용을 위해 LSTM을 Bidirectional()로 감싸주었습니다. validation_data로는 테스트 데이터를 기재하여 학습 중간에 테스트 데이터의 정확도를 확인하였습니다.\\n레이블에 대해서 원-핫 인코딩을 하고 손실 함수를 categorical_crossentropy를 사용할 수도 있겠지만, 만약 레이블에 원-핫 인코딩을 하지 않고 학습을 진행하고자 한다면 손실 함수를  categorical_crossentropy 대신 sparse_categorical_crossentropy를 선택합니다. 여기서는 후자의 방법을 택합니다.\\nfrom tensorflow.keras.models import Sequential', \"from tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\\nfrom tensorflow.keras.optimizers import Adam\\nembedding_dim = 128\\nhidden_units = 128\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim, mask_zero=True))\\nmodel.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))\\nmodel.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))\", 'model.add(TimeDistributed(Dense(tag_size, activation=(\\'softmax\\'))))\\nmodel.compile(loss=\\'sparse_categorical_crossentropy\\', optimizer=Adam(0.001), metrics=[\\'accuracy\\'])\\nmodel.fit(X_train, y_train, batch_size=128, epochs=7, validation_data=(X_test, y_test))\\n총 7번의 에포크를 마치고나서 테스트 데이터에 대한 정확도를 측정합니다.\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\\n25/25 [==============================] - 0s 6ms/step - loss: 0.0720 - accuracy: 0.9016\\n테스트 정확도: 0.9016', '테스트 정확도: 0.9016\\n실제로 맞추고 있는지를 특정 테스트 샘플(10번 인덱스)을 통해 확인해보겠습니다. 정수로부터 단어와 품사 태깅 정보를 리턴하는 index_to_word와 index_to_tag를 만들고 이를 이용하여 실제값과 예측값을 출력합니다.\\nindex_to_word = src_tokenizer.index_word\\nindex_to_tag = tar_tokenizer.index_word\\ni = 10 # 확인하고 싶은 테스트용 샘플의 인덱스.\\ny_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측값 y를 리턴\\ny_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 레이블로 변환.\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")', 'print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor word, tag, pred in zip(X_test[i], y_test[i], y_predicted[0]):\\nif word != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_tag[tag].upper(), index_to_tag[pred].upper()))\\n단어             |실제값  |예측값\\n-----------------------------------\\nin               : IN      IN\\naddition         : NN      NN\\n,                : ,       ,\\nbuick            : NNP     NNP\\nis               : VBZ     VBZ', ',                : ,       ,\\nbuick            : NNP     NNP\\nis               : VBZ     VBZ\\na                : DT      DT\\nrelatively       : RB      RB\\nrespected        : VBN     VBN\\nnameplate        : NN      NN\\namong            : IN      IN\\namerican         : NNP     NNP\\nexpress          : NNP     NNP\\ncard             : NN      NN\\nholders          : NNS     NNS\\n,                : ,       ,\\nsays             : VBZ     VBZ\\n0                : -NONE-  -NONE-\\n*t*-1            : -NONE-  -NONE-', 'says             : VBZ     VBZ\\n0                : -NONE-  -NONE-\\n*t*-1            : -NONE-  -NONE-\\nan               : DT      DT\\namerican         : NNP     NNP\\nexpress          : NNP     NNP\\nspokeswoman      : NN      NN\\n.                : .       .\\n==================================================\\n--- 12-03 개체명 인식(Named Entity Recognition) ---\\n```\\n(S\\n(PERSON James/NNP)\\nis/VBZ\\nworking/VBG\\nat/IN\\n(ORGANIZATION Disney/NNP)\\nin/IN\\n(GPE London/NNP))', '(S\\n(PERSON James/NNP)\\nis/VBZ\\nworking/VBG\\nat/IN\\n(ORGANIZATION Disney/NNP)\\nin/IN\\n(GPE London/NNP))\\n```코퍼스로부터 각 개체(entity)의 유형을 인식하는 개체명 인식(Named Entity Recognition)에 대해서 학습합니다. 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지를 찾을 수 있습니다.']\n",
      "['개체명 인식(Named Entity Recognition)이란 말 그대로 이름을 가진 개체(named entity)를 인식하겠다는 것을 의미합니다. 좀 더 쉽게 설명하면, 어떤 이름을 의미하는 단어를 보고는 그 단어가 어떤 유형인지를 인식하는 것을 말합니다.\\n예를 들어 유정이는 2018년에 골드만삭스에 입사했다. 라는 문장이 있을 때, 사람(person), 조직(organization), 시간(time)에 대해 개체명 인식을 수행하는 모델이라면 다음과 같은 결과를 보여줍니다.\\n유정 - 사람\\n2018년 - 시간\\n골드만삭스 - 조직']\n",
      "['NLTK에서는 개체명 인식기(NER chunker)를 지원하고 있으므로, 별도 개체명 인식기를 구현할 필요없이 NLTK를 사용해서 개체명 인식을 수행할 수 있습니다. 만약 아래의 실습에서 nltk.download(\\'maxent_ne_chunker\\'), nltk.download(\\'words\\') 등의 설치를 요구하는 에러 문구가 뜬다면, 지시하는대로 설치하면 됩니다.\\nfrom nltk import word_tokenize, pos_tag, ne_chunk\\nsentence = \"James is working at Disney in London\"\\n# 토큰화 후 품사 태깅\\ntokenized_sentence = pos_tag(word_tokenize(sentence))\\nprint(tokenized_sentence)', \"# 토큰화 후 품사 태깅\\ntokenized_sentence = pos_tag(word_tokenize(sentence))\\nprint(tokenized_sentence)\\n[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\\n# 개체명 인식\\nner_sentence = ne_chunk(tokenized_sentence)\\nprint(ner_sentence)\\n(S\\n(PERSON James/NNP)\\nis/VBZ\\nworking/VBG\\nat/IN\\n(ORGANIZATION Disney/NNP)\\nin/IN\\n(GPE London/NNP))\", '(S\\n(PERSON James/NNP)\\nis/VBZ\\nworking/VBG\\nat/IN\\n(ORGANIZATION Disney/NNP)\\nin/IN\\n(GPE London/NNP))\\nne_chunk는 개체명을 태깅하기 위해서 앞서 품사 태깅(pos_tag)이 수행되어야 합니다. 위의 결과에서 James는 PERSON(사람), Disney는 조직(ORGANIZATION), London은 위치(GPE)라고 정상적으로 개체명 인식이 수행된 것을 볼 수 있습니다.\\n이어서 인공 신경망을 이용하여 개체명 인식 모델을 만들어보겠습니다.\\n==================================================\\n--- 12-04 개체명 인식의 BIO 표현 이해하기 ---\\n```\\n단어             |실제값  |예측값\\n-----------------------------------\\nsarah            : B-PER   B-PER', '```\\n단어             |실제값  |예측값\\n-----------------------------------\\nsarah            : B-PER   B-PER\\nbrady            : I-PER   I-PER\\n,                : O       O\\nwhose            : O       O\\nrepublican       : B-MISC  B-MISC\\nhusband          : O       O\\nwas              : O       O\\nOOV              : O       O\\nOOV              : O       O\\nin               : O       O\\nan               : O       O\\nOOV              : O       O\\nattempt          : O       O\\non               : O       O', 'OOV              : O       O\\nattempt          : O       O\\non               : O       O\\npresident        : O       O\\nronald           : B-PER   B-PER\\nreagan           : I-PER   I-PER\\n,                : O       O\\ntook             : O       O\\ncentre           : O       O\\nstage            : O       O\\nat               : O       O\\nthe              : O       O\\ndemocratic       : B-MISC  B-MISC\\nnational         : I-MISC  I-MISC\\nconvention       : I-MISC  I-MISC\\non               : O       O', \"national         : I-MISC  I-MISC\\nconvention       : I-MISC  I-MISC\\non               : O       O\\nmonday           : O       O\\nnight            : O       O\\nto               : O       O\\nOOV              : O       O\\npresident        : O       O\\nbill             : B-PER   B-PER\\nclinton          : I-PER   I-PER\\n's               : O       O\\ngun              : O       O\\ncontrol          : O       O\\nefforts          : O       O\\n.                : O       O\", 'control          : O       O\\nefforts          : O       O\\n.                : O       O\\n```개체명 인식은 챗봇 등에서 필요한 주요 전처리 작업이면서 그 자체로도 까다로운 작업입니다. 도메인 또는 목적에 특화되도록 개체명 인식을 정확하게 하는 방법 중 하나는 기존에 공개된 개체명 인식기를 사용하는 것이 아니라, 직접 목적에 맞는 데이터를 준비하여 모델을 만드는 것입니다. 양방향 LSTM을 이용해서 개체명 인식기를 만들어봅니다.']\n",
      "[\"개체명 인식에서 코퍼스로부터 개체명을 인식하기 위한 방법으로는 여러 방법이 있지만, 여기서는 가장 보편적인 방법 중 하나인 BIO 태깅 방법을 소개합니다. B는 Begin의 약자로 개체명이 시작되는 부분, I는 Inside의 약자로 개체명의 내부 부분을 의미하며, O는 Outside의 약자로 개체명이 아닌 부분을 의미합니다.\\n예를 들어 영화에 대한 코퍼스 중에서 영화 제목에 대한 개체명을 뽑아내고 싶다고 가정합시다.\\n해 B\\n리 I\\n포 I\\n터 I\\n보 O\\n러 O\\n가 O\\n자 O\\n다음과 같이 영화 제목에 대해서만 개체명을 인식하는데, 영화 제목이 시작되는 글자인 '해'에서는 B가 사용되었고, 그리고 영화 제목이 끝나는 순간까지 I가 사용됩니다. 그리고 영화 제목이 아닌 부분에 대해서만 O가 사용됩니다. 이처럼 B와 I는 개체명을 위해 사용되고, O는 개체명이 아니라는 의미를 갖게 됩니다.\", '물론 개체명 인식이라는 것은 보통 한 종류의 개체에 대해서만 언급하는 것이 아니라 여러 종류의 개체가 있을 수 있습니다. 예를 들어 영화에 대한 대화에서는 영화 제목에 대한 개체명과 극장에 대한 개체명이 있을 수 있습니다. 이 경우 각 개체가 어떤 종류인지도 함께 태깅이 될 것입니다.\\n해 B-movie\\n리 I-movie\\n포 I-movie\\n터 I-movie\\n보 O\\n러 O\\n메 B-theater\\n가 I-theater\\n박 I-theater\\n스 I-theater\\n가 O\\n자 O']\n",
      "['실습을 통해 양방향 LSTM을 이용한 개체명 인식에 대해서 더 자세히 알아보겠습니다. CONLL2003은 개체명 인식을 위한 전통적인 영어 데이터셋입니다. 여기서 다룰 데이터의 앞 부분을 일부 보겠습니다.\\nEU NNP B-NP B-ORG\\nrejects VBZ B-VP O\\nGerman JJ B-NP B-MISC\\ncall NN I-NP O\\nto TO B-VP O\\nboycott VB I-VP O\\nBritish JJ B-NP B-MISC\\nlamb NN I-NP O\\n. . O O\\nPeter NNP B-NP B-PER\\nBlackburn NNP I-NP I-PER\\n데이터의 형식은 [단어] [품사 태깅] [청크 태깅] [개체명 태깅]의 형식으로 되어있습니다.\\n품사 태깅이 의미하는 바는 아래 링크에서 자세하게 확인할 수 있는데, 예를 들어서 EU 옆에 붙어있는 NNP는 고유 명사 단수형을 의미하며, rejects 옆에 있는 VBZ는 3인칭 단수 동사 현재형을 의미합니다.', '링크 : https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\\n개체명 태깅의 경우에는 LOC는 location, ORG는 organization, PER은 person, MISC는 miscellaneous를 의미합니다. 해당 데이터는 BIO 표현 방법을 사용하고 있기 때문에, 개체명의 시작 부분이면서 Organization을 의미하는 EU에는 B-ORG라는 개체명 태깅이 붙습니다. 다만, EU 그 자체로 개체명 하나이기 때문에 거기서 개체명 인식은 종료되면서 뒤에 I가 별도로 붙는 단어가 나오지는 않았습니다. 이에 EU 뒤에 나오는 call은 개체명이 아니기 때문에 O가 태깅이 됩니다.', '또 하나 기억해두어야할 것은 9번째 줄인. . O O 다음에 11번째 줄 Peter가 나오는 부분 사이에서 10번째 줄은 공란으로 되어 있는데, 이는 9번째 줄에서 문장이 끝나고 11번째 줄에서 새로운 문장이 시작됨을 의미합니다.\\n그 다음 문장이 시작되는 11번째 줄에서는 개체명이 하나의 단어로 끝나지 않았을 때, 어떻게 다음 단어로 개체명 인식이 이어지는지를 보여줍니다. Peter는 개체명이 시작되면서 person에 해당되기 때문에 B-PER이라는 개체명 태깅이 붙습니다. 그리고 아직 개체명에 대한 인식은 끝나지 않았기 때문에 뒤에 붙는 Blackburn에서는 I가 나오면서 I-PER이 개체명 태깅으로 붙게 됩니다. 즉, Peter Blackburn이 person에 속하는 하나의 개체명입니다.']\n",
      "['import re\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport urllib.request\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\nfrom sklearn.model_selection import train_test_split\\n위에서 설명한 개체명 인식 데이터를 읽어 전처리를 수행합니다.', 'from sklearn.model_selection import train_test_split\\n위에서 설명한 개체명 인식 데이터를 읽어 전처리를 수행합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20RNN%20Sequence%20Labeling/dataset/train.txt\", filename=\"train.txt\")\\nf = open(\\'train.txt\\', \\'r\\')\\ntagged_sentences = []\\nsentence = []\\nfor line in f:\\nif len(line)==0 or line.startswith(\\'-DOCSTART\\') or line[0]==\"\\\\n\":\\nif len(sentence) > 0:\\ntagged_sentences.append(sentence)\\nsentence = []\\ncontinue', 'if len(sentence) > 0:\\ntagged_sentences.append(sentence)\\nsentence = []\\ncontinue\\nsplits = line.split(\\' \\') # 공백을 기준으로 속성을 구분한다.\\nsplits[-1] = re.sub(r\\'\\\\n\\', \\'\\', splits[-1]) # 줄바꿈 표시 \\\\n을 제거한다.\\nword = splits[0].lower() # 단어들은 소문자로 바꿔서 저장한다.\\nsentence.append([word, splits[-1]]) # 단어와 개체명 태깅만 기록한다.\\n전체 샘플 개수를 확인해보겠습니다.\\nprint(\"전체 샘플 개수: \", len(tagged_sentences))\\n전체 샘플 개수 : 14041\\n이 중 첫번째 샘플만 출력해보겠습니다.\\nprint(\\'첫번째 샘플 :\\',tagged_sentences[0])', \"전체 샘플 개수 : 14041\\n이 중 첫번째 샘플만 출력해보겠습니다.\\nprint('첫번째 샘플 :',tagged_sentences[0])\\n첫번째 샘플 : [['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\", \"위와 같은 샘플이 총 14,041개가 있습니다. 훈련을 시키려면 훈련 데이터에서 단어에 해당되는 부분과 개체명 태깅 정보에 해당되는 부분을 분리시켜야 합니다. 즉, [('eu', 'B-ORG'), ('rejects', 'O')]와 같은 문장 샘플이 있다면 eu와 rejects는 같이 저장하고, B-ORG와 O를 같이 저장할 필요가 있습니다. 이 경우 파이썬 함수 중에서 zip()함수가 유용한 역할을 합니다. zip()함수는 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할을 합니다.\\nsentences, ner_tags = [], []\\nfor tagged_sentence in tagged_sentences: # 14,041개의 문장 샘플을 1개씩 불러온다.\\nsentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\", \"sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\\nsentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\\nner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다.\\n각 문장 샘플에 대해서 단어는 sentences에 태깅 정보는 ner_tags에 저장하였습니다. 임의로 첫번째 샘플을 출력해보겠습니다.\\nprint('첫번째 샘플의 문장 :',sentences[0])\\nprint('첫번째 샘플의 레이블 :',ner_tags[0])\\n첫번째 샘플의 문장 : ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\", '첫번째 샘플의 문장 : [\\'eu\\', \\'rejects\\', \\'german\\', \\'call\\', \\'to\\', \\'boycott\\', \\'british\\', \\'lamb\\', \\'.\\']\\n첫번째 샘플의 레이블 : [\\'B-ORG\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\', \\'O\\', \\'B-MISC\\', \\'O\\', \\'O\\']\\n첫번째 샘플에 대해서 단어에 대해서만 sentences[0]에 저장되었으며 개체명에 대해서만 ner_tags[0]에 저장된 것을 볼 수 있습니다. 뒤에서 보겠지만, sentences는 예측을 위한 X에 해당되며 ner_tags는 예측 대상인 y에 해당됩니다. 다른 샘플들도 확인하기 위해 임의로 12번 인덱스 샘플에 대해서도 확인해보겠습니다.\\nprint(sentences[12])\\nprint(ner_tags[12])\\n[\\'only\\', \\'france\\', \\'and\\', \\'britain\\', \\'backed\\', \\'fischler\\', \"\\'s\", \\'proposal\\', \\'.\\']', '[\\'only\\', \\'france\\', \\'and\\', \\'britain\\', \\'backed\\', \\'fischler\\', \"\\'s\", \\'proposal\\', \\'.\\']\\n[\\'O\\', \\'B-LOC\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'B-PER\\', \\'O\\', \\'O\\', \\'O\\']\\n단어에 대해서만 sentences[12]에, 또한 개체명에 대해서만 ner_tags[12]에 저장되었습니다. 또한 첫번째 샘플과 길이가 다른 것을 볼 수 있습니다. 사실 14,041개의 문장 샘플의 길이는 전부 제각각입니다. 전체 데이터의 길이 분포를 확인해봅시다.\\nprint(\\'샘플의 최대 길이 : %d\\' % max(len(sentence) for sentence in sentences))\\nprint(\\'샘플의 평균 길이 : %f\\' % (sum(map(len, sentences))/len(sentences)))\\nplt.hist([len(sentence) for sentence in sentences], bins=50)', \"plt.hist([len(sentence) for sentence in sentences], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n샘플의 최대 길이 : 113\\n샘플의 평균 길이 : 14.501887\\n[이미지: ]\\n위의 그래프는 샘플들의 길이가 대체적으로 0~40의 길이를 가지며, 특히 0~20의 길이를 가진 샘플이 상당한 비율을 차지하는 것을 보여줍니다. 길이가 가장 긴 샘플의 길이는 113입니다. 케라스 토크나이저를 통해서 정수 인코딩을 진행합니다. 이번에는 문장 데이터에 있는 모든 단어를 사용하지 않고 높은 빈도수를 가진 상위 약 4,000개의 단어만을 사용합니다.\\nvocab_size = 4000\\nsrc_tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')\", \"vocab_size = 4000\\nsrc_tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')\\nsrc_tokenizer.fit_on_texts(sentences)\\ntar_tokenizer = Tokenizer()\\ntar_tokenizer.fit_on_texts(ner_tags)\\n문장 데이터에 대해서는 src_tokenizer를, 레이블에 해당되는 개체명 태깅 정보에 대해서는 tar_tokenizer를 사용합니다.\\ntag_size = len(tar_tokenizer.word_index) + 1\\nprint('단어 집합의 크기 : {}'.format(vocab_size))\\nprint('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))\\n단어 집합의 크기 : 4000\\n개체명 태깅 정보 집합의 크기 : 10\\n정수 인코딩을 수행합니다.\", \"단어 집합의 크기 : 4000\\n개체명 태깅 정보 집합의 크기 : 10\\n정수 인코딩을 수행합니다.\\nX_train = src_tokenizer.texts_to_sequences(sentences)\\ny_train = tar_tokenizer.texts_to_sequences(ner_tags)\\n문장 데이터에 대해서 정수 인코딩이 수행된 결과는 X_train, 개체명 태깅 데이터에 대해서 정수 인코딩이 수행된 결과는 y_train에 저장되었습니다. 정수 인코딩이 되었는지 확인을 위해 임의로 첫번째 샘플을 출력해보겠습니다.\\nprint('첫번째 샘플의 문장 :',X_train[0])\\nprint('첫번째 샘플의 레이블 :',y_train[0])\\n첫번째 샘플의 문장 : [989, 1, 205, 629, 7, 3939, 216, 1, 3]\\n첫번째 샘플의 레이블 : [4, 1, 7, 1, 1, 1, 7, 1, 1]\", \"첫번째 샘플의 문장 : [989, 1, 205, 629, 7, 3939, 216, 1, 3]\\n첫번째 샘플의 레이블 : [4, 1, 7, 1, 1, 1, 7, 1, 1]\\n현재 문장 데이터에 대해서는 일부 단어가 'OOV'로 대체된 상황입니다. 이를 확인하기 위해 디코딩 작업을 진행해봅시다. 이를 위해 정수로부터 단어로 변환하는 index_to_word를 만듭니다.\\nindex_to_word = src_tokenizer.index_word\\nindex_to_ner = tar_tokenizer.index_word\\n정수 인코딩 된 첫번째 문장을 다시 디코딩해보겠습니다.\\ndecoded = []\\nfor index in X_train[0] : # 첫번째 샘플 안의 각 정수로 변환된 단어에 대해서\\ndecoded.append(index_to_word[index]) # 단어로 변환\\nprint('기존 문장 : {}'.format(sentences[0]))\", \"decoded.append(index_to_word[index]) # 단어로 변환\\nprint('기존 문장 : {}'.format(sentences[0]))\\nprint('빈도수가 낮은 단어가 OOV 처리된 문장 : {}'.format(decoded))\\n기존 문장 : ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\\n빈도수가 낮은 단어가 OOV 처리된 문장 : ['eu', 'OOV', 'german', 'call', 'to', 'boycott', 'british', 'OOV', '.']\\n일부 단어가 'OOV'로 대체되었습니다. 앞서 본 그래프에 따르면, 대부분의 샘플은 길이가 70 이내입니다. X에 해당되는 데이터 X_train의 샘플들과 y에 해당되는 데이터 y_train 샘플들의 모든 길이를 임의로 70정도로 맞추어 보겠습니다. 패딩을 진행합니다.\\nmax_len = 70\", \"max_len = 70\\nX_train = pad_sequences(X_train, padding='post', maxlen=max_len)\\ny_train = pad_sequences(y_train, padding='post', maxlen=max_len)\\n모든 샘플의 길이가 70이 되었습니다. 훈련 데이터와 테스트 데이터를 8:2의 비율로 분리합니다.\\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)\\n레이블에 해당하는 태깅 정보에 대해서 원-핫 인코딩을 수행합니다.\\ny_train = to_categorical(y_train, num_classes=tag_size)\\ny_test = to_categorical(y_test, num_classes=tag_size)\\n각 데이터에 대한 크기(shape)를 확인해보겠습니다.\", \"y_test = to_categorical(y_test, num_classes=tag_size)\\n각 데이터에 대한 크기(shape)를 확인해보겠습니다.\\nprint('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\\nprint('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\\nprint('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\\n훈련 샘플 문장의 크기 : (11232, 70)\\n훈련 샘플 레이블의 크기 : (11232, 70, 10)\\n테스트 샘플 문장의 크기 : (2809, 70)\\n테스트 샘플 레이블의 크기 : (2809, 70, 10)\"]\n",
      "['하이퍼파라미터인 임베딩 벡터의 차원은 128, 은닉 상태의 크기는 128입니다. 모델은 다 대 다 구조의 LSTM을 사용합니다. 이 경우 LSTM의 return_sequences의 인자값은 True로 주어야만 합니다. 이번 실습과 같이 각 데이터의 길이가 달라서 패딩을 하느라 숫자 0이 많아질 경우에는 Embedding()에 mask_zero=True를 설정하여 숫자 0은 연산에서 제외시킨다는 옵션을 줄 수 있습니다. 출력층에 TimeDistributed()를 사용했는데, TimeDistributed()는 LSTM을 다 대 다 구조로 사용하여 LSTM의 모든 시점에 대해서 출력층을 사용할 필요가 있을 때 사용합니다.', '해당 모델은 모든 시점에 대해서 개체명 레이블 개수만큼의 선택지 중 하나를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 하이퍼파라미터인 배치 크기는 128이며, 8 에포크를 수행합니다. validation_data로 테스트 데이터를 선택하여 학습하는 동안 테스트 데이터에 대한 정확도를 확인합니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\\nfrom tensorflow.keras.optimizers import Adam\\nembedding_dim = 128\\nhidden_units = 128', \"from tensorflow.keras.optimizers import Adam\\nembedding_dim = 128\\nhidden_units = 128\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True))\\nmodel.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))\\nmodel.add(TimeDistributed(Dense(tag_size, activation='softmax')))\\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\", 'model.compile(loss=\\'categorical_crossentropy\\', optimizer=Adam(0.001), metrics=[\\'accuracy\\'])\\nmodel.fit(X_train, y_train, batch_size=128, epochs=8, validation_data=(X_test, y_test))\\n학습이 종료되었다면 테스트 데이터에 대한 정확도를 측정합니다.\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\\n2809/2809 [==============================] - 9s 3ms/step\\n테스트 정확도: 0.9573\\n실제로 맞추고 있는지를 임의의 테스트 샘플로부터(인덱스 10번) 직접 실제값과 비교해보겠습니다.  index_to_word와 index_to_ner를 사용하여 테스트 데이터에 대한 예측값과 실제값을 비교 출력합니다.', 'i = 10 # 확인하고 싶은 테스트용 샘플의 인덱스.\\n# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\\ny_predicted = model.predict(np.array([X_test[i]]))\\n# 확률 벡터를 정수 레이블로 변경.\\ny_predicted = np.argmax(y_predicted, axis=-1)\\n# 원-핫 벡터를 정수 인코딩으로 변경.\\nlabels = np.argmax(y_test[i], -1)\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\\nif word != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag].upper(), index_to_ner[pred].upper()))', '단어             |실제값  |예측값\\n-----------------------------------\\nsarah            : B-PER   B-PER\\nbrady            : I-PER   I-PER\\n,                : O       O\\nwhose            : O       O\\nrepublican       : B-MISC  B-MISC\\nhusband          : O       O\\nwas              : O       O\\nOOV              : O       O\\nOOV              : O       O\\nin               : O       O\\nan               : O       O\\nOOV              : O       O\\nattempt          : O       O\\non               : O       O', 'OOV              : O       O\\nattempt          : O       O\\non               : O       O\\npresident        : O       O\\nronald           : B-PER   B-PER\\nreagan           : I-PER   I-PER\\n,                : O       O\\ntook             : O       O\\ncentre           : O       O\\nstage            : O       O\\nat               : O       O\\nthe              : O       O\\ndemocratic       : B-MISC  B-MISC\\nnational         : I-MISC  I-MISC\\nconvention       : I-MISC  I-MISC\\non               : O       O', \"national         : I-MISC  I-MISC\\nconvention       : I-MISC  I-MISC\\non               : O       O\\nmonday           : O       O\\nnight            : O       O\\nto               : O       O\\nOOV              : O       O\\npresident        : O       O\\nbill             : B-PER   B-PER\\nclinton          : I-PER   I-PER\\n's               : O       O\\ngun              : O       O\\ncontrol          : O       O\\nefforts          : O       O\\n.                : O       O\", \"control          : O       O\\nefforts          : O       O\\n.                : O       O\\n정확도를 계산하고, 테스트용 샘플에 대해서 예측한 개체명도 출력해보았습니다. 사실 이번에 사용한 정확도 측정 방법이 그다지 적절하지는 않았는데, 대부분의 단어가 개체명이 아니라는 'O'가 태깅된 상황에서 정확도가 수많은 'O'로 인해 결정되고 있기 때문입니다. 이를 해결하기 위해서 다음 실습에서 F1-score를 도입해보겠습니다.\\n==================================================\\n--- 12-05 BiLSTM을 이용한 개체명 인식(Named Entity Recognition, NER) ---\\n```\\nF1-score: 78.5%\\nprecision    recall  f1-score   support\\nart       0.11      0.02      0.03        63\", 'precision    recall  f1-score   support\\nart       0.11      0.02      0.03        63\\neve       0.28      0.29      0.29        52\\ngeo       0.84      0.84      0.84      7620\\ngpe       0.96      0.94      0.95      3145\\nnat       0.46      0.30      0.36        37\\norg       0.57      0.58      0.57      4033\\nper       0.73      0.70      0.71      3545\\ntim       0.84      0.85      0.84      4067\\nmicro avg       0.79      0.78      0.78     22562', 'tim       0.84      0.85      0.84      4067\\nmicro avg       0.79      0.78      0.78     22562\\nmacro avg       0.60      0.56      0.57     22562\\nweighted avg       0.79      0.78      0.78     22562\\n```양방향 LSTM을 이용하여 개체명 인식기를 만든 후에 F1-score를 사용하여 모델을 평가합니다.']\n",
      "['앞서 사용한 데이터와는 다른 데이터를 사용하여 개체명 인식을 수행해보겠습니다. 데이터는 아래의 링크에서 다운로드합니다.\\n링크 : https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport urllib.request\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.utils import to_categorical', 'from tensorflow.keras.utils import to_categorical\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20Sequence%20Labeling/dataset/ner_dataset.csv\", filename=\"ner_dataset.csv\")\\ndata = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\\ndata[:5]\\n[이미지: ]', 'data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\\ndata[:5]\\n[이미지: ]\\n데이터의 형식을 이해해봅시다. 첫번째 열 \\'Sentence :#\\'은 다음과 같은 패턴을 가지고 있습니다. Sentence: 1이 등장하고 Null 값이 이어지다가 다시 Sentence: 2가 등장하고 다시 Null 값이 이어지다가 Sentence: 3이 등장하고 다시 Null 값이 이어지다가를 반복합니다. 사실 이는 하나의 문장을 여러 행으로 나눠놓은 것입니다. 숫자값을 t라고 합시다. 첫번째 Sentence: t부터 Null 값이 나오다가 Sentence: t+1이 나오기 전까지의 모든 행은 기존에 하나의 문장이었습니다. t번째 문장을 단어 토큰화 후 각 행으로 나눠놓은 데이터이기 때문입니다. 뒤에서 Pandas의 fillna를 통해 하나로 묶는 작업을 해줍니다.\\nprint(\\'데이터프레임 행의 개수 : {}\\'.format(len(data)))', \"print('데이터프레임 행의 개수 : {}'.format(len(data)))\\n데이터프레임 행의 개수 : 1048575\\n현재 data의 행의 개수는 1,048,575개입니다. 하지만 뒤에서 문장 1개를 다수의 행들으로 나누어 놓은 것을 다시 1개의 행으로 병합하는 작업을 해야하기 때문에 최종 샘플의 개수는 이보다 줄어들게 됩니다. 결측값 유무를 살펴봅시다.\\nprint('데이터에 Null 값이 있는지 유무 : ' + str(data.isnull().values.any()))\\n데이터에 Null 값이 있는지 유무 : True\\nSentence #열에 Null 값들이 존재하고 있어 isnull().values.any()를 수행하였을 때 True가 나옵니다. isnull().sum()을 수행하면 각 열마다의 Null 값의 개수를 보여줍니다.\\nprint('어떤 열에 Null값이 있는지 출력')\\nprint('==============================')\", \"print('어떤 열에 Null값이 있는지 출력')\\nprint('==============================')\\ndata.isnull().sum()\\n어떤 열에 Null값이 있는지 출력\\n==============================\\nSentence #    1000616\\nWord                0\\nPOS                 0\\nTag                 0\\ndtype: int64\\n다른 열은 0개인데 오직 Sentences #열에서만 1,000,616개가 나온 것을 볼 수 있습니다. 전체 데이터에서 중복을 허용하지 않고 유일한 값의 개수를 셀 수 있게 해주는 nunique()를 사용해봅시다.\\nprint('sentence # 열의 중복을 제거한 값의 개수 : {}'.format(data['Sentence #'].nunique()))\", \"print('sentence # 열의 중복을 제거한 값의 개수 : {}'.format(data['Sentence #'].nunique()))\\nprint('Word 열의 중복을 제거한 값의 개수 : {}'.format(data.Word.nunique()))\\nprint('Tag 열의 중복을 제거한 값의 개수 : {}'.format(data.Tag.nunique()))\\nsentence # 열의 중복을 제거한 값의 개수 : 47959\\nWord 열의 중복을 제거한 값의 개수 : 35178\\nTag 열의 중복을 제거한 값의 개수 : 17\\n이 데이터에는 47,959개의 문장이 있으며 문장들은 35,178개의 단어를 가지고 17개 종류의 개체명 태깅을 가집니다. 17개의 개체명 태깅이 전체 데이터에서 몇 개가 있는지, 개체명 태깅 개수의 분포를 확인해보도록 하겠습니다.\\nprint('Tag 열의 각각의 값의 개수 카운트')\\nprint('================================')\", \"print('Tag 열의 각각의 값의 개수 카운트')\\nprint('================================')\\nprint(data.groupby('Tag').size().reset_index(name='count'))\\nTag 열의 각각의 값의 개수 카운트\\n================================\\nTag   count\\n0   B-art     402\\n1   B-eve     308\\n2   B-geo   37644\\n3   B-gpe   15870\\n4   B-nat     201\\n5   B-org   20143\\n6   B-per   16990\\n7   B-tim   20333\\n8   I-art     297\\n9   I-eve     253\\n10  I-geo    7414\\n11  I-gpe     198\\n12  I-nat      51\\n13  I-org   16784\\n14  I-per   17251\\n15  I-tim    6528\", '11  I-gpe     198\\n12  I-nat      51\\n13  I-org   16784\\n14  I-per   17251\\n15  I-tim    6528\\n16      O  887908\\nBIO 표현 방법에서 아무런 태깅도 의미하지 않는 O가 가장 887,908개로 가장 많은 개수를 차지함을 볼 수 있습니다. 데이터를 원하는 형태로 가공해보겠습니다. 우선 Null 값을 제거합니다. Pandas의 fillna(method=\\'ffill\\')는 Null 값을 가진 행의 바로 앞의 행의 값으로 Null 값을 채우는 작업을 수행합니다. t번째 문장에 속하면서 Null 값을 가진 샘플들은 전부 첫번째 열에 Sentence: t의 값이 들어갑니다. 이번에는 뒤의 5개의 샘플을 출력해서 정상적으로 수행되었는지 확인해봅시다.\\ndata = data.fillna(method=\"ffill\")\\nprint(data.tail())\\nSentence #       Word  POS Tag', 'data = data.fillna(method=\"ffill\")\\nprint(data.tail())\\nSentence #       Word  POS Tag\\n1048570  Sentence: 47959       they  PRP   O\\n1048571  Sentence: 47959  responded  VBD   O\\n1048572  Sentence: 47959         to   TO   O\\n1048573  Sentence: 47959        the   DT   O\\n1048574  Sentence: 47959     attack   NN   O\\n뒤의 5개 샘플의 첫번째 열이 Sentence: 47959로 채워졌습니다. 이는 47,959번째 문장임을 의미하며, Null 값을 가진 행들의 바로 앞 행의 Sentence # 열의 값이 Sentence: 47959이었음을 의미합니다. 전체 데이터에 Null 값이 존재하는지 확인해봅시다.', \"print('데이터에 Null 값이 있는지 유무 : ' + str(data.isnull().values.any()))\\n데이터에 Null 값이 있는지 유무 : False\\n없는 것으로 나옵니다. 모든 단어를 소문자화하여 단어의 개수를 줄여보겠습니다.\\ndata['Word'] = data['Word'].str.lower()\\nprint('Word 열의 중복을 제거한 값의 개수 : {}'.format(data.Word.nunique()))\\nWord 열의 중복을 제거한 값의 개수 : 31817\\n정상적으로 소문자화가 되었는지 앞의 샘플 5개만 출력해보겠습니다.\\nprint(data[:5])\\nSentence #           Word  POS Tag\\n0  Sentence: 1      thousands  NNS   O\\n1  Sentence: 1             of   IN   O\\n2  Sentence: 1  demonstrators  NNS   O\", '1  Sentence: 1             of   IN   O\\n2  Sentence: 1  demonstrators  NNS   O\\n3  Sentence: 1           have  VBP   O\\n4  Sentence: 1        marched  VBN   O\\n하나의 문장에 등장한 단어와 개체명 태깅 정보끼리 쌍(pair)으로 묶는 작업을 수행합니다.\\nfunc = lambda temp: [(w, t) for w, t in zip(temp[\"Word\"].values.tolist(), temp[\"Tag\"].values.tolist())]\\ntagged_sentences=[t for t in data.groupby(\"Sentence #\").apply(func)]\\nprint(\"전체 샘플 개수: {}\".format(len(tagged_sentences)))\\n전체 샘플 개수: 47959', 'print(\"전체 샘플 개수: {}\".format(len(tagged_sentences)))\\n전체 샘플 개수: 47959\\n1,000,616개의 행의 개수가 각 문장당 하나의 샘플로 묶이면서 47,959개의 샘플이 되었습니다. 정상적으로 수행되었는지 첫번째 샘플을 출력해봅시다.\\nprint(tagged_sentences[0]) # 첫번째 샘플 출력', \"print(tagged_sentences[0]) # 첫번째 샘플 출력\\n[('thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('london', 'B-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O'), ('war', 'O'), ('in', 'O'), ('iraq', 'B-geo'), ('and', 'O'), ('demand', 'O'), ('the', 'O'), ('withdrawal', 'O'), ('of', 'O'), ('british', 'B-gpe'), ('troops', 'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\", \"전처리가 수행된 첫번째 샘플이 출력됩니다. 이러한 샘플이 총 47,959개가 있습니다. 그런데 훈련을 시키려면 훈련 데이터에서 단어에 해당되는 부분과 개체명 태깅 정보에 해당되는 부분을 분리시켜야 합니다. 즉, [('thousands', 'O'), ('of', 'O')]와 같은 문장 샘플이 있다면 thousands와 of는 같이 저장하고, O와 O를 같이 저장할 필요가 있습니다. 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할을 하는 zip()을 사용하여 단어와 개체명 태깅 정보를 분리해봅시다.\\nsentences, ner_tags = [], []\\nfor tagged_sentence in tagged_sentences: # 47,959개의 문장 샘플을 1개씩 불러온다.\\n# 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\\nsentence, tag_info = zip(*tagged_sentence)\", '# 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\\nsentence, tag_info = zip(*tagged_sentence)\\nsentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\\nner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다.\\n각 문장 샘플에서 단어는 sentences에 태깅 정보는 ner_tags에 저장하였습니다. 임의로 첫번째 문장 샘플을 출력해보겠습니다.\\nprint(sentences[0])\\nprint(ner_tags[0])', \"print(sentences[0])\\nprint(ner_tags[0])\\n['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\\n['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\", \"첫번째 샘플에 대해서 단어에 대해서만 sentences[0]에, 또한 개체명에 대해서만 ner_tags[0]에 저장된 것을 볼 수 있습니다. 뒤에서 보겠지만, sentences는 예측을 위한 X에 해당되며 ner_tags는 예측 대상인 y에 해당됩니다. 다른 샘플들에 대해서도 처리가 되었는지 확인하기 위해 임의로 98번 인덱스의 샘플에 대해서도 확인해보겠습니다.\\nprint(sentences[98])\\nprint(ner_tags[98])\\n['she', 'had', 'once', 'received', 'a', 'kidney', 'transplant', '.']\\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\", \"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\\n단어에 대해서만 sentences[98]에, 또한 개체명에 대해서만 ner_tags[98]에 저장된 것을 확인할 수 있습니다. 또한 첫번째 샘플과 길이가 다릅니다. 47,959개의 문장 샘플의 길이는 서로 다를 수 있습니다. 전체 데이터의 길이 분포를 확인해봅시다.\\nprint('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\\nprint('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\\nplt.hist([len(s) for s in sentences], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n샘플의 최대 길이 : 104\\n샘플의 평균 길이 : 21.863987989741236\\n[이미지: ]\", \"plt.ylabel('number of samples')\\nplt.show()\\n샘플의 최대 길이 : 104\\n샘플의 평균 길이 : 21.863987989741236\\n[이미지: ]\\n위의 그래프는 샘플들의 길이가 대체적으로 0~40의 길이를 가지는 것을 보여줍니다. 길이가 가장 긴 샘플의 길이는 104입니다. 케라스 토크나이저를 통해서 정수 인코딩을 진행합니다. 이번에는 문장 데이터에 있는 모든 단어를 사용하겠습니다.\\n# 모든 단어를 사용하며 인덱스 1에는 단어 'OOV'를 할당.\\nsrc_tokenizer = Tokenizer(oov_token='OOV')\\n# 태깅 정보들은 내부적으로 대문자를 유지한 채 저장\\ntar_tokenizer = Tokenizer(lower=False)\\nsrc_tokenizer.fit_on_texts(sentences)\\ntar_tokenizer.fit_on_texts(ner_tags)\", \"src_tokenizer.fit_on_texts(sentences)\\ntar_tokenizer.fit_on_texts(ner_tags)\\n문장 데이터에 대해서는 src_tokenizer를, 레이블에 해당되는 개체명 태깅 정보에 대해서는 tar_tokenizer를 사용합니다.\\nvocab_size = len(src_tokenizer.word_index) + 1\\ntag_size = len(tar_tokenizer.word_index) + 1\\nprint('단어 집합의 크기 : {}'.format(vocab_size))\\nprint('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))\\n단어 집합의 크기 : 31819\\n개체명 태깅 정보 집합의 크기 : 18\\n앞서 src_tokenizer를 만들때 Tokenizer의 인자로 oov_token='OOV'를 선택했습니다. 인덱스1에 단어 'OOV'가 할당됩니다.\", \"앞서 src_tokenizer를 만들때 Tokenizer의 인자로 oov_token='OOV'를 선택했습니다. 인덱스1에 단어 'OOV'가 할당됩니다.\\nprint('단어 OOV의 인덱스 : {}'.format(src_tokenizer.word_index['OOV']))\\n단어 OOV의 인덱스 : 1\\n정수 인코딩을 수행합니다.\\nX_data = src_tokenizer.texts_to_sequences(sentences)\\ny_data = tar_tokenizer.texts_to_sequences(ner_tags)\\n문장 데이터에 대해서 정수 인코딩이 수행된 결과는 X_data, 개체명 태깅 데이터에 대해서 정수 인코딩이 수행된 결과는 y_data에 저장되었습니다. 정수 인코딩이 되었는지 확인을 위해 임의로 첫번째 샘플을 출력해보겠습니다.\\nprint(X_data[0])\\nprint(y_data[0])\", \"print(X_data[0])\\nprint(y_data[0])\\n[254, 6, 967, 16, 1795, 238, 468, 7, 523, 2, 129, 5, 61, 9, 571, 2, 833, 6, 186, 90, 22, 15, 56, 3]\\n[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1]\\n모델 훈련 후 결과 확인을 위해 인덱스로부터 단어를 리턴하는 index_to_word와 인덱스로부터 개체명 태깅 정보를 리턴하는 index_to_ner를 만듭니다. 인덱스 0은 'PAD'란 단어를 할당해둡니다. index_to_ner은 개수가 적으니 출력해봅시다.\\nword_to_index = src_tokenizer.word_index\\nindex_to_word = src_tokenizer.index_word\\nner_to_index = tar_tokenizer.word_index\", \"index_to_word = src_tokenizer.index_word\\nner_to_index = tar_tokenizer.word_index\\nindex_to_ner = tar_tokenizer.index_word\\nindex_to_ner[0] = 'PAD'\\nprint(index_to_ner)\\n{1: 'O', 2: 'B-geo', 3: 'B-tim', 4: 'B-org', 5: 'I-per', 6: 'B-per', 7: 'I-org', 8: 'B-gpe', 9: 'I-geo', 10: 'I-tim', 11: 'B-art', 12: 'B-eve', 13: 'I-art', 14: 'I-eve', 15: 'B-nat', 16: 'I-gpe', 17: 'I-nat', 0: 'PAD'}\\nindex_to_word를 통해 첫번째 샘플의 정수 시퀀스를 텍스트 시퀀스로 변환하는 디코딩 작업을 해보겠습니다.\\ndecoded = []\", \"index_to_word를 통해 첫번째 샘플의 정수 시퀀스를 텍스트 시퀀스로 변환하는 디코딩 작업을 해보겠습니다.\\ndecoded = []\\nfor index in X_data[0] : # 첫번째 샘플 안의 인덱스들에 대해서\\ndecoded.append(index_to_word[index]) # 다시 단어로 변환\\nprint('기존의 문장 : {}'.format(sentences[0]))\\nprint('디코딩 문장 : {}'.format(decoded))\\n기존의 문장 : ['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\", \"디코딩 문장 : ['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\\nX 데이터와 y 데이터가 구성되었습니다. 패딩 작업을 진행해봅시다. 앞서 확인하였듯이 대부분의 데이터의 길이는 40~60에 분포되어져 있습니다. 그러므로 가장 긴 샘플의 길이인 104가 아니라 70정도로 max_len을 정해보겠습니다.\\nmax_len = 70\\nX_data = pad_sequences(X_data, padding='post', maxlen=max_len)\\ny_data = pad_sequences(y_data, padding='post', maxlen=max_len)\", \"y_data = pad_sequences(y_data, padding='post', maxlen=max_len)\\n모든 샘플의 길이가 70이 되었습니다. 훈련 데이터와 테스트 데이터를 8:2의 비율로 분리합니다.\\nX_train, X_test, y_train_int, y_test_int = train_test_split(X_data, y_data, test_size=.2, random_state=777)\\n레이블에 해당하는 태깅 정보에 대해서 원-핫 인코딩을 수행합니다.\\ny_train = to_categorical(y_train_int, num_classes=tag_size)\\ny_test = to_categorical(y_test_int, num_classes=tag_size)\\n각 데이터 크기를 확인해보겠습니다.\\nprint('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\", \"각 데이터 크기를 확인해보겠습니다.\\nprint('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\\nprint('훈련 샘플 레이블(정수 인코딩)의 크기 : {}'.format(y_train_int.shape))\\nprint('훈련 샘플 레이블(원-핫 인코딩)의 크기 : {}'.format(y_train.shape))\\nprint('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블(정수 인코딩)의 크기 : {}'.format(y_test_int.shape))\\nprint('테스트 샘플 레이블(원-핫 인코딩)의 크기 : {}'.format(y_test.shape))\\n훈련 샘플 문장의 크기 : (38367, 70)\\n훈련 샘플 레이블(정수 인코딩)의 크기 : (38367, 70)\\n훈련 샘플 레이블(원-핫 인코딩)의 크기 : (38367, 70, 18)\\n테스트 샘플 문장의 크기 : (9592, 70)\", '훈련 샘플 레이블(원-핫 인코딩)의 크기 : (38367, 70, 18)\\n테스트 샘플 문장의 크기 : (9592, 70)\\n테스트 샘플 레이블(정수 인코딩)의 크기 : (9592, 70)\\n테스트 샘플 레이블(원-핫 인코딩)의 크기 : (9592, 70, 18)']\n",
      "['하이퍼파라미터인 임베딩 벡터의 차원은 128, 은닉 상태의 크기는 256입니다. 모델은 다 대 다 구조의 양방향 LSTM을 사용합니다. 이 경우 LSTM의 return_sequences의 인자값은 True로 주어야만 합니다. 이번 실습과 같이 각 데이터의 길이가 달라서 패딩을 하느라 숫자 0이 많아질 경우에는 Embedding()에 mask_zero=True를 설정하여 숫자 0은 연산에서 제외시킨다는 옵션을 줄 수 있습니다. 출력층에 TimeDistributed()를 사용했는데, TimeDistributed()는 LSTM을 다 대 다 구조로 사용하여 LSTM의 모든 시점에 대해서 출력층을 사용할 필요가 있을 때 사용합니다.', '해당 모델은 모든 시점에 대해서 개체명 레이블 개수만큼의 선택지 중 하나를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용합니다. 하이퍼파라미터인 배치 크기는 128이며, 6 에포크를 수행합니다. validation_split=0.1을 사용하여 훈련 데이터의 10%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding', \"from tensorflow.keras.optimizers import Adam\\nembedding_dim = 128\\nhidden_units = 256\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embedding_dim, mask_zero=True))\\nmodel.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))\\nmodel.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))\\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=6, validation_split=0.1)\", 'history = model.fit(X_train, y_train, batch_size=128, epochs=6, validation_split=0.1)\\n검증 데이터에 대해서 약 95%의 정확도를 얻습니다. 테스트 데이터의 임의의 인덱스 13번 샘플에 대해서 실제값과 예측값을 비교해봅시다.\\ni = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\\ny_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\\ny_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 인코딩으로 변경함.\\nlabels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")', 'print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\\nif word != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))\\n단어             |실제값  |예측값\\n-----------------------------------\\nthe              : O       O\\nstatement        : O       O\\ncame             : O       O\\nas               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org', \"as               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org\\nkofi             : B-per   B-per\\nannan            : I-per   I-per\\nmet              : O       O\\nwith             : O       O\\nofficials        : O       O\\nin               : O       O\\namman            : B-geo   B-geo\\nto               : O       O\\ndiscuss          : O       O\\nwednesday        : B-tim   B-tim\\n's               : O       O\\nattacks          : O       O\\n.                : O       O\", \"'s               : O       O\\nattacks          : O       O\\n.                : O       O\\n정확하게 예측했습니다. F1-score라는 성능 평가 방법에 대해서 이해하고, 테스트 데이터에 대해서 성능을 측정해봅시다.\"]\n",
      "[\"개체명 인식에서는 그 어떤 개체도 아니라는 의미의 'O'라는 태깅이 존재합니다. 그런데 이런 정보는 보통 대다수의 레이블을 차지하기 때문에 기존에 사용했던 정확도(accuracy)를 평가 방법으로 사용하는 것이 적절하지 않을 수 있습니다.\\n예를 들어 모델이 단 1개의 개체도 맞추지 못하고 전부 'O'로 예상했을 경우를 봅시다. 실제값은 위에서 출력했던 값을 실제값으로 재사용하겠습니다. 아래 코드에서는 labels라는 변수에 저장하였습니다. 그리고 개체를 하나도 맞추지 못했다는 가정하에 전부 'O'로만 채워진 예측값 predicted를 생성합니다.\", \"labels = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','B-MISC','I-MISC','I-MISC','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O']\\npredicted = ['O'] * len(labels)\\nprint('예측값 :',predicted)\\n예측값 : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\", '실제로는 PER, MISC, PER, MISC, PER이라는 총 5개의 개체가 존재함에도 불구하고 예측값인 predicted는 단 1개의 개체도 맞추지 못한 상황을 시뮬레이션하는 것입니다. 이에 대한 정확도를 계산해봅시다.\\nhit = 0 # 정답 개수\\nfor tag, pred in zip(labels, predicted):\\nif tag == pred:\\nhit +=1 # 정답인 경우에만 +1\\naccuracy = hit/len(labels) # 정답 개수를 총 개수로 나눈다.\\nprint(\"정확도: {:.1%}\".format(accuracy))\\n정확도: 74.4%\\n실제값에서도 대부분의 값이 \\'O\\'이기 때문에 그 어떤 개체도 찾지 못하였음에도 74%의 정확도를 얻습니다. 이는 정확도가 뻥튀기되어 모델의 성능을 오해할 수 있다는 문제가 있습니다. 그래서 여기서는 위와 같은 상황에서 더 적절한 평가 방법을 도입하고자 합니다. 파이썬 패키지 seqeval를 설치합니다.', 'pip install seqeval\\n앞서 머신 러닝 훑어보기 챕터에서 정밀도(precision)과 재현률(recall)을 언급한 바 있습니다. 개체명 인식 모델의 성능 측정을 위해 정밀도와 재현률 개념을 사용해보겠습니다. 이를 개체명 인식 문제에 맞도록 해석해보면 다음과 같습니다.\\n재현률\\n$$\\n\\\\text{정밀도} = \\\\frac{TP}{TP + FP} = \\\\text{특정 개체라고 예측한 경우 중에서 실제 특정 개체로 판명되어 예측이 일치한 비율}\\n$$\\n$$\\n\\\\text{재현률} = \\\\frac{TP}{TP + FN} = \\\\text{전체 특정 개체 중에서 실제 특정 개체라고 정답을 맞춘 비율}\\n$$\\n정밀도와 재현률로부터 조화 평균(harmonic mean)을 구한 것을 f1-score라고 합니다.\\n$$\\nf1\\\\ score = 2 × \\\\frac{\\\\text{정밀도 × 재현률}}{\\\\text{정밀도 + 재현률}}\\n$$', '$$\\nf1\\\\ score = 2 × \\\\frac{\\\\text{정밀도 × 재현률}}{\\\\text{정밀도 + 재현률}}\\n$$\\npredicted의 성능을 평가하기 위해서 정밀도, 재현률, f1-score를 계산해보도록 하겠습니다.\\nfrom seqeval.metrics import classification_report\\nprint(classification_report([labels], [predicted]))\\nprecision    recall  f1-score   support\\nMISC       0.00      0.00      0.00         2\\nPER       0.00      0.00      0.00         3\\nmicro avg       0.00      0.00      0.00         5\\nmacro avg       0.00      0.00      0.00         5', \"macro avg       0.00      0.00      0.00         5\\nweighted avg       0.00      0.00      0.00         5\\n이러한 측정 방법을 사용하면 PER과 MISC 두 특정 개체 중에서 실제 predicted가 맞춘 것은 단 1개도 없는 것을 확인할 수 있습니다. 이번에는 어느 정도는 정답을 맞추었다고 가정하고 예측값인 predicted를 수정하여 정밀도, 재현률, f1-score를 확인해봅시다.\\nlabels = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','B-MISC','I-MISC','I-MISC','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O']\", \"predicted = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O']\\nprint(classification_report([labels], [predicted]))\\nprecision    recall  f1-score   support\\nMISC       1.00      0.50      0.67         2\\nPER       1.00      0.67      0.80         3\\nmicro avg       1.00      0.60      0.75         5\\nmacro avg       1.00      0.58      0.73         5\", 'macro avg       1.00      0.58      0.73         5\\nweighted avg       1.00      0.60      0.75         5\\n특정 개체로 예측한 경우에 대해서는 모두 제대로 예측을 하였으므로 정밀도는 1이 나옵니다. 하지만 재현률에서는 MISC는 실제로는 4개임에도 2개만을 맞추었으므로 0.5, PER은 실제로는 3개임에도 2개만을 맞추었으므로 0.67이 나온 것을 볼 수 있습니다.']\n",
      "['F1-score를 계산하기 위해서 개체명 태깅의 확률 벡터 또는 원-핫 벡터로부터 태깅 정보 시퀀스로 변환하는 함수인 sequences_to_tag를 만듭니다. 해당 함수를 통해 모델의 예측값인 y_predicted와 실제값에 해당하는 y_test를 태깅 정보 시퀀스로 변환합니다. 그리고 두 개를 비교하여 f1-score를 계산합니다.\\nfrom seqeval.metrics import f1_score, classification_report\\ndef sequences_to_tag(sequences):\\nresult = []\\n# 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\\nfor sequence in sequences:\\nword_sequence = []\\n# 시퀀스로부터 확률 벡터 또는 원-핫 벡터를 하나씩 꺼낸다.\\nfor pred in sequence:\\n# 정수로 변환. 예를 들어 pred가 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.', 'for pred in sequence:\\n# 정수로 변환. 예를 들어 pred가 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\\npred_index = np.argmax(pred)\\n# index_to_ner을 사용하여 정수를 태깅 정보로 변환. \\'PAD\\'는 \\'O\\'로 변경.\\nword_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\\nresult.append(word_sequence)\\nreturn result\\ny_predicted = model.predict([X_test])\\npred_tags = sequences_to_tag(y_predicted)\\ntest_tags = sequences_to_tag(y_test)\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))', 'print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nprint(classification_report(test_tags, pred_tags))\\nF1-score: 78.5%\\nprecision    recall  f1-score   support\\nart       0.11      0.02      0.03        63\\neve       0.28      0.29      0.29        52\\ngeo       0.84      0.84      0.84      7620\\ngpe       0.96      0.94      0.95      3145\\nnat       0.46      0.30      0.36        37\\norg       0.57      0.58      0.57      4033\\nper       0.73      0.70      0.71      3545', 'org       0.57      0.58      0.57      4033\\nper       0.73      0.70      0.71      3545\\ntim       0.84      0.85      0.84      4067\\nmicro avg       0.79      0.78      0.78     22562\\nmacro avg       0.60      0.56      0.57     22562\\nweighted avg       0.79      0.78      0.78     22562\\n이어서 CRF 층을 추가하여 성능을 높여봅시다.\\n==================================================\\n--- 12-06 BiLSTM-CRF를 이용한 개체명 인식 ---\\n```\\nF1-score: 79.1%\\nprecision    recall  f1-score   support', '--- 12-06 BiLSTM-CRF를 이용한 개체명 인식 ---\\n```\\nF1-score: 79.1%\\nprecision    recall  f1-score   support\\nart       0.00      0.00      0.00        63\\neve       0.91      0.19      0.32        52\\ngeo       0.82      0.85      0.83      7620\\ngpe       0.95      0.93      0.94      3145\\nnat       0.00      0.00      0.00        37\\norg       0.62      0.57      0.60      4033\\nper       0.76      0.70      0.73      3545\\ntim       0.87      0.83      0.85      4067', 'per       0.76      0.70      0.73      3545\\ntim       0.87      0.83      0.85      4067\\nmicro avg       0.80      0.78      0.79     22562\\nmacro avg       0.62      0.51      0.53     22562\\nweighted avg       0.80      0.78      0.79     22562\\n```이번 실습은 아래의 실습은 이미 실행한 상태라고 가정합니다.\\n이전 실습 링크 : https://wikidocs.net/147219\\n이번에는 기존의 양방향 LSTM 모델에 CRF(Conditional Random Field)라는 새로운 층을 추가하여 보다 모델을 개선시킨 양방향 LSTM + CRF 모델을 사용하여 개체명 인식(Named Entity Recognition)을 수행합니다.', '논문 링크 : https://arxiv.org/pdf/1508.01991v1.pdf\\n논문 링크 : https://arxiv.org/pdf/1603.01360.pdf']\n",
      "['CRF는 Conditional Random Field의 약자로 양방향 LSTM을 위해 탄생한 모델이 아니라 이전에 독자적으로 존재해왔던 모델입니다. 이를 양방향 LSTM 모델 위에 하나의 층으로 추가하여, 양방향 LSTM + CRF 모델이 탄생하였습니다. 여기서는 CRF의 수식적 이해가 아니라 양방향 LSTM + CRF 모델의 직관에 대해서 이해합니다.\\nCRF 층의 역할을 이해하기 위해서 간단한 개체명 인식 작업의 예를 들어보겠습니다. 사람(Person), 조직(Organization) 두 가지만을 태깅하는 간단한 태깅 작업에 BIO 표현을 사용한다면 여기서 사용하는 태깅의 종류는 아래의 5가지입니다.\\nB-Per, I-Per, B-Org, I-Org, O\\n아래의 그림은 위의 태깅을 수행하는 기존의 양방향 LSTM 개체명 인식 모델의 예를 보여줍니다.\\n[이미지: ]', 'B-Per, I-Per, B-Org, I-Org, O\\n아래의 그림은 위의 태깅을 수행하는 기존의 양방향 LSTM 개체명 인식 모델의 예를 보여줍니다.\\n[이미지: ]\\n위 모델은 각 단어를 벡터로 입력받고, 모델의 출력층에서 활성화 함수를 통해 개체명을 예측합니다. 사실 입력 단어들과 실제 개체명이 무엇인지 모르는 상황이므로 이 모델이 정확하게 개체명을 예측했는지는 위 그림만으로는 알 수 없습니다. 또 다른 예를 보겠습니다.\\n[이미지: ]\\n위 모델은 명확히 틀린 예측을 포함하고 있습니다. 입력 단어들과 실제값의 여부와 상관없이 이 사실을 알 수 있습니다. BIO 표현에 따르면 우선, 첫번째 단어의 레이블에서 I가 등장할 수 없습니다. 또한 I-Per은 반드시 B-Per 뒤에서만 등장할 수 있습니다. 뿐만 아니라, I-Org도 마찬가지로 B-Org 뒤에서만 등장할 수 있는데 위 모델은 이런 BIO 표현 방법의 제약사항들을 모두 위반하고 있습니다.', '여기서 양방향 LSTM 위에 CRF 층을 추가하여 얻을 수 있는 이점을 언급하겠습니다. CRF 층을 추가하면 모델은 예측 개체명, 다시 말해 레이블 사이의 의존성을 고려할 수 있습니다. 아래의 그림은 양방향 LSTM + CRF 모델을 보여줍니다.\\n[이미지: ]\\n앞서봤듯이, 기존에 CRF 층이 존재하지 않았던 양방향 LSTM 모델은 활성화 함수를 지난 시점에서 개체명을 결정했지만, CRF 층을 추가한 모델에서는 활성화 함수의 결과들이 CRF 층의 입력으로 전달됩니다. 예를 들어 $word_{1}$에 대한 양방향 LSTM 셀과 활성화 함수를 지난 출력값 [0.7, 0.12, 0.08, 0.04, 0.06]은 CRF 층의 입력이 됩니다. 마찬가지로 모든 단어에 대한 활성화 함수를 지난 출력값은 CRF 층의 입력이 되고, CRF 층은 레이블 시퀀스에 대해서 가장 높은 점수를 가지는 시퀀스를 예측합니다.', '이러한 구조에서 CRF 층은 점차적으로 훈련 데이터로부터 아래와 같은 제약사항 등을 학습하게 됩니다.\\n문장의 첫번째 단어에서는 I가 나오지 않습니다.\\nO-I 패턴은 나오지 않습니다.\\nB-I-I 패턴에서 개체명은 일관성을 유지합니다. 예를 들어 B-Per 다음에 I-Org는 나오지 않습니다.\\n요약하면 양방향 LSTM은 입력 단어에 대한 양방향 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영합니다.']\n",
      "['CRF 층을 손쉽게 사용하기 위한 keras-crf를 설치합니다.\\npip install keras-crf\\n깃허브 링크 : https://github.com/luozhouyang/keras-crf']\n",
      "['이전과 동일한 데이터에 대해서 모델을 학습해봅시다. 마지막 층에 CRF 층을 추가하기 위하여 함수형 API를 사용합니다. 하이퍼파라미터인 임베딩 벡터의 차원은 128, 은닉 상태의 크기는 64입니다. 모델은 다 대 다 구조의 양방향 LSTM을 사용합니다. 이 경우 LSTM의 return_sequences의 인자값은 True로 주어야만 합니다. 출력층에 TimeDistributed()를 사용했는데, TimeDistributed()는 LSTM을 다 대 다 구조로 사용하여 LSTM의 모든 시점에 대해서 출력층을 사용할 필요가 있을 때 사용합니다.\\n해당 모델은 모든 시점에 대해서 개체명 레이블 개수만큼의 선택지 중 하나를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 여기서는 최종 출력층이 CRF 층으로 CRF 층에 분류해야 하는 선택지 개수를 의미하는 tag_size를 전달해줍니다.\\nimport tensorflow as tf', \"import tensorflow as tf\\nfrom tensorflow.keras import Model\\nfrom tensorflow.keras.layers import Dense, LSTM, Input, Bidirectional, TimeDistributed, Embedding, Dropout\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nfrom keras_crf import CRFModel\\nfrom seqeval.metrics import f1_score, classification_report\\nembedding_dim = 128\\nhidden_units = 64\\ndropout_ratio = 0.3\\nsequence_input = Input(shape=(max_len,),dtype=tf.int32, name='sequence_input')\", \"dropout_ratio = 0.3\\nsequence_input = Input(shape=(max_len,),dtype=tf.int32, name='sequence_input')\\nmodel_embedding = Embedding(input_dim=vocab_size,\\noutput_dim=embedding_dim,\\ninput_length=max_len)(sequence_input)\\nmodel_bilstm = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(model_embedding)\\nmodel_dropout = TimeDistributed(Dropout(dropout_ratio))(model_bilstm)\\nmodel_dense = TimeDistributed(Dense(tag_size, activation='relu'))(model_dropout)\", \"model_dense = TimeDistributed(Dense(tag_size, activation='relu'))(model_dropout)\\nbase = Model(inputs=sequence_input, outputs=model_dense)\\nmodel = CRFModel(base, tag_size)\\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\", \"model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\\n하이퍼파라미터인 배치 크기는 128이며, 15 에포크를 수행합니다. validation_split=0.1을 사용하여 훈련 데이터의 10%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인합니다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용됩니다. 조기 종료를 사용하기 위해서 콜백을 정의합니다. keras-crf가 원-핫 인코딩 된 레이블은 지원하지 않으므로 y_train이 아니라 y_train_int를 사용함을 주의합니다.\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\", \"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('bilstm_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\\nhistory = model.fit(X_train, y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])\\n조기 종료로 학습이 끝났다면 검증 데이터에 대해서 정확도가 가장 높았을 당시를 저장해둔 가중치를 불러온 후, 임의로 선정한 테스트 데이터의 13번 인덱스의 샘플에 대해서 예측해봅시다.\\nmodel.load_weights('bilstm_crf/cp.ckpt')\", 'model.load_weights(\\'bilstm_crf/cp.ckpt\\')\\ni = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\\ny_predicted = model.predict(np.array([X_test[i]]))[0] # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\\nlabels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경.\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\\nif word != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))\\n단어             |실제값  |예측값', '단어             |실제값  |예측값\\n-----------------------------------\\nthe              : O       O\\nstatement        : O       O\\ncame             : O       O\\nas               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org\\nkofi             : B-per   B-per\\nannan            : I-per   I-per\\nmet              : O       O\\nwith             : O       O\\nofficials        : O       O\\nin               : O       O\\namman            : B-geo   B-geo\\nto               : O       O', \"in               : O       O\\namman            : B-geo   B-geo\\nto               : O       O\\ndiscuss          : O       O\\nwednesday        : B-tim   B-tim\\n's               : O       O\\nattacks          : O       O\\n.                : O       O\\n정확하게 잘 예측한 것 같습니다. 테스트 데이터에 대해서 성능을 측정해봅시다. 테스트 데이터에 대한 예측 시퀀스인 y_predicted를 얻습니다.\\ny_predicted = model.predict(X_test)[0]\\n상위 2개만 출력해봅시다.\\nprint(y_predicted[:2])\\n[[ 1  3 10  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0\", 'print(y_predicted[:2])\\n[[ 1  3 10  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0\\n0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\\n0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[ 1  1  1  1  1  1  3  1  1  1  1  1  1  1  2  9  9  1  0  0  0  0  0  0\\n0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\\n0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]', '0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\\n예측값으로 확률 벡터가 아니라 정수 시퀀스가 출력됩니다. 이 경우 이전 실습에서 사용했던 함수인 sequences_to_tag를 사용할 수 없으므로 함수를 수정해야 합니다. 확률 벡터가 아닌 정수 시퀀스를 입력으로 받아서 태깅 정보 시퀀스를 리턴하는 함수로 sequences_to_tag_for_crf를 만듭니다. 해당 함수를 사용하여 예측값과 레이블에 해당하는 y_test를 태깅 정보 시퀀스로 변환하여 F1-score를 계산합니다.\\ndef sequences_to_tag_for_crf(sequences):\\nresult = []\\n# 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\\nfor sequence in sequences:\\nword_sequence = []\\n# 시퀀스로부터 예측 정수 레이블을 하나씩 꺼낸다.\\nfor pred_index in sequence:', 'word_sequence = []\\n# 시퀀스로부터 예측 정수 레이블을 하나씩 꺼낸다.\\nfor pred_index in sequence:\\n# index_to_ner을 사용하여 정수를 태깅 정보로 변환. \\'PAD\\'는 \\'O\\'로 변경.\\nword_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\\nresult.append(word_sequence)\\nreturn result\\npred_tags = sequences_to_tag_for_crf(y_predicted)\\ntest_tags = sequences_to_tag(y_test)\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nprint(classification_report(test_tags, pred_tags))\\nF1-score: 79.1%\\nprecision    recall  f1-score   support', 'F1-score: 79.1%\\nprecision    recall  f1-score   support\\nart       0.00      0.00      0.00        63\\neve       0.91      0.19      0.32        52\\ngeo       0.82      0.85      0.83      7620\\ngpe       0.95      0.93      0.94      3145\\nnat       0.00      0.00      0.00        37\\norg       0.62      0.57      0.60      4033\\nper       0.76      0.70      0.73      3545\\ntim       0.87      0.83      0.85      4067\\nmicro avg       0.80      0.78      0.79     22562', 'tim       0.87      0.83      0.85      4067\\nmicro avg       0.80      0.78      0.79     22562\\nmacro avg       0.62      0.51      0.53     22562\\nweighted avg       0.80      0.78      0.79     22562\\n==================================================\\n--- 12-07 문자 임베딩(Character Embedding) 활용하기 ---\\n```\\nF1-score: 80.9%\\nprecision    recall  f1-score   support\\nart       0.29      0.03      0.06        63\\neve       1.00      0.04      0.07        52\\ngeo       0.83      0.86      0.85      7620', 'eve       1.00      0.04      0.07        52\\ngeo       0.83      0.86      0.85      7620\\ngpe       0.95      0.94      0.95      3145\\nnat       0.35      0.16      0.22        37\\norg       0.67      0.58      0.62      4033\\nper       0.79      0.74      0.77      3545\\ntim       0.88      0.84      0.86      4067\\nmicro avg       0.83      0.79      0.81     22562\\nmacro avg       0.72      0.52      0.55     22562\\nweighted avg       0.82      0.79      0.80     22562', 'weighted avg       0.82      0.79      0.80     22562\\n```이번 실습은 아래의 실습을 이미 실행한 상태라고 가정합니다.\\n이전 실습 링크 : https://wikidocs.net/147234\\n개체명 인식기의 성능을 올리기 위한 방법으로 문자 임베딩을 워드 임베딩과 함께 입력으로 사용하는 방법이 있습니다. 워드 임베딩에 문자 임베딩을 연결(concatenate)하여 성능을 높여봅시다.']\n",
      "[\"문자 임베딩을 위해서 하고자 하는 전처리는 문자 단위 정수 인코딩입니다. 가령 단어 'book'이 있고, b가 21번 o가 7번, k가 11번이라고 한다면 단어 'book'을 [21 7 7 11]로 인코딩합니다. 만약 단어 1개가 아니라 단어구 내지는 문장이라면 어떻게 될까요? 'good book'이란 문장이 있고, g가 12번, d가 17번이라고 한다면 이 문장을 문자 단위 정수 인코딩 후에는 다음과 같은 결과를 얻을 수 있습니다.\\n'good book의 정수 인코딩 결과'\\n[[12 7 7 17]\\n[21 7 7 11]]\\n이 각 문자와 맵핑된 정수를 각각 임베딩 층(Embedding layer)을 거치도록 하여, 문자 단위 임베딩을 얻게 됩니다. 향후 임베딩 층을 통과시키기 위해 문자에 대한 정수 인코딩을 진행해봅시다. 우선 전체 데이터의 모든 단어를 문자 레벨로 분해하여, 문자 집합을 만듭니다.\\n# char_vocab 만들기\", '# char_vocab 만들기\\nwords = list(set(data[\"Word\"].values))\\nchars = set([w_i for w in words for w_i in w])\\nchars = sorted(list(chars))\\nprint(\\'문자 집합 :\\',chars)', 'chars = set([w_i for w in words for w_i in w])\\nchars = sorted(list(chars))\\nprint(\\'문자 집합 :\\',chars)\\n문자 집합 : [\\'!\\', \\'\"\\', \\'#\\', \\'$\\', \\'%\\', \\'&\\', \"\\'\", \\'(\\', \\')\\', \\'+\\', \\',\\', \\'-\\', \\'.\\', \\'/\\', \\'0\\', \\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\', \\':\\', \\';\\', \\'?\\', \\'@\\', \\'[\\', \\']\\', \\'_\\', \\'`\\', \\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\', \\'f\\', \\'g\\', \\'h\\', \\'i\\', \\'j\\', \\'k\\', \\'l\\', \\'m\\', \\'n\\', \\'o\\', \\'p\\', \\'q\\', \\'r\\', \\'s\\', \\'t\\', \\'u\\', \\'v\\', \\'w\\', \\'x\\', \\'y\\', \\'z\\', \\'~\\', \\'\\\\x85\\', \\'\\\\x91\\', \\'\\\\x92\\', \\'\\\\x93\\', \\'\\\\x94\\', \\'\\\\x96\\', \\'\\\\x97\\', \\'\\\\xa0\\', \\'°\\', \\'é\\', \\'ë\\', \\'ö\\', \\'ü\\']', '이렇게 얻은 문자 집합으로부터 문자를 정수로 변환할 수 있는 딕셔너리인 char_to_index와 반대로 정수로부터 문자를 얻을 수 있는 딕셔너리인 index_to_char를 만듭니다.\\nchar_to_index = {c: i + 2 for i, c in enumerate(chars)}\\nchar_to_index[\"OOV\"] = 1\\nchar_to_index[\"PAD\"] = 0\\nindex_to_char = {}\\nfor key, value in char_to_index.items():\\nindex_to_char[value] = key\\n단어를 표현하는 문자 시퀀스의 최대 길이는 15로 제한 후 패딩합니다.\\nmax_len_char = 15\\n# 문자 시퀀스에 대한 패딩하는 함수\\ndef padding_char_indice(char_indice, max_len_char):\\nreturn pad_sequences(', \"# 문자 시퀀스에 대한 패딩하는 함수\\ndef padding_char_indice(char_indice, max_len_char):\\nreturn pad_sequences(\\nchar_indice, maxlen=max_len_char, padding='post', value = 0)\\n# 각 단어를 문자 시퀀스로 변환 후 패딩 진행\\ndef integer_coding(sentences):\\nchar_data = []\\nfor ts in sentences:\\nword_indice = [word_to_index[t] for t in ts]\\nchar_indice = [[char_to_index[char] for char in t]\\nfor t in ts]\\nchar_indice = padding_char_indice(char_indice, max_len_char)\\nfor chars_of_token in char_indice:\\nif len(chars_of_token) > max_len_char:\", \"for chars_of_token in char_indice:\\nif len(chars_of_token) > max_len_char:\\ncontinue\\nchar_data.append(char_indice)\\nreturn char_data\\n# 문자 단위 정수 인코딩 결과\\nX_char_data = integer_coding(sentences)\\n동일한 문장에 대해서 단어 단위 정수 인코딩과 문자 단위 정수 인코딩의 차이를 확인해봅시다. 첫번째 샘플은 다음과 같습니다.\\n# 정수 인코딩 이전의 기존 문장\\nprint('기존 문장 :',sentences[0])\", \"# 정수 인코딩 이전의 기존 문장\\nprint('기존 문장 :',sentences[0])\\n기존 문장 : ['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\\n위 문장을 정수 인코딩 및 패딩한 결과는 다음과 같습니다.\\n# 단어 단위 정수 인코딩 + 패딩\\nprint('단어 단위 정수 인코딩 :')\\nprint(X_data[0])\\n단어 단위 정수 인코딩 :\\n[ 254    6  967   16 1795  238  468    7  523    2  129    5   61    9\", \"단어 단위 정수 인코딩 :\\n[ 254    6  967   16 1795  238  468    7  523    2  129    5   61    9\\n571    2  833    6  186   90   22   15   56    3    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0]\\n254는 기존의 thousands, 6은 기존의 of에 해당됩니다. 해당 샘플을 문자 단위 정수 인코딩한 결과는 다음과 같습니다.\\n# 문자 단위 정수 인코딩\\nprint('문자 단위 정수 인코딩 :')\\nprint(X_char_data[0])\", \"# 문자 단위 정수 인코딩\\nprint('문자 단위 정수 인코딩 :')\\nprint(X_char_data[0])\\n문자 단위 정수 인코딩 :\\n[[53 41 48 54 52 34 47 37 52  0  0  0  0  0  0]\\n[48 39  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[37 38 46 48 47 52 53 51 34 53 48 51 52  0  0]\\n[41 34 55 38  0  0  0  0  0  0  0  0  0  0  0]\\n[46 34 51 36 41 38 37  0  0  0  0  0  0  0  0]\\n[53 41 51 48 54 40 41  0  0  0  0  0  0  0  0]\\n[45 48 47 37 48 47  0  0  0  0  0  0  0  0  0]\\n[53 48  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[49 51 48 53 38 52 53  0  0  0  0  0  0  0  0]\", '[53 48  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[49 51 48 53 38 52 53  0  0  0  0  0  0  0  0]\\n[53 41 38  0  0  0  0  0  0  0  0  0  0  0  0]\\n[56 34 51  0  0  0  0  0  0  0  0  0  0  0  0]\\n[42 47  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[42 51 34 50  0  0  0  0  0  0  0  0  0  0  0]\\n[34 47 37  0  0  0  0  0  0  0  0  0  0  0  0]\\n[37 38 46 34 47 37  0  0  0  0  0  0  0  0  0]\\n[53 41 38  0  0  0  0  0  0  0  0  0  0  0  0]\\n[56 42 53 41 37 51 34 56 34 45  0  0  0  0  0]', '[53 41 38  0  0  0  0  0  0  0  0  0  0  0  0]\\n[56 42 53 41 37 51 34 56 34 45  0  0  0  0  0]\\n[48 39  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[35 51 42 53 42 52 41  0  0  0  0  0  0  0  0]\\n[53 51 48 48 49 52  0  0  0  0  0  0  0  0  0]\\n[39 51 48 46  0  0  0  0  0  0  0  0  0  0  0]\\n[53 41 34 53  0  0  0  0  0  0  0  0  0  0  0]\\n[36 48 54 47 53 51 58  0  0  0  0  0  0  0  0]\\n[14  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]', '[36 48 54 47 53 51 58  0  0  0  0  0  0  0  0]\\n[14  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\\n위 출력 결과에서 각 행은 각 단어를 의미합니다. 가령, thousands는 첫번째 행 [53 41 48 54 52 34 47 37 52  0  0  0  0  0  0]에 해당됩니다. 단어의 최대 길이를 15(max_len_char)로 제한하였으므로, 길이가 15보다 짧은 단어는 뒤에 0으로 패딩됩니다. 53은 t, 41은 h, 48은 o, 54는 u에 각각 해당됩니다. X_data는 뒤에 0으로 패딩되어 길이가 70인 것에 비해 X_char_data는 현재 0번 단어는 무시되어 길이가 70이 아닙니다. 다시 말해 위 출력 결과에서 행의 개수가 70이 아닌 상태입니다. 길이 70으로 맞춰주기 위해서 문장 길이 방향으로도 패딩을 해줍니다.', \"X_char_data = pad_sequences(X_char_data, maxlen=max_len, padding='post', value = 0)\\n단어 단위 정수 인코딩 결과는 이미 X_train, y_train, X_test, y_test로 훈련 데이터와 테스트 데이터가 분리된 상태입니다. 문자 단위 정수 인코딩 결과에 대해서도 마찬가지로 X_char_train, X_char_test로 나누어줍니다.\\nX_char_train, X_char_test, _, _ = train_test_split(X_char_data, y_data, test_size=.2, random_state=777)\\nX_char_train = np.array(X_char_train)\\nX_char_test = np.array(X_char_test)\\n첫번째 훈련 데이터를 출력해봅시다.\\nprint(X_train[0])\", 'X_char_test = np.array(X_char_test)\\n첫번째 훈련 데이터를 출력해봅시다.\\nprint(X_train[0])\\n[ 150  928  361   17 2624    9 4131 3567    9    8 2893 1250  880  107\\n3    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0]\\n첫번째 훈련 샘플의 첫번째 단어인 150번은 원래 어떤 단어였을까요?\\nprint(index_to_word[150])\\nsoldiers', \"첫번째 훈련 샘플의 첫번째 단어인 150번은 원래 어떤 단어였을까요?\\nprint(index_to_word[150])\\nsoldiers\\nsoldiers라는 단어였습니다. 그렇다면 X_char_train의 첫번째 훈련 샘플의 첫번째 단어의 문자 정수 인코딩 결과로부터 soldiers라는 단어와 일치하는지 확인해보겠습니다.\\nprint(' '.join([index_to_char[index] for index in X_char_train[0][0]]))\\ns o l d i e r s PAD PAD PAD PAD PAD PAD PAD\\n각 데이터와 레이블의 크기를 확인해봅시다.\\nprint('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\\nprint('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\\nprint('훈련 샘플 char 데이터의 크기 : {}'.format(X_char_train.shape))\", \"print('훈련 샘플 char 데이터의 크기 : {}'.format(X_char_train.shape))\\nprint('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\\n훈련 샘플 문장의 크기 : (38367, 70)\\n훈련 샘플 레이블의 크기 : (38367, 70, 18)\\n훈련 샘플 char 데이터의 크기 : (38367, 70, 15)\\n테스트 샘플 문장의 크기 : (9592, 70)\\n테스트 샘플 레이블의 크기 : (9592, 70, 18)\"]\n",
      "['우선 문자 임베딩이 활용되는 과정을 보겠습니다. 하나의 단어는 문자 단위로 토큰화되었고, 토큰화 된 각 문자는 위의 전처리를 통해 정수로 맵핑된 상태입니다. 정수로 맵핑된 각 문자는 임베딩 층을 통과하면 64차원의 벡터가 됩니다. 이후 1D 합성곱 층의 입력으로 사용되는데, 1D 합성곱 층의 커널의 크기는 3이며 해당 커널은 총 30개 사용합니다. 1D 합성곱 층의 결과로 하나의 단어에 대한 단어 벡터를 얻습니다. 해당 단어 벡터는 일반적으로 워드 임베딩이라고 부르던 과정을 통해 얻은 단어의 임베딩 벡터와 연결(concatenate)됩니다. 이를 양방향 LSTM의 입력으로 사용하게 되는데, 이후에는 이전 실습들과 같습니다.', 'LSTM의 은닉 상태의 크기는 256입니다. 모델은 다 대 다 구조의 양방향 LSTM을 사용합니다. 이 경우 LSTM의 return_sequences의 인자값은 True로 주어야만 합니다. 출력층에 TimeDistributed()를 사용하여 LSTM의 모든 시점에 대해서 출력층을 사용합니다.\\nimport tensorflow as tf\\nfrom tensorflow.keras.layers import Embedding, Input, TimeDistributed, Dropout, concatenate, Bidirectional, LSTM, Conv1D, Dense, MaxPooling1D, Flatten\\nfrom tensorflow.keras import Model\\nfrom tensorflow.keras.initializers import RandomUniform\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint', \"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\nfrom tensorflow.keras.models import load_model\\nfrom seqeval.metrics import f1_score, classification_report\\nfrom keras_crf import CRFModel\\nembedding_dim = 128\\nchar_embedding_dim = 64\\ndropout_ratio = 0.5\\nhidden_units = 256\\nnum_filters = 30\\nkernel_size = 3\\n# 단어 임베딩\\nword_ids = Input(shape=(None,),dtype='int32', name='words_input')\\nword_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\\n# char 임베딩\", \"word_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\\n# char 임베딩\\nchar_ids = Input(shape=(None, max_len_char,), name='char_input')\\nembed_char_out = TimeDistributed(Embedding(len(char_to_index), char_embedding_dim, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_ids)\\ndropout = Dropout(dropout_ratio)(embed_char_out)\\n# char 임베딩에 대해서는 Conv1D 수행\", \"dropout = Dropout(dropout_ratio)(embed_char_out)\\n# char 임베딩에 대해서는 Conv1D 수행\\nconv1d_out = TimeDistributed(Conv1D(kernel_size=kernel_size, filters=num_filters, padding='same', activation='tanh', strides=1))(dropout)\\nmaxpool_out = TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\\nchar_embeddings = TimeDistributed(Flatten())(maxpool_out)\\nchar_embeddings = Dropout(dropout_ratio)(char_embeddings)\\n# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\\noutput = concatenate([word_embeddings, char_embeddings])\", \"# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\\noutput = concatenate([word_embeddings, char_embeddings])\\n# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\\noutput = Bidirectional(LSTM(hidden_units, return_sequences=True, dropout=dropout_ratio))(output)\\n# 출력층\\noutput = TimeDistributed(Dense(tag_size, activation='softmax'))(output)\\nmodel = Model(inputs=[word_ids, char_ids], outputs=[output])\\nmodel.compile(loss='categorical_crossentropy', optimizer='nadam',  metrics=['acc'])\", \"model.compile(loss='categorical_crossentropy', optimizer='nadam',  metrics=['acc'])\\n조기 종료를 조건으로 콜백을 정의합니다. 이전 실습과 동일한 조건으로 배치 크기는 128로 하고, 15 에포크를 학습합니다. 훈련 데이터의 10%를 검증 데이터로 사용합니다.\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('bilstm_cnn.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\\nhistory = model.fit([X_train, X_char_train], y_train, batch_size=128, epochs=15, validation_split=0.1, verbose=1, callbacks=[es, mc])\", '조기 종료로 학습이 끝났다면 검증 데이터에 대해서 정확도가 가장 높았을 당시를 저장해둔 가중치를 불러옵니다. 해당 모델에 대해서 테스트 데이터의 13번 인덱스의 샘플에 대해서 예측해봅시다.\\nmodel = load_model(\\'bilstm_cnn.h5\\')\\ni = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\\n# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\\ny_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])\\ny_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 인코딩으로 변경.\\nlabels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 정수 인코딩으로 변경.\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")', 'print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\\nif word != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))\\n단어             |실제값  |예측값\\n-----------------------------------\\nthe              : O       O\\nstatement        : O       O\\ncame             : O       O\\nas               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org', \"as               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org\\nkofi             : B-per   B-per\\nannan            : I-per   I-per\\nmet              : O       O\\nwith             : O       O\\nofficials        : O       O\\nin               : O       O\\namman            : B-geo   B-geo\\nto               : O       O\\ndiscuss          : O       O\\nwednesday        : B-tim   B-tim\\n's               : O       O\\nattacks          : O       O\\n.                : O       O\", '\\'s               : O       O\\nattacks          : O       O\\n.                : O       O\\n정확하게 잘 예측한 것 같습니다. 테스트 데이터에 대해서 성능을 측정해봅시다. 테스트 데이터에 대한 예측 시퀀스인 y_predicted를 얻습니다. 그리고 예측값과 실제값에 대한 태깅 정보 시퀀스를 얻은 후 F1-score를 계산합니다.\\ny_predicted = model.predict([X_test, X_char_test])\\npred_tags = sequences_to_tag(y_predicted)\\ntest_tags = sequences_to_tag(y_test)\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nprint(classification_report(test_tags, pred_tags))\\nF1-score: 79.0%', 'print(classification_report(test_tags, pred_tags))\\nF1-score: 79.0%\\nprecision    recall  f1-score   support\\nart       0.00      0.00      0.00        63\\neve       1.00      0.08      0.14        52\\ngeo       0.81      0.86      0.84      7620\\ngpe       0.95      0.94      0.94      3145\\nnat       0.00      0.00      0.00        37\\norg       0.59      0.56      0.57      4033\\nper       0.73      0.72      0.73      3545\\ntim       0.87      0.84      0.85      4067', 'per       0.73      0.72      0.73      3545\\ntim       0.87      0.84      0.85      4067\\nmicro avg       0.79      0.79      0.79     22562\\nmacro avg       0.62      0.50      0.51     22562\\nweighted avg       0.79      0.79      0.79     22562']\n",
      "[\"저자의 경우 '양방향 LSTM에 CRF 층을 추가적으로 사용한 모델' 또는 '양방향 LSTM에 문자 임베딩을 사용한 모델' 이렇게 두 가지 모델이 '양방향 LSTM만을 사용한 모델' 보다는성능이 더 좋은 것을 확인했습니다. 그렇다면 두 가지 모두를 활용해보는 것은 어떨까요? 이번에는 문자 임베딩을 사용한 위 모델에 CRF 층까지 추가적으로 사용해보겠습니다.\\nembedding_dim = 128\\nchar_embedding_dim = 64\\ndropout_ratio = 0.5\\nhidden_units = 256\\nnum_filters = 30\\nkernel_size = 3\\n# 단어 임베딩\\nword_ids = Input(shape=(None,),dtype='int32', name='words_input')\\nword_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\\n# char 임베딩\", \"word_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\\n# char 임베딩\\nchar_ids = Input(shape=(None, max_len_char,), name='char_input')\\nembed_char_out = TimeDistributed(Embedding(len(char_to_index), char_embedding_dim, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_ids)\\ndropout = Dropout(dropout_ratio)(embed_char_out)\\n# char 임베딩에 대해서는 Conv1D 수행\", \"dropout = Dropout(dropout_ratio)(embed_char_out)\\n# char 임베딩에 대해서는 Conv1D 수행\\nconv1d_out = TimeDistributed(Conv1D(kernel_size=kernel_size, filters=num_filters, padding='same',activation='tanh', strides=1))(dropout)\\nmaxpool_out=TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\\nchar_embeddings = TimeDistributed(Flatten())(maxpool_out)\\nchar_embeddings = Dropout(dropout_ratio)(char_embeddings)\\n# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\\noutput = concatenate([word_embeddings, char_embeddings])\", \"# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\\noutput = concatenate([word_embeddings, char_embeddings])\\n# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\\noutput = Bidirectional(LSTM(hidden_units, return_sequences=True, dropout=dropout_ratio))(output)\\n# 출력층\\noutput = TimeDistributed(Dense(tag_size, activation='relu'))(output)\\nbase = Model(inputs=[word_ids, char_ids], outputs=[output])\\nmodel = CRFModel(base, tag_size)\\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\", \"model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('bilstm_cnn_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\\nCRF 층은 원-핫 인코딩 된 레이블은 지원하지 않으므로 y_train이 아니라 y_train_int를 사용함을 주의합니다.\", \"CRF 층은 원-핫 인코딩 된 레이블은 지원하지 않으므로 y_train이 아니라 y_train_int를 사용함을 주의합니다.\\nhistory = model.fit([X_train, X_char_train], y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])\\n조기 종료로 학습이 끝났다면 검증 데이터에 대해서 정확도가 가장 높았을 당시를 저장해둔 가중치를 불러온 후, 테스트 데이터의 13번 인덱스의 샘플에 대해서 예측해봅시다.\\nmodel.load_weights('bilstm_cnn_crf/cp.ckpt')\\ni = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\\n# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\\ny_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0]\", 'y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0]\\nlabels = np.argmax(y_test[i], -1) # 원-핫 벡터를 정수 인코딩으로 변경.\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\\nif word != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))\\n단어             |실제값  |예측값\\n-----------------------------------\\nthe              : O       O', '단어             |실제값  |예측값\\n-----------------------------------\\nthe              : O       O\\nstatement        : O       O\\ncame             : O       O\\nas               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org\\nkofi             : B-per   B-per\\nannan            : I-per   I-per\\nmet              : O       O\\nwith             : O       O\\nofficials        : O       O\\nin               : O       O\\namman            : B-geo   B-geo\\nto               : O       O', \"in               : O       O\\namman            : B-geo   B-geo\\nto               : O       O\\ndiscuss          : O       O\\nwednesday        : B-tim   B-tim\\n's               : O       O\\nattacks          : O       O\\n.                : O       O\\n정확하게 잘 예측한 것 같습니다. 테스트 데이터에 대해서 성능을 측정해봅시다. 테스트 데이터에 대한 예측 시퀀스인 y_predicted를 얻습니다. 예측값과 실제값에 대한 태깅 정보 시퀀스를 얻은 후 F1-score를 계산합니다.\\ny_predicted = model.predict([X_test, X_char_test])[0]\\npred_tags = sequences_to_tag_for_crf(y_predicted)\", 'pred_tags = sequences_to_tag_for_crf(y_predicted)\\ntest_tags = sequences_to_tag(y_test)\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nprint(classification_report(test_tags, pred_tags))\\nF1-score: 81.0%\\nprecision    recall  f1-score   support\\nart       0.25      0.06      0.10        63\\neve       0.61      0.27      0.37        52\\ngeo       0.85      0.84      0.84      7620\\ngpe       0.94      0.94      0.94      3145\\nnat       0.33      0.05      0.09        37', 'gpe       0.94      0.94      0.94      3145\\nnat       0.33      0.05      0.09        37\\norg       0.66      0.60      0.63      4033\\nper       0.76      0.77      0.77      3545\\ntim       0.89      0.85      0.87      4067\\nmicro avg       0.82      0.80      0.81     22562\\nmacro avg       0.66      0.55      0.58     22562\\nweighted avg       0.82      0.80      0.81     22562']\n",
      "['문자 임베딩이 활용되는 과정을 보겠습니다. 하나의 단어는 문자 단위로 토큰화되었고, 토큰화 된 각 문자는 위의 전처리를 통해 정수로 맵핑된 상태입니다. 정수로 맵핑된 각 문자는 임베딩 층을 통과하면 64차원의 벡터가 됩니다. 이후 양방향 LSTM 입력으로 사용되는데, 이때 사용되는 LSTM의 은닉 상태의 크기는 64입니다. 해당 LSTM은 다 대 일(many-to-one)구조로 순방향 LSTM의 은닉 상태와 역방향 LSTM의 은닉 상태가 연결(concatenate)된 값이 양방향 LSTM의 출력입니다. 해당 출력을 하나의 단어에 대한 단어 벡터로 간주합니다. 해당 단어 벡터는 일반적으로 워드 임베딩이라고 부르던 과정을 통해 얻은 단어의 임베딩 벡터와 연결(concatenate)됩니다. 이를 개체명 인식을 위한 양방향 LSTM의 입력으로 사용하게 되는데, 이후에는 이전 실습들과 같습니다. CRF 층은 원-핫 인코딩 된 레이블은 지원하지 않으므로 y_train_int를 사용합니다.', \"embedding_dim = 128\\nchar_embedding_dim = 64\\ndropout_ratio = 0.3\\nhidden_units = 64\\n# 단어 임베딩\\nword_ids = Input(batch_shape=(None, None), dtype='int32', name='word_input')\\nword_embeddings = Embedding(input_dim=vocab_size,\\noutput_dim=embedding_dim,\\nname='word_embedding')(word_ids)\\n# char 임베딩\\nchar_ids = Input(batch_shape=(None, None, None), dtype='int32', name='char_input')\\nchar_embeddings = Embedding(input_dim=(len(char_to_index)),\\noutput_dim=char_embedding_dim,\", \"char_embeddings = Embedding(input_dim=(len(char_to_index)),\\noutput_dim=char_embedding_dim,\\nembeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5),\\nname='char_embedding')(char_ids)\\n# char 임베딩을 BiLSTM을 통과 시켜 단어 벡터를 얻고 단어 임베딩과 연결\\nchar_embeddings = TimeDistributed(Bidirectional(LSTM(hidden_units)))(char_embeddings)\\noutput = concatenate([word_embeddings, char_embeddings])\\n# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\\noutput = Dropout(dropout_ratio)(output)\", \"# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\\noutput = Dropout(dropout_ratio)(output)\\noutput = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(output)\\n# 출력층\\noutput = TimeDistributed(Dense(tag_size, activation='relu'))(output)\\nbase = Model(inputs=[word_ids, char_ids], outputs=[output])\\nmodel = CRFModel(base, tag_size)\\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\", \"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\\nmc = ModelCheckpoint('bilstm_bilstm_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\\nhistory = model.fit([X_train, X_char_train], y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])\\n조기 종료로 학습이 끝났다면 검증 데이터에 대해서 정확도가 가장 높았을 당시를 저장해둔 가중치를 불러온 후, 테스트 데이터의 13번 인덱스의 샘플에 대해서 예측합니다.\", '조기 종료로 학습이 끝났다면 검증 데이터에 대해서 정확도가 가장 높았을 당시를 저장해둔 가중치를 불러온 후, 테스트 데이터의 13번 인덱스의 샘플에 대해서 예측합니다.\\nmodel.load_weights(\\'bilstm_bilstm_crf/cp.ckpt\\')\\ni = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\\n# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\\ny_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0]\\nlabels = np.argmax(y_test[i], -1) # 원-핫 벡터를 정수 인코딩으로 변경.\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\\nif word != 0: # PAD값은 제외함.', 'for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\\nif word != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))\\n단어             |실제값  |예측값\\n-----------------------------------\\nthe              : O       O\\nstatement        : O       O\\ncame             : O       O\\nas               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org\\nkofi             : B-per   B-per\\nannan            : I-per   I-per', \"secretary-general: I-org   I-org\\nkofi             : B-per   B-per\\nannan            : I-per   I-per\\nmet              : O       O\\nwith             : O       O\\nofficials        : O       O\\nin               : O       O\\namman            : B-geo   B-geo\\nto               : O       O\\ndiscuss          : O       O\\nwednesday        : B-tim   B-tim\\n's               : O       O\\nattacks          : O       O\\n.                : O       O\", '\\'s               : O       O\\nattacks          : O       O\\n.                : O       O\\n정확하게 잘 예측한 것 같습니다. 테스트 데이터에 대해서 성능을 측정해봅시다. 테스트 데이터에 대한 예측 시퀀스인 y_predicted를 얻습니다. 예측값과 실제값에 대한 태깅 정보 시퀀스를 얻은 후 F1-score를 계산합니다.\\ny_predicted = model.predict([X_test, X_char_test])[0]\\npred_tags = sequences_to_tag_for_crf(y_predicted)\\ntest_tags = sequences_to_tag(y_test)\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nprint(classification_report(test_tags, pred_tags))\\nF1-score: 80.9%', 'print(classification_report(test_tags, pred_tags))\\nF1-score: 80.9%\\nprecision    recall  f1-score   support\\nart       0.29      0.03      0.06        63\\neve       1.00      0.04      0.07        52\\ngeo       0.83      0.86      0.85      7620\\ngpe       0.95      0.94      0.95      3145\\nnat       0.35      0.16      0.22        37\\norg       0.67      0.58      0.62      4033\\nper       0.79      0.74      0.77      3545\\ntim       0.88      0.84      0.86      4067', 'per       0.79      0.74      0.77      3545\\ntim       0.88      0.84      0.86      4067\\nmicro avg       0.83      0.79      0.81     22562\\nmacro avg       0.72      0.52      0.55     22562\\nweighted avg       0.82      0.79      0.80     22562\\n==================================================\\n--- 99) ===텐서플로우 1버전 코드 (구버전)=== ---\\n마지막 편집일시 : 2022년 11월 12일 6:11 오후\\n==================================================\\n--- 12-01 양방향 LSTM을 이용한 개체명 인식(Named Entity Recognition using Bi-LSTM) ---\\n```', '--- 12-01 양방향 LSTM을 이용한 개체명 인식(Named Entity Recognition using Bi-LSTM) ---\\n```\\nF1-score: 76.9%\\n```이번 챕터에서는 개체명 인식 데이터에 대한 전처리를 진행하고, 양방향 LSTM을 이용하여 개체명 인식기를 만듭니다. 그리고 F1-Score를 사용하여 모델을 평가합니다.']\n",
      "['이번에 사용할 CRF layer는 현재 텐서플로우 1.14.0버전과 케라스 2.2.4에서 원활하게 동작합니다. 그러므로 우선 버전을 맞춰줍시다. 로컬 환경의 버전은 건드리지 않기 위해 구글 Colab에서의 실습을 권장합니다.\\n!pip uninstall keras-nightly\\n!pip uninstall -y tensorflow\\n!pip install tensorflow==1.14.0\\n!pip install keras==2.2.4\\n!pip install tensorflow-gpu==1.14.0\\n!pip install h5py==2.10.0\\nCRF를 사용하기 위해 keras_contrib를 설치해야 합니다. 아래의 명령을 수행하여 설치합니다.\\n!pip install git+https://www.github.com/keras-team/keras-contrib.git']\n",
      "['이번에는 양방향 LSTM과 CRF를 함께 사용하여 앞서 사용한 데이터 외에 다른 데이터를 사용하여 개체명 인식을 수행해보도록 하겠습니다. 데이터는 아래의 링크에서 다운로드 가능합니다.\\n링크 : https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\\nimport pandas as pd\\nimport numpy as np\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.utils import to_categorical', 'from tensorflow.keras.utils import to_categorical\\ndata = pd.read_csv(\"ner_dataset.csv 파일의 경로\", encoding=\"latin1\")\\ndata[:5]\\n[이미지: ]', 'data = pd.read_csv(\"ner_dataset.csv 파일의 경로\", encoding=\"latin1\")\\ndata[:5]\\n[이미지: ]\\n이번에 사용할 데이터는 앞서 사용한 데이터랑 양식이 조금 다릅니다. 첫번째 열은 다음과 같은 패턴을 가지고 있습니다. Sentence: 1 있고, Null 값이 이어지다가 다시 Sentence: 2가 나오고 다시 Null 값이 이어지다가 Sentence: 3이 나오고 다시 Null 값이 이어지다가를 반복합니다. 그런데 사실 이는 하나의 문장을 여러 행으로 나눠놓은 것입니다. 숫자값을 t라고 합시다. 첫번째 Sentence: t부터 Null 값이 나오다가 Sentence: t+1이 나오기 전까지의 모든 데이터는 원래 하나의 행. 즉, 하나의 샘플이어야 합니다. t번째 문장을 각 단어마다 각 하나의 행으로 나눠놓은 데이터이기 때문입니다. 이는 뒤에서 Pandas의 fillna를 통해 하나로 묶는 작업을 해줍니다.', \"print('데이터프레임 행의 개수 : {}'.format(len(data)))\\n데이터프레임 행의 개수 : 1048575\\n현재 data의 행의 개수는 1,048,575개입니다. 하지만 뒤에서 기존에 문장 1개였던 행들을 1개의 행으로 병합하는 작업을 해야하기 때문에 최종 샘플의 개수는 이보다 줄어들게 됩니다. 우선, 데이터를 좀 더 살펴봅시다.\\nprint('데이터에 Null 값이 있는지 유무 : ' + str(data.isnull().values.any()))\\n데이터에 Null 값이 있는지 유무 : True\\nSentence #열에 Null 값들이 존재하고 있으므로, isnull().values.any()를 수행하였을 때 True가 나옵니다.\\nprint('어떤 열에 Null값이 있는지 출력')\\nprint('==============================')\\ndata.isnull().sum()\\n어떤 열에 Null값이 있는지 출력\", \"print('==============================')\\ndata.isnull().sum()\\n어떤 열에 Null값이 있는지 출력\\n==============================\\nSentence #    1000616\\nWord                0\\nPOS                 0\\nTag                 0\\ndtype: int64\\nisnull().sum()을 수행하면 각 열마다의 Null 값의 개수를 보여줍니다. 다른 열은 0개인데 오직 Sentences #열에서만 1,000,616개가 나온 것을 볼 수 있습니다. 전체 데이터에서 중복을 허용하지 않고, 유일한 값의 개수를 셀 수 있게 해주는 nunique()를 사용해봅시다.\\nprint('sentence # 열의 중복을 제거한 값의 개수 : {}'.format(data['Sentence #'].nunique()))\", \"print('sentence # 열의 중복을 제거한 값의 개수 : {}'.format(data['Sentence #'].nunique()))\\nprint('Word 열의 중복을 제거한 값의 개수 : {}'.format(data.Word.nunique()))\\nprint('Tag 열의 중복을 제거한 값의 개수 : {}'.format(data.Tag.nunique()))\\nsentence # 열의 중복을 제거한 값의 개수 : 47959\\nWord 열의 중복을 제거한 값의 개수 : 35178\\nTag 열의 중복을 제거한 값의 개수 : 17\\n이 데이터에는 47,959개의 문장이 있으며 문장들은 35,178개의 단어를 가지고 17개 종류의 개체명 태깅을 가집니다. 17개의 개체명 태깅이 전체 데이터에서 몇 개가 있는지, 개체명 태깅 개수의 분포를 확인해보도록 하겠습니다.\\nprint('Tag 열의 각각의 값의 개수 카운트')\\nprint('================================')\", \"print('Tag 열의 각각의 값의 개수 카운트')\\nprint('================================')\\nprint(data.groupby('Tag').size().reset_index(name='count'))\\nTag 열의 각각의 값의 개수 카운트\\n================================\\nTag   count\\n0   B-art     402\\n1   B-eve     308\\n2   B-geo   37644\\n3   B-gpe   15870\\n4   B-nat     201\\n5   B-org   20143\\n6   B-per   16990\\n7   B-tim   20333\\n8   I-art     297\\n9   I-eve     253\\n10  I-geo    7414\\n11  I-gpe     198\\n12  I-nat      51\\n13  I-org   16784\\n14  I-per   17251\\n15  I-tim    6528\", '11  I-gpe     198\\n12  I-nat      51\\n13  I-org   16784\\n14  I-per   17251\\n15  I-tim    6528\\n16      O  887908\\nBIO 표현 방법에서 아무런 태깅도 의미하지 않는 O가 가장 887,908개로 가장 많은 개수를 차지함을 볼 수 있습니다. 이제 데이터를 원하는 형태로 가공해보겠습니다. 우선 Null 값을 제거합니다.\\ndata = data.fillna(method=\"ffill\")\\nPandas의 (method=\\'ffill\\')는 Null 값을 가진 행의 바로 앞의 행의 값으로 Null 값을 채우는 작업을 수행합니다. 이렇게 하면 t번째 문장에 속하면서 Null 값을 가진 샘플들은 전부 첫번째 열에 Sentence: t의 값이 들어갑니다. 이번에는 뒤의 5개의 샘플을 출력해서 정상적으로 수행되었는지 확인해봅시다.\\nprint(data.tail())\\nSentence #       Word  POS Tag', 'print(data.tail())\\nSentence #       Word  POS Tag\\n1048570  Sentence: 47959       they  PRP   O\\n1048571  Sentence: 47959  responded  VBD   O\\n1048572  Sentence: 47959         to   TO   O\\n1048573  Sentence: 47959        the   DT   O\\n1048574  Sentence: 47959     attack   NN   O\\n뒤의 5개 샘플의 첫번째 열이 Sentence: 47959로 채워졌습니다. 이는 47,959번째 문장임을 의미하며, Null 값을 가진 행들의 바로 앞 행의 Sentence # 열의 값이 Sentence: 47959이었음을 의미합니다. 전체 데이터에 Null 값이 존재하는지 확인해봅시다.', \"print('데이터에 Null 값이 있는지 유무 : ' + str(data.isnull().values.any()))\\n데이터에 Null 값이 있는지 유무 : False\\n없는 것으로 나옵니다. 모든 단어를 소문자화하여 단어의 개수를 줄여보겠습니다.\\ndata['Word'] = data['Word'].str.lower()\\nprint('Word 열의 중복을 제거한 값의 개수 : {}'.format(data.Word.nunique()))\\nWord 열의 중복을 제거한 값의 개수 : 31817\\n정상적으로 소문자화가 되었는지 앞의 샘플 5개만 출력해보겠습니다.\\nprint(data[:5])\\nSentence #           Word  POS Tag\\n0  Sentence: 1      thousands  NNS   O\\n1  Sentence: 1             of   IN   O\\n2  Sentence: 1  demonstrators  NNS   O\", '1  Sentence: 1             of   IN   O\\n2  Sentence: 1  demonstrators  NNS   O\\n3  Sentence: 1           have  VBP   O\\n4  Sentence: 1        marched  VBN   O\\n이제 하나의 문장에 등장한 단어와 개체명 태깅 정보끼리 쌍(pair)으로 묶는 작업을 수행합니다.\\nfunc = lambda temp: [(w, t) for w, t in zip(temp[\"Word\"].values.tolist(), temp[\"Tag\"].values.tolist())]\\ntagged_sentences=[t for t in data.groupby(\"Sentence #\").apply(func)]\\nprint(\"전체 샘플 개수: {}\".format(len(tagged_sentences)))\\n전체 샘플 개수: 47959', 'print(\"전체 샘플 개수: {}\".format(len(tagged_sentences)))\\n전체 샘플 개수: 47959\\n1,000,616개의 행의 개수가 각 문장당 하나의 샘플로 묶이면서 47,959개의 샘플로 변환된 것을 확인할 수 있습니다. 정상적으로 수행이 되었는지 첫번째 샘플에 대해서 출력을 해봅시다.\\nprint(tagged_sentences[0]) # 첫번째 샘플 출력', \"print(tagged_sentences[0]) # 첫번째 샘플 출력\\n[('thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('london', 'B-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O'), ('war', 'O'), ('in', 'O'), ('iraq', 'B-geo'), ('and', 'O'), ('demand', 'O'), ('the', 'O'), ('withdrawal', 'O'), ('of', 'O'), ('british', 'B-gpe'), ('troops', 'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\", \"전처리가 수행된 첫번째 샘플이 출력된 것을 볼 수 있습니다. 이러한 샘플이 총 47,959개가 있습니다. 그런데 훈련을 시키려면 훈련 데이터에서 단어에 해당되는 부분과 개체명 태깅 정보에 해당되는 부분을 분리시켜야 합니다. 즉, [('thousands', 'O'), ('of', 'O')]와 같은 문장 샘플이 있다면 thousands와 of는 같이 저장하고, O와 O를 같이 저장할 필요가 있습니다.\\n이런 경우 파이썬 함수 중에서 zip()함수가 유용한 역할을 합니다. zip()함수는 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할을 합니다. (2챕터의 데이터의 분리 챕터 참고)\\nsentences, ner_tags = [], []\\nfor tagged_sentence in tagged_sentences: # 47,959개의 문장 샘플을 1개씩 불러온다.\", 'sentences, ner_tags = [], []\\nfor tagged_sentence in tagged_sentences: # 47,959개의 문장 샘플을 1개씩 불러온다.\\nsentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\\nsentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\\nner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다.\\n각 문장 샘플에 대해서 단어는 sentences에 태깅 정보는 ner_tags에 저장하였습니다. 임의로 첫번째 문장 샘플을 출력해보겠습니다.\\nprint(sentences[0])\\nprint(ner_tags[0])', \"print(sentences[0])\\nprint(ner_tags[0])\\n['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\\n['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\", \"첫번째 샘플에 대해서 단어에 대해서만 sentences[0]에, 또한 개체명에 대해서만 ner_tags[0]에 저장된 것을 볼 수 있습니다. 뒤에서 보겠지만, sentences는 예측을 위한 X에 해당되며 ner_tags는 예측 대상인 y에 해당됩니다. 다른 샘플들에 대해서도 처리가 되었는지 확인하기 위해 임의로 99번째 샘플에 대해서도 확인해보겠습니다.\\nprint(sentences[98])\\nprint(ner_tags[98])\\n['she', 'had', 'once', 'received', 'a', 'kidney', 'transplant', '.']\\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\", \"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\\n단어에 대해서만 sentences[98]에, 또한 개체명에 대해서만 ner_tags[98]에 저장된 것을 확인할 수 있습니다. 또한 첫번째 샘플과 길이가 다른 것을 볼 수 있습니다. 사실 47,959개의 문장 샘플의 길이는 전부 제각각입니다. 전체 데이터의 길이 분포를 확인해봅시다.\\nprint('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\\nprint('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\\nplt.hist([len(s) for s in sentences], bins=50)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n샘플의 최대 길이 : 104\\n샘플의 평균 길이 : 21.863987989741236\", \"plt.ylabel('number of samples')\\nplt.show()\\n샘플의 최대 길이 : 104\\n샘플의 평균 길이 : 21.863987989741236\\n[이미지: ]\\n위의 그래프는 샘플들의 길이가 대체적으로 0~40의 길이를 가지는 것을 보여줍니다. 길이가 가장 긴 샘플의 길이는 104입니다. 이제 케라스 토크나이저를 통해서 정수 인코딩을 진행합니다. 이번에는 문장 데이터에 있는 모든 단어를 사용하겠습니다.\\nsrc_tokenizer = Tokenizer(oov_token='OOV') # 모든 단어를 사용하지만 인덱스 1에는 단어 'OOV'를 할당한다.\\nsrc_tokenizer.fit_on_texts(sentences)\\ntar_tokenizer = Tokenizer(lower=False) # 태깅 정보들은 내부적으로 대문자를 유지한채로 저장\\ntar_tokenizer.fit_on_texts(ner_tags)\", \"tar_tokenizer.fit_on_texts(ner_tags)\\n문장 데이터에 대해서는 src_tokenizer를, 레이블에 해당되는 개체명 태깅 정보에 대해서는 tar_tokenizer를 사용합니다.\\nvocab_size = len(src_tokenizer.word_index) + 1\\ntag_size = len(tar_tokenizer.word_index) + 1\\nprint('단어 집합의 크기 : {}'.format(vocab_size))\\nprint('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))\\n단어 집합의 크기 : 31819\\n개체명 태깅 정보 집합의 크기 : 18\\n앞서 src_tokenizer를 만들때 Tokenizer의 인자로 oov_token='OOV'를 선택했습니다. 이렇게 하면 인덱스1에 단어 'OOV'가 할당됩니다.\\nprint('단어 OOV의 인덱스 : {}'.format(src_tokenizer.word_index['OOV']))\", \"print('단어 OOV의 인덱스 : {}'.format(src_tokenizer.word_index['OOV']))\\n단어 OOV의 인덱스 : 1\\n이제 정수 인코딩을 수행합니다.\\nX_data = src_tokenizer.texts_to_sequences(sentences)\\ny_data = tar_tokenizer.texts_to_sequences(ner_tags)\\n이제 문장 데이터에 대해서 정수 인코딩이 수행된 결과는 X_data, 개체명 태깅 데이터에 대해서 정수 인코딩이 수행된 결과는 y_data에 저장되었습니다. 정수 인코딩이 되었는지 확인을 위해 임의로 첫번째 샘플을 출력해보겠습니다.\\nprint(X_data[0])\\nprint(y_data[0])\\n[254, 6, 967, 16, 1795, 238, 468, 7, 523, 2, 129, 5, 61, 9, 571, 2, 833, 6, 186, 90, 22, 15, 56, 3]\", \"[254, 6, 967, 16, 1795, 238, 468, 7, 523, 2, 129, 5, 61, 9, 571, 2, 833, 6, 186, 90, 22, 15, 56, 3]\\n[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1]\\n모델 훈련 후 결과 확인을 위해 인덱스로부터 단어를 리턴하는 index_to_word를 만듭니다. 그와 동시에 뒤에서 사용할 index_to_ner도 만듭니다. 이때, 인덱스 0은 'PAD'란 단어를 할당해두겠습니다. index_to_ner은 개수가 적으니 출력까지 해봅시다.\\nword_to_index = src_tokenizer.word_index\\nindex_to_word = src_tokenizer.index_word\\nner_to_index = tar_tokenizer.word_index\\nindex_to_ner = tar_tokenizer.index_word\", \"ner_to_index = tar_tokenizer.word_index\\nindex_to_ner = tar_tokenizer.index_word\\nindex_to_ner[0] = 'PAD'\\nprint(index_to_ner)\\n{1: 'O', 2: 'B-geo', 3: 'B-tim', 4: 'B-org', 5: 'I-per', 6: 'B-per', 7: 'I-org', 8: 'B-gpe', 9: 'I-geo', 10: 'I-tim', 11: 'B-art', 12: 'B-eve', 13: 'I-art', 14: 'I-eve', 15: 'B-nat', 16: 'I-gpe', 17: 'I-nat', 0: 'PAD'}\\nindex_to_word를 만들었으니 시험삼아 첫번째 샘플에 대해서 다시 디코딩(정수에서 다시 텍스트 데이터로 변환) 작업을 해보겠습니다.\\ndecoded = []\\nfor index in X_data[0] : # 첫번째 샘플 안의 인덱스들에 대해서\", \"decoded = []\\nfor index in X_data[0] : # 첫번째 샘플 안의 인덱스들에 대해서\\ndecoded.append(index_to_word[index]) # 다시 단어로 변환\\nprint('기존의 문장 : {}'.format(sentences[0]))\\nprint('디코딩 문장 : {}'.format(decoded))\\n기존의 문장 : ['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\", \"디코딩 문장 : ['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\\n이제 X 데이터와 y 데이터가 구성되었습니다. 이제 패딩 작업을 진행해봅시다. 앞서 확인하였듯이 대부분의 데이터의 길이는 40~60에 분포되어져 있습니다. 그러므로 가장 긴 샘플의 길이인 104가 아니라 70정도로 max_len을 정해보겠습니다.\\nmax_len = 70\\n# 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\\nX_data = pad_sequences(X_data, padding='post', maxlen=max_len)\", \"# 모든 샘플들의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\\nX_data = pad_sequences(X_data, padding='post', maxlen=max_len)\\ny_data = pad_sequences(y_data, padding='post', maxlen=max_len)\\n모든 샘플의 길이가 70이 되었습니다. 이제 훈련 데이터와 테스트 데이터를 8:2의 비율로 분리합니다.\\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=.2, random_state=777)\\n레이블에 해당하는 태깅 정보에 대해서 원-핫 인코딩을 수행합니다.\\ny_train = to_categorical(y_train, num_classes=tag_size)\\ny_test = to_categorical(y_test, num_classes=tag_size)\\n이제 각 데이터 크기를 확인해보겠습니다.\", \"y_test = to_categorical(y_test, num_classes=tag_size)\\n이제 각 데이터 크기를 확인해보겠습니다.\\nprint('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\\nprint('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\\nprint('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\\n훈련 샘플 문장의 크기 : (38367, 70)\\n훈련 샘플 레이블의 크기 : (38367, 70, 18)\\n테스트 샘플 문장의 크기 : (9592, 70)\\n테스트 샘플 레이블의 크기 : (9592, 70, 18)\"]\n",
      "[\"시퀀스 레이블링 모델을 평가할 때는 한 가지 주의할 점이 있습니다. 이런 모델의 경우에는 보통 큰 의미를 갖지 않는 레이블 정보가 존재합니다. 예를 들어 개체명 인식에서는 그 어떤 개체도 아니라는 의미의 'O'라는 태깅이 존재합니다. 그런데 이런 정보는 보통 대다수의 레이블을 차지하기 때문에 기존에 사용했던 정확도 평가 방법을 사용하는 것이 적절하지 않을 수 있습니다.\\n예를 들어 모델이 단 1개의 개체도 맞추지 못하고 전부 'O'로 예상했을 경우를 봅시다. 우선 실제값은 바로 위에서 출력했던 값을 실제값으로 재사용하겠습니다. 아래 코드에서는 true라는 변수에 저장하였습니다. 그리고 개체를 하나도 맞추지 못했다는 가정하에 전부 'O'로만 채워진 예측값 predicted를 생성합니다.\", \"true=['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','B-MISC','I-MISC','I-MISC','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O']\\n# 실제값\\npredicted=['O'] * len(true) #실제값의 길이만큼 전부 'O'로 채워진 리스트 생성. 예측값으로 사용.\\nprint(predicted)\\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\", '실제로는 PER, MISC, PER, MISC, PER이라는 총 5개의 개체가 존재함에도 불구하고 예측값인 predicted는 단 1개의 개체도 맞추지 못한 상황을 시뮬레이션하는 것입니다. 이제 이에 대한 정확도를 계산해봅시다.\\nhit = 0 # 정답 개수\\nfor t, p in zip(true, predicted):\\nif t == p:\\nhit +=1 # 정답인 경우에만 +1\\naccuracy = hit/len(true) # 정답 개수를 총 개수로 나눈다.\\nprint(\"정확도: {:.1%}\".format(accuracy))\\n정확도: 74.4%', 'accuracy = hit/len(true) # 정답 개수를 총 개수로 나눈다.\\nprint(\"정확도: {:.1%}\".format(accuracy))\\n정확도: 74.4%\\n실제값에서도 대부분의 값이 \\'O\\'이기 때문에 그 어떤 개체도 찾지 못하였음에도 74%의 정확도를 얻습니다. 이는 정확도가 뻥튀기되어 모델의 성능을 오해할 수 있다는 문제가 있습니다. 그래서 여기서는 위와 같은 상황에서 더 적절한 평가 방법을 도입하고자 합니다. 윈도우의 명령 프롬프트나 UNIX의 터미널에서 아래의 명령을 수행하여 파이썬 패키지 seqeval를 설치합니다.\\n!pip install seqeval\\n앞서 머신 러닝 훑어보기 챕터에서 정밀도(precision)과 재현률(recall)을 배운 바 있습니다. 개체명 인식 모델의 성능 측정을 위해 정밀도와 재현률 개념을 사용해보겠습니다. 이를 개체명 인식 문제에 맞도록 해석해보면 다음과 같습니다.', '$$ \\\\text{정밀도} = \\\\frac{TP}{TP + FP} = \\\\text{특정 개체라고 예측한 경우 중에서 실제 특정 개체로 판명되어 예측이 일치한 비율}$$\\n$$ \\\\text{재현률} = \\\\frac{TP}{TP + FN} = \\\\text{전체 특정 개체 중에서 실제 특정 개체라고 정답을 맞춘 비율}$$\\n정밀도와 재현률로부터 조화 평균(harmonic mean)을 구한 것을 f1-score라고 합니다.\\n$$ f1\\\\ score = 2 × \\\\frac{\\\\text{정밀도 × 재현률}}{\\\\text{정밀도 + 재현률}}$$\\npredicted의 성능을 평가하기 위해서 정밀도, 재현률, f1-score를 계산해보도록 하겠습니다.\\nfrom seqeval.metrics import classification_report\\nprint(classification_report([true], [predicted]))\\nprecision    recall  f1-score   support', 'print(classification_report([true], [predicted]))\\nprecision    recall  f1-score   support\\nMISC       0.00      0.00      0.00         2\\nPER       0.00      0.00      0.00         3\\nmicro avg       0.00      0.00      0.00         5\\nmacro avg       0.00      0.00      0.00         5\\nweighted avg       0.00      0.00      0.00         5\\n이러한 측정 방법을 사용하면 PER과 MISC 두 특정 개체 중에서 실제 predicted가 맞춘 것은 단 1개도 없는 것을 확인할 수 있습니다. 이번에는 어느 정도는 정답을 맞추었다고 가정하고 예측값인 predicted를 수정하여 정밀도, 재현률, f1-score를 확인해봅시다.', \"true=['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','B-MISC','I-MISC','I-MISC','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O']\\npredicted=['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O']\\nprint(classification_report([true], [predicted]))\\nprecision    recall  f1-score   support\", 'print(classification_report([true], [predicted]))\\nprecision    recall  f1-score   support\\nMISC       1.00      0.50      0.67         2\\nPER       1.00      0.67      0.80         3\\nmicro avg       1.00      0.60      0.75         5\\nmacro avg       1.00      0.58      0.73         5\\nweighted avg       1.00      0.60      0.75         5\\n특정 개체로 예측한 경우에 대해서는 모두 제대로 예측을 하였으므로 정밀도는 1이 나옵니다. 하지만 재현률에서는 MISC는 실제로는 4개임에도 2개만을 맞추었으므로 0.5, PER은 실제로는 3개임에도 2개만을 맞추었으므로 0.67이 나온 것을 볼 수 있습니다.']\n",
      "['from keras.callbacks import Callback\\nfrom seqeval.metrics import f1_score, classification_report\\n모델을 학습하는 과정에서 검증 데이터에 대한 F1-score를 출력하기 위해 다음과 같은 클래스를 구현합니다. 이렇게 클래스를 구현해두면 모델을 검증 데이터를 통해 검증하는 과정에서 F1-score를 지속적으로 확인할 수 있습니다. 그리고 F1-score가 가장 높아질 때마다 모델을 저장합니다.\\nclass F1score(Callback):\\ndef __init__(self, value = 0.0, use_char=True):\\nsuper(F1score, self).__init__()\\nself.value = value\\nself.use_char = use_char\\ndef sequences_to_tags(self, sequences): # 예측값을 index_to_ner를 사용하여 태깅 정보로 변경하는 함수.', 'def sequences_to_tags(self, sequences): # 예측값을 index_to_ner를 사용하여 태깅 정보로 변경하는 함수.\\nresult = []\\nfor sequence in sequences: # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\\ntag = []\\nfor pred in sequence: # 시퀀스로부터 예측값을 하나씩 꺼낸다.\\npred_index = np.argmax(pred) # 예를 들어 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\\ntag.append(index_to_ner[pred_index].replace(\"PAD\", \"O\")) # \\'PAD\\'는 \\'O\\'로 변경\\nresult.append(tag)\\nreturn result\\n# 에포크가 끝날 때마다 실행되는 함수\\ndef on_epoch_end(self, epoch, logs={}):\\n# char Embedding을 사용하는 경우\\nif self.use_char:', 'def on_epoch_end(self, epoch, logs={}):\\n# char Embedding을 사용하는 경우\\nif self.use_char:\\nX_test = self.validation_data[0]\\nX_char_test = self.validation_data[1]\\ny_test = self.validation_data[2]\\ny_predicted = self.model.predict([X_test, X_char_test])\\nelse:\\nX_test = self.validation_data[0]\\ny_test = self.validation_data[1]\\ny_predicted = self.model.predict([X_test])\\npred_tags = self.sequences_to_tags(y_predicted)\\ntest_tags = self.sequences_to_tags(y_test)\\nscore = f1_score(pred_tags, test_tags)', \"test_tags = self.sequences_to_tags(y_test)\\nscore = f1_score(pred_tags, test_tags)\\nprint(' - f1: {:04.2f}'.format(score * 100))\\nprint(classification_report(test_tags, pred_tags))\\n# F1-score가 지금까지 중 가장 높은 경우\\nif score > self.value:\\nprint('f1_score improved from %f to %f, saving model to best_model.h5'%(self.value, score))\\nself.model.save('best_model.h5')\\nself.value = score\\nelse:\\nprint('f1_score did not improve from %f'%(self.value))\"]\n",
      "[\"from keras.models import Sequential\\nfrom keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\\nfrom keras.optimizers import Adam\\nfrom keras.models import load_model\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 128, input_length=max_len, mask_zero=True))\\nmodel.add(Bidirectional(LSTM(256, return_sequences=True)))\\nmodel.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))\", \"model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))\\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=10,  validation_split=0.1, callbacks=[F1score(use_char=False)])\\nbilstm_model = load_model('best_model.h5')\\ni=13 # 확인하고 싶은 테스트용 샘플의 인덱스.\\ny_predicted = bilstm_model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\", 'y_predicted = bilstm_model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\\ny_predicted = np.argmax(y_predicted, axis=-1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\\ntrue = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor w, t, pred in zip(X_test[i], true, y_predicted[0]):\\nif w != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_ner[t], index_to_ner[pred]))\\n단어             |실제값  |예측값', '단어             |실제값  |예측값\\n-----------------------------------\\nthe              : O       O\\nstatement        : O       O\\ncame             : O       O\\nas               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org\\nkofi             : B-per   B-per\\nannan            : I-per   I-per\\nmet              : O       O\\nwith             : O       O\\nofficials        : O       O\\nin               : O       O\\namman            : B-geo   B-geo\\nto               : O       O', \"in               : O       O\\namman            : B-geo   B-geo\\nto               : O       O\\ndiscuss          : O       O\\nwednesday        : B-tim   B-tim\\n's               : O       O\\nattacks          : O       O\\n.                : O       O\\nf1score = F1score()\\ny_predicted = bilstm_model.predict([X_test])\\npred_tags = f1score.sequences_to_tags(y_predicted)\\ntest_tags = f1score.sequences_to_tags(y_test)\\nprint(classification_report(test_tags, pred_tags))\\nprecision    recall  f1-score   support\", 'print(classification_report(test_tags, pred_tags))\\nprecision    recall  f1-score   support\\nart       0.00      0.00      0.00        63\\neve       0.64      0.31      0.42        52\\ngeo       0.82      0.82      0.82      7620\\ngpe       0.95      0.94      0.95      3145\\nnat       1.00      0.08      0.15        37\\norg       0.57      0.56      0.57      4033\\nper       0.70      0.75      0.72      3545\\ntim       0.76      0.83      0.79      4067', 'per       0.70      0.75      0.72      3545\\ntim       0.76      0.83      0.79      4067\\nmicro avg       0.76      0.78      0.77     22562\\nmacro avg       0.68      0.54      0.55     22562\\nweighted avg       0.76      0.78      0.77     22562\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nF1-score: 76.9%\\n==================================================\\n--- 12-02 양방향 LSTM과 CRF(Bidirectional LSTM + CRF) ---\\n```\\n단어             ||예측값\\n==============================', '```\\n단어             ||예측값\\n==============================\\nmr.            : B-per\\nheo            : I-per\\nsaid           : O\\nsouth          : B-geo\\nkorea          : I-geo\\nhas            : O\\nbecome         : O\\na              : O\\nworldwide      : O\\nleader         : O\\n```이번 챕터는 아래의 챕터를 이미 실행한 상태라고 가정합니다.\\n이전 챕터 링크 : https://wikidocs.net/97519\\n이번 챕터에서는 기존의 양방향 LSTM 모델에 CRF(Conditional Random Field)라는 새로운 층을 추가하여 보다 모델을 개선시킨 양방향 LSTM + CRF 모델을 사용하여 개체명 인식(Named Entity Recognition)을 수행합니다.', '논문 링크 : https://arxiv.org/pdf/1508.01991v1.pdf\\n논문 링크 : https://arxiv.org/pdf/1603.01360.pdf\\n이번 챕터는 텐서플로우와 케라스 버전이 다른 실습과는 다른 버전에서 동작하므로 구글의 Colab을 사용하기를 권장합니다.']\n",
      "['CRF는 Conditional Random Field의 약자로 양방향 LSTM을 위해 탄생한 모델이 아니라 이전에 독자적으로 존재해왔던 모델입니다. 이를 양방향 LSTM 모델 위에 하나의 층으로 추가하여, 양방향 LSTM + CRF 모델이 탄생하였습니다. 여기서는 CRF의 수식적 이해가 아니라 양방향 LSTM + CRF 모델의 직관에 대해서 이해합니다.\\nCRF 층의 역할을 이해하기 위해서 간단한 개체명 인식 작업의 예를 들어보겠습니다. 사람(Person), 조직(Organization) 두 가지만을 태깅하는 간단한 태깅 작업에 BIO 표현을 사용한다면 여기서 사용하는 태깅의 종류는 아래의 5가지입니다.\\nB-Per, I-Per, B-Org, I-Org, O\\n아래의 그림은 위의 태깅을 수행하는 기존의 양방향 LSTM 개체명 인식 모델의 예를 보여줍니다.\\n[이미지: ]', 'B-Per, I-Per, B-Org, I-Org, O\\n아래의 그림은 위의 태깅을 수행하는 기존의 양방향 LSTM 개체명 인식 모델의 예를 보여줍니다.\\n[이미지: ]\\n위 모델은 각 단어를 벡터로 입력받고, 모델의 출력층에서 활성화 함수를 통해 개체명을 예측합니다. 사실 입력 단어들과 실제 개체명이 무엇인지 모르는 상황이므로 이 모델이 정확하게 개체명을 예측했는지는 위 그림만으로는 알 수 없습니다. 또 다른 예를 보겠습니다.\\n[이미지: ]\\n위 모델은 명확히 틀린 예측을 포함하고 있습니다. 입력 단어들과 실제값의 여부와 상관없이 이 사실을 알 수 있습니다. BIO 표현에 따르면 우선, 첫번째 단어의 레이블에서 I가 등장할 수 없습니다. 또한 I-Per은 반드시 B-Per 뒤에서만 등장할 수 있습니다. 뿐만 아니라, I-Org도 마찬가지로 B-Org 뒤에서만 등장할 수 있는데 위 모델은 이런 BIO 표현 방법의 제약사항들을 모두 위반하고 있습니다.', '여기서 양방향 LSTM 위에 CRF 층을 추가하여 얻을 수 있는 이점을 언급하겠습니다. CRF 층을 추가하면 모델은 예측 개체명, 다시 말해 레이블 사이의 의존성을 고려할 수 있습니다. 아래의 그림은 양방향 LSTM + CRF 모델을 보여줍니다.\\n[이미지: ]\\n앞서봤듯이, 기존에 CRF 층이 존재하지 않았던 양방향 LSTM 모델은 활성화 함수를 지난 시점에서 개체명을 결정했지만, CRF 층을 추가한 모델에서는 활성화 함수의 결과들이 CRF 층의 입력으로 전달됩니다. 예를 들어 $word_{1}$에 대한 BiLSTM 셀과 활성화 함수를 지난 출력값 [0.7, 0.12, 0.08, 0.04, 0.06]은 CRF 층의 입력이 됩니다. 마찬가지로 모든 단어에 대한 활성화 함수를 지난 출력값은 CRF 층의 입력이 되고, CRF 층은 레이블 시퀀스에 대해서 가장 높은 점수를 가지는 시퀀스를 예측합니다.', '이러한 구조에서 CRF 층은 점차적으로 훈련 데이터로부터 아래와 같은 제약사항 등을 학습하게 됩니다.\\n문장의 첫번째 단어에서는 I가 나오지 않습니다.\\nO-I 패턴은 나오지 않습니다.\\nB-I-I 패턴에서 개체명은 일관성을 유지합니다. 예를 들어 B-Per 다음에 I-Org는 나오지 않습니다.\\n요약하면 양방향 LSTM은 입력 단어에 대한 양방향 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영합니다.']\n",
      "['CRF layer는 현재 텐서플로우 1.14.0버전과 케라스 2.2.4에서 가장 원활하게 동작합니다. 텐서플로우와 케라스 버전을 높이면 CRF layer가 동작하지 않거나, mask_zero=True가 되지 않는 등의 문제가 발생합니다. 그러므로 우선 버전을 맞춰줍시다. 로컬 환경의 버전은 건드리지 않기 위해 구글 Colab에서의 실습을 권장합니다.\\n!pip install tensorflow==1.14.0\\n!pip install keras==2.2.4\\n!pip install tensorflow-gpu==1.14.0\\nCRF를 사용하기 위해 keras_contrib를 설치해야 합니다. 아래의 명령을 수행하여 설치합니다.\\n!pip install git+https://www.github.com/keras-team/keras-contrib.git']\n",
      "[\"데이터셋 로드와 전처리는 5) 양방향 LSTM을 이용한 개체명 인식에서 진행한 것과 동일하게 진행되었다고 가정합니다.\\n링크 : https://wikidocs.net/97519\\nprint('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\\nprint('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\\nprint('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\\n훈련 샘플 문장의 크기 : (38367, 70)\\n훈련 샘플 레이블의 크기 : (38367, 70, 18)\\n테스트 샘플 문장의 크기 : (9592, 70)\\n테스트 샘플 레이블의 크기 : (9592, 70, 18)\"]\n",
      "['아래 링크에서 사용된 콜백 클래스를 동일하게 구현하였다고 가정합니다.\\n링크 : https://wikidocs.net/97519\\nclass F1score(Callback):\\n... 중략 ...']\n",
      "['from keras.models import Sequential\\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\\nfrom keras.models import load_model\\nfrom keras_contrib.layers import CRF\\nfrom keras_contrib.losses import crf_loss\\nfrom keras_contrib.metrics import crf_viterbi_accuracy\\n위와 같이 필요한 도구들을 임포트합니다.\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len, mask_zero=True))\\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))', 'model.add(Bidirectional(LSTM(128, return_sequences=True)))\\nmodel.add(TimeDistributed(Dense(50, activation=\"relu\")))\\ncrf = CRF(tag_size)\\nmodel.add(crf)\\n모델에 양방향 LSTM을 사용하고, 모델의 출력층에 CRF 층을 배치하였습니다. 모델을 훈련합니다.\\nmodel.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_split = 0.1, verbose = 1, callbacks=[F1score(use_char=False)])', 'bilstm_crf_model = load_model(\\'best_model.h5\\', custom_objects={\\'CRF\\':CRF,\\n\\'crf_loss\\':crf_loss,\\n\\'crf_viterbi_accuracy\\':crf_viterbi_accuracy})\\ni=13 # 확인하고 싶은 테스트용 샘플의 인덱스.\\ny_predicted = bilstm_crf_model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\\ny_predicted = np.argmax(y_predicted, axis=-1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\\ntrue = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\\nprint(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")', 'print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\\nprint(35 * \"-\")\\nfor w, t, pred in zip(X_test[i], true, y_predicted[0]):\\nif w != 0: # PAD값은 제외함.\\nprint(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_ner[t], index_to_ner[pred]))\\n단어             |실제값  |예측값\\n-----------------------------------\\nthe              : O       O\\nstatement        : O       O\\ncame             : O       O\\nas               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org', \"as               : O       O\\nu.n.             : B-org   B-org\\nsecretary-general: I-org   I-org\\nkofi             : B-per   B-per\\nannan            : I-per   I-per\\nmet              : O       O\\nwith             : O       O\\nofficials        : O       O\\nin               : O       O\\namman            : B-geo   B-geo\\nto               : O       O\\ndiscuss          : O       O\\nwednesday        : B-tim   B-tim\\n's               : O       O\\nattacks          : O       O\\n.                : O       O\"]\n",
      "['이제 앞서 구현한 모델에 대해서 위에서 배운 f1-score를 적용해봅시다. 이를 위해서는 모델이 리턴하는 예측값은 숫자로 구성되어져 있으므로, 이를 먼저 태깅이 나열되어 있는 리스트로 치환하는 작업이 필요합니다. 이를 위해서 sequences_to_tag함수를 사용합니다.\\nf1score = F1score(use_char=False)\\ny_predicted = bilstm_crf_model.predict([X_test])\\npred_tags = f1score.sequences_to_tags(y_predicted)\\ntest_tags = f1score.sequences_to_tags(y_test)\\nprint(classification_report(test_tags, pred_tags))\\nprecision    recall  f1-score   support\\nart       0.00      0.00      0.00        63', 'precision    recall  f1-score   support\\nart       0.00      0.00      0.00        63\\neve       0.82      0.27      0.41        52\\ngeo       0.82      0.87      0.85      7620\\ngpe       0.97      0.93      0.95      3145\\nnat       1.00      0.19      0.32        37\\norg       0.71      0.56      0.63      4033\\nper       0.79      0.75      0.77      3545\\ntim       0.89      0.85      0.87      4067\\nmicro avg       0.83      0.80      0.81     22562', 'tim       0.89      0.85      0.87      4067\\nmicro avg       0.83      0.80      0.81     22562\\nmacro avg       0.75      0.55      0.60     22562\\nweighted avg       0.83      0.80      0.81     22562\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nF1-score: 81.4%']\n",
      "[\"이제 임의로 만든 새로운 문장에 대해서 앞서 만든 개체명 인식 모델을 수행해보겠습니다.\\nnew_sentence='Mr. Heo said South Korea has become a worldwide leader'.lower().split()\\n저자가 임의로 만든 문장을 띄어쓰기 수준의 토큰화 상태로 new_sentence에 저장하였습니다. 이제 이를 정수 인코딩합니다.\\nnew_encoded=[]\\nfor w in new_sentence:\\ntry:\\nnew_encoded.append(word_to_index.get(w,1))\\nexcept KeyError:\\nnew_encoded.append(word_to_index['OOV'])\\n# 모델이 모르는 단어에 대해서는 'OOV'의 인덱스인 1로 인코딩\\nprint(new_encoded)\\n[38, 1, 18, 117, 243, 12, 762, 8, 1154, 130]\\n정수 인코딩 과정에서 Heo의 경우 OOV로 치환되었습니다.\", 'print(new_encoded)\\n[38, 1, 18, 117, 243, 12, 762, 8, 1154, 130]\\n정수 인코딩 과정에서 Heo의 경우 OOV로 치환되었습니다.\\nnew_padded = pad_sequences([new_encoded], padding=\"post\", value=0, maxlen=max_len)\\n임의로 만든 문장을 max_len의 길이로 패딩해줍니다. 이제 예측을 시작해볼까요?\\np = bilstm_crf_model.predict(np.array([new_padded[0]]))\\np = np.argmax(p, axis=-1)\\nprint(\"{:15}||{}\".format(\"단어\", \"예측값\"))\\nprint(30 * \"=\")\\nfor w, pred in zip(new_sentence, p[0]):\\nprint(\"{:15}: {:5}\".format(w, index_to_ner[pred]))\\n단어             ||예측값', 'print(\"{:15}: {:5}\".format(w, index_to_ner[pred]))\\n단어             ||예측값\\n==============================\\nmr.            : B-per\\nheo            : I-per\\nsaid           : O\\nsouth          : B-geo\\nkorea          : I-geo\\nhas            : O\\nbecome         : O\\na              : O\\nworldwide      : O\\nleader         : O\\nhttps://github.com/floydhub/named-entity-recognition-template/blob/master/ner.ipynb\\n==================================================\\n--- 12-03 양방향 LSTM과 문자 임베딩(Char embedding) ---\\n```', '--- 12-03 양방향 LSTM과 문자 임베딩(Char embedding) ---\\n```\\nF1-score: 82.4%\\n```이번 챕터는 아래의 챕터를 이미 실행한 상태라고 가정합니다.\\n이전 챕터 링크 : https://wikidocs.net/97519\\n개체명 인식기의 성능을 올리기 위한 방법으로 12챕터에서 배운 문자 임베딩을 워드 임베딩과 함께 입력으로 사용하는 방법이 있습니다. 워드 임베딩에 문자 임베딩을 연결(concatenate)하여 성능을 높여봅시다.']\n",
      "[\"문자 임베딩을 위해서 하고자 하는 전처리는 문자 단위 정수 인코딩입니다. 가령 단어 'book'이 있고, b가 21번 o가 7번, k가 11번이라고 한다면 단어 'book'을 [21 7 7 11]로 인코딩하는 것입니다. 만약 단어 1개가 아니라 단어구나 문장이라면 어떨까요? 'good book'이란 문장이 있고, g가 12번, d가 17번이라고 한다면 이 문장을 문자 단위 정수 인코딩한다면 다음과 같은 결과를 얻을 수 있습니다.\\n'good book의 정수 인코딩 결과'\\n[[12 7 7 17]\\n[21 7 7 11]]\\n이 각 정수를 각각 임베딩 층(Embedding layer)를 거치도록 하여, 문자 단위 임베딩을 얻게 됩니다. 임베딩 층은 모델을 설계할 때 추가하게 되므로, 여기서는 정수 인코딩까지만 진행합니다. 우선 전체 데이터의 모든 단어를 문자 레벨로 분해하여, 문자 집합을 만듭니다.\\n# char_vocab 만들기\", '# char_vocab 만들기\\nwords = list(set(data[\"Word\"].values))\\nchars = set([w_i for w in words for w_i in w])\\nchars = sorted(list(chars))\\nprint(chars)', 'chars = set([w_i for w in words for w_i in w])\\nchars = sorted(list(chars))\\nprint(chars)\\n[\\'!\\', \\'\"\\', \\'#\\', \\'$\\', \\'%\\', \\'&\\', \"\\'\", \\'(\\', \\')\\', \\'+\\', \\',\\', \\'-\\', \\'.\\', \\'/\\', \\'0\\', \\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\', \\':\\', \\';\\', \\'?\\', \\'@\\', \\'[\\', \\']\\', \\'_\\', \\'`\\', \\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\', \\'f\\', \\'g\\', \\'h\\', \\'i\\', \\'j\\', \\'k\\', \\'l\\', \\'m\\', \\'n\\', \\'o\\', \\'p\\', \\'q\\', \\'r\\', \\'s\\', \\'t\\', \\'u\\', \\'v\\', \\'w\\', \\'x\\', \\'y\\', \\'z\\', \\'~\\', \\'\\\\x85\\', \\'\\\\x91\\', \\'\\\\x92\\', \\'\\\\x93\\', \\'\\\\x94\\', \\'\\\\x96\\', \\'\\\\x97\\', \\'\\\\xa0\\', \\'°\\', \\'é\\', \\'ë\\', \\'ö\\', \\'ü\\']', '이렇게 얻은 문자 집합으로부터 문자를 정수로 변환할 수 있는 딕셔너리인 char_to_index와 반대로 정수로부터 문자를 얻을 수 있는 딕셔너리인 index_to_char를 만듭니다.\\nchar_to_index = {c: i + 2 for i, c in enumerate(chars)}\\nchar_to_index[\"OOV\"] = 1\\nchar_to_index[\"PAD\"] = 0\\nindex_to_char = {}\\nfor key, value in char_to_index.items():\\nindex_to_char[value] = key\\nmax_len_char = 15\\ndef padding_char_indice(char_indice, max_len_char):\\nreturn pad_sequences(\\nchar_indice, maxlen=max_len_char, padding=\\'post\\', value = 0)\\ndef integer_coding(sentences):\\nchar_data = []', 'def integer_coding(sentences):\\nchar_data = []\\nfor ts in sentences:\\nword_indice = [word_to_index[t] for t in ts]\\nchar_indice = [[char_to_index[char] for char in t]\\nfor t in ts]\\nchar_indice = padding_char_indice(char_indice, max_len_char)\\nfor chars_of_token in char_indice:\\nif len(chars_of_token) > max_len_char:\\ncontinue\\nchar_data.append(char_indice)\\nreturn char_data\\nX_char_data = integer_coding(sentences)\\n동일한 문장에 대해서 단어 단위 정수 인코딩과 문자 단위 정수 인코딩의 차이를 확인해봅시다. 첫번째 샘플은 다음과 같습니다.\\n# 정수 인코딩 이전의 기존 문장', \"동일한 문장에 대해서 단어 단위 정수 인코딩과 문자 단위 정수 인코딩의 차이를 확인해봅시다. 첫번째 샘플은 다음과 같습니다.\\n# 정수 인코딩 이전의 기존 문장\\nprint(sentences[0])\\n['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\\n이 샘플을 정수 인코딩하고, 다른 샘플들과 길이를 동일하게 하기 위해 패딩한 결과는 다음과 같습니다.\\n# 단어 단위 정수 인코딩 + 패딩\\nprint(X_data[0])\\n[ 254    6  967   16 1795  238  468    7  523    2  129    5   61    9\", 'print(X_data[0])\\n[ 254    6  967   16 1795  238  468    7  523    2  129    5   61    9\\n571    2  833    6  186   90   22   15   56    3    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0]\\n254번 단어는 thousands, 6번 단어는 of에 해당됩니다. 해당 샘플을 문자 단위 정수 인코딩한 결과는 다음과 같습니다.\\n# 문자 단위 정수 인코딩\\nprint(X_char_data[0])', '# 문자 단위 정수 인코딩\\nprint(X_char_data[0])\\n[[53 41 48 54 52 34 47 37 52  0  0  0  0  0  0]\\n[48 39  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[37 38 46 48 47 52 53 51 34 53 48 51 52  0  0]\\n[41 34 55 38  0  0  0  0  0  0  0  0  0  0  0]\\n[46 34 51 36 41 38 37  0  0  0  0  0  0  0  0]\\n[53 41 51 48 54 40 41  0  0  0  0  0  0  0  0]\\n[45 48 47 37 48 47  0  0  0  0  0  0  0  0  0]\\n[53 48  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[49 51 48 53 38 52 53  0  0  0  0  0  0  0  0]', '[53 48  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[49 51 48 53 38 52 53  0  0  0  0  0  0  0  0]\\n[53 41 38  0  0  0  0  0  0  0  0  0  0  0  0]\\n[56 34 51  0  0  0  0  0  0  0  0  0  0  0  0]\\n[42 47  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[42 51 34 50  0  0  0  0  0  0  0  0  0  0  0]\\n[34 47 37  0  0  0  0  0  0  0  0  0  0  0  0]\\n[37 38 46 34 47 37  0  0  0  0  0  0  0  0  0]\\n[53 41 38  0  0  0  0  0  0  0  0  0  0  0  0]\\n[56 42 53 41 37 51 34 56 34 45  0  0  0  0  0]', '[53 41 38  0  0  0  0  0  0  0  0  0  0  0  0]\\n[56 42 53 41 37 51 34 56 34 45  0  0  0  0  0]\\n[48 39  0  0  0  0  0  0  0  0  0  0  0  0  0]\\n[35 51 42 53 42 52 41  0  0  0  0  0  0  0  0]\\n[53 51 48 48 49 52  0  0  0  0  0  0  0  0  0]\\n[39 51 48 46  0  0  0  0  0  0  0  0  0  0  0]\\n[53 41 34 53  0  0  0  0  0  0  0  0  0  0  0]\\n[36 48 54 47 53 51 58  0  0  0  0  0  0  0  0]\\n[14  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]', '[36 48 54 47 53 51 58  0  0  0  0  0  0  0  0]\\n[14  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\\n위 출력 결과에서 각 행은 각 단어를 의미합니다. 가령, thousands는 첫번째 행 [53 41 48 54 52 34 47 37 52  0  0  0  0  0  0]에 해당됩니다. 단어의 최대 길이를 15(max_len_char)로 제한하였으므로, 길이가 15보다 짧은 단어는 뒤에 0으로 패딩됩니다. 53은 t, 41은 h, 48은 o, 54는 u에 각각 해당됩니다. X_data는 뒤에 0으로 패딩되어 길이가 70인 것에 비해, X_char_data는 0번 단어는 무시되어 길이가 70이 아닙니다. 즉, 위 출력 결과에서 행의 개수가 70이 아닌 상태입니다. 이를 위해 문장 길이 방향으로도 패딩을 해줍니다.', \"X_char_data = pad_sequences(X_char_data, maxlen=max_len, padding='post', value = 0)\\n이미 단어 단위 정수 인코딩 결과는 X_train, y_train, X_test, y_test로 훈련 데이터와 테스트 데이터가 분리된 상태입니다. 문자 단위 정수 인코딩 결과에 대해서도 마찬가지로 X_char_train, X_char_test로 나누어줍니다.\\nX_char_train, X_char_test, _, _ = train_test_split(X_char_data, y_data, test_size=.2, random_state=777)\\nX_char_train = np.array(X_char_train)\\nX_char_test = np.array(X_char_test)\\nprint(X_train[0])\\n[ 150  928  361   17 2624    9 4131 3567    9    8 2893 1250  880  107\", \"print(X_train[0])\\n[ 150  928  361   17 2624    9 4131 3567    9    8 2893 1250  880  107\\n3    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0]\\nprint(index_to_word[150])\\nsoldiers\\nprint(' '.join([index_to_char[index] for index in X_char_train[0][0]]))\", \"soldiers\\nprint(' '.join([index_to_char[index] for index in X_char_train[0][0]]))\\ns o l d i e r s PAD PAD PAD PAD PAD PAD PAD\\nprint('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\\nprint('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\\nprint('훈련 샘플 char 데이터의 크기 : {}'.format(X_char_train.shape))\\nprint('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\\nprint('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\\n훈련 샘플 문장의 크기 : (38367, 70)\\n훈련 샘플 레이블의 크기 : (38367, 70, 18)\\n훈련 샘플 char 데이터의 크기 : (38367, 70, 15)\", '훈련 샘플 문장의 크기 : (38367, 70)\\n훈련 샘플 레이블의 크기 : (38367, 70, 18)\\n훈련 샘플 char 데이터의 크기 : (38367, 70, 15)\\n테스트 샘플 문장의 크기 : (9592, 70)\\n테스트 샘플 레이블의 크기 : (9592, 70, 18)']\n",
      "[\"위에서 전처리한 문자 단위 정수 인코딩 입력을 1D CNN의 입력으로 사용하여 문자 임베딩을 얻고, 워드 임베딩과 연결(concatenate)하여 양방향 LSTM의 입력으로 사용해봅시다.\\nfrom keras.layers import Embedding, TimeDistributed, Dropout, concatenate, Bidirectional, LSTM, Conv1D, Dense, MaxPooling1D, Flatten\\nfrom keras import Input, Model\\nfrom keras.initializers import RandomUniform\\nfrom keras.models import load_model\\n# 워드 임베딩\\nword_ids = Input(shape=(None,),dtype='int32',name='words_input')\", \"# 워드 임베딩\\nword_ids = Input(shape=(None,),dtype='int32',name='words_input')\\nword_embeddings = Embedding(input_dim = vocab_size, output_dim = 64)(word_ids)\\n# char 임베딩\\nchar_ids = Input(shape=(None, max_len_char,),name='char_input')\\nembed_char_out = TimeDistributed(Embedding(len(char_to_index), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_ids)\\ndropout = Dropout(0.5)(embed_char_out)\\n# char 임베딩에 대해서는 Conv1D 수행\", \"dropout = Dropout(0.5)(embed_char_out)\\n# char 임베딩에 대해서는 Conv1D 수행\\nconv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\\nmaxpool_out=TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\\nchar_embeddings = TimeDistributed(Flatten())(maxpool_out)\\nchar_embeddings = Dropout(0.5)(char_embeddings)\\n# char 임베딩을 Conv1D 수행한 뒤에 워드 임베딩과 연결\\noutput = concatenate([word_embeddings, char_embeddings])\\n# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\", \"output = concatenate([word_embeddings, char_embeddings])\\n# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\\noutput = Bidirectional(LSTM(50, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\\n# 출력층\\noutput = TimeDistributed(Dense(tag_size, activation='softmax'))(output)\\nmodel = Model(inputs=[word_ids, char_ids], outputs=[output])\\nmodel.compile(loss='categorical_crossentropy', optimizer='nadam',  metrics=['acc'])\\ncallbacks으로 F1score를 넣어줄 때, 문자 임베딩을 사용하고 있으므로 use_char를 True로 해주어야 합니다.\", \"callbacks으로 F1score를 넣어줄 때, 문자 임베딩을 사용하고 있으므로 use_char를 True로 해주어야 합니다.\\nhistory = model.fit([X_train, X_char_train], y_train, batch_size = 32, epochs = 10, validation_split = 0.1, verbose = 1, callbacks=[F1score(use_char=True)])\\n학습이 끝났다면 모델을 로드하여 테스트 데이터에 대해서 F1 score를 측정해봅시다.\\nbilstm_cnn_model = load_model('best_model.h5')\\nf1score = F1score(use_char=True)\\ny_predicted = bilstm_cnn_model.predict([X_test, X_char_test])\\npred_tags = f1score.sequences_to_tags(y_predicted)\", 'pred_tags = f1score.sequences_to_tags(y_predicted)\\ntest_tags = f1score.sequences_to_tags(y_test)\\nprint(classification_report(test_tags, pred_tags))\\nprecision    recall  f1-score   support\\nart       0.27      0.14      0.19        63\\neve       0.46      0.35      0.40        52\\ngeo       0.82      0.86      0.84      7620\\ngpe       0.96      0.95      0.95      3145\\nnat       0.55      0.46      0.50        37\\norg       0.62      0.59      0.60      4033', 'nat       0.55      0.46      0.50        37\\norg       0.62      0.59      0.60      4033\\nper       0.73      0.71      0.72      3545\\ntim       0.87      0.84      0.86      4067\\nmicro avg       0.80      0.79      0.80     22562\\nmacro avg       0.66      0.61      0.63     22562\\nweighted avg       0.80      0.79      0.79     22562\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nF1-score: 79.5%']\n",
      "[\"이번에는 문자 임베딩을 사용한 위의 모델에 CRF층을 추가적으로 사용해보겠습니다.\\nfrom keras_contrib.layers import CRF\\nfrom keras_contrib.losses import crf_loss\\nfrom keras_contrib.metrics import crf_viterbi_accuracy\\n# 워드 임베딩\\nword_ids = Input(shape=(None,),dtype='int32',name='words_input')\\nword_embeddings = Embedding(input_dim = vocab_size, output_dim = 64)(word_ids)\\n# char 임베딩\\nchar_ids = Input(shape=(None, max_len_char,),name='char_input')\", \"# char 임베딩\\nchar_ids = Input(shape=(None, max_len_char,),name='char_input')\\nembed_char_out = TimeDistributed(Embedding(len(char_to_index), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_ids)\\ndropout = Dropout(0.5)(embed_char_out)\\n# char 임베딩에 대해서는 Conv1D 수행\\nconv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\\nmaxpool_out=TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\", 'maxpool_out=TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\\nchar_embeddings = TimeDistributed(Flatten())(maxpool_out)\\nchar_embeddings = Dropout(0.5)(char_embeddings)\\n# char 임베딩을 Conv1D 수행한 뒤에 워드 임베딩과 연결\\noutput = concatenate([word_embeddings, char_embeddings])\\n# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\\noutput = Bidirectional(LSTM(50, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\\n# 출력층에 CRF 층을 추가 (위의 모델과 이 부분이 다릅니다.)', '# 출력층에 CRF 층을 추가 (위의 모델과 이 부분이 다릅니다.)\\noutput = TimeDistributed(Dense(50, activation=\\'relu\\'))(output)\\ncrf = CRF(tag_size)\\noutput = crf(output)\\nmodel = Model(inputs=[words_input, character_input], outputs=[output])\\nmodel.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\\nhistory = model.fit([X_train, X_char_train], y_train, batch_size = 32, epochs = 15, validation_split = 0.1, verbose = 1, callbacks=[F1score(use_char=True)])', \"bilstm_cnn_crf_model = load_model('best_model.h5', custom_objects={'CRF':CRF,\\n'crf_loss':crf_loss,\\n'crf_viterbi_accuracy':crf_viterbi_accuracy})\\nf1score = F1score(use_char=True)\\ny_predicted = bilstm_cnn_crf_model.predict([X_test, X_char_test])\\npred_tags = f1score.sequences_to_tags(y_predicted)\\ntest_tags = f1score.sequences_to_tags(y_test)\\nprint(classification_report(test_tags, pred_tags))\\nprecision    recall  f1-score   support\\nper       0.80      0.76      0.78      3545\", 'precision    recall  f1-score   support\\nper       0.80      0.76      0.78      3545\\ngpe       0.96      0.94      0.95      3145\\ngeo       0.85      0.86      0.86      7620\\ntim       0.88      0.87      0.87      4067\\norg       0.66      0.60      0.63      4033\\nart       0.00      0.00      0.00        63\\neve       0.61      0.27      0.37        52\\nnat       0.68      0.51      0.58        37\\nmicro avg       0.83      0.81      0.82     22562', 'nat       0.68      0.51      0.58        37\\nmicro avg       0.83      0.81      0.82     22562\\nmacro avg       0.83      0.81      0.82     22562\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nF1-score: 81.9%']\n",
      "[\"문자 임베딩은 1D CNN이 아니라 양방향 LSTM을 통해서도 얻을 수 있습니다. 양방향 LSTM을 이용해 얻은 문자 임베딩을 워드 임베딩과 연결(concatenate)하여 양방향 LSTM의 입력으로 사용하고, 마지막으로 CRF 층을 추가하여 모델을 만듭니다.\\n# 워드 임베딩\\nword_ids = Input(batch_shape=(None, None), dtype='int32', name='word_input')\\nword_embeddings = Embedding(input_dim=vocab_size,\\noutput_dim=64,\\nmask_zero=True,\\nname='word_embedding')(word_ids)\\n# char 임베딩\\nchar_ids = Input(batch_shape=(None, None, None), dtype='int32', name='char_input')\\nchar_embeddings = Embedding(input_dim=(len(char_to_index)),\", \"char_embeddings = Embedding(input_dim=(len(char_to_index)),\\noutput_dim=30,\\nmask_zero=True,\\nembeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5),\\nname='char_embedding')(char_ids)\\nchar_embeddings = TimeDistributed(Bidirectional(LSTM(64)))(char_embeddings)\\n# char 임베딩을 워드 임베딩과 연결\\nword_embeddings = concatenate([word_embeddings, char_embeddings])\\nword_embeddings = Dropout(0.3)(word_embeddings)\\nz = Bidirectional(LSTM(units=64, return_sequences=True))(word_embeddings)\", 'z = Bidirectional(LSTM(units=64, return_sequences=True))(word_embeddings)\\nz = Dense(tag_size, activation=\\'tanh\\')(z)\\ncrf = CRF(tag_size)\\noutput = crf(z)\\nmodel = Model(inputs=[word_ids, char_ids], outputs=[output])\\nmodel.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\\nhistory = model.fit([X_train, X_char_train], y_train, batch_size = 32, epochs = 15, validation_split = 0.1, verbose = 1, callbacks=[F1score(use_char=True)])', \"bilstm_bilstm_crf_model = load_model('best_model.h5', custom_objects={'CRF':CRF,\\n'crf_loss':crf_loss,\\n'crf_viterbi_accuracy':crf_viterbi_accuracy})\\nf1score = F1score(use_char=True)\\ny_predicted = bilstm_bilstm_crf_model.predict([X_test, X_char_test])\\npred_tags = f1score.sequences_to_tags(y_predicted)\\ntest_tags = f1score.sequences_to_tags(y_test)\\nprint(classification_report(test_tags, pred_tags))\\nprecision    recall  f1-score   support\\nart       0.25      0.02      0.03        63\", 'precision    recall  f1-score   support\\nart       0.25      0.02      0.03        63\\neve       0.45      0.29      0.35        52\\ngeo       0.85      0.87      0.86      7620\\ngpe       0.96      0.95      0.95      3145\\nnat       0.55      0.16      0.25        37\\norg       0.73      0.57      0.64      4033\\nper       0.78      0.78      0.78      3545\\ntim       0.89      0.86      0.87      4067\\nmicro avg       0.84      0.81      0.82     22562', 'tim       0.89      0.86      0.87      4067\\nmicro avg       0.84      0.81      0.82     22562\\nmacro avg       0.68      0.56      0.59     22562\\nweighted avg       0.84      0.81      0.82     22562\\nprint(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\\nF1-score: 82.4%\\n==================================================\\n--- 13. Part 2. 심화 과정 ---\\n마지막 편집일시 : 2022년 11월 13일 3:18 오후\\n==================================================\\n--- 13. 서브워드 토크나이저(Subword Tokenizer) ---', '==================================================\\n--- 13. 서브워드 토크나이저(Subword Tokenizer) ---\\n기계에게 아무리 많은 단어를 학습시켜도, 세상의 모든 단어를 알려줄 수는 없는 노릇입니다. 만약, 기계가 모르는 단어가 등장하면 그 단어를 단어 집합에 없는 단어란 의미에서 OOV(Out-Of-Vocabulary) 또는 UNK(Unknown Token)라고 표현합니다. 기계가 문제를 풀 때, 모르는 단어가 등장하면 (사람도 마찬가지지만) 주어진 문제를 푸는 것이 까다로워 집니다. 이와 같이 모르는 단어로 인해 문제를 푸는 것이 까다로워지는 상황을 OOV 문제라고 합니다.', '서브워드 분리(Subword segmenation) 작업은 하나의 단어는 더 작은 단위의 의미있는 여러 서브워드들(Ex) birthplace = birth + place)의 조합으로 구성된 경우가 많기 때문에, 하나의 단어를 여러 서브워드로 분리해서 단어를 인코딩 및 임베딩하겠다는 의도를 가진 전처리 작업입니다. 이를 통해 OOV나 희귀 단어, 신조어와 같은 문제를 완화시킬 수 있습니다. 실제로 언어의 특성에 따라 영어권 언어나 한국어는 서브워드 분리를 시도했을 때 어느정도 의미있는 단위로 나누는 것이 가능합니다. 이 책에서는 이런 작업을 하는 토크나이저를 서브워드 토크나이저라고 명명하겠습니다.\\n이번 챕터에서는 서브워드 토크나이저의 주요 알고리즘인 바이트 페어 인코딩과 실제 실무에서 사용하는 서브워드 토크나이저 구현체인 SentencePiece와 Huggingface의 Tokenizers에 대해서 소개합니다.', \"==================================================\\n--- 13-01 바이트 페어 인코딩(Byte Pair Encoding, BPE) ---\\n```\\nword split into characters: ('h', 'i', 'g', 'h', 'i', 'n', 'g', '')\\nIteration 1:\\nbigrams in the word: {('n', 'g'), ('g', 'h'), ('h', 'i'), ('g', '</w>'), ('i', 'n'), ('i', 'g')}\\ncandidate for merging: ('n', 'g')\\nCandidate not in BPE merges, algorithm stops.\\n('h', 'i', 'g', 'h', 'i', 'n', 'g')\", \"Candidate not in BPE merges, algorithm stops.\\n('h', 'i', 'g', 'h', 'i', 'n', 'g')\\n```기계에게 아무리 많은 단어를 학습시켜도 세상의 모든 단어를 알려줄 수는 없는 노릇입니다. 만약 기계가 모르는 단어가 등장하면 그 단어를 단어 집합에 없는 단어란 의미에서 해당 토큰을 UNK(Unknown Token)라고 표현합니다. 기계가 문제를 풀 때 모르는 단어가 등장하면 (사람도 마찬가지지만) 주어진 문제를 푸는 것이 까다로워집니다. 이와 같이 모르는 단어로 인해 문제를 푸는 것이 까다로워지는 상황을 OOV(Out-Of-Vocabulary) 문제라고 합니다.\", '서브워드 분리(Subword segmenation) 작업은 하나의 단어는 더 작은 단위의 의미있는 여러 서브워드들(Ex) birthplace = birth + place)의 조합으로 구성된 경우가 많기 때문에, 하나의 단어를 여러 서브워드로 분리해서 단어를 인코딩 및 임베딩하겠다는 의도를 가진 전처리 작업입니다. 이를 통해 OOV나 희귀 단어, 신조어와 같은 문제를 완화시킬 수 있습니다. 실제로 언어의 특성으로 인해 영어나 한국어는 서브워드 분리를 시도했을 때 어느정도 의미있는 단위로 나누는 것이 가능합니다. 이 책에서는 이런 작업을 하는 토크나이저를 서브워드 토크나이저라고 명명합니다. 여기서는 OOV(Out-Of-Vocabulary) 문제를 완화하는 대표적인 서브워드 분리 알고리즘인 BPE(Byte Pair Encoding) 알고리즘을 소개합니다.']\n",
      "[\"BPE(Byte pair encoding) 알고리즘은 1994년에 제안된 데이터 압축 알고리즘입니다. 하지만 후에 자연어 처리의 서브워드 분리 알고리즘으로 응용되었는데 이에 대해서는 뒤에 언급하도록 하고, 우선 기존의 BPE의 작동 방법에 대해서 이해해보겠습니다. 아래와 같은 문자열이 주어졌을 때 BPE을 수행한다고 해봅시다.\\naaabdaaabac\\nBPE은 기본적으로 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식을 수행합니다. 태생이 압축 알고리즘인 만큼, 여기서는 글자 대신 바이트(byte)라는 표현을 사용하겠습니다. 예를 들어 위의 문자열 중 가장 자주 등장하고 있는 바이트의 쌍(byte pair)은 'aa'입니다. 이 'aa'라는 바이트의 쌍을 하나의 바이트인 'Z'로 치환해보겠습니다.\\nZabdZabac\\nZ=aa\\n위 문자열 중에서 가장 많이 등장하고 있는 바이트의 쌍은 'ab'입니다. 이 'ab'를 'Y'로 치환해봅시다.\\nZYdZYac\\nY=ab\", \"ZabdZabac\\nZ=aa\\n위 문자열 중에서 가장 많이 등장하고 있는 바이트의 쌍은 'ab'입니다. 이 'ab'를 'Y'로 치환해봅시다.\\nZYdZYac\\nY=ab\\nZ=aa\\n가장 많이 등장하고 있는 바이트의 쌍은 'ZY'입니다. 이를 'X'로 치환해봅시다.\\nXdXac\\nX=ZY\\nY=ab\\nZ=aa\\n더 이상 병합할 바이트의 쌍은 없으므로 BPE는 위의 결과를 최종 결과로 하여 종료됩니다.\"]\n",
      "['논문 : https://arxiv.org/pdf/1508.07909.pdf\\n자연어 처리에서의 BPE는 서브워드 분리(subword segmentation) 알고리즘입니다. 기존에 있던 단어를 분리한다는 의미입니다. BPE을 요약하면, 글자(charcter) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 Bottom up 방식의 접근을 사용합니다. 우선 훈련 데이터에 있는 단어들을 모든 글자(chracters) 또는 유니코드(unicode) 단위로 단어 집합(vocabulary)를 만들고, 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합합니다.\\nBPE을 자연어 처리에 사용한다고 제안한 논문(Sennrich et al. (2016))에서 이미 BPE의 코드를 공개하였기 때문에, 바로 파이썬 실습이 가능합니다. 코드 실습을 진행하기 전에 육안으로 확인할 수 있는 간단한 예를 들어보겠습니다.\\n1) 기존의 접근', \"1) 기존의 접근\\n어떤 훈련 데이터로부터 각 단어들의 빈도수를 카운트했다고 해보겠습니다. 그리고 각 단어와 각 단어의 빈도수가 기록되어져 있는 해당 결과는 임의로 딕셔너리(dictionary)란 이름을 붙였습니다.\\n# dictionary\\n# 훈련 데이터에 있는 단어와 등장 빈도수\\nlow : 5, lower : 2, newest : 6, widest : 3\\n이 훈련 데이터에는 'low'란 단어가 5회 등장하였고, 'lower'란 단어는 2회 등장하였으며, 'newest'란 단어는 6회, 'widest'란 단어는 3회 등장하였다는 의미입니다. 그렇다면 딕셔너리로부터 이 훈련 데이터의 단어 집합(vocabulary)을 얻는 것은 간단합니다.\\n# vocabulary\\nlow, lower, newest, widest\", \"# vocabulary\\nlow, lower, newest, widest\\n단어 집합은 중복을 배제한 단어들의 집합을 의미하므로 기존에 배운 단어 집합의 정의라면, 이 훈련 데이터의 단어 집합에는 'low', 'lower', 'newest', 'widest'라는 4개의 단어가 존재합니다. 그리고 이 경우 테스트 과정에서 'lowest'란 단어가 등장한다면 기계는 이 단어를 학습한 적이 없으므로 해당 단어에 대해서 제대로 대응하지 못하는 OOV 문제가 발생합니다. 그렇다면 BPE를 적용한다면 어떨까요?\\n2) BPE 알고리즘을 사용한 경우\\n위의 딕셔너리에 BPE를 적용해봅시다. 우선 딕셔너리의 모든 단어들을 글자(chracter) 단위로 분리합니다. 이 경우 딕셔너리는 아래와 같습니다. 이제부터 딕셔너리는 자신 또한 업데이트되며 앞으로 단어 집합을 업데이트하기 위해 지속적으로 참고되는 참고 자료의 역할을 합니다.\\n# dictionary\", '# dictionary\\nl o w : 5,  l o w e r : 2,  n e w e s t : 6,  w i d e s t : 3\\n딕셔너리를 참고로 한 초기 단어 집합(vocabulary)을 아래와 같습니다. 간단히 말해 초기 구성은 글자 단위로 분리된 상태입니다.\\n# vocabulary\\nl, o, w, e, r, n, s, t, i, d\\nBPE의 특징은 알고리즘의 동작을 몇 회 반복(iteration)할 것인지를 사용자가 정한다는 점입니다. 여기서는 총 10회를 수행한다고 가정합니다. 다시 말해 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합하는 과정을 총 10회 반복합니다. 위의 딕셔너리에 따르면 빈도수가 현재 가장 높은 유니그램의 쌍은 (e, s)입니다.\\n1회 - 딕셔너리를 참고로 하였을 때 빈도수가 9로 가장 높은 (e, s)의 쌍을 es로 통합합니다.\\n# dictionary update!\\nl o w : 5,\\nl o w e r : 2,', '# dictionary update!\\nl o w : 5,\\nl o w e r : 2,\\nn e w es t : 6,\\nw i d es t : 3\\n# vocabulary update!\\nl, o, w, e, r, n, s, t, i, d, es\\n2회 - 빈도수가 9로 가장 높은 (es, t)의 쌍을 est로 통합합니다.\\n# dictionary update!\\nl o w : 5,\\nl o w e r : 2,\\nn e w est : 6,\\nw i d est : 3\\n# vocabulary update!\\nl, o, w, e, r, n, s, t, i, d, es, est\\n3회 - 빈도수가 7로 가장 높은 (l, o)의 쌍을 lo로 통합합니다.\\n# dictionary update!\\nlo w : 5,\\nlo w e r : 2,\\nn e w est : 6,\\nw i d est : 3\\n# vocabulary update!\\nl, o, w, e, r, n, s, t, i, d, es, est, lo', 'n e w est : 6,\\nw i d est : 3\\n# vocabulary update!\\nl, o, w, e, r, n, s, t, i, d, es, est, lo\\n이와 같은 방식으로 총 10회 반복하였을 때 얻은 딕셔너리와 단어 집합은 아래와 같습니다.\\n# dictionary update!\\nlow : 5,\\nlow e r : 2,\\nnewest : 6,\\nwidest : 3\\n# vocabulary update!\\nl, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest', \"l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest\\n이 경우 테스트 과정에서 'lowest'란 단어가 등장한다면, 기존에는 OOV에 해당되는 단어가 되었겠지만 BPE 알고리즘을 사용한 위의 단어 집합에서는 더 이상 'lowest'는 OOV가 아닙니다. 기계는 우선 'lowest'를 전부 글자 단위로 분할합니다. 즉, 'l, o, w, e, s, t'가 됩니다. 그리고 기계는 위의 단어 집합을 참고로 하여 'low'와 'est'를 찾아냅니다. 즉, 'lowest'를 기계는 'low'와 'est' 두 단어로 인코딩합니다. 그리고 이 두 단어는 둘 다 단어 집합에 있는 단어이므로 OOV가 아닙니다.\\n이 동작 과정을 그림으로 표현한다면 다음과 같습니다.\\n[이미지: ]\\n실제 코드를 통해 구현해봅시다.\\n3) 코드 실습하기\", \"이 동작 과정을 그림으로 표현한다면 다음과 같습니다.\\n[이미지: ]\\n실제 코드를 통해 구현해봅시다.\\n3) 코드 실습하기\\n아래 코드는 원 논문에서 공개한 코드를 참고로 하여 수정한 코드입니다. 우선 필요한 도구들을 임포트합니다.\\nimport re, collections\\nfrom IPython.display import display, Markdown, Latex\\nBPE을 몇 회 수행할 것인지를 정합니다. 여기서는 10회로 정했습니다.\\nnum_merges = 10\\nBPE에 사용할 단어가 low, lower, newest, widest일 때, BPE의 입력으로 사용하는 실제 단어 집합은 아래와 같습니다. </w>는 단어의 맨 끝에 붙이는 특수 문자이며, 각 단어는 글자(character) 단위로 분리합니다.\\ndictionary = {'l o w </w>' : 5,\\n'l o w e r </w>' : 2,\\n'n e w e s t </w>':6,\\n'w i d e s t </w>':3\\n}\", \"dictionary = {'l o w </w>' : 5,\\n'l o w e r </w>' : 2,\\n'n e w e s t </w>':6,\\n'w i d e s t </w>':3\\n}\\nBPE의 코드는 아래와 같습니다. 알고리즘은 위에서 설명했던 것과 동일하게 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합하는 과정으로 num_merges회 반복합니다.\\ndef get_stats(dictionary):\\n# 유니그램의 pair들의 빈도수를 카운트\\npairs = collections.defaultdict(int)\\nfor word, freq in dictionary.items():\\nsymbols = word.split()\\nfor i in range(len(symbols)-1):\\npairs[symbols[i],symbols[i+1]] += freq\\nprint('현재 pair들의 빈도수 :', dict(pairs))\\nreturn pairs\", 'pairs[symbols[i],symbols[i+1]] += freq\\nprint(\\'현재 pair들의 빈도수 :\\', dict(pairs))\\nreturn pairs\\ndef merge_dictionary(pair, v_in):\\nv_out = {}\\nbigram = re.escape(\\' \\'.join(pair))\\np = re.compile(r\\'(?<!\\\\S)\\' + bigram + r\\'(?!\\\\S)\\')\\nfor word in v_in:\\nw_out = p.sub(\\'\\'.join(pair), word)\\nv_out[w_out] = v_in[word]\\nreturn v_out\\nbpe_codes = {}\\nbpe_codes_reverse = {}\\nfor i in range(num_merges):\\ndisplay(Markdown(\"### Iteration {}\".format(i + 1)))\\npairs = get_stats(dictionary)\\nbest = max(pairs, key=pairs.get)', 'pairs = get_stats(dictionary)\\nbest = max(pairs, key=pairs.get)\\ndictionary = merge_dictionary(best, dictionary)\\nbpe_codes[best] = i\\nbpe_codes_reverse[best[0] + best[1]] = best\\nprint(\"new merge: {}\".format(best))\\nprint(\"dictionary: {}\".format(dictionary))\\n이를 실행하면 출력 결과는 아래와 같으며 이는 글자들의 통합 과정을 보여주고 있습니다.\\nIteration 1', \"이를 실행하면 출력 결과는 아래와 같으며 이는 글자들의 통합 과정을 보여주고 있습니다.\\nIteration 1\\n현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\\nnew merge: ('e', 's')\\ndictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\\nIteration 2\", \"Iteration 2\\n현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3}\\nnew merge: ('es', 't')\\ndictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\\nIteration 3\", \"Iteration 3\\n현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}\\nnew merge: ('est', '</w>')\\ndictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\\nIteration 4\", \"Iteration 4\\n현재 pair들의 빈도수 : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\\nnew merge: ('l', 'o')\\ndictionary: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\\nIteration 5\", \"Iteration 5\\n현재 pair들의 빈도수 : {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\\nnew merge: ('lo', 'w')\\ndictionary: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\\nIteration 6\", \"dictionary: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\\nIteration 6\\n현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\\nnew merge: ('n', 'e')\\ndictionary: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\\nIteration 7\", \"dictionary: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\\nIteration 7\\n현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\\nnew merge: ('ne', 'w')\\ndictionary: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\\nIteration 8\", \"dictionary: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\\nIteration 8\\n현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\\nnew merge: ('new', 'est</w>')\\ndictionary: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\\nIteration 9\", \"dictionary: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\\nIteration 9\\n현재 pair들의 빈도수 : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\\nnew merge: ('low', '</w>')\\ndictionary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\\nIteration 10\\n현재 pair들의 빈도수 : {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\\nnew merge: ('w', 'i')\", \"new merge: ('w', 'i')\\ndictionary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\\ne와 s의 쌍은 초기 단어 집합에서 총 9회 등장했습니다. 그렇기 때문에 es로 통합됩니다. 그 다음으로는 es와 t의 쌍을, 그 다음으로는 est와 </w>의 쌍을 통합시킵니다. 빈도수가 가장 높은 순서대로 통합하는 이 과정을 총 num_merges회 반복한 것입니다.\\nbpe_codes를 출력하면 merge 했던 기록이 출력됩니다.\\nprint(bpe_codes)\\n{('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('w', 'i'): 9}\", '이 기록은 새로운 단어가 등장하였을 때, 현재 가지고 있는 서브워드 단어 집합에 의거하여 분리하는 일에 참고할 수 있습니다.\\n4) OOV에 대처하기\\ndef get_pairs(word):\\n\"\"\"Return set of symbol pairs in a word.\\nWord is represented as a tuple of symbols (symbols being variable-length strings).\\n\"\"\"\\npairs = set()\\nprev_char = word[0]\\nfor char in word[1:]:\\npairs.add((prev_char, char))\\nprev_char = char\\nreturn pairs\\ndef encode(orig):\\n\"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\\nword = tuple(orig) + (\\'</w>\\',)', 'word = tuple(orig) + (\\'</w>\\',)\\ndisplay(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\\npairs = get_pairs(word)\\nif not pairs:\\nreturn orig\\niteration = 0\\nwhile True:\\niteration += 1\\ndisplay(Markdown(\"__Iteration {}:__\".format(iteration)))\\nprint(\"bigrams in the word: {}\".format(pairs))\\nbigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float(\\'inf\\')))\\nprint(\"candidate for merging: {}\".format(bigram))\\nif bigram not in bpe_codes:', 'print(\"candidate for merging: {}\".format(bigram))\\nif bigram not in bpe_codes:\\ndisplay(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\\nbreak\\nfirst, second = bigram\\nnew_word = []\\ni = 0\\nwhile i < len(word):\\ntry:\\nj = word.index(first, i)\\nnew_word.extend(word[i:j])\\ni = j\\nexcept:\\nnew_word.extend(word[i:])\\nbreak\\nif word[i] == first and i < len(word)-1 and word[i+1] == second:\\nnew_word.append(first+second)\\ni += 2\\nelse:\\nnew_word.append(word[i])\\ni += 1\\nnew_word = tuple(new_word)', 'i += 2\\nelse:\\nnew_word.append(word[i])\\ni += 1\\nnew_word = tuple(new_word)\\nword = new_word\\nprint(\"word after merging: {}\".format(word))\\nif len(word) == 1:\\nbreak\\nelse:\\npairs = get_pairs(word)\\n# 특별 토큰인 </w>는 출력하지 않는다.\\nif word[-1] == \\'</w>\\':\\nword = word[:-1]\\nelif word[-1].endswith(\\'</w>\\'):\\nword = word[:-1] + (word[-1].replace(\\'</w>\\',\\'\\'),)\\nreturn word\\n단어 \\'loki\\'가 들어오면 BPE 알고리즘 해당 단어를 어떻게 분리할까요?\\nencode(\"loki\")\\nword split into characters: (\\'l\\', \\'o\\', \\'k\\', \\'i\\', \\'\\')\\nIteration 1:', 'encode(\"loki\")\\nword split into characters: (\\'l\\', \\'o\\', \\'k\\', \\'i\\', \\'\\')\\nIteration 1:\\nbigrams in the word: {(\\'i\\', \\'</w>\\'), (\\'o\\', \\'k\\'), (\\'l\\', \\'o\\'), (\\'k\\', \\'i\\')}\\ncandidate for merging: (\\'l\\', \\'o\\')\\nword after merging: (\\'lo\\', \\'k\\', \\'i\\', \\'</w>\\')\\nIteration 2:\\nbigrams in the word: {(\\'i\\', \\'</w>\\'), (\\'k\\', \\'i\\'), (\\'lo\\', \\'k\\')}\\ncandidate for merging: (\\'i\\', \\'</w>\\')\\nCandidate not in BPE merges, algorithm stops.\\n(\\'lo\\', \\'k\\', \\'i\\')\\n현재 서브워드 단어집합에는 \\'lo\\'가 존재하므로, \\'lo\\'는 유지하고 \\'k\\'와 \\'i\\'는 분리시킵니다. 단어 \\'lowest\\'에 대해서도 수행해봅시다.', '(\\'lo\\', \\'k\\', \\'i\\')\\n현재 서브워드 단어집합에는 \\'lo\\'가 존재하므로, \\'lo\\'는 유지하고 \\'k\\'와 \\'i\\'는 분리시킵니다. 단어 \\'lowest\\'에 대해서도 수행해봅시다.\\nencode(\"lowest\")\\nword split into characters: (\\'l\\', \\'o\\', \\'w\\', \\'e\\', \\'s\\', \\'t\\', \\'\\')\\nIteration 1:\\nbigrams in the word: {(\\'e\\', \\'s\\'), (\\'s\\', \\'t\\'), (\\'t\\', \\'</w>\\'), (\\'o\\', \\'w\\'), (\\'w\\', \\'e\\'), (\\'l\\', \\'o\\')}\\ncandidate for merging: (\\'e\\', \\'s\\')\\nword after merging: (\\'l\\', \\'o\\', \\'w\\', \\'es\\', \\'t\\', \\'</w>\\')\\nIteration 2:\\nbigrams in the word: {(\\'w\\', \\'es\\'), (\\'es\\', \\'t\\'), (\\'t\\', \\'</w>\\'), (\\'o\\', \\'w\\'), (\\'l\\', \\'o\\')}', \"Iteration 2:\\nbigrams in the word: {('w', 'es'), ('es', 't'), ('t', '</w>'), ('o', 'w'), ('l', 'o')}\\ncandidate for merging: ('es', 't')\\nword after merging: ('l', 'o', 'w', 'est', '</w>')\\nIteration 3:\\nbigrams in the word: {('o', 'w'), ('l', 'o'), ('est', '</w>'), ('w', 'est')}\\ncandidate for merging: ('est', '</w>')\\nword after merging: ('l', 'o', 'w', 'est</w>')\\nIteration 4:\\nbigrams in the word: {('o', 'w'), ('l', 'o'), ('w', 'est</w>')}\\ncandidate for merging: ('l', 'o')\", \"bigrams in the word: {('o', 'w'), ('l', 'o'), ('w', 'est</w>')}\\ncandidate for merging: ('l', 'o')\\nword after merging: ('lo', 'w', 'est</w>')\\nIteration 5:\\nbigrams in the word: {('lo', 'w'), ('w', 'est</w>')}\\ncandidate for merging: ('lo', 'w')\\nword after merging: ('low', 'est</w>')\\nIteration 6:\\nbigrams in the word: {('low', 'est</w>')}\\ncandidate for merging: ('low', 'est</w>')\\nCandidate not in BPE merges, algorithm stops.\\n('low', 'est')\", 'Candidate not in BPE merges, algorithm stops.\\n(\\'low\\', \\'est\\')\\n현재 서브워드 단어집합에 \\'low\\'와 \\'est\\'가 존재하므로, \\'low\\'와 \\'est\\'를 분리시킵니다. 단어 \\'lowing\\'에 대해서도 수행해봅시다.\\nencode(\"lowing\")\\nword split into characters: (\\'l\\', \\'o\\', \\'w\\', \\'i\\', \\'n\\', \\'g\\', \\'\\')\\nIteration 1:\\nbigrams in the word: {(\\'n\\', \\'g\\'), (\\'w\\', \\'i\\'), (\\'g\\', \\'</w>\\'), (\\'i\\', \\'n\\'), (\\'o\\', \\'w\\'), (\\'l\\', \\'o\\')}\\ncandidate for merging: (\\'l\\', \\'o\\')\\nword after merging: (\\'lo\\', \\'w\\', \\'i\\', \\'n\\', \\'g\\', \\'</w>\\')\\nIteration 2:', \"word after merging: ('lo', 'w', 'i', 'n', 'g', '</w>')\\nIteration 2:\\nbigrams in the word: {('lo', 'w'), ('n', 'g'), ('w', 'i'), ('g', '</w>'), ('i', 'n')}\\ncandidate for merging: ('lo', 'w')\\nword after merging: ('low', 'i', 'n', 'g', '</w>')\\nIteration 3:\\nbigrams in the word: {('n', 'g'), ('g', '</w>'), ('i', 'n'), ('low', 'i')}\\ncandidate for merging: ('n', 'g')\\nCandidate not in BPE merges, algorithm stops.\\n('low', 'i', 'n', 'g')\", 'Candidate not in BPE merges, algorithm stops.\\n(\\'low\\', \\'i\\', \\'n\\', \\'g\\')\\n현재 서브워드 단어집합에 \\'low\\'가 존재하지만, \\'i\\', \\'n\\', \\'g\\'의 바이그램 조합으로 이루어진 서브워드는 존재하지 않으므로 \\'i\\', \\'n\\', \\'g\\'로 전부 분리합니다. 훈련된 데이터 중에서 어떤 서브워드도 존재하지 않는 \\'highing\\'은 어떨까요?\\nencode(\"highing\")\\nword split into characters: (\\'h\\', \\'i\\', \\'g\\', \\'h\\', \\'i\\', \\'n\\', \\'g\\', \\'\\')\\nIteration 1:\\nbigrams in the word: {(\\'n\\', \\'g\\'), (\\'g\\', \\'h\\'), (\\'h\\', \\'i\\'), (\\'g\\', \\'</w>\\'), (\\'i\\', \\'n\\'), (\\'i\\', \\'g\\')}\\ncandidate for merging: (\\'n\\', \\'g\\')\\nCandidate not in BPE merges, algorithm stops.', \"candidate for merging: ('n', 'g')\\nCandidate not in BPE merges, algorithm stops.\\n('h', 'i', 'g', 'h', 'i', 'n', 'g')\\n모든 알파벳이 분리됩니다. BPE 외에도 BPE를 참고하여 만들어진 Wordpiece Tokenizer나 Unigram Language Model Tokenizer와 같은 서브워드 분리 알고리즘이 존재합니다. 이 두 알고리즘에 대해서는 간략히 이런 것들이 존재한다 정도로만 언급하고 넘어가겠습니다.\"]\n",
      "['논문 : https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf\\n구글이 위 WordPiece Tokenizer를 변형하여 번역기에 사용했다는 논문 : https://arxiv.org/pdf/1609.08144.pdf\\nWordPiece Tokenizer은 BPE의 변형 알고리즘입니다. 해당 알고리즘은 BPE가 빈도수에 기반하여 가장 많이 등장한 쌍을 병합하는 것과는 달리, 병합되었을 때 코퍼스의 우도(Likelihood)를 가장 높이는 쌍을 병합합니다. 2016년의 위 논문에서 구글은 구글 번역기에서 WordPiece Tokenizer가 수행된 결과에 대해서 기술하였습니다.\\n수행하기 이전의 문장: Jet makers feud over seat width with big orders at stake', '수행하기 이전의 문장: Jet makers feud over seat width with big orders at stake\\nWordPiece Tokenizer를 수행한 결과(wordpieces): _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake\\nJet는 J와 et로 나누어졌으며, feud는 fe와 ud로 나누어진 것을 볼 수 있습니다. WordPiece Tokenizer는 모든 단어의 맨 앞에 _를 붙이고, 단어는 서브 워드(subword)로 통계에 기반하여 띄어쓰기로 분리합니다. 여기서 언더바 _는 문장 복원을 위한 장치입니다.', '예컨대, WordPiece Tokenizer의 결과로 나온 문장을 보면, Jet → _J et와 같이 기존에 없던 띄어쓰기가 추가되어 서브 워드(subwords)들을 구분하는 구분자 역할을 하고 있습니다. 그렇다면 기존에 있던 띄어쓰기와 구분자 역할의 띄어쓰기는 어떻게 구별할까요? 이 역할을 수행하는 것이 단어들 앞에 붙은 언더바 _입니다. WordPiece Tokenizer이 수행된 결과로부터 다시 수행 전의 결과로 돌리는 방법은 현재 있는 모든 띄어쓰기를 전부 제거하고, 언더바를 띄어쓰기로 바꾸면 됩니다.\\n이 알고리즘은 유명 딥 러닝 모델 BERT를 훈련하기 위해서 사용되기도 하였습니다.']\n",
      "['논문 : https://arxiv.org/pdf/1804.10959.pdf\\n유니그램 언어 모델 토크나이저는 각각의 서브워드들에 대해서 손실(loss)을 계산합니다. 여기서 서브 단어의 손실이라는 것은 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 우도(Likelihood)가 감소하는 정도를 말합니다. 이렇게 측정된 서브워드들을 손실의 정도로 정렬하여, 최악의 영향을 주는 10~20%의 토큰을 제거합니다. 이를 원하는 단어 집합의 크기에 도달할 때까지 반복합니다.\\n지금까지 서브워드 토크나이징 알고리즘들에 대해서 정리해보았습니다. 이어서 이를 실무에서 사용하기 위한 패키지인 센텐스피스(SentencePiece)나 토크나이저스(tokenizers)의 사용법에 대해서 학습합니다.\\n==================================================\\n--- 13-02 센텐스피스(SentencePiece) ---\\n```', \"==================================================\\n--- 13-02 센텐스피스(SentencePiece) ---\\n```\\n['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\\n[54, 200, 821, 85]\\n```앞서 서브워드 토큰화를 위한 BPE(Byte Pair Encoding) 알고리즘과 그 외 BPE의 변형 알고리즘에 대해서 간단히 언급했습니다. BPE를 포함하여 기타 서브워드 토크나이징 알고리즘들을 내장한 센텐스피스(SentencePiece)는 일반적으로 실무에서 선택할 수 있는 최선의 선택 중 하나입니다.\"]\n",
      "['논문 : https://arxiv.org/pdf/1808.06226.pdf\\n센텐스피스 깃허브 : https://github.com/google/sentencepiece\\n내부 단어 분리를 위한 유용한 패키지로 구글의 센텐스피스(Sentencepiece)가 있습니다. 구글은 BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 센텐스피스를 깃허브에 공개하였습니다.', '내부 단어 분리 알고리즘을 사용하기 위해서, 데이터에 단어 토큰화를 먼저 진행한 상태여야 한다면 이 단어 분리 알고리즘을 모든 언어에 사용하는 것은 쉽지 않습니다. 영어와 달리 한국어와 같은 언어는 단어 토큰화부터가 쉽지 않기 때문입니다. 그런데, 이런 사전 토큰화 작업(pretokenization)없이 전처리를 하지 않은 데이터(raw data)에 바로 단어 분리 토크나이저를 사용할 수 있다면, 이 토크나이저는 그 어떤 언어에도 적용할 수 있는 토크나이저가 될 것입니다. 센텐스피스는 이 이점을 살려서 구현되었습니다. 센텐스피스는 사전 토큰화 작업없이 단어 분리 토큰화를 수행하므로 언어에 종속되지 않습니다.\\npip install sentencepiece']\n",
      "['import sentencepiece as spm\\nimport pandas as pd\\nimport urllib.request\\nimport csv\\nIMDB 리뷰 데이터를 다운로드하고 이를 데이터프레임에 저장합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\\ntrain_df = pd.read_csv(\\'IMDb_Reviews.csv\\')\\ntrain_df[\\'review\\']\\n0        My family and I normally do not watch local mo...\\n1        Believe it or not, this was at one time the wo...', '1        Believe it or not, this was at one time the wo...\\n2        After some internet surfing, I found the \"Home...\\n3        One of the most unheralded great works of anim...\\n4        It was the Sixties, and anyone with long hair ...\\n...\\n49995    the people who came up with this are SICK AND ...\\n49996    The script is so so laughable... this in turn,...\\n49997    \"So there\\'s this bride, you see, and she gets ...\\n49998    Your mind will not be satisfied by this no\\x97bud...', \"49998    Your mind will not be satisfied by this no\\x97bud...\\n49999    The chaser's war on everything is a weekly sho...\\nName: review, Length: 50000, dtype: object\\nprint('리뷰 개수 :',len(train_df)) # 리뷰 개수 출력\\n리뷰 개수 : 50000\\n총 5만개의 샘플이 존재합니다. 센텐스피스의 입력으로 사용하기 위해서 데이터프레임을 txt 파일로 저장합니다.\\nwith open('imdb_review.txt', 'w', encoding='utf8') as f:\\nf.write('\\\\n'.join(train_df['review']))\\n센텐스피스로 단어 집합과 각 단어에 고유한 정수를 부여해보겠습니다.\", \"f.write('\\\\n'.join(train_df['review']))\\n센텐스피스로 단어 집합과 각 단어에 고유한 정수를 부여해보겠습니다.\\nspm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')\\n각 인자가 의미하는 바는 다음과 같습니다.\\ninput : 학습시킬 파일\\nmodel_prefix : 만들어질 모델 이름\\nvocab_size : 단어 집합의 크기\\nmodel_type : 사용할 모델 (unigram(default), bpe, char, word)\\nmax_sentence_length: 문장의 최대 길이\\npad_id, pad_piece: pad token id, 값\\nunk_id, unk_piece: unknown token id, 값\", \"pad_id, pad_piece: pad token id, 값\\nunk_id, unk_piece: unknown token id, 값\\nbos_id, bos_piece: begin of sentence token id, 값\\neos_id, eos_piece: end of sequence token id, 값\\nuser_defined_symbols: 사용자 정의 토큰\\nvocab 생성이 완료되면 imdb.model, imdb.vocab 파일 두개가 생성 됩니다. vocab 파일에서 학습된 서브워드들을 확인할 수 있습니다. 단어 집합의 크기를 확인하기 위해 vocab 파일을 데이터프레임에 저장해봅시다.\\nvocab_list = pd.read_csv('imdb.vocab', sep='\\\\t', header=None, quoting=csv.QUOTE_NONE)\\nvocab_list.sample(10)\", 'vocab_list.sample(10)\\n위에서 vocab_size의 인자를 통해 단어 집합의 크기를 5,000개로 제한하였으므로 단어 집합의 크기는 5,000개입니다.\\nlen(vocab_list)\\n5000\\nmodel 파일을 로드하여 단어 시퀀스를 정수 시퀀스로 바꾸는 인코딩 작업이나 반대로 변환하는 디코딩 작업을 할 수 있습니다.\\nsp = spm.SentencePieceProcessor()\\nvocab_file = \"imdb.model\"\\nsp.load(vocab_file)\\nTrue\\n아래의 두 가지 도구를 테스트해보겠습니다.\\nencode_as_pieces : 문장을 입력하면 서브 워드 시퀀스로 변환합니다.\\nencode_as_ids : 문장을 입력하면 정수 시퀀스로 변환합니다.\\nlines = [\\n\"I didn\\'t at all think of it this way.\",\\n\"I have waited a long time for someone to film\"\\n]', '\"I didn\\'t at all think of it this way.\",\\n\"I have waited a long time for someone to film\"\\n]\\nfor line in lines:\\nprint(line)\\nprint(sp.encode_as_pieces(line))\\nprint(sp.encode_as_ids(line))\\nprint()\\nI didn\\'t at all think of it this way.\\n[\\'▁I\\', \\'▁didn\\', \"\\'\", \\'t\\', \\'▁at\\', \\'▁all\\', \\'▁think\\', \\'▁of\\', \\'▁it\\', \\'▁this\\', \\'▁way\\', \\'.\\']\\n[41, 623, 4950, 4926, 138, 169, 378, 30, 58, 73, 413, 4945]\\nI have waited a long time for someone to film', \"I have waited a long time for someone to film\\n['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\\n[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\\nGetPieceSize() : 단어 집합의 크기를 확인합니다.\\nsp.GetPieceSize()\\n5000\\nidToPiece : 정수로부터 맵핑되는 서브 워드로 변환합니다.\\nsp.IdToPiece(430)\\n▁character\\nPieceToId : 서브워드로부터 맵핑되는 정수로 변환합니다.\\nsp.PieceToId('▁character')\\n430\\nDecodeIds : 정수 시퀀스로부터 문장으로 변환합니다.\\nsp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91])\", \"sp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91])\\nDecodePieces : 서브워드 시퀀스로부터 문장으로 변환합니다.\\nI have waited a long time for someone to film\\nsp.DecodePieces(['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film'])\\nencode : 문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환 가능합니다.\\nI have waited a long time for someone to film\\nprint(sp.encode('I have waited a long time for someone to film', out_type=str))\", \"print(sp.encode('I have waited a long time for someone to film', out_type=str))\\nprint(sp.encode('I have waited a long time for someone to film', out_type=int))\\n['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\\n[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\"]\n",
      "['네이버 영화 리뷰 데이터에 대해서 위의 IMDB 리뷰 데이터와 동일한 과정을 진행해보겠습니다.\\nimport pandas as pd\\nimport sentencepiece as spm\\nimport urllib.request\\nimport csv\\n네이버 영화 리뷰 데이터를 다운로드하여 데이터프레임에 저장합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\\nnaver_df = pd.read_table(\\'ratings.txt\\')\\nnaver_df[:5]\\n총 20만개의 샘플이 존재합니다.\\nprint(\\'리뷰 개수 :\\',len(naver_df)) # 리뷰 개수 출력\\n리뷰 개수 : 200000\\n네이버 영화 리뷰 데이터의 경우 Null 값이 존재하므로 이를 제거한 후에 수행합니다.', \"리뷰 개수 : 200000\\n네이버 영화 리뷰 데이터의 경우 Null 값이 존재하므로 이를 제거한 후에 수행합니다.\\nprint(naver_df.isnull().values.any())\\nTrue\\nnaver_df = naver_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\\nprint(naver_df.isnull().values.any()) # Null 값이 존재하는지 확인\\nFalse\\nprint('리뷰 개수 :',len(naver_df)) # 리뷰 개수 출력\\n리뷰 개수 : 199992\\n최종적으로 199,992개의 샘플을 naver_review.txt 파일에 저장한 후에 센텐스피스를 통해 단어 집합을 생성합니다.\\nwith open('naver_review.txt', 'w', encoding='utf8') as f:\\nf.write('\\\\n'.join(naver_df['document']))\", \"with open('naver_review.txt', 'w', encoding='utf8') as f:\\nf.write('\\\\n'.join(naver_df['document']))\\nspm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')\\nvocab 생성이 완료되면 naver.model, naver.vocab 파일 두개가 생성 됩니다.\\n.vocab 에서 학습된 subwords를 확인할 수 있습니다.\\nvocab_list = pd.read_csv('naver.vocab', sep='\\\\t', header=None, quoting=csv.QUOTE_NONE)\\nvocab_list[:10]\\nvocab_list.sample(10)\", 'vocab_list[:10]\\nvocab_list.sample(10)\\nVocabulary 에는 unknown, 문장의 시작, 문장의 끝을 의미하는 special token이 0, 1, 2에 사용되었습니다.\\nlen(vocab_list)\\n5000\\n설정한대로 5000개의 서브워드가 단어 집합에 존재합니다.\\nsp = spm.SentencePieceProcessor()\\nvocab_file = \"naver.model\"\\nsp.load(vocab_file)\\nTrue\\nlines = [\\n\"뭐 이딴 것도 영화냐.\",\\n\"진짜 최고의 영화입니다 ㅋㅋ\",\\n]\\nfor line in lines:\\nprint(line)\\nprint(sp.encode_as_pieces(line))\\nprint(sp.encode_as_ids(line))\\nprint()\\n뭐 이딴 것도 영화냐.\\n[\\'▁뭐\\', \\'▁이딴\\', \\'▁것도\\', \\'▁영화냐\\', \\'.\\']\\n[132, 966, 1296, 2590, 3276]\\n진짜 최고의 영화입니다 ㅋㅋ', \"print()\\n뭐 이딴 것도 영화냐.\\n['▁뭐', '▁이딴', '▁것도', '▁영화냐', '.']\\n[132, 966, 1296, 2590, 3276]\\n진짜 최고의 영화입니다 ㅋㅋ\\n['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\\n[54, 200, 821, 85]\\nGetPieceSize() : 단어 집합의 크기를 확인합니다.\\nsp.GetPieceSize()\\n5000\\nidToPiece : 정수로부터 맵핑되는 서브 워드로 변환합니다.\\nsp.IdToPiece(4)\\n'영화'\\nPieceToId : 서브워드로부터 맵핑되는 정수로 변환합니다.\\nsp.PieceToId('영화')\\n4\\nDecodeIds : 정수 시퀀스로부터 문장으로 변환합니다.\\nsp.DecodeIds([54, 200, 821, 85])\\n진짜 최고의 영화입니다 ᄏᄏ\\nDecodePieces : 서브워드 시퀀스로부터 문장으로 변환합니다.\\nsp.DecodePieces(['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ'])\", \"DecodePieces : 서브워드 시퀀스로부터 문장으로 변환합니다.\\nsp.DecodePieces(['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ'])\\n진짜 최고의 영화입니다 ᄏᄏ\\nencode : 문장으로부터 인자값에 따라서 정수 시퀀스 또는 서브워드 시퀀스로 변환 가능합니다.\\nprint(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=str))\\nprint(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=int))\\n['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\\n[54, 200, 821, 85]\\n==================================================\\n--- 13-03 서브워드텍스트인코더(SubwordTextEncoder) ---\\n```\\n570 ----> 보면서\\n892 ----> 웃\\n36 ----> 지\\n584 ----> 않는\\n159 ----> 건\\n7091 ----> 불가능\", '```\\n570 ----> 보면서\\n892 ----> 웃\\n36 ----> 지\\n584 ----> 않는\\n159 ----> 건\\n7091 ----> 불가능\\n201 ----> 하다\\n```SubwordTextEncoder는 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저입니다.  BPE와 유사한 알고리즘인 Wordpiece Model을 채택하였으며, 패키지를 통해 쉽게 단어들을 서브워드들로 분리할 수 있습니다. SubwordTextEncoder를 통해서 IMDB 영화 리뷰 데이터와 네이버 영화 리뷰 데이터에 대해서 토큰화 작업을 수행해봅시다.\\nTensorflow 2.3+ 버전에서는 tfds.features.text 대신 tfds.deprecated.text라고 작성해야 합니다.']\n",
      "['import pandas as pd\\nimport urllib.request\\nimport tensorflow_datasets as tfds\\n다운로드한 데이터를 데이터프레임에 저장합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\\ntrain_df = pd.read_csv(\\'IMDb_Reviews.csv\\')\\n데이터프레임에서 \\'review\\'에 해당하는 열이 토큰화를 수행해야 할 데이터입니다.\\ntrain_df[\\'review\\']\\n0        My family and I normally do not watch local mo...\\n1        Believe it or not, this was at one time the wo...', '1        Believe it or not, this was at one time the wo...\\n2        After some internet surfing, I found the \"Home...\\n3        One of the most unheralded great works of anim...\\n4        It was the Sixties, and anyone with long hair ...\\n...\\n49995    the people who came up with this are SICK AND ...\\n49996    The script is so so laughable... this in turn,...\\n49997    \"So there\\'s this bride, you see, and she gets ...\\n49998    Your mind will not be satisfied by this no\\x97bud...', \"49998    Your mind will not be satisfied by this no\\x97bud...\\n49999    The chaser's war on everything is a weekly sho...\\nName: review, Length: 50000, dtype: object\\ntfds.features.text.SubwordTextEncoder.build_from_corpus의 인자로 토큰화 할 데이터를 넣어줍니다. 이 작업을 통해서 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여합니다.\\ntokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\\ntrain_df['review'], target_vocab_size=2**13)\\n.subwords를 통해서 토큰화 된 서브워드들을 확인할 수 있습니다. 100개만 출력해봅시다.\", \"train_df['review'], target_vocab_size=2**13)\\n.subwords를 통해서 토큰화 된 서브워드들을 확인할 수 있습니다. 100개만 출력해봅시다.\\nprint(tokenizer.subwords[:100])\", \"['the_', ', ', '\", '. \\', \\'a_\\', \\'and_\\', \\'of_\\', \\'to_\\', \\'s_\\', \\'is_\\', \\'br\\', \\'in_\\', \\'I_\\', \\'that_\\', \\'this_\\', \\'it_\\', \\' /><\\', \\' />\\', \\'was_\\', \\'The_\\', \\'t_\\', \\'as_\\', \\'with_\\', \\'for_\\', \\'.<\\', \\'on_\\', \\'but_\\', \\'movie_\\', \\'are_\\', \\' (\\', \\'have_\\', \\'his_\\', \\'film_\\', \\'not_\\', \\'be_\\', \\'you_\\', \\'ing_\\', \\' \"\\', \\'ed_\\', \\'it\\', \\'d_\\', \\'an_\\', \\'at_\\', \\'by_\\', \\'he_\\', \\'one_\\', \\'who_\\', \\'from_\\', \\'y_\\', \\'or_\\', \\'e_\\', \\'like_\\', \\'all_\\', \\'\" \\', \\'they_\\', \\'so_\\', \\'just_\\', \\'has_\\', \\') \\', \\'about_\\', \\'her_\\', \\'out_\\', \\'This_\\', \\'some_\\', \\'movie\\', \\'ly_\\', \\'film\\', \\'very_\\', \\'more_\\',', \"'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\", \"임의로 선택한 20번 인덱스의 샘플을 출력해보고, 정수 인코딩을 수행한 결과와 비교해보겠습니다.\\nprint(train_df['review'][20])\", \"Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable\", '. Strange, who would go on to play Frankenstein\\'s monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)', \"encode()를 통해서 입력한 데이터에 대해서 정수 인코딩을 수행한 결과를 얻을 수 있습니다.\\nprint('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))\", 'Tokenized sample question: [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043,', '1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16,', '2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]', '임의로 선택한 짧은 문장에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩해보겠습니다. 디코딩 할 때는 인코딩할 때 encode()를 사용한 것과 유사하게 decode()를 통해서 할 수 있습니다.\\n# train_df에 존재하는 문장 중 일부를 발췌\\nsample_string = \"It\\'s mind-blowing to me that this film was even made.\"\\n# 인코딩한 결과를 tokenized_string에 저장\\ntokenized_string = tokenizer.encode(sample_string)\\nprint (\\'정수 인코딩 후의 문장 : {}\\'.format(tokenized_string))\\n# 이를 다시 디코딩\\noriginal_string = tokenizer.decode(tokenized_string)\\nprint (\\'기존 문장 : {}\\'.format(original_string))', \"original_string = tokenizer.decode(tokenized_string)\\nprint ('기존 문장 : {}'.format(original_string))\\n정수 인코딩 후의 문장 : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\\n기존 문장 : It's mind-blowing to me that this film was even made.\\n.vocab_size를 통해 단어 집합의 크기를 확인할 수 있습니다.\\nprint('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)\\n단어 집합의 크기(Vocab size) : 8268\\n현재 단어 집합의 크기는 8,268개입니다. 디코딩 결과를 병렬적으로 나열하여 각 단어와 맵핑된 정수를 확인해봅시다.\\nfor ts in tokenized_string:\", \"현재 단어 집합의 크기는 8,268개입니다. 디코딩 결과를 병렬적으로 나열하여 각 단어와 맵핑된 정수를 확인해봅시다.\\nfor ts in tokenized_string:\\nprint ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\\n137 ----> It\\n8051 ----> '\\n8 ----> s\\n910 ----> mind\\n8057 ----> -\\n2169 ----> blow\\n36 ----> ing\\n7 ----> to\\n103 ----> me\\n13 ----> that\\n14 ----> this\\n32 ----> film\\n18 ----> was\\n79 ----> even\\n681 ----> made\\n8058 ----> .\\n이번에는 기존 예제 문장 중 even이라는 단어에 임의로 xyz라는 3개의 글자를 추가해봤습니다. 현재 토크나이저가 even이라는 단어를 이미 하나의 서브워드로 인식하고 있는 상황에서 나머지 xyz를 어떻게 분리하는지를 확인하기 위함입니다.\", '# 앞서 실습한 문장에 even 뒤에 임의로 xyz 추가\\nsample_string = \"It\\'s mind-blowing to me that this film was evenxyz made.\"\\n# 인코딩한 결과를 tokenized_string에 저장\\ntokenized_string = tokenizer.encode(sample_string)\\nprint (\\'정수 인코딩 후의 문장 : {}\\'.format(tokenized_string))\\n# 이를 다시 디코딩\\noriginal_string = tokenizer.decode(tokenized_string)\\nprint (\\'기존 문장 : {}\\'.format(original_string))\\n정수 인코딩 후의 문장 [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]', \"기존 문장: It's mind-blowing to me that this film was evenxyz made.\\nfor ts in tokenized_string:\\nprint ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\\n137 ----> It\\n8051 ----> '\\n8 ----> s\\n910 ----> mind\\n8057 ----> -\\n2169 ----> blow\\n36 ----> ing\\n7 ----> to\\n103 ----> me\\n13 ----> that\\n14 ----> this\\n32 ----> film\\n18 ----> was\\n7974 ----> even\\n8132 ----> x\\n8133 ----> y\\n997 ----> z\\n681 ----> made\\n8058 ----> .\\nevenxyz에서 even을 독립적으로 분리하고 xyz는 훈련 데이터에서 하나의 단어로서 등장한 적이 없으므로 각각 전부 분리됩니다.\"]\n",
      "['네이버 영화 리뷰에 대해서도 위에서 IMDB 영화 리뷰에 대해서 수행한 동일한 작업을 진행해봅시다.\\nimport pandas as pd\\nimport urllib.request\\nimport tensorflow_datasets as tfds\\n다운로드한 데이터를 데이터프레임에 저장합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\\ntrain_data = pd.read_table(\\'ratings_train.txt\\')\\n이 데이터에는 Null 값이 존재하므로 이를 제거해줍니다.\\nprint(train_data.isnull().sum())\\nid          0\\ndocument    5\\nlabel       0\\ndtype: int64', \"print(train_data.isnull().sum())\\nid          0\\ndocument    5\\nlabel       0\\ndtype: int64\\ntrain_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\\nprint(train_data.isnull().values.any()) # Null 값이 존재하는지 확인\\ntfds.features.text.SubwordTextEncoder.build_from_corpus의 인자로 네이버 영화 리뷰 데이터를 넣어서, 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여합니다.\\ntokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\\ntrain_data['document'], target_vocab_size=2**13)\\n토큰화 된 100개의 서브워드들을 출력해봅시다.\", \"train_data['document'], target_vocab_size=2**13)\\n토큰화 된 100개의 서브워드들을 출력해봅시다.\\nprint(tokenizer.subwords[:100])\", \"['. ', '..', '영화', '이_', '...', '의_', '는_', '도_', '다', ', ', '을_', '고_', '은_', '가_', '에_', '.. ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '..\", \". ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '... ', '진짜_', '이', '다_', '요', '만_', '? ', '과_', '나', '가', '서_', '지_', '로_', '으로_', '아', '어', '....', '음', '한', '수_', '와_', '도', '네', '그냥_', '나_', '더_', '왜_', '이런_', '면_', '기', '하고_', '보고_', '하는_', '서', '좀_', '리', '자', '스', '안', '! ', '에서_', '영화를_', '미', 'ㅋㅋ', '네요', '시', '주', '라', '는', '오', '없는_', '에', '해', '사', '!!', '영화는_', '마', '잘_', '수', '영화가_', '만', '본_', '로', '그_', '지만_', '대', '은', '비', '의', '일', '개', '있는_', '없다', '함', '구', '하']\", \"encode()를 통해 임의로 선택한 20번 인덱스의 샘플을 출력해보고, 정수 인코딩을 수행한 결과와 비교해보겠습니다.\\nprint(train_data['document'][20])\\n나름 심오한 뜻도 있는 듯. 그냥 학생이 선생과 놀아나는 영화는 절대 아님\\nprint('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))\\nTokenized sample question: [669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\\n21번 인덱스 샘플에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩해보겠습니다. 디코딩 할 때는 인코딩할 때 encode()를 사용한 것과 유사하게 decode()를 통해서 할 수 있습니다.\", \"sample_string = train_data['document'][21]\\n# 인코딩한 결과를 tokenized_string에 저장\\ntokenized_string = tokenizer.encode(sample_string)\\nprint ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\\n# 이를 다시 디코딩\\noriginal_string = tokenizer.decode(tokenized_string)\\nprint ('기존 문장 : {}'.format(original_string))\\n정수 인코딩 후의 문장 : [570, 892, 36, 584, 159, 7091, 201]\\n기존 문장 : 보면서 웃지 않는 건 불가능하다\\nfor ts in tokenized_string:\\nprint ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\\n570 ----> 보면서\\n892 ----> 웃\\n36 ----> 지\", \"print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\\n570 ----> 보면서\\n892 ----> 웃\\n36 ----> 지\\n584 ----> 않는\\n159 ----> 건\\n7091 ----> 불가능\\n201 ----> 하다\\n==================================================\\n--- 13-04 허깅페이스 토크나이저(Huggingface Tokenizer) ---\\n```\\n['▁이', '▁영화는', '▁정말', '▁재미있', '습니다.']\\n```자연어 처리 스타트업 허깅페이스가 개발한 패키지 tokenizers는 자주 등장하는 서브워드들을 하나의 토큰으로 취급하는 다양한 서브워드 토크나이저를 제공합니다. 이번 실습에서는 이 중에서 WordPiece Tokenizer를 실습해보겠습니다. 실습을 위해 우선 tokenizers를 설치합니다.\\npip install tokenizers\"]\n",
      "['구글이 공개한 딥 러닝 모델 BERT에는 WordPiece Tokenizer가 사용되었습니다. 허깅페이스는 해당 토크나이저를 직접 구현하여 tokenizers라는 패키지를 통해 버트워드피스토크나이저(BertWordPieceTokenizer)를 제공합니다.\\n여기서는 네이버 영화 리뷰 데이터를 해당 토크나이저에 학습시키고, 이로부터 서브워드의 단어 집합(Vocabulary)을 얻습니다. 그리고 임의의 문장에 대해서 학습된 토크나이저를 사용하여 토큰화를 진행합니다. 우선 네이버 영화 리뷰 데이터를 로드합니다.\\nimport pandas as pd\\nimport urllib.request\\nfrom tokenizers import BertWordPieceTokenizer\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")', \"센텐스피스(SentencePiece) 실습에서 진행했던 전처리와 동일한 과정을 진행합니다. ratings.txt라는 파일을 데이터프레임으로 로드한 후, 결측값을 제거하고, 실질적인 리뷰 데이터인 document열에 대해서 naver_review.txt라는 파일로 저장합니다.\\nnaver_df = pd.read_table('ratings.txt')\\nnaver_df = naver_df.dropna(how='any')\\nwith open('naver_review.txt', 'w', encoding='utf8') as f:\\nf.write('\\\\n'.join(naver_df['document']))\\n버트워드피스토크나이저를 설정합니다.\\ntokenizer = BertWordPieceTokenizer(lowercase=False, trip_accents=False)\\n각 인자가 의미하는 바는 다음과 같습니다.\\nlowercase : 대소문자를 구분 여부. True일 경우 구분하지 않음.\", \"각 인자가 의미하는 바는 다음과 같습니다.\\nlowercase : 대소문자를 구분 여부. True일 경우 구분하지 않음.\\nstrip_accents : True일 경우 악센트 제거.\\nex) é → e, ô → o\\n네이버 영화 리뷰 데이터를 학습하여 단어 집합을 얻어봅시다.\\ndata_file = 'naver_review.txt'\\nvocab_size = 30000\\nlimit_alphabet = 6000\\nmin_frequency = 5\\ntokenizer.train(files=data_file,\\nvocab_size=vocab_size,\\nlimit_alphabet=limit_alphabet,\\nmin_frequency=min_frequency)\\n각 인자가 의미하는 바는 다음과 같습니다.\\nfiles : 단어 집합을 얻기 위해 학습할 데이터\\nvocab_size : 단어 집합의 크기\\nlimit_alphabet : 병합 전의 초기 토큰의 허용 개수.\", \"files : 단어 집합을 얻기 위해 학습할 데이터\\nvocab_size : 단어 집합의 크기\\nlimit_alphabet : 병합 전의 초기 토큰의 허용 개수.\\nmin_frequency : 최소 해당 횟수만큼 등장한 쌍(pair)의 경우에만 병합 대상이 된다.\\n학습이 다 되었다면 vocab을 저장합니다. 경로를 지정해주어야 하는데 여기서는 현재 경로에 저장하겠습니다.\\n# vocab 저장\\ntokenizer.save_model('./')\\nvocab을 데이터프레임으로 로드합니다.\\n# vocab 로드\\ndf = pd.read_fwf('vocab.txt', header=None)\\ndf\\n[이미지: ]\\n총 30,000개의 단어가 존재합니다. 이는 단어 집합의 크기를 30,000으로 지정하였기 때문입니다. 실제 토큰화를 수행해봅시다.\\nencoded = tokenizer.encode('아 배고픈데 짜장면먹고싶다')\\nprint('토큰화 결과 :',encoded.tokens)\", \"encoded = tokenizer.encode('아 배고픈데 짜장면먹고싶다')\\nprint('토큰화 결과 :',encoded.tokens)\\nprint('정수 인코딩 :',encoded.ids)\\nprint('디코딩 :',tokenizer.decode(encoded.ids))\\n토큰화 결과 : ['아', '배고', '##픈', '##데', '짜장면', '##먹고', '##싶다']\\n정수 인코딩 : [2111, 20629, 3979, 3244, 24682, 7871, 7379]\\n디코딩 : 아 배고픈데 짜장면먹고싶다\\n.ids는 실질적인 딥 러닝 모델의 입력으로 사용되는 정수 인코딩 결과를 출력합니다. tokens는 해당 토크나이저가 어떻게 토큰화를 진행했는지를 보여줍니다. decode()는 정수 시퀀스를 문자열로 복원합니다.\\nencoded = tokenizer.encode('커피 한잔의 여유를 즐기다')\\nprint('토큰화 결과 :',encoded.tokens)\", \"encoded = tokenizer.encode('커피 한잔의 여유를 즐기다')\\nprint('토큰화 결과 :',encoded.tokens)\\nprint('정수 인코딩 :',encoded.ids)\\nprint('디코딩 :',tokenizer.decode(encoded.ids))\\n토큰화 결과 : ['커피', '한잔', '##의', '여유', '##를', '즐기', '##다']\\n정수 인코딩 : [12825, 25641, 3435, 12696, 3419, 10784, 3260]\\n디코딩 : 커피 한잔의 여유를 즐기다\"]\n",
      "['이 외 ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer 등이 존재하며 선택에 따라서 사용할 수 있습니다.\\nBertWordPieceTokenizer : BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\\nCharBPETokenizer : 오리지널 BPE\\nByteLevelBPETokenizer : BPE의 바이트 레벨 버전\\nSentencePieceBPETokenizer : 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체\\nfrom tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer\\ntokenizer = SentencePieceBPETokenizer()', 'tokenizer = SentencePieceBPETokenizer()\\ntokenizer.train(\\'naver_review.txt\\', vocab_size=10000, min_frequency=5)\\nencoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\\nprint(encoded.tokens)\\n[\\'▁이\\', \\'▁영화는\\', \\'▁정말\\', \\'▁재미있\\', \\'습니다.\\']\\n==================================================\\n--- 14. RNN을 이용한 인코더-디코더 ---', '==================================================\\n--- 14. RNN을 이용한 인코더-디코더 ---\\n앞서 RNN의 다 대 일(many-to-one) 구조로 텍스트 분류를 풀 수 있었고, 다 대 다(many-to-many) 구조로는 개체명 인식이나 품사 태깅과 같은 문제를 풀 수 있었습니다. 이번에 살펴볼 RNN의 구조는 앞에서 살펴본 구조와는 다소 차이가 있는데, 하나의 RNN을 인코더. 또 다른 하나의 RNN을 디코더라는 모듈로 명명하고 두 개의 RNN을 연결해서 사용하는 인코더-디코더 구조입니다.', '이러한 인코더-디코더 구조는 주로 입력 문장과 출력 문장의 길이가 다를 경우에 사용하는데, 대표적인 분야가 번역기나 텍스트 요약과 같은 경우가 있습니다. 영어 문장을 한국어 문장으로 번역한다고 하였을 때,  입력 문장인 영어 문장과 번역된 결과인 한국어 문장의 길이는 똑같을 필요가 없습니다. 텍스트 요약의 경우에는 출력 문장이 요약된 문장이므로 입력 문장보다는 당연히 길이가 짧을 것입니다.\\n이번 챕터에서는 RNN의 인코더와 디코더가 각각 어떤 방식으로 동작하여 입력 문장으로부터 출력 문장을 연산해내는지 번역기 구현 프로젝트를 통해서 학습합니다. 그리고 번역이라는 섬세한 자연어 처리 태스크를 기계적으로 평가할 수 있는 방법인 BLEU(Bilingual Evaluation Understudy Score)라는 평가 방법에 대해서 설명합니다.\\n==================================================', '==================================================\\n--- 14-01 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq) ---\\n```\\n-----------------------------------\\n입력 문장: Hi.\\n정답 문장: Salut !\\n번역 문장: Salut.\\n-----------------------------------\\n입력 문장: I see.\\n정답 문장: Aha.\\n번역 문장: Je change.\\n-----------------------------------\\n입력 문장: Hug me.\\n정답 문장: Serrez-moi dans vos bras !\\n번역 문장: Serre-moi dans vos patents !\\n-----------------------------------\\n입력 문장: Help me.\\n정답 문장: Aidez-moi.\\n번역 문장: Aidez-moi.', '-----------------------------------\\n입력 문장: Help me.\\n정답 문장: Aidez-moi.\\n번역 문장: Aidez-moi.\\n-----------------------------------\\n입력 문장: I beg you.\\n정답 문장: Je vous en prie.\\n번역 문장: Je vous en prie.\\n```이번 실습은 케라스 함수형 API에 대한 이해가 필요합니다. 함수형 API(functional API, https://wikidocs.net/38861 )에 대해서 우선 숙지 후 실습을 진행해주세요.', '시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델입니다. 예를 들어 챗봇(Chatbot)과 기계 번역(Machine Translation)이 그러한 대표적인 예인데, 입력 시퀀스와 출력 시퀀스를 각각 질문과 대답으로 구성하면 챗봇으로 만들 수 있고, 입력 시퀀스와 출력 시퀀스를 각각 입력 문장과 번역 문장으로 만들면 번역기로 만들 수 있습니다. 그 외에도 내용 요약(Text Summarization), STT(Speech to Text) 등에서 쓰일 수 있습니다.\\n여기서는 기계 번역을 예제로 시퀀스-투-시퀀스를 설명합니다. 줄여서 seq2seq이라는 이름으로 명명하겠습니다.']\n",
      "[\"seq2seq는 번역기에서 대표적으로 사용되는 모델입니다. 앞으로의 설명 방식은 내부가 보이지 않는 커다란 블랙 박스에서 점차적으로 확대해가는 방식으로 설명합니다. 여기서 설명하는 내용의 대부분은 RNN 챕터에서 언급한 내용들로 단지 RNN을 어떻게 조립했느냐에 따라서 seq2seq라는 구조가 만들어집니다.\\n[이미지: ]\\n위의 그림은 seq2seq 모델로 만들어진 번역기가 'I am a student'라는 영어 문장을 입력받아서, 'je suis étudiant'라는 프랑스 문장을 출력하는 모습을 보여줍니다. 그렇다면, seq2seq 모델 내부의 모습은 어떻게 구성되었을까요?\\n[이미지: ]\", '[이미지: ]\\nseq2seq는 크게 인코더와 디코더라는 두 개의 모듈로 구성됩니다. 인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만드는데, 이를 컨텍스트 벡터(context vector)라고 합니다. 입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송합니다. 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력합니다.\\n[이미지: ]\\n컨텍스트 벡터에 대해서는 뒤에서 다시 언급하겠습니다. 위의 그림에서는 컨텍스트 벡터를 4의 사이즈로 표현하였지만, 실제 현업에서 사용되는 seq2seq 모델에서는 보통 수백 이상의 차원을 갖고있습니다. 인코더와 디코더의 내부를 좀 더 확대해보겠습니다.\\n[이미지: ]', '[이미지: ]\\n인코더 아키텍처와 디코더 아키텍처의 내부는 사실 두 개의 RNN 아키텍처 입니다. 입력 문장을 받는 RNN 셀을 인코더라고 하고, 출력 문장을 출력하는 RNN 셀을 디코더라고 합니다. 여기서는 인코더의 RNN 셀을 주황색으로, 디코더의 RNN 셀을 초록색으로 표현합니다. 물론, 성능 문제로 인해 실제로는 바닐라 RNN이 아니라 LSTM 셀 또는 GRU 셀들로 구성됩니다. 우선 인코더를 자세히보면, 입력 문장은 단어 토큰화를 통해서 단어 단위로 쪼개지고 단어 토큰 각각은 RNN 셀의 각 시점의 입력이 됩니다. 인코더 RNN 셀은 모든 단어를 입력받은 뒤에 인코더 RNN 셀의 마지막 시점의 은닉 상태를 디코더 RNN 셀로 넘겨주는데 이를 컨텍스트 벡터라고 합니다. 컨텍스트 벡터는 디코더 RNN 셀의 첫번째 은닉 상태에 사용됩니다.', '디코더는 기본적으로 RNNLM(RNN Language Model)입니다. RNNLM의 개념을 기억하고 있다면 좀 더 이해하기 쉽습니다. 디코더는 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>가 들어갑니다. 디코더는 <sos>가 입력되면, 다음에 등장할 확률이 높은 단어를 예측합니다. 첫번째 시점(time step)의 디코더 RNN 셀은 다음에 등장할 단어로 je를 예측하였습니다. 첫번째 시점의 디코더 RNN 셀은 예측된 단어 je를 다음 시점의 RNN 셀의 입력으로 입력합니다. 그리고 두번째 시점의 디코더 RNN 셀은 입력된 단어 je로부터 다시 다음에 올 단어인 suis를 예측하고, 또 다시 이것을 다음 시점의 RNN 셀의 입력으로 보냅니다. 디코더는 이런 식으로 기본적으로 다음에 올 단어를 예측하고, 그 예측한 단어를 다음 시점의 RNN 셀의 입력으로 넣는 행위를 반복합니다. 이 행위는 문장의 끝을 의미하는 심볼인 <eos>가 다음 단어로 예측될 때까지 반복됩니다', '. 이 행위는 문장의 끝을 의미하는 심볼인 <eos>가 다음 단어로 예측될 때까지 반복됩니다. 지금 설명하는 것은 테스트 과정 동안의 이야기입니다.', 'seq2seq는 훈련 과정과 테스트 과정(또는 실제 번역기를 사람이 쓸 때)의 작동 방식이 조금 다릅니다. 훈련 과정에서는 디코더에게 인코더가 보낸 컨텍스트 벡터와 실제 정답인 상황인 <sos> je suis étudiant를 입력 받았을 때, je suis étudiant <eos>가 나와야 된다고 정답을 알려주면서 훈련합니다. 이에 대해서는 뒤에 교사 강요(teacher forcing) 를 설명하면서 재언급하겠습니다. 반면 테스트 과정에서는 앞서 설명한 과정과 같이 디코더는 오직 컨텍스트 벡터와 <sos>만을 입력으로 받은 후에 다음에 올 단어를 예측하고, 그 단어를 다음 시점의 RNN 셀의 입력으로 넣는 행위를 반복합니다. 즉, 앞서 설명한 과정과 위의 그림은 테스트 과정에 해당됩니다. 이번에는 입, 출력에 쓰이는 단어 토큰들이 있는 부분을 좀 더 확대해보겠습니다.\\n[이미지: ]', '[이미지: ]\\n기계는 텍스트보다 숫자를 잘 처리합니다. 자연어 처리에서 텍스트를 벡터로 바꾸는 방법으로 주로 워드 임베딩이 사용된다고 설명한 바 있습니다. 즉, seq2seq에서 사용되는 모든 단어들은 임베딩 벡터로 변환 후 입력으로 사용됩니다. 위 그림은 모든 단어에 대해서 임베딩 과정을 거치게 하는 단계인 임베딩 층(embedding layer)의 모습을 보여줍니다.\\n[이미지: ]\\n예를 들어 I, am, a, student라는 단어들에 대한 임베딩 벡터는 위와 같은 모습을 가집니다. 여기서는 그림으로 표현하고자 사이즈를 4로 하였지만, 보통 실제 임베딩 벡터는 수백 개의 차원을 가질 수 있습니다. RNN 셀에 대해서 확대해보겠습니다. 이전에 설명하였지만, 하나의 RNN 셀은 각각의 시점(time step)마다 두 개의 입력을 받습니다.\\n[이미지: ]', '[이미지: ]\\n현재 시점(time step)을 t라고 할 때, RNN 셀은 t-1에서의 은닉 상태와 t에서의 입력 벡터를 입력으로 받고, t에서의 은닉 상태를 만듭니다. 이때 t에서의 은닉 상태는 바로 위에 또 다른 은닉층이나 출력층이 존재할 경우에는 위의 층으로 보내거나, 필요없으면 값을 무시할 수 있습니다. 그리고 RNN 셀은 다음 시점에 해당하는 t+1의 RNN 셀의 입력으로 현재 t에서의 은닉 상태를 입력으로 보냅니다.\\nRNN 챕터에서도 언급했지만, 이런 구조에서 현재 시점 t에서의 은닉 상태는 과거 시점의 동일한 RNN 셀에서의 모든 은닉 상태의 값들의 영향을 누적해서 받아온 값이라고 할 수 있습니다. 그렇기 때문에 앞서 언급했던 컨텍스트 벡터는 사실 인코더에서의 마지막 RNN 셀의 은닉 상태값을 말하는 것이며, 이는 입력 문장의 모든 단어 토큰들의 정보를 요약해서 담고있다고 할 수 있습니다.', '디코더는 인코더의 마지막 RNN 셀의 은닉 상태인 컨텍스트 벡터를 첫번째 은닉 상태의 값으로 사용합니다. 디코더의 첫번째 RNN 셀은 이 첫번째 은닉 상태의 값과, 현재 t에서의 입력값인 <sos>로부터, 다음에 등장할 단어를 예측합니다. 그리고 이 예측된 단어는 다음 시점인 t+1 RNN에서의 입력값이 되고, 이 t+1에서의 RNN 또한 이 입력값과 t에서의 은닉 상태로부터 t+1에서의 출력 벡터. 즉, 또 다시 다음에 등장할 단어를 예측하게 될 것입니다. 디코더가 다음에 등장할 단어를 예측하는 부분을 확대해보겠습니다.\\n[이미지: ]', '[이미지: ]\\n출력 단어로 나올 수 있는 단어들은 다양한 단어들이 있습니다. seq2seq 모델은 선택될 수 있는 모든 단어들로부터 하나의 단어를 골라서 예측해야 합니다. 이를 예측하기 위해서 쓸 수 있는 함수로는 뭐가 있을까요? 바로 소프트맥스 함수입니다. 디코더에서 각 시점(time step)의 RNN 셀에서 출력 벡터가 나오면, 해당 벡터는 소프트맥스 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환하고, 디코더는 출력 단어를 결정합니다.', '지금까지 가장 기본적인 seq2seq에 대해서 배워보았습니다. 사실 seq2seq는 어떻게 구현하느냐에 따라서 충분히 더 복잡해질 수 있습니다. 컨텍스트 벡터를 디코더의 초기 은닉 상태로만 사용할 수도 있고, 거기서 더 나아가 컨텍스트 벡터를 디코더가 단어를 예측하는 매 시점마다 하나의 입력으로 사용할 수도 있으며 거기서 더 나아가면 어텐션 메커니즘이라는 방법을 통해 지금 알고있는 컨텍스트 벡터보다 더욱 문맥을 반영할 수 있는 컨텍스트 벡터를 구하여 매 시점마다 하나의 입력으로 사용할 수도 있습니다. 어텐션 메커니즘에 대해서는 다음 챕터에서 배웁니다.']\n",
      "[\"seq2seq를 이용해서 기계 번역기를 만들어보겠습니다. 시작하기에 앞서 참고하면 좋은 게시물을 소개합니다. 인터넷에 케라스로 seq2seq를 구현하는 많은 유사 예제들이 나와있지만 대부분은 케라스 개발자 프랑수아 숄레의 블로그의 유명 게시물인 'sequence-to-sequence 10분만에 이해하기'가 원본입니다. 이번 실습 또한 해당 게시물의 예제에 많이 영향받았습니다.\\n해당 게시물 링크 : https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\", '실제 성능이 좋은 기계 번역기를 구현하려면 정말 방대한 데이터가 필요하므로 여기서는 방금 배운 seq2seq를 실습해보는 수준에서 아주 간단한 기계 번역기를 구축해보겠습니다. 기계 번역기를 훈련시키기 위해서는 훈련 데이터로 병렬 코퍼스(parallel corpus)가 필요합니다. 병렬 코퍼스란, 두 개 이상의 언어가 병렬적으로 구성된 코퍼스를 의미합니다.\\n다운로드 링크 : http://www.manythings.org/anki\\n이번 실습에서는 프랑스-영어 병렬 코퍼스인 fra-eng.zip 파일을 사용할 겁니다. 위의 링크에서 해당 파일을 다운받으시면 됩니다. 해당 파일의 압축을 풀면 fra.txt라는 파일이 있는데 이 파일이 이번 실습에서 사용할 파일입니다.\\n1) 병렬 코퍼스 데이터에 대한 이해와 전처리', \"1) 병렬 코퍼스 데이터에 대한 이해와 전처리\\n우선 병렬 코퍼스 데이터에 대한 이해를 해보겠습니다. 병렬 데이터라고 하면 앞서 수행한 태깅 작업의 데이터를 생각할 수 있지만, 앞서 수행한 태깅 작업의 병렬 데이터와 seq2seq가 사용하는 병렬 데이터는 성격이 조금 다릅니다. 태깅 작업의 병렬 데이터는 쌍이 되는 모든 데이터가 길이가 같았지만 여기서는 쌍이 된다고 해서 길이가 같지않습니다.\\n실제 번역기를 생각해보면 구글 번역기에 '나는 학생이다.'라는 토큰의 개수가 2인 문장을 넣었을 때 'I am a student.'라는 토큰의 개수가 4인 문장이 나오는 것과 같은 이치입니다. seq2seq는 기본적으로 입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있다고 가정합니다. 지금은 기계 번역기가 예제지만 seq2seq의 또 다른 유명한 예제 중 하나인 챗봇을 만든다고 가정해보면, 대답의 길이가 질문의 길이와 항상 똑같아야 한다고하면 그 또한 이상합니다.\", 'Watch me.           Regardez-moi !\\n여기서 사용할 fra.txt 데이터는 위와 같이 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장 사이에 탭으로 구분되는 구조가 하나의 샘플입니다. 그리고 이와 같은 형식의 약 16만개의 병렬 문장 샘플을 포함하고 있습니다. 해당 데이터를 읽고 전처리를 진행해보겠습니다. 앞으로의 코드에서 src는 source의 줄임말로 입력 문장을 나타내며, tar는 target의 줄임말로 번역하고자 하는 문장을 나타냅니다.\\nimport os\\nimport shutil\\nimport zipfile\\nimport pandas as pd\\nimport tensorflow as tf\\nimport urllib3\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\nimport requests', 'from tensorflow.keras.utils import to_categorical\\nimport requests\\nheaders = {\\n\\'User-Agent\\': \\'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\\'\\n}\\ndef download_zip(url, output_path):\\nresponse = requests.get(url, headers=headers, stream=True)\\nif response.status_code == 200:\\nwith open(output_path, \\'wb\\') as f:\\nfor chunk in response.iter_content(chunk_size=8192):\\nf.write(chunk)\\nprint(f\"ZIP file downloaded to {output_path}\")\\nelse:', 'f.write(chunk)\\nprint(f\"ZIP file downloaded to {output_path}\")\\nelse:\\nprint(f\"Failed to download. HTTP Response Code: {response.status_code}\")\\nurl = \"http://www.manythings.org/anki/fra-eng.zip\"\\noutput_path = \"fra-eng.zip\"\\ndownload_zip(url, output_path)\\npath = os.getcwd()\\nzipfilename = os.path.join(path, output_path)\\nwith zipfile.ZipFile(zipfilename, \\'r\\') as zip_ref:\\nzip_ref.extractall(path)\\nlines = pd.read_csv(\\'fra.txt\\', names=[\\'src\\', \\'tar\\', \\'lic\\'], sep=\\'\\\\t\\')\\ndel lines[\\'lic\\']', \"lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\\\t')\\ndel lines['lic']\\nprint('전체 샘플의 개수 :',len(lines))\\n전체 샘플의 개수 : 191954\\n전체 샘플의 개수는 총 약 19만 2천개입니다.\\nlines = lines.loc[:, 'src':'tar']\\nlines = lines[0:60000] # 6만개만 저장\\nlines.sample(10)\\n해당 데이터는 약 19만 2천개의 병렬 문장 샘플로 구성되어있지만 여기서는 간단히 60,000개의 샘플만 가지고 기계 번역기를 구축해보도록 하겠습니다. 우선 전체 데이터 중 60,000개의 샘플만 저장하고 현재 데이터가 어떤 구성이 되었는지 확인해보겠습니다.\\n[이미지: ]\", \"[이미지: ]\\n위의 테이블은 랜덤으로 선택된 10개의 샘플을 보여줍니다. 번역 문장에 해당되는 프랑스어 데이터는 앞서 배웠듯이 시작을 의미하는 심볼 <sos>과 종료를 의미하는 심볼 <eos>을 넣어주어야 합니다. 여기서는 <sos>와 <eos> 대신 \\\\t를 시작 심볼, \\\\n을 종료 심볼로 간주하여 추가하고 다시 데이터를 출력해보겠습니다.\\nlines.tar = lines.tar.apply(lambda x : '\\\\t '+ x + ' \\\\n')\\nlines.sample(10)\\n[이미지: ]\\n랜덤으로 10개의 샘플을 선택하여 출력하였습니다. 프랑스어 데이터에서 시작 심볼과 종료 심볼이 추가된 것을 볼 수 있습니다. 문자 집합을 생성해보겠습니다. 단어 집합이 아니라 문자 집합이라고 하는 이유는 토큰 단위가 단어가 아니라 문자이기 때문입니다.\\n# 문자 집합 구축\\nsrc_vocab = set()\\nfor line in lines.src: # 1줄씩 읽음\", \"# 문자 집합 구축\\nsrc_vocab = set()\\nfor line in lines.src: # 1줄씩 읽음\\nfor char in line: # 1개의 문자씩 읽음\\nsrc_vocab.add(char)\\ntar_vocab = set()\\nfor line in lines.tar:\\nfor char in line:\\ntar_vocab.add(char)\\n문자 집합의 크기를 보겠습니다.\\nsrc_vocab_size = len(src_vocab)+1\\ntar_vocab_size = len(tar_vocab)+1\\nprint('source 문장의 char 집합 :',src_vocab_size)\\nprint('target 문장의 char 집합 :',tar_vocab_size)\\nsource 문장의 char 집합 : 79\\ntarget 문장의 char 집합 : 105\", \"print('target 문장의 char 집합 :',tar_vocab_size)\\nsource 문장의 char 집합 : 79\\ntarget 문장의 char 집합 : 105\\n영어와 프랑스어는 각각 79개와 105개의 문자가 존재합니다. 이 중에서 인덱스를 임의로 부여하여 일부만 출력해봅시다. 현 상태에서 인덱스를 사용하려고 하면 에러가 납니다. 하지만 정렬하여 순서를 정해준 뒤에 인덱스를 사용하여 출력해주면 됩니다.\\nsrc_vocab = sorted(list(src_vocab))\\ntar_vocab = sorted(list(tar_vocab))\\nprint(src_vocab[45:75])\\nprint(tar_vocab[45:75])\", \"tar_vocab = sorted(list(tar_vocab))\\nprint(src_vocab[45:75])\\nprint(tar_vocab[45:75])\\n['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\\n['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\\n문자 집합에 문자 단위로 저장된 것을 확인할 수 있습니다. 각 문자에 인덱스를 부여하겠습니다.\", '문자 집합에 문자 단위로 저장된 것을 확인할 수 있습니다. 각 문자에 인덱스를 부여하겠습니다.\\nsrc_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\\ntar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\\nprint(src_to_index)\\nprint(tar_to_index)\\n{\\' \\': 1, \\'!\\': 2, \\'\"\\': 3, \\'$\\': 4, \\'%\\': 5, ... 중략 ... \\'x\\': 73, \\'y\\': 74, \\'z\\': 75, \\'é\\': 76, \\'’\\': 77, \\'€\\': 78}\\n{\\'\\\\t\\': 1, \\'\\\\n\\': 2, \\' \\': 3, \\'!\\': 4, \\'\"\\': 5, ... 중략 ... \\'û\\': 98, \\'œ\\': 99, \\'С\\': 100, \\'\\\\u2009\\': 101, \\'‘\\': 102, \\'’\\': 103, \\'\\\\u202f\\': 104}', \"인덱스가 부여된 문자 집합으로부터 갖고있는 훈련 데이터에 정수 인코딩을 수행합니다. 우선 인코더의 입력이 될 영어 문장 샘플에 대해서 정수 인코딩을 수행해보고, 5개의 샘플을 출력해봅시다.\\nencoder_input = []\\n# 1개의 문장\\nfor line in lines.src:\\nencoded_line = []\\n# 각 줄에서 1개의 char\\nfor char in line:\\n# 각 char을 정수로 변환\\nencoded_line.append(src_to_index[char])\\nencoder_input.append(encoded_line)\\nprint('source 문장의 정수 인코딩 :',encoder_input[:5])\\nsource 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10], [31, 58, 10]]\", \"source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10], [31, 58, 10]]\\n정수 인코딩이 수행된 것을 볼 수 있습니다. 디코더의 입력이 될 프랑스어 데이터에 대해서 정수 인코딩을 수행해보겠습니다.\\ndecoder_input = []\\nfor line in lines.tar:\\nencoded_line = []\\nfor char in line:\\nencoded_line.append(tar_to_index[char])\\ndecoder_input.append(encoded_line)\\nprint('target 문장의 정수 인코딩 :',decoder_input[:5])\", \"decoder_input.append(encoded_line)\\nprint('target 문장의 정수 인코딩 :',decoder_input[:5])\\ntarget 문장의 정수 인코딩 : [[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [1, 3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2]]\", \"정상적으로 정수 인코딩이 수행된 것을 볼 수 있습니다. 아직 정수 인코딩을 수행해야 할 데이터가 하나 더 남았습니다. 디코더의 예측값과 비교하기 위한 실제값이 필요합니다. 그런데 이 실제값에는 시작 심볼에 해당되는 <sos>가 있을 필요가 없습니다. 이해가 되지 않는다면 이전 페이지의 그림으로 돌아가 Dense와 Softmax 위에 있는 단어들을 다시 보시기 바랍니다. 그래서 이번에는 정수 인코딩 과정에서 <sos>를 제거합니다. 즉, 모든 프랑스어 문장의 맨 앞에 붙어있는 '\\\\t'를 제거하도록 합니다.\\ndecoder_target = []\\nfor line in lines.tar:\\ntimestep = 0\\nencoded_line = []\\nfor char in line:\\nif timestep > 0:\\nencoded_line.append(tar_to_index[char])\\ntimestep = timestep + 1\\ndecoder_target.append(encoded_line)\", \"encoded_line.append(tar_to_index[char])\\ntimestep = timestep + 1\\ndecoder_target.append(encoded_line)\\nprint('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])\\ntarget 문장 레이블의 정수 인코딩 : [[3, 48, 53, 3, 4, 3, 2], [3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2]]\", \"앞서 먼저 만들었던 디코더의 입력값에 해당되는 decoder_input 데이터와 비교하면 decoder_input에서는 모든 문장의 앞에 붙어있던 숫자 1이 decoder_target에서는 제거된 것을 볼 수 있습니다. '\\\\t'가 인덱스가 1이므로 정상적으로 제거된 것입니다. 모든 데이터에 대해서 정수 인덱스로 변경하였으니 패딩 작업을 수행합니다. 패딩을 위해서 영어 문장과 프랑스어 문장 각각에 대해서 가장 길이가 긴 샘플의 길이를 확인합니다.\\nmax_src_len = max([len(line) for line in lines.src])\\nmax_tar_len = max([len(line) for line in lines.tar])\\nprint('source 문장의 최대 길이 :',max_src_len)\\nprint('target 문장의 최대 길이 :',max_tar_len)\\nsource 문장의 최대 길이 : 23\\ntarget 문장의 최대 길이 : 76\", \"print('target 문장의 최대 길이 :',max_tar_len)\\nsource 문장의 최대 길이 : 23\\ntarget 문장의 최대 길이 : 76\\n각각 23와 76의 길이를 가집니다. 이번 병렬 데이터는 영어와 프랑스어의 길이는 하나의 쌍이라고 하더라도 전부 다르므로 패딩을 할 때도 이 두 개의 데이터의 길이를 전부 동일하게 맞춰줄 필요는 없습니다. 영어 데이터는 영어 샘플들끼리, 프랑스어는 프랑스어 샘플들끼리 길이를 맞추어서 패딩하면 됩니다. 여기서는 가장 긴 샘플의 길이에 맞춰서 영어 데이터의 샘플은 전부 길이가 23이 되도록 패딩하고, 프랑스어 데이터의 샘플은 전부 길이가 76이 되도록 패딩합니다.\\nencoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\", \"encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\\ndecoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\\ndecoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\\n모든 값에 대해서 원-핫 인코딩을 수행합니다. 문자 단위 번역기므로 워드 임베딩은 별도로 사용되지 않으며, 예측값과의 오차 측정에 사용되는 실제값뿐만 아니라 입력값도 원-핫 벡터를 사용하겠습니다.\\nencoder_input = to_categorical(encoder_input)\\ndecoder_input = to_categorical(decoder_input)\\ndecoder_target = to_categorical(decoder_target)\", 'decoder_input = to_categorical(decoder_input)\\ndecoder_target = to_categorical(decoder_target)\\n데이터에 대한 전처리가 모두 끝났습니다. 본격적으로 seq2seq 모델을 설계해보겠습니다.\\n2) 교사 강요(Teacher forcing)\\n모델을 설계하기 전에 혹시 의아한 점은 없으신가요? 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다고 설명하였는데 decoder_input이 왜 필요할까요?', '훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 겁니다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다. 이런 상황이 반복되면 훈련 시간이 느려집니다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다.\\n3) seq2seq 기계 번역기 훈련시키기\\nseq2seq 모델을 설계하고 교사 강요를 사용하여 훈련시켜보도록 하겠습니다.', '3) seq2seq 기계 번역기 훈련시키기\\nseq2seq 모델을 설계하고 교사 강요를 사용하여 훈련시켜보도록 하겠습니다.\\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense\\nfrom tensorflow.keras.models import Model\\nimport numpy as np\\nencoder_inputs = Input(shape=(None, src_vocab_size))\\nencoder_lstm = LSTM(units=256, return_state=True)\\n# encoder_outputs은 여기서는 불필요\\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\\n# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\\nencoder_states = [state_h, state_c]', '# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\\nencoder_states = [state_h, state_c]\\n인코더를 주목해보면 functional API를 사용한다는 것 외에는 앞서 다른 실습에서 본 LSTM 설계와 크게 다르지는 않습니다. 우선 LSTM의 은닉 상태 크기는 256으로 선택하였습니다. 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정합니다. 인코더에 입력을 넣으면 내부 상태를 리턴합니다.', 'LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 LSTM을 설명할 때 언급하였던 배운 은닉 상태와 셀 상태에 해당됩니다. 앞서 이론을 설명할 때는 셀 상태는 설명에서 생략하고 은닉 상태만 언급하였으나 사실 LSTM은 은닉 상태와 셀 상태라는 두 가지 상태를 가진다는 사실을 기억해야 합니다. 갑자기 어려워진 게 아닙니다. 단지 은닉 상태만 전달하는 게 아니라 은닉 상태와 셀 상태 두 가지를 전달한다고 생각하면 됩니다. 이 두 가지 상태를 encoder_states에 저장합니다. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달합니다. 이것이 앞서 배운 컨텍스트 벡터입니다.\\ndecoder_inputs = Input(shape=(None, tar_vocab_size))\\ndecoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)', 'decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\\n# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\\ndecoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\\ndecoder_softmax_layer = Dense(tar_vocab_size, activation=\\'softmax\\')\\ndecoder_outputs = decoder_softmax_layer(decoder_outputs)\\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")', 'model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\\n디코더는 인코더의 마지막 은닉 상태를 초기 은닉 상태로 사용합니다. 위에서 initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당됩니다. 또한 동일하게 디코더의 은닉 상태 크기도 256으로 주었습니다. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않습니다. 그 후 출력층에 프랑스어의 단어 집합의 크기만큼 뉴런을 배치한 후 소프트맥스 함수를 사용하여 실제값과의 오차를 구합니다.\\nmodel.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)', '입력으로는 인코더 입력과 디코더 입력이 들어가고, 디코더의 실제값인 decoder_target도 필요합니다. 배치 크기는 64로 하였으며 총 40 에포크를 학습합니다. 위에서 설정한 은닉 상태의 크기와 에포크 수는 실제로는 훈련 데이터에 과적합 상태를 불러옵니다. 중간부터 검증 데이터에 대한 오차인 val_loss의 값이 올라가는데, 사실 이번 실습에서는 주어진 데이터의 양과 태스크의 특성으로 인해 훈련 과정에서 훈련 데이터의 정확도와 과적합 방지라는 두 마리 토끼를 동시에 잡기에는 쉽지 않습니다. 여기서는 우선 seq2seq의 메커니즘과 짧은 문장과 긴 문장에 대한 성능 차이에 대한 확인을 중점으로 두고 훈련 데이터에 과적합 된 상태로 동작 단계로 넘어갑니다.\\n4) seq2seq 기계 번역기 동작시키기\\n앞서 seq2seq는 훈련할 때와 동작할 때의 방식이 다르다고 언급한 바 있습니다. 이번에는 입력한 문장에 대해서 기계 번역을 하도록 모델을 조정하고 동작시켜보도록 하겠습니다.', '앞서 seq2seq는 훈련할 때와 동작할 때의 방식이 다르다고 언급한 바 있습니다. 이번에는 입력한 문장에 대해서 기계 번역을 하도록 모델을 조정하고 동작시켜보도록 하겠습니다.\\n전체적인 번역 동작 단계를 정리하면 아래와 같습니다.']\n",
      "[]\n",
      "[]\n",
      "['encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\\n우선 인코더를 정의합니다. encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용하는 것입니다. 디코더를 설계해보겠습니다.\\n# 이전 시점의 상태들을 저장하는 텐서\\ndecoder_state_input_h = Input(shape=(256,))\\ndecoder_state_input_c = Input(shape=(256,))\\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\\n# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\\n# 뒤의 함수 decode_sequence()에 동작을 구현 예정', '# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\\n# 뒤의 함수 decode_sequence()에 동작을 구현 예정\\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\\n# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\\ndecoder_states = [state_h, state_c]\\ndecoder_outputs = decoder_softmax_layer(decoder_outputs)\\ndecoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)', 'index_to_src = dict((i, char) for char, i in src_to_index.items())\\nindex_to_tar = dict((i, char) for char, i in tar_to_index.items())\\n단어로부터 인덱스를 얻는 것이 아니라 인덱스로부터 단어를 얻을 수 있는 index_to_src와 index_to_tar를 만들었습니다.\\ndef decode_sequence(input_seq):\\n# 입력으로부터 인코더의 상태를 얻음\\nstates_value = encoder_model.predict(input_seq)\\n# <SOS>에 해당하는 원-핫 벡터 생성\\ntarget_seq = np.zeros((1, 1, tar_vocab_size))\\ntarget_seq[0, 0, tar_to_index[\\'\\\\t\\']] = 1.\\nstop_condition = False\\ndecoded_sentence = \"\"', 'target_seq[0, 0, tar_to_index[\\'\\\\t\\']] = 1.\\nstop_condition = False\\ndecoded_sentence = \"\"\\n# stop_condition이 True가 될 때까지 루프 반복\\nwhile not stop_condition:\\n# 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\\noutput_tokens, h, c = decoder_model.predict([target_seq] + states_value)\\n# 예측 결과를 문자로 변환\\nsampled_token_index = np.argmax(output_tokens[0, -1, :])\\nsampled_char = index_to_tar[sampled_token_index]\\n# 현재 시점의 예측 문자를 예측 문장에 추가\\ndecoded_sentence += sampled_char\\n# <eos>에 도달하거나 최대 길이를 넘으면 중단.', \"# 현재 시점의 예측 문자를 예측 문장에 추가\\ndecoded_sentence += sampled_char\\n# <eos>에 도달하거나 최대 길이를 넘으면 중단.\\nif (sampled_char == '\\\\n' or\\nlen(decoded_sentence) > max_tar_len):\\nstop_condition = True\\n# 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\\ntarget_seq = np.zeros((1, 1, tar_vocab_size))\\ntarget_seq[0, 0, sampled_token_index] = 1.\\n# 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\\nstates_value = [h, c]\\nreturn decoded_sentence\\nfor seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\\ninput_seq = encoder_input[seq_index:seq_index+1]\", 'for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\\ninput_seq = encoder_input[seq_index:seq_index+1]\\ndecoded_sentence = decode_sequence(input_seq)\\nprint(35 * \"-\")\\nprint(\\'입력 문장:\\', lines.src[seq_index])\\nprint(\\'정답 문장:\\', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # \\'\\\\t\\'와 \\'\\\\n\\'을 빼고 출력\\nprint(\\'번역 문장:\\', decoded_sentence[1:len(decoded_sentence)-1]) # \\'\\\\n\\'을 빼고 출력\\n-----------------------------------\\n입력 문장: Hi.\\n정답 문장: Salut !\\n번역 문장: Salut.\\n-----------------------------------\\n입력 문장: I see.', '입력 문장: Hi.\\n정답 문장: Salut !\\n번역 문장: Salut.\\n-----------------------------------\\n입력 문장: I see.\\n정답 문장: Aha.\\n번역 문장: Je change.\\n-----------------------------------\\n입력 문장: Hug me.\\n정답 문장: Serrez-moi dans vos bras !\\n번역 문장: Serre-moi dans vos patents !\\n-----------------------------------\\n입력 문장: Help me.\\n정답 문장: Aidez-moi.\\n번역 문장: Aidez-moi.\\n-----------------------------------\\n입력 문장: I beg you.\\n정답 문장: Je vous en prie.\\n번역 문장: Je vous en prie.', '입력 문장: I beg you.\\n정답 문장: Je vous en prie.\\n번역 문장: Je vous en prie.\\n지금까지 문자 단위의 seq2seq를 구현하였습니다. 다음 실습에서는 이번 실습에서 배운 내용을 바탕으로 문자 단위에서 단어 단위로 확장해서 기계 번역기를 구현해보겠습니다.\\n==================================================\\n--- 14-02 Word-Level 번역기 만들기(Neural Machine Translation (seq2seq) Tutorial) ---\\n```\\n입력문장 : we are busy men .\\n정답문장 : nous sommes des hommes occupes .\\n번역문장 : nous sommes tres vieux .\\n--------------------------------------------------\\n입력문장 : it was very ugly .', '--------------------------------------------------\\n입력문장 : it was very ugly .\\n정답문장 : ce n etait vraiment pas beau a voir .\\n번역문장 : c etait tres fort .\\n--------------------------------------------------\\n입력문장 : tom looks shocked .\\n정답문장 : tom a l air choque .\\n번역문장 : tom a l air bien .\\n--------------------------------------------------\\n입력문장 : cross the street .\\n정답문장 : traversez la rue .\\n번역문장 : la ?\\n--------------------------------------------------\\n입력문장 : you nearly died .\\n정답문장 : tu es presque mort .', \"입력문장 : you nearly died .\\n정답문장 : tu es presque mort .\\n번역문장 : tu es presque mort .\\n--------------------------------------------------\\n```seq2seq를 이용해서 기계 번역기를 만들어보겠습니다. 시작하기에 앞서 참고하면 좋은 게시물을 소개합니다. 인터넷에 케라스로 seq2seq를 구현하는 많은 유사 예제들이 나와있지만 대부분은 케라스 개발자 프랑수아 숄레의 블로그의 유명 게시물인 'sequence-to-sequence 10분만에 이해하기'가 원본입니다. 이번 실습 또한 해당 게시물의 예제에 많이 영향받았습니다.\\n링크 : https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\"]\n",
      "['실제 성능이 좋은 기계 번역기를 구현하려면 방대한 데이터가 필요하므로 여기서는 seq2seq를 간단히 실습해보는 수준의 간단한 기계 번역기를 구현해보겠습니다. 기계 번역기를 훈련시키기 위해서는 훈련 데이터로 병렬 코퍼스(parallel corpus)가 필요합니다. 병렬 코퍼스란, 두 개 이상의 언어가 병렬적으로 구성된 코퍼스를 의미합니다.\\n링크 : http://www.manythings.org/anki\\n이번 실습에서는 프랑스어-영어 병렬 코퍼스인 fra-eng.zip 파일을 사용합니다. 위의 링크에서 해당 파일을 다운받은 후 압축을 풀면 fra.txt라는 파일을 얻을 수 있는데 해당 파일을이 실습에서 사용합니다.', '병렬 코퍼스 데이터에 대해서 이해해봅시다. 병렬 데이터라고 하면 앞서 수행한 태깅 작업 챕터의 개체명 인식과 같은 데이터를 생각할 수 있지만, 앞서 수행한 태깅 작업의 병렬 데이터와 seq2seq가 사용하는 병렬 데이터는 성격이 다릅니다. 태깅 작업의 병렬 데이터는 쌍이 되는 데이터와 레이블이 길이가 동일하였으나 여기서는 쌍이 된다고 해서 반드시 길이가 같지는 않습니다.', \"실제 번역기를 생각해보면 구글 번역기에 '나는 학생이다.'라는 토큰의 개수가 2인 문장을 넣었을 때 'I am a student.'라는 토큰의 개수가 4인 문장이 나오는 것과 같은 이치입니다. seq2seq는 기본적으로 입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있다고 가정합니다. 지금 구현 예제는 기계 번역기이지만 seq2seq로 구현할 수 있는 또 다른 예제인 챗봇을 만든다고 가정해보면, 대답의 길이가 질문의 길이와 항상 똑같아야 한다고하면 그 또한 이상합니다. 여기서 사용할 fra.txt 데이터는 아래와 같이 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장 사이에 탭으로 구분되는 형식이 하나의 샘플입니다.\\nWatch me.           Regardez-moi !\", 'Watch me.           Regardez-moi !\\n데이터는 위와 동일한 형식의 약 19만개의 병렬 문장 샘플을 포함하고 있습니다. 데이터를 읽고 전처리를 진행해보겠습니다. 앞으로의 코드에서 src는 source의 줄임말로 입력 문장을 나타내며, tar는 target의 줄임말로 번역하고자 하는 문장을 나타냅니다.\\nimport os\\nimport re\\nimport shutil\\nimport zipfile\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nimport unicodedata\\nimport urllib3\\nfrom tensorflow.keras.layers import Embedding, GRU, Dense\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences', \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfra-eng.zip 파일을 다운로드하고 압축을 풀겠습니다.\\nimport requests\\nheaders = {\\n'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\\n}\\ndef download_zip(url, output_path):\\nresponse = requests.get(url, headers=headers, stream=True)\\nif response.status_code == 200:\\nwith open(output_path, 'wb') as f:\", 'if response.status_code == 200:\\nwith open(output_path, \\'wb\\') as f:\\nfor chunk in response.iter_content(chunk_size=8192):\\nf.write(chunk)\\nprint(f\"ZIP file downloaded to {output_path}\")\\nelse:\\nprint(f\"Failed to download. HTTP Response Code: {response.status_code}\")\\nurl = \"http://www.manythings.org/anki/fra-eng.zip\"\\noutput_path = \"fra-eng.zip\"\\ndownload_zip(url, output_path)\\npath = os.getcwd()\\nzipfilename = os.path.join(path, output_path)\\nwith zipfile.ZipFile(zipfilename, \\'r\\') as zip_ref:', \"zipfilename = os.path.join(path, output_path)\\nwith zipfile.ZipFile(zipfilename, 'r') as zip_ref:\\nzip_ref.extractall(path)\\n이번 실습에서는 약 19만개의 데이터 중 33,000개의 샘플만을 사용할 예정입니다.\\nnum_samples = 33000\\n전처리 함수들을 구현합니다. 구두점 등을 제거하거나 단어와 구분해주기 위한 전처리입니다.\\ndef to_ascii(s):\\n# 프랑스어 악센트(accent) 삭제\\n# 예시 : 'déjà diné' -> deja dine\\nreturn ''.join(c for c in unicodedata.normalize('NFD', s)\\nif unicodedata.category(c) != 'Mn')\\ndef preprocess_sentence(sent):\\n# 악센트 제거 함수 호출\\nsent = to_ascii(sent.lower())\", 'def preprocess_sentence(sent):\\n# 악센트 제거 함수 호출\\nsent = to_ascii(sent.lower())\\n# 단어와 구두점 사이에 공백 추가.\\n# ex) \"I am a student.\" => \"I am a student .\"\\nsent = re.sub(r\"([?.!,¿])\", r\" \\\\1\", sent)\\n# (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\\nsent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\\n# 다수 개의 공백을 하나의 공백으로 치환\\nsent = re.sub(r\"\\\\s+\", \" \", sent)\\nreturn sent\\n구현한 전처리 함수들을 임의의 문장을 입력으로 테스트해봅시다.\\n# 전처리 테스트\\nen_sent = u\"Have you had dinner?\"\\nfr_sent = u\"Avez-vous déjà diné?\"\\nprint(\\'전처리 전 영어 문장 :\\', en_sent)', 'en_sent = u\"Have you had dinner?\"\\nfr_sent = u\"Avez-vous déjà diné?\"\\nprint(\\'전처리 전 영어 문장 :\\', en_sent)\\nprint(\\'전처리 후 영어 문장 :\\',preprocess_sentence(en_sent))\\nprint(\\'전처리 전 프랑스어 문장 :\\', fr_sent)\\nprint(\\'전처리 후 프랑스어 문장 :\\', preprocess_sentence(fr_sent))\\n전처리 전 영어 문장 : Have you had dinner?\\n전처리 후 영어 문장 : have you had dinner ?\\n전처리 전 프랑스어 문장 : Avez-vous déjà diné?\\n전처리 후 프랑스어 문장 : avez vous deja dine ?', '전처리 전 프랑스어 문장 : Avez-vous déjà diné?\\n전처리 후 프랑스어 문장 : avez vous deja dine ?\\n전체 데이터에서 33,000개의 샘플에 대해서 전처리를 수행합니다. 또한 훈련 과정에서 교사 강요(Teacher Forcing)을 사용할 예정이므로, 훈련 시 사용할 디코더의 입력 시퀀스와 실제값. 즉, 레이블에 해당되는 출력 시퀀스를 따로 분리하여 저장합니다. 입력 시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가하고, 출력 시퀀스에는 종료를 의미하는 토큰인 <eos>를 추가합니다.\\ndef load_preprocessed_data():\\nencoder_input, decoder_input, decoder_target = [], [], []\\nwith open(\"fra.txt\", \"r\") as lines:\\nfor i, line in enumerate(lines):\\n# source 데이터와 target 데이터 분리', 'with open(\"fra.txt\", \"r\") as lines:\\nfor i, line in enumerate(lines):\\n# source 데이터와 target 데이터 분리\\nsrc_line, tar_line, _ = line.strip().split(\\'\\\\t\\')\\n# source 데이터 전처리\\nsrc_line = [w for w in preprocess_sentence(src_line).split()]\\n# target 데이터 전처리\\ntar_line = preprocess_sentence(tar_line)\\ntar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\\ntar_line_out = [w for w in (tar_line + \" <eos>\").split()]\\nencoder_input.append(src_line)\\ndecoder_input.append(tar_line_in)\\ndecoder_target.append(tar_line_out)', \"decoder_input.append(tar_line_in)\\ndecoder_target.append(tar_line_out)\\nif i == num_samples - 1:\\nbreak\\nreturn encoder_input, decoder_input, decoder_target\\n이렇게 얻은 3개의 데이터셋 인코더의 입력, 디코더의 입력, 디코더의 레이블을 상위 5개 샘플만 출력해봅시다.\\nsents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\\nprint('인코더의 입력 :',sents_en_in[:5])\\nprint('디코더의 입력 :',sents_fra_in[:5])\\nprint('디코더의 레이블 :',sents_fra_out[:5])\\n인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\", \"인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\\n디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\\n디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]\\n모델을 설계하기 전 의아한 점이 있을 수 있습니다. 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다고 설명하였는데 디코더의 입력에 해당하는 데이터인 sents_fra_in이 왜 필요할까요?\", '훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 겁니다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다. 이런 상황이 반복되면 훈련 시간이 느려집니다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다.\\n케라스 토크나이저를 통해 단어 집합을 생성, 정수 인코딩을 진행 후 이어서 패딩을 진행합니다.\\ntokenizer_en = Tokenizer(filters=\"\", lower=False)', 'tokenizer_en = Tokenizer(filters=\"\", lower=False)\\ntokenizer_en.fit_on_texts(sents_en_in)\\nencoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\\nencoder_input = pad_sequences(encoder_input, padding=\"post\")\\ntokenizer_fra = Tokenizer(filters=\"\", lower=False)\\ntokenizer_fra.fit_on_texts(sents_fra_in)\\ntokenizer_fra.fit_on_texts(sents_fra_out)\\ndecoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\\ndecoder_input = pad_sequences(decoder_input, padding=\"post\")', 'decoder_input = pad_sequences(decoder_input, padding=\"post\")\\ndecoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\\ndecoder_target = pad_sequences(decoder_target, padding=\"post\")\\n데이터의 크기(shape)를 확인합니다.\\nprint(\\'인코더의 입력의 크기(shape) :\\',encoder_input.shape)\\nprint(\\'디코더의 입력의 크기(shape) :\\',decoder_input.shape)\\nprint(\\'디코더의 레이블의 크기(shape) :\\',decoder_target.shape)\\n인코더의 입력의 크기(shape) : (33000, 8)\\n디코더의 입력의 크기(shape) : (33000, 16)\\n디코더의 레이블의 크기(shape) : (33000, 16)', '인코더의 입력의 크기(shape) : (33000, 8)\\n디코더의 입력의 크기(shape) : (33000, 16)\\n디코더의 레이블의 크기(shape) : (33000, 16)\\n샘플은 총 33,000개 존재하며 영어 문장의 길이는 8, 프랑스어 문장의 길이는 16입니다. 단어 집합의 크기를 정의합니다.\\nsrc_vocab_size = len(tokenizer_en.word_index) + 1\\ntar_vocab_size = len(tokenizer_fra.word_index) + 1\\nprint(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\\n영어 단어 집합의 크기 : 4647, 프랑스어 단어 집합의 크기 : 8022', \"영어 단어 집합의 크기 : 4647, 프랑스어 단어 집합의 크기 : 8022\\n단어 집합의 크기는 각각 4,647개와 8,022개입니다. 단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 만들어줍니다. 이들은 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용됩니다.\\nsrc_to_index = tokenizer_en.word_index\\nindex_to_src = tokenizer_en.index_word\\ntar_to_index = tokenizer_fra.word_index\\nindex_to_tar = tokenizer_fra.index_word\\n테스트 데이터를 분리하기 전 데이터를 섞어줍니다. 이를 위해서 순서가 섞인 정수 시퀀스 리스트를 만듭니다.\\nindices = np.arange(encoder_input.shape[0])\\nnp.random.shuffle(indices)\\nprint('랜덤 시퀀스 :',indices)\", \"indices = np.arange(encoder_input.shape[0])\\nnp.random.shuffle(indices)\\nprint('랜덤 시퀀스 :',indices)\\n랜덤 시퀀스 : [16412  5374  8832 ...  5652 24040 10002]\\n이를 데이터셋의 순서로 지정해주면 샘플들이 기존 순서와 다른 순서로 섞이게 됩니다.\\nencoder_input = encoder_input[indices]\\ndecoder_input = decoder_input[indices]\\ndecoder_target = decoder_target[indices]\\n임의로 30,997번째 샘플을 출력해봅시다. 이때 decoder_input과 decoder_target은 데이터의 구조상으로 앞에 붙은 <sos> 토큰과 뒤에 붙은 <eos>을 제외하면 동일한 정수 시퀀스를 가져야 합니다.\\nencoder_input[30997]\", \"encoder_input[30997]\\narray([  5,   7, 638,   1,   0,   0,   0,   0], dtype=int32)\\ndecoder_input[30997]\\narray([  2,  18,   5,  16, 173,   1,   0,   0,   0,   0,   0,   0,   0,\\n0,   0,   0], dtype=int32)\\ndecoder_target[30997]\\narray([ 18,   5,  16, 173,   1,   3,   0,   0,   0,   0,   0,   0,   0,\\n0,   0,   0], dtype=int32)\\n저자의 경우 18, 5, 16, 173, 1이라는 동일 시퀀스를 확인했습니다. 이제 훈련 데이터의 10%를 테스트 데이터로 분리하겠습니다.\\nn_of_val = int(33000*0.1)\\nprint('검증 데이터의 개수 :',n_of_val)\\n검증 데이터의 개수 : 3300\", \"n_of_val = int(33000*0.1)\\nprint('검증 데이터의 개수 :',n_of_val)\\n검증 데이터의 개수 : 3300\\n33,000개의 10%에 해당되는 3,300개의 데이터를 테스트 데이터로 사용합니다.\\nencoder_input_train = encoder_input[:-n_of_val]\\ndecoder_input_train = decoder_input[:-n_of_val]\\ndecoder_target_train = decoder_target[:-n_of_val]\\nencoder_input_test = encoder_input[-n_of_val:]\\ndecoder_input_test = decoder_input[-n_of_val:]\\ndecoder_target_test = decoder_target[-n_of_val:]\\n훈련 데이터와 테스트 데이터의 크기(shape)를 출력해봅시다.\", \"decoder_target_test = decoder_target[-n_of_val:]\\n훈련 데이터와 테스트 데이터의 크기(shape)를 출력해봅시다.\\nprint('훈련 source 데이터의 크기 :',encoder_input_train.shape)\\nprint('훈련 target 데이터의 크기 :',decoder_input_train.shape)\\nprint('훈련 target 레이블의 크기 :',decoder_target_train.shape)\\nprint('테스트 source 데이터의 크기 :',encoder_input_test.shape)\\nprint('테스트 target 데이터의 크기 :',decoder_input_test.shape)\\nprint('테스트 target 레이블의 크기 :',decoder_target_test.shape)\\n훈련 source 데이터의 크기 : (29700, 8)\\n훈련 target 데이터의 크기 : (29700, 16)\", '훈련 source 데이터의 크기 : (29700, 8)\\n훈련 target 데이터의 크기 : (29700, 16)\\n훈련 target 레이블의 크기 : (29700, 16)\\n테스트 source 데이터의 크기 : (3300, 8)\\n테스트 target 데이터의 크기 : (3300, 16)\\n테스트 target 레이블의 크기 : (3300, 16)\\n훈련 데이터의 샘플은 29,700개, 테스트 데이터의 샘플은 3,300개가 존재합니다. 이제 모델을 설계합니다.']\n",
      "['from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\\nfrom tensorflow.keras.models import Model\\n임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64로 사용합니다.\\nembedding_dim = 64\\nhidden_units = 64\\n인코더를 설계합니다. 인코더를 주목해보면 함수형 API(functional API)를 사용한다는 것 외에는 앞서 다른 실습에서 본 LSTM 설계와 크게 다르지는 않습니다. Masking은 패딩 토큰인 숫자 0의 경우에는 연산을 제외하는 역할을 수행합니다. 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정합니다. 인코더에 입력을 넣으면 내부 상태를 리턴합니다.', 'LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 RNN 챕터에서 LSTM을 처음 설명할 때 언급하였던 은닉 상태와 셀 상태에 해당됩니다. 이 두 가지 상태를 encoder_states에 저장합니다. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달할 예정입니다. 이것이 앞서 배운 컨텍스트 벡터입니다.\\n# 인코더\\nencoder_inputs = Input(shape=(None,))\\nenc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\\nenc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\\nencoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True', 'encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\\nencoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\\nencoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장', 'encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장\\n디코더는 인코더의 마지막 은닉 상태로부터 초기 은닉 상태를 얻습니다. initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당됩니다. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않습니다. seq2seq의 디코더는 기본적으로 각 시점마다 다중 클래스 분류 문제를 풀고있습니다. 매 시점마다 프랑스어 단어 집합의 크기(tar_vocab_size)의 선택지에서 단어를 1개 선택하여 이를 이번 시점에서 예측한 단어로 택합니다. 다중 클래스 분류 문제이므로 출력층으로 소프트맥스 함수와 손실 함수를 크로스 엔트로피 함수를 사용합니다.', 'categorical_crossentropy를 사용하려면 레이블은 원-핫 인코딩이 된 상태여야 합니다. 그런데 현재 decoder_outputs의 경우에는 원-핫 인코딩을 하지 않은 상태입니다. 원-핫 인코딩을 하지 않은 상태로 정수 레이블에 대해서 다중 클래스 분류 문제를 풀고자 하는 경우에는 categorical_crossentropy가 아니라 sparse_categorical_crossentropy를 사용하면 됩니다.\\n# 디코더\\ndecoder_inputs = Input(shape=(None,))\\ndec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\\ndec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\\ndec_masking = Masking(mask_value=0.0)(dec_emb)', \"dec_masking = Masking(mask_value=0.0)(dec_emb)\\n# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\\ndecoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\\n# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\\ndecoder_outputs, _, _ = decoder_lstm(dec_masking,\\ninitial_state=encoder_states)\\n# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\\ndecoder_dense = Dense(tar_vocab_size, activation='softmax')\\ndecoder_outputs = decoder_dense(decoder_outputs)\\n# 모델의 입력과 출력을 정의.\", \"decoder_outputs = decoder_dense(decoder_outputs)\\n# 모델의 입력과 출력을 정의.\\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\\n모델을 훈련합니다. 128개의 배치 크기로 총 50 에포크 학습합니다. 테스트 데이터를 검증 데이터로 사용하여 훈련이 제대로 되고있는지 모니터링하겠습니다.\\nmodel.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\\\\nvalidation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\\nbatch_size=128, epochs=50)\", 'batch_size=128, epochs=50)\\n저자의 경우 최종 에포크에서 훈련 데이터는 92%의 정확도를, 테스트 데이터에서는 86%의 정확도를 얻었습니다.']\n",
      "['seq2seq는 훈련 과정(교사 강요)과 테스트 과정에서의 동작 방식이 다릅니다. 그래서 테스트 과정을 위해 모델을 다시 설계해주어야 합니다. 특히 디코더를 수정해야 합니다. 이번에는 번역 단계를 위해 모델을 수정하고 동작시켜보겠습니다.\\n전체적인 번역 단계를 정리하면 아래와 같습니다.\\n번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻습니다.\\n인코더의 은닉 상태와 셀 상태, 그리고 토큰 <sos>를 디코더로 보냅니다.\\n디코더가 토큰 <eos>가 나올 때까지 다음 단어를 예측하는 행동을 반복합니다.', '인코더의 은닉 상태와 셀 상태, 그리고 토큰 <sos>를 디코더로 보냅니다.\\n디코더가 토큰 <eos>가 나올 때까지 다음 단어를 예측하는 행동을 반복합니다.\\n인코더의 입, 출력으로 사용하는 encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용합니다. 이렇게 되면 훈련 단계에 encoder_inputs와 encoder_states 사이에 있는 모든 층까지 전부 불러오게 되므로 결과적으로 훈련 단계에서 사용한 인코더를 그대로 재사용하게 됩니다. 이어서 디코더를 설계합니다. 테스트 단계에서는 디코더를 매 시점 별로 컨트롤 할 예정으로, 이를 위해서 이전 시점의 상태를 저장할 텐서인 decoder_state_input_h, decoder_state_input_c를 정의합니다. 매 시점 별로 디코더를 컨트롤하는 함수는 뒤에서 정의할 decode_sequence()로 해당 함수를 자세히 살펴봐야 합니다.\\n# 인코더', '# 인코더\\nencoder_model = Model(encoder_inputs, encoder_states)\\n# 디코더 설계 시작\\n# 이전 시점의 상태를 보관할 텐서\\ndecoder_state_input_h = Input(shape=(hidden_units,))\\ndecoder_state_input_c = Input(shape=(hidden_units,))\\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\\n# 훈련 때 사용했던 임베딩 층을 재사용\\ndec_emb2 = dec_emb_layer(decoder_inputs)\\n# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)', 'decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\\ndecoder_states2 = [state_h2, state_c2]\\n# 모든 시점에 대해서 단어 예측\\ndecoder_outputs2 = decoder_dense(decoder_outputs2)\\n# 수정된 디코더\\ndecoder_model = Model(\\n[decoder_inputs] + decoder_states_inputs,\\n[decoder_outputs2] + decoder_states2)', '테스트 단계에서의 동작을 위한 decode_sequence 함수를 구현합니다. 입력 문장이 들어오면 인코더는 마지막 시점까지 전개하여 마지막 시점의 은닉 상태와 셀 상태를 리턴합니다. 이 두 개의 값을 states_value에 저장합니다. 그리고 디코더의 초기 입력으로 <SOS>를 준비합니다. 이를 target_seq에 저장합니다. 이 두 가지 입력을 가지고 while문 안으로 진입하여 이 두 가지를 디코더의 입력으로 사용합니다. 이제 디코더는 현재 시점에 대해서 예측을 하게 되는데, 현재 시점의 예측 벡터가 output_tokens, 현재 시점의 은닉 상태가 h, 현재 시점의 셀 상태가 c입니다. 예측 벡터로부터 현재 시점의 예측 단어인 target_seq를 얻고, h와 c 이 두 개의 값은 states_value에 저장합니다. 그리고 while문의 다음 루프. 즉, 두번째 시점의 디코더의 입력으로 다시 target_seq와 states_value를 사용합니다', '. 그리고 while문의 다음 루프. 즉, 두번째 시점의 디코더의 입력으로 다시 target_seq와 states_value를 사용합니다. 이를 현재 시점의 예측 단어로 <eos>를 예측하거나 번역 문장의 길이가 50이 넘는 순간까지 반복합니다. 각 시점마다 번역된 다어는 decoded_sentence에 누적하여 저장하였다가 최종 번역 시퀀스로 리턴합니다.', \"def decode_sequence(input_seq):\\n# 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\\nstates_value = encoder_model.predict(input_seq)\\n# <SOS>에 해당하는 정수 생성\\ntarget_seq = np.zeros((1,1))\\ntarget_seq[0, 0] = tar_to_index['<sos>']\\nstop_condition = False\\ndecoded_sentence = ''\\n# stop_condition이 True가 될 때까지 루프 반복\\n# 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\\nwhile not stop_condition:\\n# 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\\noutput_tokens, h, c = decoder_model.predict([target_seq] + states_value)\\n# 예측 결과를 단어로 변환\", \"output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\\n# 예측 결과를 단어로 변환\\nsampled_token_index = np.argmax(output_tokens[0, -1, :])\\nsampled_char = index_to_tar[sampled_token_index]\\n# 현재 시점의 예측 단어를 예측 문장에 추가\\ndecoded_sentence += ' '+sampled_char\\n# <eos>에 도달하거나 정해진 길이를 넘으면 중단.\\nif (sampled_char == '<eos>' or\\nlen(decoded_sentence) > 50):\\nstop_condition = True\\n# 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\\ntarget_seq = np.zeros((1,1))\\ntarget_seq[0, 0] = sampled_token_index\", \"target_seq = np.zeros((1,1))\\ntarget_seq[0, 0] = sampled_token_index\\n# 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\\nstates_value = [h, c]\\nreturn decoded_sentence\\n결과 확인을 위한 함수를 만듭니다. seq_to_src 함수는 영어 문장에 해당하는 정수 시퀀스를 입력받으면 정수로부터 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환합니다. seq_to_tar은 프랑스어에 해당하는 정수 시퀀스를 입력받으면 정수로부터 프랑스어 단어를 리턴하는 index_to_tar을 통해 프랑스어 문장으로 변환합니다.\\n# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\\ndef seq_to_src(input_seq):\\nsentence = ''\\nfor encoded_word in input_seq:\\nif(encoded_word != 0):\", \"def seq_to_src(input_seq):\\nsentence = ''\\nfor encoded_word in input_seq:\\nif(encoded_word != 0):\\nsentence = sentence + index_to_src[encoded_word] + ' '\\nreturn sentence\\n# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\\ndef seq_to_tar(input_seq):\\nsentence = ''\\nfor encoded_word in input_seq:\\nif(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\\nsentence = sentence + index_to_tar[encoded_word] + ' '\\nreturn sentence\\n훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다.\", 'return sentence\\n훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다.\\nfor seq_index in [3, 50, 100, 300, 1001]:\\ninput_seq = encoder_input_train[seq_index: seq_index + 1]\\ndecoded_sentence = decode_sequence(input_seq)\\nprint(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\\nprint(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\\nprint(\"번역문장 :\",decoded_sentence[1:-5])\\nprint(\"-\"*50)\\n입력문장 : when does it end ?\\n정답문장 : quand est ce que ca finit ?\\n번역문장 : quand est ce que ca marche ?', '입력문장 : when does it end ?\\n정답문장 : quand est ce que ca finit ?\\n번역문장 : quand est ce que ca marche ?\\n--------------------------------------------------\\n입력문장 : it s sand .\\n정답문장 : c est du sable .\\n번역문장 : c est de l eau .\\n--------------------------------------------------\\n입력문장 : i didn t go .\\n정답문장 : je n y suis pas allee .\\n번역문장 : je ne suis pas encore .\\n--------------------------------------------------\\n입력문장 : it was a mistake .\\n정답문장 : ce fut une erreur .\\n번역문장 : il s agit d une blague .', '입력문장 : it was a mistake .\\n정답문장 : ce fut une erreur .\\n번역문장 : il s agit d une blague .\\n--------------------------------------------------\\n입력문장 : it boggles my mind .\\n정답문장 : ca me laisse perplexe .\\n번역문장 : ca m en femme .\\n--------------------------------------------------\\n테스트 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다.\\nfor seq_index in [3, 50, 100, 300, 1001]:\\ninput_seq = encoder_input_test[seq_index: seq_index + 1]\\ndecoded_sentence = decode_sequence(input_seq)', 'decoded_sentence = decode_sequence(input_seq)\\nprint(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\\nprint(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\\nprint(\"번역문장 :\",decoded_sentence[1:-5])\\nprint(\"-\"*50)\\n입력문장 : we are busy men .\\n정답문장 : nous sommes des hommes occupes .\\n번역문장 : nous sommes tres vieux .\\n--------------------------------------------------\\n입력문장 : it was very ugly .\\n정답문장 : ce n etait vraiment pas beau a voir .\\n번역문장 : c etait tres fort .', '입력문장 : it was very ugly .\\n정답문장 : ce n etait vraiment pas beau a voir .\\n번역문장 : c etait tres fort .\\n--------------------------------------------------\\n입력문장 : tom looks shocked .\\n정답문장 : tom a l air choque .\\n번역문장 : tom a l air bien .\\n--------------------------------------------------\\n입력문장 : cross the street .\\n정답문장 : traversez la rue .\\n번역문장 : la ?\\n--------------------------------------------------\\n입력문장 : you nearly died .\\n정답문장 : tu es presque mort .\\n번역문장 : tu es presque mort .', '입력문장 : you nearly died .\\n정답문장 : tu es presque mort .\\n번역문장 : tu es presque mort .\\n--------------------------------------------------\\n==================================================\\n--- 14-03 BLEU Score(Bilingual Evaluation Understudy Score) ---\\n```\\n실습 코드의 BLEU : 0.5045666840058485\\n패키지 NLTK의 BLEU : 0.5045666840058485\\n```앞서 언어 모델(Language Model)의 성능 측정을 위한 평가 방법으로 펄플렉서티(perplexity, PPL)를 소개한 바 있습니다. 기계 번역기에도 PPL을 평가에 사용할 수는 있지만, PPL은 번역의 성능을 직접적으로 반영하는 수치라 보기엔 어렵습니다.', '자연어 처리에서는 그 외에도 수많은 평가 방법들이 존재하는데, 기계 번역의 성능이 얼마나 뛰어난가를 측정하기 위해 사용되는 대표적인 방법인 BLEU(Bilingual Evaluation Understudy) 대해서 학습해보겠습니다. 앞으로 진행되는 설명은 논문 BLEU: a Method for Automatic Evaluation of Machine Translation를 참고로 하여 작성되었습니다.\\nimport numpy as np\\nfrom collections import Counter\\nfrom nltk import ngrams']\n",
      "['BLEU는 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법입니다. 측정 기준은 n-gram에 기반합니다. n-gram의 정의는 언어 모델 챕터를 참고하시기 바랍니다.\\nBLEU는 완벽한 방법이라고는 할 수는 없지만 몇 가지 이점을 가집니다. 언어에 구애받지 않고 사용할 수 있으며, 계산 속도가 빠릅니다. BLEU는 PPL과는 달리 높을 수록 성능이 더 좋음을 의미합니다. BLEU를 이해하기 위해 기계 번역 성능 평가를 위한 몇 가지 직관적인 방법을 먼저 제시하고, 문제점을 보완해나가는 방식으로 설명합니다.\\n1) 단어 개수 카운트로 측정하기(Unigram Precision)', '1) 단어 개수 카운트로 측정하기(Unigram Precision)\\n한국어-영어 번역기의 성능을 측정한다고 가정해봅시다. 두 개의 기계 번역기가 존재하고 두 기계 번역기에 같은 한국어 문장을 입력하여 번역된 영어 문장의 성능을 측정하고자 합니다. 번역된 문장을 각각 Candidate1, 2라고 해봅시다. 이 문장의 성능을 평가하기 위해서는 정답으로 비교되는 문장이 있어야 합니다. 세 명의 사람에게 한국어를 보고 영작해보라고 하여 세 개의 번역 문장을 만들어냈습니다. 이 세 문장을 각각 Reference1, 2, 3라고 해봅시다.\\nExample 1\\nCandidate1 : It is a guide to action which ensures that the military always obeys the commands of the party.', 'Candidate2 : It is to insure the troops forever hearing the activity guidebook that party direct.\\nReference1 : It is a guide to action that ensures that the military will forever heed Party commands.\\nReference2 : It is the guiding principle which guarantees the military forces always being under the command of the Party.\\nReference3 : It is the practical guide for the army always to heed the directions of the party.', 'Reference3 : It is the practical guide for the army always to heed the directions of the party.\\n편의상 Candidate를 Ca로, Reference를 Ref로 축약하여 부르겠습니다. Ca 1, 2를 Ref 1, 2, 3과 비교하여 성능을 측정하고자 합니다. 가장 직관적인 성능 평가 방법은 Ref 1, 2, 3 중 어느 한 문장이라도 등장한 단어의 개수를 Ca에서 세는 것입니다. 그리고 그 후에 Ca의 모든 단어의 카운트의 합. 즉, Ca에서의 총 단어의 수으로 나눠줍니다.\\n이러한 측정 방법을 유니그램 정밀도(Unigram Precision)라고 합니다. 이를 식으로 표현하면 다음과 같습니다.', '이러한 측정 방법을 유니그램 정밀도(Unigram Precision)라고 합니다. 이를 식으로 표현하면 다음과 같습니다.\\n$$\\\\text{Unigram Precision =}\\\\frac{\\\\text{Ref들 중에서 존재하는 Ca의 단어의 수}}{\\\\text{Ca의 총 단어 수}} = \\\\frac{\\\\text{the number of Ca words(unigrams) which occur in any Ref}}{\\\\text{the total number of words in the Ca}}$$', 'Ca1의 단어들은 얼추 훑어만봐도 Ref1, Ref2, Ref3에서 전반적으로 등장하는 반면, Ca2는 그렇지 않습니다. 이는 Ca1이 Ca2보다 더 좋은 번역 문장임을 의미합니다. 예를 들어 Ca1의 It is a guide to action은 Ref1에서, which는 Ref2에서, ensures that the militrary는 Ref1에서, always는 Ref2와 Ref3에서, commands는 Ref1에서, of the party는 Ref2에서 등장하였습니다. (대소문자 구분은 없다고 합시다.) Ca1에 있는 단어 중 Ref1, Ref2, Ref3 어디에도 등장하지 않은 단어는 obeys뿐입니다. 반면, Ca2는 Ca1과 비교하여 상대적으로 Ref1, 2, 3에 등장한 단어들이 적습니다.\\n위의 계산 방법에 따르면 Ca1과 Ca2의 유니그램 정밀도는 각각 아래와 같습니다.\\n$$\\\\text{Ca1 Unigram Precision =} \\\\frac{17}{18}$$', '위의 계산 방법에 따르면 Ca1과 Ca2의 유니그램 정밀도는 각각 아래와 같습니다.\\n$$\\\\text{Ca1 Unigram Precision =} \\\\frac{17}{18}$$\\n$$\\\\text{Ca2 Unigram Precision =} \\\\frac{8}{14}$$\\n이제부터는 단어라는 표현보다는 유니그램이라는 용어로 설명하겠습니다. 지금까지 설명한 유니그램 정밀도는 나름 의미있는 측정 방법으로 보이지만 사실 허술한 점이 있습니다. 아래와 같은 새로운 예가 있다고 해봅시다.\\n2) 중복을 제거하여 보정하기(Modified Unigram Precision)\\nExample 2\\nCandidate : the the the the the the the\\nReference1 : the cat is on the mat\\nReference2 : there is a cat on the mat', 'Reference1 : the cat is on the mat\\nReference2 : there is a cat on the mat\\n위의 Ca는 the만 7개가 등장한 터무니 없는 번역입니다. 하지만 이 번역은 앞서 배운 유니그램 정밀도에 따르면 $\\\\frac{7}{7}=1$이라는 최고의 성능 평가를 받게 됩니다. 이에 유니그램 정밀도를 다소 보정할 필요를 느낍니다. 이를 보정하기 위해서는 정밀도의 분자를 계산하기 위해 Ref와 매칭하며 카운트하는 과정에서 Ca의 유니그램이 이미 Ref에서 매칭된 적이 있었는지를 고려해야 합니다.\\n$$\\\\text{Unigram Precision =}\\\\frac{\\\\text{Ref들과 Ca를 고려한 새로운 카운트 방법이 필요!}}{\\\\text{Ca의 총 유니그램 수}}$$', '$$\\\\text{Unigram Precision =}\\\\frac{\\\\text{Ref들과 Ca를 고려한 새로운 카운트 방법이 필요!}}{\\\\text{Ca의 총 유니그램 수}}$$\\n정밀도의 분자를 계산하기 위한 각 유니그램의 카운트는 다음과 같이 수정합시다. 우선, 유니그램이 하나의 Ref에서 최대 몇 번 등장했는지를 카운트합니다. 이 값을 maximum reference count를 줄인 의미에서 Max_Ref_Count라고 부르겠습니다. Max_Ref_Count가 기존의 단순 카운트한 값보다 작은 경우에는 이 값을 최종 카운트 값으로 대체합니다. 정밀도의 분자 계산을 위한 새로운 카운트 방식을 식으로 표현하면 다음과 같습니다.\\n$Count_{clip}\\\\ =\\\\ min(Count,\\\\ Max$_$Ref$_$Count)$\\n위의 카운트를 사용하여 분자를 계산한 정밀도를 보정된 유니그램 정밀도(Modified Unigram Precision)라고 합니다.', '위의 카운트를 사용하여 분자를 계산한 정밀도를 보정된 유니그램 정밀도(Modified Unigram Precision)라고 합니다.\\n$$\\\\text{Modified Unigram Precision =}\\\\frac{\\\\text{Ca의 각 유니그램에 대해 }Count_{clip}\\\\text{을 수행한 값의 총 합}}{\\\\text{Ca의 총 유니그램 수}}=\\\\frac{\\\\sum_{unigram∈Candidate}\\\\ Count_{clip}(unigram)}\\n{\\\\sum_{unigram∈Candidate}\\\\ Count(unigram)}$$\\n분모의 경우에는 이전과 동일하게 Ca의 모든 유니그램에 대해서 각각 $Count$하고 모두 합한 값을 사용합니다.', '분모의 경우에는 이전과 동일하게 Ca의 모든 유니그램에 대해서 각각 $Count$하고 모두 합한 값을 사용합니다.\\n보정된 유니그램 정밀도를 예제를 통해 이해해봅시다. Example 2를 볼까요? the의 경우에는 Ref1에서 총 두 번 등장하였으므로, the의 카운트는 2로 보정됩니다. Ca의 기존 유니그램 정밀도는 $\\\\frac{7}{7}=1$이었으나 보정된 유니그램 정밀도는 $\\\\frac{2}{7}$와 같이 변경됩니다.', '다른 예로 Example 1에서의 Ca1의 보정된 유니그램 정밀도를 계산해보면 보정되기 이전과 동일하게 $\\\\frac{17}{18}$이지만 결과를 얻는 과정은 다릅니다. Ca1에서 the는 3번 등장하지만, Re2와 Ref3에서 the가 4번 등장하므로 3이 4보다 작으므로 the는 3으로 카운트 됩니다. the 외에 Ca1의 모든 유니그램은 전부 1개씩 등장하므로 보정 전과 동일하게 카운트하면 됩니다. 결과적으로 보정 이전의 정밀도와 동일하게 $\\\\frac{17}{18}$의 값을 가집니다.\\n3) 보정된 유니그램 정밀도 (Modified Unigram Precision) 구현하기', '3) 보정된 유니그램 정밀도 (Modified Unigram Precision) 구현하기\\n보정된 유니그램 정밀도를 파이썬 함수로 구현해보겠습니다. 보정된 유니그램 정밀도를 구현하기 위해서는 유니그램을 카운트 하는 $Count$ 함수와 $Count_{clip}$ 함수 두 가지 함수를 구현해야 합니다. 분모를 구하기 위해서 $Count$ 함수를 사용하고, 분자를 구하기 위해서 $Count_{clip}$ 함수를 사용하면 보정된 유니그램 정밀도를 구할 수 있습니다. 우선 유니그램을 단순히 $Count$하는 함수를 simple_count라는 이름의 아래 함수로 구현합니다.\\n# 토큰화 된 문장(tokens)에서 n-gram을 카운트\\ndef simple_count(tokens, n):\\nreturn Counter(ngrams(tokens, n))', '# 토큰화 된 문장(tokens)에서 n-gram을 카운트\\ndef simple_count(tokens, n):\\nreturn Counter(ngrams(tokens, n))\\n위 함수는 토큰화 된 문장을 입력받아서 문장 내의 n-gram의 개수를 카운트하는 함수입니다. 구하고자 하는 것은 유니그램 정밀도이므로 카운트하고자 하는 n-gram의 단위를 결정하는 simple_count 함수의 두번째 인자인 n의 값을 1로 하여 함수를 실행하면 됩니다. Example 1의 Ca1를 가져와 함수가 어떤 결과를 출력하는지 확인해봅시다.\\ncandidate = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\\ntokens = candidate.split() # 토큰화\\nresult = simple_count(tokens, 1) # n = 1은 유니그램', \"tokens = candidate.split() # 토큰화\\nresult = simple_count(tokens, 1) # n = 1은 유니그램\\nprint('유니그램 카운트 :',result)\\n유니그램 카운트 : Counter({('the',): 3, ('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('military',): 1, ('always',): 1, ('obeys',): 1, ('commands',): 1, ('of',): 1, ('party.',): 1})\\n위의 출력 결과는 모든 유니그램을 카운트한 결과를 보여줍니다. 대부분의 유니그램이 1개씩 카운트되었으나 유니그램 the는 문장에서 3번 등장하였으므로 유일하게 3의 값을 가집니다. 이번에는 Example 2의 Ca를 가지고 함수를 수행해봅시다.\", \"candidate = 'the the the the the the the'\\ntokens = candidate.split() # 토큰화\\nresult = simple_count(tokens, 1) # n = 1은 유니그램\\nprint('유니그램 카운트 :',result)\\n유니그램 카운트 : Counter({('the',): 7})\\nsimple_count 함수는 단순 카운트를 수행하므로 the에 대해서 7이라는 카운트 값을 리턴합니다. $Count$에 대한 함수를 구현하였으니 이번에는 $Count_{clip}$을 아래의 count_clip 이름을 가진 함수로 구현해보겠습니다.\\ndef count_clip(candidate, reference_list, n):\\n# Ca 문장에서 n-gram 카운트\\nca_cnt = simple_count(candidate, n)\\nmax_ref_cnt_dict = dict()\\nfor ref in reference_list:\\n# Ref 문장에서 n-gram 카운트\", 'max_ref_cnt_dict = dict()\\nfor ref in reference_list:\\n# Ref 문장에서 n-gram 카운트\\nref_cnt = simple_count(ref, n)\\n# 각 Ref 문장에 대해서 비교하여 n-gram의 최대 등장 횟수를 계산.\\nfor n_gram in ref_cnt:\\nif n_gram in max_ref_cnt_dict:\\nmax_ref_cnt_dict[n_gram] = max(ref_cnt[n_gram], max_ref_cnt_dict[n_gram])\\nelse:\\nmax_ref_cnt_dict[n_gram] = ref_cnt[n_gram]\\nreturn {\\n# count_clip = min(count, max_ref_count)\\nn_gram: min(ca_cnt.get(n_gram, 0), max_ref_cnt_dict.get(n_gram, 0)) for n_gram in ca_cnt\\n}', \"n_gram: min(ca_cnt.get(n_gram, 0), max_ref_cnt_dict.get(n_gram, 0)) for n_gram in ca_cnt\\n}\\ncount_clip 함수는 candidate 문장과 reference 문장들, 그리고 카운트 단위가 되는 n-gram에서의 n의 값 이 세 가지를 인자로 입력받아서 $count_{clip}$을 수행합니다. 여기서는 유니그램 정밀도를 구현하고 있으므로 역시나 n=1로 하여 함수를 실행하면 됩니다.\\n또한 count_clip 함수 내부에는 기존에 구현했던 simple_count 함수가 사용된 것을 확인할 수 있습니다. $Count_{clip}$을 구하기 위해서는 $Max$_$Ref$_$Count$값과 비교하기 위해 $Count$값이 필요하기 때문입니다. Example 2를 통해 함수가 정상 작동되는지 확인해봅시다.\\ncandidate = 'the the the the the the the'\\nreferences = [\", \"candidate = 'the the the the the the the'\\nreferences = [\\n'the cat is on the mat',\\n'there is a cat on the mat'\\n]\\nresult = count_clip(candidate.split(),list(map(lambda ref: ref.split(), references)),1)\\nprint('보정된 유니그램 카운트 :',result)\\n보정된 유니그램 카운트 : {('the',): 2}\\n동일한 예제 문장에 대해서 위의 simple_count 함수는 the가 7개로 카운트되었던 것과는 달리 이번에는 2개로 카운트되었습니다. 위의 두 함수를 사용하여 예제 문장에 대해서 보정된 정밀도를 연산하는 함수를 modified_precision란 이름의 함수로 구현해봅시다.\\ndef modified_precision(candidate, reference_list, n):\", \"def modified_precision(candidate, reference_list, n):\\nclip_cnt = count_clip(candidate, reference_list, n)\\ntotal_clip_cnt = sum(clip_cnt.values()) # 분자\\ncnt = simple_count(candidate, n)\\ntotal_cnt = sum(cnt.values()) # 분모\\n# 분모가 0이 되는 것을 방지\\nif total_cnt == 0:\\ntotal_cnt = 1\\n# 분자 : count_clip의 합, 분모 : 단순 count의 합 ==> 보정된 정밀도\\nreturn (total_clip_cnt / total_cnt)\\nresult = modified_precision(candidate.split(), list(map(lambda ref: ref.split(), references)), n=1)\\nprint('보정된 유니그램 정밀도 :',result)\", \"print('보정된 유니그램 정밀도 :',result)\\n보정된 유니그램 정밀도 : 0.2857142857142857\\n소수 값이 나오는데 이는 $\\\\frac{2}{7}$의 값을 의미합니다. 이는 앞서 육안으로 계산했던 Example 2에서 Ca의 보정된 정밀도와 동일합니다. 지금까지 보정된 유니그램 정밀도에 대해서 설명하고, 직접 구현까지 해보았습니다.\\n이제부터 설명에서 언급하는 '정밀도'는 기본적으로 보정된 정밀도(Modified Precision)라고 가정합니다. 정밀도를 보정하므로서 Ca에서 발생하는 단어 중복에 대한 문제점은 해결되었습니다. 하지만 유니그램 정밀도가 가지는 본질적인 문제점이 있기에 유니그램을 넘어 바이그램, 트라이그램 등과 같이 n-gram으로 확장해야 합니다. 문제점이 무엇인지 이해하고, 어떻게 n-gram으로 확장하는지 학습해봅시다.\\n4) 순서를 고려하기 위해서 n-gram으로 확장하기\", '4) 순서를 고려하기 위해서 n-gram으로 확장하기\\nBoW 표현과 유사하게, 유니그램 정밀도와 같이 각 단어의 빈도수로 접근하는 방법은 결국 단어의 순서를 고려하지 않는다는 특징이 있습니다. Example 1에 Ca3이라는 새로운 문장을 추가해보고 기존의 Ca1과 비교해봅시다.\\nExample 1\\nCandidate1 : It is a guide to action which ensures that the military always obeys the commands of the party.\\nCandidate2 : It is to insure the troops forever hearing the activity guidebook that party direct.\\nCandidate3 : the that military a is It guide ensures which to commands the of action obeys always party the.', 'Reference1 : It is a guide to action that ensures that the military will forever heed Party commands.\\nReference2 : It is the guiding principle which guarantees the military forces always being under the command of the Party.\\nReference3 : It is the practical guide for the army always to heed the directions of the party.', 'Reference3 : It is the practical guide for the army always to heed the directions of the party.\\nCa3은 사실 Ca1에서 모든 유니그램의 순서를 랜덤으로 섞은 실제 영어 문법에 맞지 않은 문장입니다. 하지만 Ref 1, 2, 3과 비교하여 유니그램 정밀도를 적용하면 Ca1과 Ca3의 두 정밀도는 동일합니다. 유니그램 정밀도는 유니그램의 순서를 전혀 고려하지 않기 때문입니다. 이를 위한 대안으로 개별적인 유니그램/단어로서 카운트하는 유니그램 정밀도에서 다음에 등장한 단어까지 함께 고려하여 카운트하도록 유니그램 외에도 Bigram, Trigram, 4-gram 단위 등으로 계산한 정밀도. 즉, n-gram을 이용한 정밀도를 도입하고자 합니다.', '이들 각각은 카운트 단위를 2개, 3개, 4개로 보느냐의 차이로 2-gram Precision, 3-gram Precision, 4-gram Precision이라고 하기도 합니다. 어떤 의미인지 바이그램(Bigram) 단위로 카운트하여 Example 1, 2의 바이그램 정밀도(Bigram Precision)를 계산해보겠습니다. 우선 좀 더 쉬운 Example 2부터 볼까요?\\nExample 2\\nCandidate1 : the the the the the the the\\nCandidate2 : the cat the cat on the mat\\nReference1 : the cat is on the mat\\nReference2 : there is a cat on the mat\\n이해를 돕고자 Example 2에 Ca2를 새로 추가했습니다. Ca2 바이그램의 $Count$와 $Count_{clip}$은 아래와 같습니다.\\n바이그램\\nthe cat\\ncat the\\ncat on\\non the\\nthe mat', '바이그램\\nthe cat\\ncat the\\ncat on\\non the\\nthe mat\\nSUM\\n$Count$\\n2\\n1\\n1\\n1\\n1\\n6\\n$Count_{clip}$\\n1\\n0\\n1\\n1\\n1\\n4\\n결과적으로 Ca2의 바이그램 정밀도는 $\\\\frac{4}{6}$가 됩니다. 반면, 당연하게도 Ca1의 바이그램 정밀도는 0입니다. Example 1은 어떨까요? Example 1에서 Ca1의 바이그램 정밀도는 $\\\\frac{10}{17}$이며, Ca2의 바이그램 정밀도는 $\\\\frac{1}{13}$입니다. Ca1에서 단어의 순서를 뒤섞은 Ca3의 바이그램 정밀도는 독자분들의 숙제로 남깁니다.\\n보정된 정밀도를 식으로 정의해보겠습니다. $p_{n}$에서 $n$은 n-gram에서의 $n$을 의미한다고 하였을 때, 앞서 배운 보정된 유니그램 정밀도의 식을 상기해봅시다.\\n$$p_{1}=\\\\frac{\\\\sum_{unigram∈Candidate}\\\\ Count_{clip}(unigram)}', '$$p_{1}=\\\\frac{\\\\sum_{unigram∈Candidate}\\\\ Count_{clip}(unigram)}\\n{\\\\sum_{unigram∈Candidate}\\\\ Count(unigram)}$$\\n이를 n-gram으로 일반화하면 아래와 같습니다.\\n$$p_{n}=\\\\frac{\\\\sum_{n\\\\text{-}gram∈Candidate}\\\\ Count_{clip}(n\\\\text{-}gram)}\\n{\\\\sum_{n\\\\text{-}gram∈Candidate}\\\\ Count(n\\\\text{-}gram)}$$\\n유니그램 정밀도에서는 $n$이 1이므로 $p_{1}$로 표현하였으나, 일반화 된 식에서는 $p_{n}$으로 표현한 것을 볼 수 있습니다.', '유니그램 정밀도에서는 $n$이 1이므로 $p_{1}$로 표현하였으나, 일반화 된 식에서는 $p_{n}$으로 표현한 것을 볼 수 있습니다.\\n여기서는 보정된 바이그램 정밀도 $p_{2}$, 보정된 트라이그램 정밀도 $p_{3}$ 등에 대한 파이썬 실습은 생략합니다. 사실 $p_{n}$을 계산하기 위한 함수를 별도로 다시 구현할 필요는 없는데, 앞서 구현한 함수 simple_count, count_clip, modified_precision은 모두 n-gram의 n을 함수의 인자로 받으므로, n을 1대신 다른 값을 넣어서 실습해보면 바이그램, 트라이그램 등에 대해서도 보정된 정밀도를 구할 수 있습니다.\\nn-gram 정밀도 식을 이해하였다면 BLEU의 최종 식까지 다 왔습니다. BLEU는 보정된 정밀도 $p_{1}, p_{2}, ..., p_{n}$를 모두 조합하여 사용합니다. 이를 모두 조합한 BLEU의 식은 아래와 같습니다.', '$$BLEU = exp(\\\\sum_{n=1}^{N}w_{n}\\\\ \\\\text{log}\\\\ p_{n})$$\\n$p_{n}$ : 각 gram의 보정된 정밀도입니다.\\n$N$ : n-gram에서 $n$의 최대 숫자입니다. 보통은 4의 값을 가집니다. $N$이 4라는 것은 $p_{1}, p_{2}, p_{3}, p_{4}$를 사용한다는 것을 의미합니다.\\n$w_{n}$ : 각 gram의 보정된 정밀도에 서로 다른 가중치를 줄 수 있습니다. 이 가중치의 합은 1로 합니다. 예를 들어 $N$이 4라고 하였을 때, $p_{1}, p_{2}, p_{3}, p_{4}$에 대해서 동일한 가중치를 주고자한다면 모두 0.25를 적용할 수 있습니다.\\nBLEU의 최종식에 거의 다 도달했습니다. 즉, 여전히 위의 BLEU식에도 문제점이 존재합니다.\\n5) 짧은 문장 길이에 대한 패널티(Brevity Penalty)', 'BLEU의 최종식에 거의 다 도달했습니다. 즉, 여전히 위의 BLEU식에도 문제점이 존재합니다.\\n5) 짧은 문장 길이에 대한 패널티(Brevity Penalty)\\nn-gram으로 단어의 순서를 고려한다고 하더라도 여전히 남아있는 문제가 있는데, 바로 Ca의 길이에 BLEU의 점수가 과한 영향을 받을 수 있다는 점입니다. 기존 Example 1에 다음의 Ca를 추가한다고 해보겠습니다.\\nExample 1\\nCandidate4 : it is', 'Example 1\\nCandidate4 : it is\\n이 문장은 유니그램 정밀도나 바이그램 정밀도가 각각 $\\\\frac{2}{2}$, $\\\\frac{1}{1}$로 두 정밀도 모두 1이라는 높은 정밀도를 얻습니다. 이과 같이 제대로 된 번역이 아님에도 문장의 길이가 짧다는 이유로 높은 점수를 받는 것은 이상합니다. 그래서 Ca가 Ref보다 문장의 길이가 짧은 경우에는 점수에 패널티를 줄 필요가 있습니다. 이를 브레버티 패널티(Brevity Penalty)라고 합니다. (직역하면 짧음 패널티) 이에 대해서 배우기 전에, 만약 반대로 Ca의 길이가 Ref보다 긴 경우에도 문제가 생길 수 있는지 보겠습니다.\\nExample 3\\nCandidate 1: I always invariably perpetually do.\\nCandidate 2: I always do.\\nReference 1: I always do.\\nReference 2: I invariably do.', 'Candidate 2: I always do.\\nReference 1: I always do.\\nReference 2: I invariably do.\\nReference 3: I perpetually do.\\nExample 3에서 Ca1은 가장 많은 단어를 사용했지만 Ca2보다 좋지 못한 번역입니다. 다시 말해 Ref의 단어를 가장 많이 사용한 것이 꼭 좋은 번역이라는 의미는 아닙니다. 그런데 다행히도 위와 같이 Ca의 길이가 불필요하게 Ref보다 긴 경우에는 BLEU 수식에서 정밀도를 n-gram으로 확장하여 바이그램, 트라이그램 정밀도 등을 모두 계산에 사용하고 있는 것만으로도 이미 패널티를 받고 있습니다. 즉, 브레버티 패널티를 설계할 때, 이 경우까지 고려할 필요는 없습니다.', '다시 Ref보다 Ca의 길이가 짧을 경우에 패널티를 주는 브레버티 패널티의 이야기로 돌아보겠습니다. 브레버티 패널티는 앞서 배운 BLEU의 식에 곱하는 방식으로 사용합니다. 브레버티 패널티를 줄여서 $BP$라고 하였을 때, 최종 BLEU의 식은 아래와 같습니다.\\n$$BLEU = BP × exp(\\\\sum_{n=1}^{N}w_{n}\\\\ \\\\text{log}\\\\ p_{n})$$\\n위의 수식은 패널티를 줄 필요가 없는 경우에는 $BP$의 값이 1이어야 함을 의미합니다. 이를 반영한 $BP$의 수식은 아래와 같습니다.\\n$$BP = \\\\begin{cases}1&\\\\text{if}\\\\space c>r\\\\\\\\ e^{(1-r/c)}&\\\\text{if}\\\\space c \\\\leq r \\\\end{cases} $$\\n$c$ : Candidate의 길이\\n$r$ : Candidate와 가장 길이 차이가 작은 Reference의 길이', '$c$ : Candidate의 길이\\n$r$ : Candidate와 가장 길이 차이가 작은 Reference의 길이\\nRef가 1개라면 Ca와 Ref의 두 문장의 길이만을 가지고 계산하면 되겠지만 여기서는 Ref가 여러 개일 때를 가정하고 있으므로 $r$은 모든 Ref들 중에서 Ca와 가장 길이 차이가 작은 Ref의 길이로 합니다. $r$을 구하는 코드는 아래와 같습니다.\\n# Ca 길이와 가장 근접한 Ref의 길이를 리턴하는 함수\\ndef closest_ref_length(candidate, reference_list):\\nca_len = len(candidate) # ca 길이\\nref_lens = (len(ref) for ref in reference_list) # Ref들의 길이\\n# 길이 차이를 최소화하는 Ref를 찾아서 Ref의 길이를 리턴', 'ref_lens = (len(ref) for ref in reference_list) # Ref들의 길이\\n# 길이 차이를 최소화하는 Ref를 찾아서 Ref의 길이를 리턴\\nclosest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - ca_len), ref_len))\\nreturn closest_ref_len\\n만약 Ca와 길이가 정확히 동일한 Ref가 있다면 길이 차이가 0인 최고 수준의 매치(best match length)입니다. 또한 만약 서로 다른 길이의 Ref이지만 Ca와 길이 차이가 동일한 경우에는 더 작은 길이의 Ref를 택합니다. 예를 들어 Ca가 길이가 10인데, Ref 1, 2가 각각 9와 11이라면 길이 차이는 동일하게 1밖에 나지 않지만 9를 택합니다. closest_ref_length 함수를 통해 $r$을 구했다면, $BP$를 구하는 함수 brevity_penalty를 구현해봅시다.', 'def brevity_penalty(candidate, reference_list):\\nca_len = len(candidate)\\nref_len = closest_ref_length(candidate, reference_list)\\nif ca_len > ref_len:\\nreturn 1\\n# candidate가 비어있다면 BP = 0 → BLEU = 0.0\\nelif ca_len == 0 :\\nreturn 0\\nelse:\\nreturn np.exp(1 - ref_len/ca_len)\\n위 함수는 앞서 배운 $BP$의 수식처럼 $c$가 $r$보다 클 경우에는 1을 리턴하고, 그 외의 경우에는 $e^{1-r/c}$를 리턴합니다. 최종적으로 BLEU 점수를 계산하는 함수 bleu_score를 구현해봅시다.\\ndef bleu_score(candidate, reference_list, weights=[0.25, 0.25, 0.25, 0.25]):', 'def bleu_score(candidate, reference_list, weights=[0.25, 0.25, 0.25, 0.25]):\\nbp = brevity_penalty(candidate, reference_list) # 브레버티 패널티, BP\\np_n = [modified_precision(candidate, reference_list, n=n) for n, _ in enumerate(weights,start=1)]\\n# p1, p2, p3, ..., pn\\nscore = np.sum([w_i * np.log(p_i) if p_i != 0 else 0 for w_i, p_i in zip(weights, p_n)])\\nreturn bp * np.exp(score)', 'return bp * np.exp(score)\\n위의 bleu_score 함수는 기본적으로는 $N$이 4에 각 gram에 대한 가중치는 동일하게 0.25라 주어진다고 가정합니다. 또한 함수 내에서는 $BP$를 구하고 bp에, $p_{1}, p_{2}, ..., p_{n}$를 구하여 p_n에 저장하도록 구현되어져 있습니다. 그리고 앞서 배운 BLEU의 식에 따라 추가 연산하여 최종 계산한 값을 리턴합니다.\\n위 함수가 동작하기 위해서는 앞서 구현한 simple_count, count_clip, modified_precision, brevity_penalty 4개의 함수 또한 모두 구현되어져 있어야 합니다. 지금까지 구현한 BLEU 코드로 계산된 점수와 NLTK 패키지에 이미 구현되어져 있는 BLEU 코드로 계산된 점수를 비교해봅시다.']\n",
      "[\"파이썬에서는 NLTK 패키지를 사용하여 BLEU를 계산할 수 있습니다.\\nimport nltk.translate.bleu_score as bleu\\ncandidate = 'It is a guide to action which ensures that the military always obeys the commands of the party'\\nreferences = [\\n'It is a guide to action that ensures that the military will forever heed Party commands',\\n'It is the guiding principle which guarantees the military forces always being under the command of the Party',\\n'It is the practical guide for the army always to heed the directions of the party'\\n]\", \"'It is the practical guide for the army always to heed the directions of the party'\\n]\\nprint('실습 코드의 BLEU :',bleu_score(candidate.split(),list(map(lambda ref: ref.split(), references))))\\nprint('패키지 NLTK의 BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), references)),candidate.split()))\\n실습 코드의 BLEU : 0.5045666840058485\\n패키지 NLTK의 BLEU : 0.5045666840058485\\n==================================================\\n--- 15. 어텐션 메커니즘 (Attention Mechanism) ---\\n마지막 편집일시 : 2022년 1월 2일 2:51 오후\", '--- 15. 어텐션 메커니즘 (Attention Mechanism) ---\\n마지막 편집일시 : 2022년 1월 2일 2:51 오후\\n==================================================\\n--- 15-01 어텐션 메커니즘 (Attention Mechanism) ---\\n```\\nQ = Query : t 시점의 디코더 셀에서의 은닉 상태\\nK = Keys : 모든 시점의 인코더 셀의 은닉 상태들\\nV = Values : 모든 시점의 인코더 셀의 은닉 상태들\\n```앞서 배운 seq2seq 모델은 인코더에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축하고, 디코더는 이 컨텍스트 벡터를 통해서 출력 시퀀스를 만들어냈습니다.\\n하지만 이러한 RNN에 기반한 seq2seq 모델에는 크게 두 가지 문제가 있습니다.\\n첫째, 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생합니다.', '하지만 이러한 RNN에 기반한 seq2seq 모델에는 크게 두 가지 문제가 있습니다.\\n첫째, 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생합니다.\\n둘째, RNN의 고질적인 문제인 기울기 소실(vanishing gradient) 문제가 존재합니다.\\n결국 이는 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상으로 나타났습니다. 이를 위한 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위한 등장한 기법인 어텐션(attention)을 소개합니다.']\n",
      "['어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 보게 됩니다.']\n",
      "['어텐션 메커니즘을 언급하기 전에 컴퓨터공학의 많은 분야에서 사용되는 Key-Value로 구성되는 자료형에 대해서 잠깐 언급하겠습니다. 가령, 이 책의 주 언어로 사용되는 파이썬에도 Key-Value로 구성되는 자료형인 딕셔너리(Dict) 자료형이 존재합니다. 파이썬의 딕셔너리 자료형은 키(Key)와 값(Value)이라는 두 개의 쌍으로 구성되는데, 키를 통해서 맵핑된 값을 찾아낼 수 있다는 특징을 갖고있습니다.\\n# 파이썬의 딕셔너리 자료형을 선언\\n# 키(Key) : 값(value)의 형식으로 키와 값의 쌍(Pair)을 선언한다.\\ndict = {\"2017\" : \"Transformer\", \"2018\" : \"BERT\"}\\n위의 자료형에서 2017은 키에 해당되며, Transformer는 2017의 키와 맵핑되는 값에 해당됩니다. 그와 마찬가지로 2018은 키에 해당되며, BERT는 2018이라는 키와 맵핑되는 값에 해당됩니다.', 'print(dict[\"2017\"]) #2017이라는 키에 해당되는 값을 출력\\nTransformer\\nprint(dict[\"2018\"])  #2018이라는 키에 해당되는 값을 출력\\nBERT\\nKey-Value 자료형에 대한 이해를 가지고 어텐션 함수에 대해서 설명해보겠습니다.\\n[이미지: ]\\n어텐션을 함수로 표현하면 주로 다음과 같이 표현됩니다.\\nAttention(Q, K, V) = Attention Value\\n어텐션 함수는 주어진 \\'쿼리(Query)\\'에 대해서 모든 \\'키(Key)\\'와의 유사도를 각각 구합니다. 그리고 구해낸 이 유사도를 키와 맵핑되어있는 각각의 \\'값(Value)\\'에 반영해줍니다. 그리고 유사도가 반영된 \\'값(Value)\\'을 모두 더해서 리턴합니다. 여기서는 이를 어텐션 값(Attention Value)이라고 하겠습니다.\\n지금부터 배우게 되는 seq2seq + 어텐션 모델에서 Q, K, V에 해당되는 각각의 Query, Keys, Values는 각각 다음과 같습니다.', '지금부터 배우게 되는 seq2seq + 어텐션 모델에서 Q, K, V에 해당되는 각각의 Query, Keys, Values는 각각 다음과 같습니다.\\nQ = Query : t 시점의 디코더 셀에서의 은닉 상태\\nK = Keys : 모든 시점의 인코더 셀의 은닉 상태들\\nV = Values : 모든 시점의 인코더 셀의 은닉 상태들\\n간단한 어텐션 예제를 통해 어텐션을 이해해보겠습니다.']\n",
      "['어텐션은 다양한 종류가 있는데 그 중에서도 가장 수식적으로 이해하기 쉽게 수식을 적용한 닷-프로덕트 어텐션(Dot-Product Attention)을 통해 어텐션을 이해해봅시다. seq2seq에서 사용되는 어텐션 중에서 닷-프로덕트 어텐션과 다른 어텐션의 차이는 주로 중간 수식의 차이로 메커니즘 자체는 거의 유사합니다.\\n[이미지: ]\\n위 그림은 디코더의 세번째 LSTM 셀에서 출력 단어를 예측할 때, 어텐션 메커니즘을 사용하는 모습을 보여줍니다. 디코더의 첫번째, 두번째 LSTM 셀은 이미 어텐션 메커니즘을 통해 je와 suis를 예측하는 과정을 거쳤다고 가정합니다. 어텐션 메커니즘에 대해 상세히 설명하기 전에 위의 그림을 통해 전체적인 개요만 이해해보겠습니다. 디코더의 세번째 LSTM 셀은 출력 단어를 예측하기 위해서 인코더의 모든 입력 단어들의 정보를 다시 한번 참고하고자 합니다.  중간 과정에 대한 설명은 현재는 생략하고 여기서 주목할 것은 인코더의 소프트맥스 함수입니다.', '소프트맥스 함수를 통해 나온 결과값은 I, am, a, student 단어 각각이 출력 단어를 예측할 때 얼마나 도움이 되는지의 정도를 수치화한 값입니다. 위의 그림에서는 빨간 직사각형의 크기로 소프트맥스 함수의 결과값의 크기를 표현했습니다. 직사각형의 크기가 클 수록 도움이 되는 정도의 크기가 큽니다. 각 입력 단어가 디코더의 예측에 도움이 되는 정도가 수치화하여 측정되면 이를 하나의 정보로 담아서 디코더로 전송됩니다. 위의 그림에서는 초록색 삼각형이 이에 해당됩니다. 결과적으로, 디코더는 출력 단어를 더 정확하게 예측할 확률이 높아집니다. 좀 더 상세히 알아보겠습니다.\\n1) 어텐션 스코어(Attention Score)를 구한다.\\n[이미지: ]', '1) 어텐션 스코어(Attention Score)를 구한다.\\n[이미지: ]\\n인코더의 시점(time step)을 각각 1, 2, ... N이라고 하였을 때 인코더의 은닉 상태(hidden state)를 각각 $h_{1}$, $h_{2}$, ... $h_{N}$라고 합시다. 디코더의 현재 시점(time step) t에서의 디코더의 은닉 상태(hidden state)를 $s_{t}$라고 합시다. 또한 여기서는 인코더의 은닉 상태와 디코더의 은닉 상태의 차원이 같다고 가정합니다. 위의 그림의 경우에는 인코더의 은닉 상태와 디코더의 은닉 상태가 동일하게 차원이 4입니다.', '어텐션 메커니즘의 첫 걸음인 어텐션 스코어(Attention score)에 대해서 배우기전에, 이전 챕터 배웠던 디코더의 현재 시점 t에서 필요한 입력값을 다시 상기해보겠습니다. 시점 t에서 출력 단어를 예측하기 위해서 디코더의 셀은 두 개의 입력값을 필요로 하는데, 바로 이전 시점인 t-1의 은닉 상태와 이전 시점 t-1에 나온 출력 단어입니다.\\n그런데 어텐션 메커니즘에서는 출력 단어 예측에 또 다른 값을 필요로 하는데 바로 어텐션 값(Attention Value)이라는 새로운 값입니다. t번째 단어를 예측하기 위한 어텐션 값을 $a_{t}$이라고 정의하겠습니다.', '어텐션 값이라는 새로운 개념이 등장한 만큼, 어텐션 값이 현재 시점 t에서의 출력 예측에 구체적으로 어떻게 반영되는지는 뒤에서 설명하겠습니다. 지금부터 배우는 모든 과정은 $a_{t}$를 구하기 위한 여정입니다. 그리고 그 여정의 첫 걸음은 바로 어텐션 스코어(Attention Score)를 구하는 일입니다. 어텐션 스코어란 현재 디코더의 시점 t에서 단어를 예측하기 위해, 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 $s_{t}$와 얼마나 유사한지를 판단하는 스코어값입니다.\\n닷-프로덕트 어텐션에서는 이 스코어 값을 구하기 위해 $s_{t}$를 전치(transpose)하고 각 은닉 상태와 내적(dot product)을 수행합니다. 즉, 모든 어텐션 스코어 값은 스칼라입니다. 예를 들어 $s_{t}$과 인코더의 i번째 은닉 상태의 어텐션 스코어의 계산 방법은 아래와 같습니다.\\n[이미지: ]\\n어텐션 스코어 함수를 정의해보면 다음과 같습니다.', '[이미지: ]\\n어텐션 스코어 함수를 정의해보면 다음과 같습니다.\\n$score(s_{t},\\\\ h_{i}) = s_{t}^Th_{i}$\\n$s_{t}$와 인코더의 모든 은닉 상태의 어텐션 스코어의 모음값을 $e^{t}$라고 정의하겠습니다. $e^{t}$의 수식은 다음과 같습니다.\\n$e^{t}=[s_{t}^Th_{1},...,s_{t}^Th_{N}]$\\n2) 소프트맥스(softmax) 함수를 통해 어텐션 분포(Attention Distribution)를 구한다.\\n[이미지: ]', '2) 소프트맥스(softmax) 함수를 통해 어텐션 분포(Attention Distribution)를 구한다.\\n[이미지: ]\\n$e^{t}$에 소프트맥스 함수를 적용하여, 모든 값을 합하면 1이 되는 확률 분포를 얻어냅니다. 이를 어텐션 분포(Attention Distribution)라고 하며, 각각의 값은 어텐션 가중치(Attention Weight)라고 합니다. 예를 들어 소프트맥스 함수를 적용하여 얻은 출력값인 I, am, a, student의 어텐션 가중치를 각각 0.1, 0.4, 0.1, 0.4라고 합시다. 이들의 합은 1입니다. 위의 그림은 각 인코더의 은닉 상태에서의 어텐션 가중치의 크기를 직사각형의 크기를 통해 시각화하였습니다. 즉, 어텐션 가중치가 클수록 직사각형이 큽니다.\\n디코더의 시점 t에서의 어텐션 가중치의 모음값인 어텐션 분포를 $α^{t}$이라고 할 때, $α^{t}$을 식으로 정의하면 다음과 같습니다.\\n$α^{t} = softmax(e^{t})$', '$α^{t} = softmax(e^{t})$\\n3) 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값(Attention Value)을 구한다.\\n[이미지: ]\\n이제 지금까지 준비해온 정보들을 하나로 합치는 단계입니다. 어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치값들을 곱하고, 최종적으로 모두 더합니다. 요약하면 가중합(Weighted Sum)을 진행합니다. 아래는 어텐션의 최종 결과. 즉, 어텐션 함수의 출력값인 어텐션 값(Attention Value) $a_{t}$에 대한 식을 보여줍니다.\\n$$\\na_{t}=\\\\sum_{i=1}^{N} α_{i}^{t}h_{i}\\n$$\\n이러한 어텐션 값 $a_{t}$은 종종 인코더의 문맥을 포함하고 있다고하여, 컨텍스트 벡터(context vector)라고도 불립니다. 앞서 배운 가장 기본적인 seq2seq에서는 인코더의 마지막 은닉 상태를 컨텍스트 벡터라고 부르는 것과 대조됩니다.', '4) 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결한다.(Concatenate)\\n[이미지: ]\\n어텐션 함수의 최종값인 어텐션 값 $a_{t}$을 구했습니다. 사실 어텐션 값이 구해지면 어텐션 메커니즘은 $a_{t}$를 $s_{t}$와 결합(concatenate)하여 하나의 벡터로 만드는 작업을 수행합니다. 이를 $v_{t}$라고 정의해보겠습니다. 그리고 이 $v_{t}$를 $\\\\hat{y}$ 예측 연산의 입력으로 사용하므로서 인코더로부터 얻은 정보를 활용하여 $\\\\hat{y}$를 좀 더 잘 예측할 수 있게 됩니다. 이것이 어텐션 메커니즘의 핵심입니다.\\n5) 출력층 연산의 입력이 되는 $\\\\tilde{{s}}_{t}$를 계산합니다.\\n[이미지: ]', '5) 출력층 연산의 입력이 되는 $\\\\tilde{{s}}_{t}$를 계산합니다.\\n[이미지: ]\\n논문에서는 $v_{t}$를 바로 출력층으로 보내기 전에 신경망 연산을 한 번 더 추가하였습니다. 가중치 행렬과 곱한 후에 하이퍼볼릭탄젠트 함수를 지나도록 하여 출력층 연산을 위한 새로운 벡터인 $\\\\tilde{{s}}_{t}$를 얻습니다. 어텐션 메커니즘을 사용하지 않는 seq2seq에서는 출력층의 입력이 t시점의 은닉 상태인 ${s}_{t}$였던 반면, 어텐션 메커니즘에서는 출력층의 입력이 $\\\\tilde{{s}}_{t}$가 되는 셈입니다.\\n식으로 표현하면 다음과 같습니다. $\\\\mathbf{W_{c}}$는 학습 가능한 가중치 행렬, $b_{c}$는 편향입니다. 그림에서 편향은 생략했습니다.\\n$$\\\\tilde{s}_{t} = \\\\tanh(\\\\mathbf{W_{c}}[{a}_t;{s}_t] + b_{c})$$\\n6) $\\\\tilde{{s}}_{t}$를 출력층의 입력으로 사용합니다.', '6) $\\\\tilde{{s}}_{t}$를 출력층의 입력으로 사용합니다.\\n$\\\\tilde{{s}}_{t}$를 출력층의 입력으로 사용하여 예측 벡터를 얻습니다.\\n$$\\n\\\\widehat{y}_t = \\\\text{Softmax}\\\\left( W_y\\\\tilde{s}_t + b_y \\\\right)\\n$$']\n",
      "['앞서 seq2seq + 어텐션(attention) 모델에 쓰일 수 있는 다양한 어텐션 종류가 있지만, 닷-프로덕트 어텐션과 다른 어텐션들의 차이는 중간 수식의 차이라고 언급한 바 있습니다. 여기서 말하는 중간 수식은 어텐션 스코어 함수를 말합니다. 위에서 배운 어텐션이 닷-프로덕트 어텐션인 이유는 어텐션 스코어를 구하는 방법이 내적이었기 때문입니다.\\n어텐션 스코어를 구하는 방법은 여러가지가 제시되어있으며, 현재 제시된 여러 종류의 어텐션 스코어 함수는 다음과 같습니다.\\n이름\\n스코어 함수\\nDefined by\\n$dot$\\n$score(s_{t},\\\\ h_{i}) = s^{T}_{t}h_{i}$\\nLuong et al. (2015)\\n$scaled\\\\ dot$\\n$$score(s_{t},\\\\ h_{i}) = \\\\frac{s^{T}_{t}h_{i}}{\\\\sqrt{n}}$$\\nVaswani et al. (2017)\\n$general$', '$$score(s_{t},\\\\ h_{i}) = \\\\frac{s^{T}_{t}h_{i}}{\\\\sqrt{n}}$$\\nVaswani et al. (2017)\\n$general$\\n$score(s_{t},\\\\ h_{i}) = s^{T}_{t}W_{a}h_{i}$ // 단, $W_{a}$는 학습 가능한 가중치 행렬\\nLuong et al. (2015)\\n$concat$\\n$score(s_{t},\\\\ h_{i}) = W_{a}^{T}\\\\ tanh(W_{b}[s_{t};h_{i}]), score(s_{t},\\\\  h_{i}) = W_{a}^{T}\\\\ tanh(W_{b}s_{t}+W_{c} h_{i})$\\nBahdanau et al. (2015)\\n$location-base$\\n$α_{t} = softmax(W_{a}s_{t})$ // $α_{t}$ 산출 시에 $s_{t}$만 사용하는 방법.\\nLuong et al. (2015)', '$α_{t} = softmax(W_{a}s_{t})$ // $α_{t}$ 산출 시에 $s_{t}$만 사용하는 방법.\\nLuong et al. (2015)\\n위에서 $s_{t}$는 Query, $h_{i}$는 Keys, $W_{a}$와 $W_{b}$는 학습 가능한 가중치 행렬입니다.\\n이름이 dot이라고 붙여진 스코어 함수가 이번에 배운 닷 프로덕트 어텐션입니다. 이 어텐션은 제안한 사람의 이름을 따서 루옹(Luong) 어텐션이라고도 합니다. 제안한 이들의 이름은 위 테이블에서 Defined By에 적혀져 있습니다. concat이라는 이름의 어텐션은 만든 사람의 이름을 따서 바다나우(Bahdanau) 어텐션 이라고도 부르며 뒤에서 설명합니다.', '지금까지 seq2seq에서 성능을 향상시켜주기 위한 기법인 어텐션에 대해서 알아봤습니다. 어텐션은 처음에는 RNN 기반의 seq2seq의 성능을 보정하기 위한 목적으로 소개되었지만, 현재에 이르러서는 어텐션 스스로가 기존의 seq2seq를 대체하는 방법이 되어가고 있습니다. 이에 대해서는 다음 챕터인 트랜스포머(Transformer) 챕터에서 더 자세히 배워보겠습니다.\\n==================================================\\n--- 15-02 바다나우 어텐션(Bahdanau Attention) ---\\n```\\nt = 어텐션 메커니즘이 수행되는 디코더 셀의 현재 시점을 의미.\\nQ = Query : t-1 시점의 디코더 셀에서의 은닉 상태\\nK = Keys : 모든 시점의 인코더 셀의 은닉 상태들\\nV = Values : 모든 시점의 인코더 셀의 은닉 상태들', 'K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\\nV = Values : 모든 시점의 인코더 셀의 은닉 상태들\\n```앞서 어텐션 메커니즘의 목적과 어텐션 메커니즘의 일종인 닷 프로덕트 어텐션(루옹 어텐션)의 전체적인 개요를 살펴보고, 마지막에 표를 통해 그 외에도 다양한 어텐션 메커니즘이 존재한다고 소개하였습니다. 이번에는 닷 프로덕트 어텐션보다는 조금 더 복잡하게 설계된 바다나우 어텐션 메커니즘을 이해해봅시다.']\n",
      "['어텐션 메커니즘을 함수 Attention()으로 정의하였을 때, 바다나우 어텐션 함수의 입, 출력은 다음과 같의 정의할 수 있습니다.\\nAttention(Q, K, V) = Attention Value\\nt = 어텐션 메커니즘이 수행되는 디코더 셀의 현재 시점을 의미.\\nQ = Query : t-1 시점의 디코더 셀에서의 은닉 상태\\nK = Keys : 모든 시점의 인코더 셀의 은닉 상태들\\nV = Values : 모든 시점의 인코더 셀의 은닉 상태들\\n여기서는 어텐션 함수의 Query가 디코더 셀의 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태임을 주목합시다.']\n",
      "['바다나우 어텐션의 연산 순서를 이해해봅시다.\\n1) 어텐션 스코어(Attention Score)를 구한다.\\n[이미지: ]\\n인코더의 시점(time step)을 각각 1, 2, ... N이라고 하였을 때 인코더의 은닉 상태(hidden state)를 각각 $h_{1}$, $h_{2}$, ... $h_{N}$라고 합시다. 디코더의 현재 시점(time step) t에서의 디코더의 은닉 상태(hidden state)를 $s_{t}$라고 합시다. 또한 여기서는 인코더의 은닉 상태와 디코더의 은닉 상태의 차원이 같다고 가정합니다. 위의 그림의 경우에는 인코더의 은닉 상태와 디코더의 은닉 상태가 동일하게 차원이 4입니다.', '앞서 루옹 어텐션에서는 Query로 디코더의 t 시점의 은닉 상태를 사용한 것과는 달리 이번에는 t-1 시점의 은닉 상태 $s_{t-1}$를 사용합니다. 바다나우 어텐션의 어텐션 스코어 함수. 즉, $s_{t-1}$과 인코더의 i번째 은닉 상태의 어텐션 스코어 계산 방법은 아래와 같습니다.\\n$score(s_{t-1},\\\\ h_{i}) = W_{a}^{T}\\\\ tanh(W_{b}s_{t-1}+W_{c}h_{i})$\\n단, $W_{a}, W_{b}, W_{c}$는 학습 가능한 가중치 행렬입니다. $s_{t-1}$와 $h_{1}, h_{2}, h_{3}, h_{4}$의 어텐션 스코어를 각각 구해야하므로 병렬 연산을 위해 $h_{1}, h_{2}, h_{3}, h_{4}$를 하나의 행렬 $H$로 두겠습니다. 수식은 다음과 같이 변경됩니다.\\n$score(s_{t-1},\\\\ H) = W_{a}^{T}\\\\ tanh(W_{b}s_{t-1}+W_{c}H)$', '$score(s_{t-1},\\\\ H) = W_{a}^{T}\\\\ tanh(W_{b}s_{t-1}+W_{c}H)$\\n그림을 통해 이해해봅시다. 우선 $W_{b}s_{t-1}$와 $W_{c}H$를 각각 구하면 다음과 같습니다.\\n[이미지: ]\\n이들을 더한 후, 하이퍼볼릭탄젠트 함수를 지나도록 합니다.\\n[이미지: ]\\n지금까지 진행된 연산의 수식은 다음과 같습니다.\\n$tanh(W_{b}s_{t-1}+W_{c}H)$\\n이제 $W_{a}^{T}$와 곱하여 $s_{t-1}$와 $h_{1}, h_{2}, h_{3}, h_{4}$의 유사도가 기록된 어텐션 스코어 벡터 $e^{t}$를 얻습니다.\\n[이미지: ]\\n$e^{t} = W_{a}^{T}\\\\ tanh(W_{b}s_{t-1}+W_{c}H)$\\n2) 소프트맥스(softmax) 함수를 통해 어텐션 분포(Attention Distribution)를 구한다.\\n[이미지: ]', '2) 소프트맥스(softmax) 함수를 통해 어텐션 분포(Attention Distribution)를 구한다.\\n[이미지: ]\\n$e^{t}$에 소프트맥스 함수를 적용하여, 모든 값을 합하면 1이 되는 확률 분포를 얻어냅니다. 이를 어텐션 분포(Attention Distribution)라고 하며, 각각의 값은 어텐션 가중치(Attention Weight)라고 합니다.\\n3) 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값(Attention Value)을 구한다.\\n[이미지: ]\\n지금까지 준비해온 정보들을 하나로 합치는 단계입니다. 어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치값들을 곱하고, 최종적으로 모두 더합니다. 요약하면 가중합(Weighted Sum)을 한다고 말할 수도 있겠습니다. 이 벡터는 인코더의 문맥을 포함하고 있다고하여, 컨텍스트 벡터(context vector)라고 부릅니다.\\n4) 컨텍스트 벡터로부터 $s_{t}$를 구합니다.', '4) 컨텍스트 벡터로부터 $s_{t}$를 구합니다.\\n기존의 LSTM이 $s_{t}$를 구할 때를 아래 그림을 통해 상기해봅시다. 기존의 LSTM은 이전 시점의 셀로부터 전달받은 은닉 상태 $s_{t-1}$와 현재 시점의 입력 $x_{t}$를 가지고 연산하였습니다. 아래의 LSTM은 seq2seq의 디코더이며 현재 시점의 입력 $x_{t}$는 임베딩된 단어 벡터입니다.\\n[이미지: ]', '[이미지: ]\\n그렇다면 어텐션 메커니즘에서는 어떨까요? 아래의 그림은 바다나우 어텐션 메커니즘에서는 컨텍스트 벡터와 현재 시점의 입력인 단어의 임베딩 벡터를 연결(concatenate)하고, 현재 시점의 새로운 입력으로 사용하는 모습을 보여줍니다. 그리고 이전 시점의 셀로부터 전달받은 은닉 상태 $s_{t-1}$와 현재 시점의 새로운 입력으로부터 $s_{t}$를 구합니다. 기존의 LSTM이 임베딩된 단어 벡터를 입력으로 하는 것에서 컨텍스트 벡터와 임베딩된 단어 벡터를 연결(concatenate)하여 입력으로 사용하는 것이 달라졌습니다.\\n[이미지: ]\\n이후에는 어텐션 메커니즘을 사용하지 않는 경우와 동일합니다. $s_{t}$는 출력층으로 전달되어 현재 시점의 예측값을 구하게 됩니다.\\n==================================================', '==================================================\\n--- 15-03 양방향 LSTM과 어텐션 메커니즘(BiLSTM with Attention mechanism) ---\\n```\\n25000/25000 [============================] - 183s 7ms/sample - loss: 0.1901 - acc: 0.8793\\n테스트 정확도: 0.8793\\n```단뱡항 LSTM으로 텍스트 분류를 수행할 수도 있지만 때로는 양방향 LSTM을 사용하는 것이 더 강력합니다. 여기에 추가적으로 어텐션 메커니즘을 사용할 수도 있습니다. 양방향 LSTM과 어텐션 메커니즘으로 IMDB 리뷰 감성 분류하기를 수행해봅시다.']\n",
      "['from tensorflow.keras.datasets import imdb\\nfrom tensorflow.keras.utils import to_categorical\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nIMDB 리뷰 데이터는 앞서 텍스트 분류하기 챕터에서 다룬 바 있으므로 데이터에 대한 상세 설명은 생략합니다. 최대 단어 개수를 10,000으로 제한하고 훈련 데이터와 테스트 데이터를 받아옵니다.\\nvocab_size = 10000\\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)', \"vocab_size = 10000\\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)\\n훈련 데이터와 이에 대한 레이블이 각각 X_train, y_train에 테스트 데이터와 이에 대한 레이블이 각각 X_test, y_test에 저장되었습니다. IMDB 리뷰 데이터는 이미 정수 인코딩이 된 상태므로 남은 전처리는 패딩뿐입니다. 리뷰의 최대 길이와 평균 길이를 확인해봅시다.\\nprint('리뷰의 최대 길이 : {}'.format(max(len(l) for l in X_train)))\\nprint('리뷰의 평균 길이 : {}'.format(sum(map(len, X_train))/len(X_train)))\\n리뷰의 최대 길이 : 2494\\n리뷰의 평균 길이 : 238.71364\\n리뷰의 최대 길이는 2,494이며 리뷰의 평균 길이는 약 238로 확인됩니다. 평균 길이보다는 조금 크게 데이터를 패딩하겠습니다.\", '리뷰의 평균 길이 : 238.71364\\n리뷰의 최대 길이는 2,494이며 리뷰의 평균 길이는 약 238로 확인됩니다. 평균 길이보다는 조금 크게 데이터를 패딩하겠습니다.\\nmax_len = 500\\nX_train = pad_sequences(X_train, maxlen=max_len)\\nX_test = pad_sequences(X_test, maxlen=max_len)\\n훈련용 리뷰와 테스트용 리뷰의 길이가 둘 다 500이 되었습니다.']\n",
      "['여기서 사용할 어텐션은 바다나우 어텐션(Bahdanau attention)입니다. 이를 이해하기 위해 앞서 배운 가장 쉬운 어텐션이었던 닷 프로덕트 어텐션과 어텐션 스코어 함수의 정의를 상기해봅시다.\\n어텐션 스코어 함수란 주어진 query와 모든 key에 대해서 유사도를 측정하는 함수를 말합니다. 그리고 닷 프로덕트 어텐션에서는 query와 key의 유사도를 구하는 방법이 내적(dot product)이었습니다. 다음은 닷 프로덕트 어텐션의 어텐션 스코어 함수를 보여줍니다.\\n$score(query,\\\\ key) = query^Tkey$\\n바다나우 어텐션은 아래와 같은 어텐션 스코어 함수를 사용합니다.\\n$score(query,\\\\ key) = V^Ttanh(W_{1}key + W_{2}query)$', '바다나우 어텐션은 아래와 같은 어텐션 스코어 함수를 사용합니다.\\n$score(query,\\\\ key) = V^Ttanh(W_{1}key + W_{2}query)$\\n이 어텐션 스코어 함수를 사용하여 어텐션 메커니즘을 구현하면 됩니다. 그런데 텍스트 분류에서 어텐션 메커니즘을 사용하는 이유는 무엇일까요? RNN의 마지막 은닉 상태는 예측을 위해 사용됩니다. 그런데 이 RNN의 마지막 은닉 상태는 몇 가지 유용한 정보들을 손실한 상태입니다. 그래서 RNN이 time step을 지나며 손실했던 정보들을 다시 참고하고자 합니다.\\n이는 다시 말해 RNN의 모든 은닉 상태들을 다시 한 번 참고하겠다는 것입니다. 그리고 이를 위해서 어텐션메커니즘을 사용합니다.\\nimport tensorflow as tf\\nclass BahdanauAttention(tf.keras.Model):\\ndef __init__(self, units):\\nsuper(BahdanauAttention, self).__init__()', 'def __init__(self, units):\\nsuper(BahdanauAttention, self).__init__()\\nself.W1 = Dense(units)\\nself.W2 = Dense(units)\\nself.V = Dense(1)\\ndef call(self, values, query): # 단, key와 value는 같음\\n# query shape == (batch_size, hidden size)\\n# hidden_with_time_axis shape == (batch_size, 1, hidden size)\\n# score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\\nhidden_with_time_axis = tf.expand_dims(query, 1)\\n# score shape == (batch_size, max_length, 1)\\n# we get 1 at the last axis because we are applying score to self.V', '# we get 1 at the last axis because we are applying score to self.V\\n# the shape of the tensor before applying self.V is (batch_size, max_length, units)\\nscore = self.V(tf.nn.tanh(\\nself.W1(values) + self.W2(hidden_with_time_axis)))\\n# attention_weights shape == (batch_size, max_length, 1)\\nattention_weights = tf.nn.softmax(score, axis=1)\\n# context_vector shape after sum == (batch_size, hidden_size)\\ncontext_vector = attention_weights * values\\ncontext_vector = tf.reduce_sum(context_vector, axis=1)', 'context_vector = attention_weights * values\\ncontext_vector = tf.reduce_sum(context_vector, axis=1)\\nreturn context_vector, attention_weights']\n",
      "[\"from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\\nfrom tensorflow.keras import Input, Model\\nfrom tensorflow.keras import optimizers\\nimport os\\n이제 모델을 설계해보겠습니다. 여기서는 케라스의 함수형 API를 사용합니다. 우선 입력층과 임베딩층을 설계합니다.\\nsequence_input = Input(shape=(max_len,), dtype='int32')\\nembedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero = True)(sequence_input)\", '10,000개의 단어들을 128차원의 벡터로 임베딩하도록 설계하였습니다. 이제 양방향 LSTM을 설계합니다. 단, 여기서는 양방향 LSTM을 두 층을 사용하겠습니다. 우선, 첫번째 층입니다. 두번째 층을 위에 쌓을 예정이므로 return_sequences를 True로 해주어야 합니다.\\nlstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)\\n두번째 층을 설계합니다. 상태를 리턴받아야 하므로 return_state를 True로 해주어야 합니다.\\nlstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\\\\n(LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)\\n각 상태의 크기(shape)를 출력해보겠습니다.', '(LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)\\n각 상태의 크기(shape)를 출력해보겠습니다.\\nprint(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\\n(None, 500, 128) (None, 64) (None, 64) (None, 64) (None, 64)\\n순방향 LSTM의 은닉 상태와 셀상태를 forward_h, forward_c에 저장하고, 역방향 LSTM의 은닉 상태와 셀 상태를 backward_h, backward_c에 저장합니다.\\n각 은닉 상태나 셀 상태의 경우에는 128차원을 가지는데, lstm의 경우에는 (500 × 128)의 크기를 가집니다. foward 방향과 backward 방향이 연결된 hidden state벡터가 모든 시점에 대해서 존재함을 의미합니다.', '양방향 LSTM을 사용할 경우에는 순방향 LSTM과 역방향 LSTM 각각 은닉 상태와 셀 상태를 가지므로, 양방향 LSTM의 은닉 상태와 셀 상태를 사용하려면 두 방향의 LSTM의 상태들을 연결(concatenate)해주면 됩니다.\\nstate_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\\nstate_c = Concatenate()([forward_c, backward_c]) # 셀 상태\\n어텐션 메커니즘에서는 은닉 상태를 사용합니다. 이를 입력으로 컨텍스트 벡터(context vector)를 얻습니다.\\nattention = BahdanauAttention(64) # 가중치 크기 정의\\ncontext_vector, attention_weights = attention(lstm, state_h)', 'context_vector, attention_weights = attention(lstm, state_h)\\n컨텍스트 벡터를 밀집층(dense layer)에 통과시키고, 이진 분류이므로 최종 출력층에 1개의 뉴런을 배치하고, 활성화 함수로 시그모이드 함수를 사용합니다.\\ndense1 = Dense(20, activation=\"relu\")(context_vector)\\ndropout = Dropout(0.5)(dense1)\\noutput = Dense(1, activation=\"sigmoid\")(dropout)\\nmodel = Model(inputs=sequence_input, outputs=output)\\n옵티마이저로 아담 옵티마이저 사용하고, 모델을 컴파일합니다.\\nmodel.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'accuracy\\'])', \"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\\n시그모이드 함수를 사용하므로 손실 함수로 binary_crossentropy를 사용하였습니다. 이제 모델을 훈련하겠습니다.\\nhistory = model.fit(X_train, y_train, epochs = 3, batch_size = 256, validation_data=(X_test, y_test), verbose=1)\\n검증 데이터로 테스트 데이터를 사용하여 에포크가 끝날 때마다 테스트 데이터에 대한 정확도를 출력하도록 하였습니다.\\nTrain on 25000 samples, validate on 25000 samples\\nEpoch 1/3\", 'Train on 25000 samples, validate on 25000 samples\\nEpoch 1/3\\n25000/25000 [==============================] - 566s 23ms/sample - loss: 0.4941 - accuracy: 0.7570 - val_loss: 0.3110 - val_accuracy: 0.8721\\nEpoch 2/3\\n25000/25000 [==============================] - 541s 22ms/sample - loss: 0.2530 - accuracy: 0.9074 - val_loss: 0.2852 - val_accuracy: 0.8835\\nEpoch 3/3\\n25000/25000 [==============================] - 543s 22ms/sample - loss: 0.1901 - accuracy: 0.9352 - val_loss: 0.3375 - val_accuracy: 0.8793', 'print(\"\\\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\\n25000/25000 [============================] - 183s 7ms/sample - loss: 0.1901 - acc: 0.8793\\n테스트 정확도: 0.8793\\n87.93%의 정확도를 얻습니다.\\n==================================================\\n--- 16. 트랜스포머(Transformer) ---\\n마지막 편집일시 : 2022년 1월 2일 2:51 오후\\n==================================================\\n--- 16-01 트랜스포머(Transformer) ---\\n```\\nText(0.5, 0, \\'Train Step\\')\\n```\\n이번 챕터는 앞서 설명한 어텐션 메커니즘 챕터에 대한 사전 이해가 필요합니다.', '```\\nText(0.5, 0, \\'Train Step\\')\\n```\\n이번 챕터는 앞서 설명한 어텐션 메커니즘 챕터에 대한 사전 이해가 필요합니다.\\n트랜스포머(Transformer)는 2017년 구글이 발표한 논문인 \"Attention is all you need\"에서 나온 모델로 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델입니다. 이 모델은 RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 우수한 성능을 보여주었습니다.\\n당장 구현에 관심이 없다면 코드 부분만 스킵해서 이론만 읽으셔도 됩니다.\\n트랜스포머 전체 코드는 아래의 링크에 공유합니다.\\n깃허브 링크 : https://github.com/ukairia777/tensorflow-transformer\\npip install tensorflow==2.12.0\\nimport numpy as np', 'pip install tensorflow==2.12.0\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport tensorflow as tf']\n",
      "['트랜스포머에 대해서 배우기 전에 기존의 seq2seq를 상기해봅시다. 기존의 seq2seq 모델은 인코더-디코더 구조로 구성되어져 있었습니다. 여기서 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 통해서 출력 시퀀스를 만들어냈습니다. 하지만 이러한 구조는 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실된다는 단점이 있었고, 이를 보정하기 위해 어텐션이 사용되었습니다. 그런데 어텐션을 RNN의 보정을 위한 용도로서 사용하는 것이 아니라 어텐션만으로 인코더와 디코더를 만들어보면 어떨까요?']\n",
      "['시작에 앞서 트랜스포머의 하이퍼파라미터를 정의합니다. 각 하이퍼파라미터의 의미에 대해서는 뒤에서 설명하기로하고, 여기서는 트랜스포머에는 이러한 하이퍼파라미터가 존재한다는 정도로만 이해해보겠습니다. 아래에서 정의하는 수치는 트랜스포머를 제안한 논문에서 사용한 수치로 하이퍼파라미터는 사용자가 모델 설계시 임의로 변경할 수 있는 값들입니다.\\n$$d_{model} = 512$$\\n트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기를 의미합니다. 임베딩 벡터의 차원 또한 $d_{model}$이며, 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때에도 이 차원을 유지합니다. 논문에서는 512입니다.\\n$$\\\\text{num_layers} = 6$$\\n트랜스포머에서 하나의 인코더와 디코더를 층으로 생각하였을 때, 트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지를 의미합니다. 논문에서는 인코더와 디코더를 각각 총 6개 쌓았습니다.', '$$\\\\text{num_heads} = 8$$\\n트랜스포머에서는 어텐션을 사용할 때, 한 번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 택했습니다. 이때 이 병렬의 개수를 의미합니다.\\n$$d_{ff} = 2048$$\\n트랜스포머 내부에는 피드 포워드 신경망이 존재하며 해당 신경망의 은닉층의 크기를 의미합니다. 피드 포워드 신경망의 입력층과 출력층의 크기는 $d_{model}$입니다.']\n",
      "['[이미지: ]\\n트랜스포머는 RNN을 사용하지 않지만 기존의 seq2seq처럼 인코더에서 입력 시퀀스를 입력받고, 디코더에서 출력 시퀀스를 출력하는 인코더-디코더 구조를 유지하고 있습니다. 이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조입니다. 트랜스포머를 제안한 논문에서는 인코더와 디코더의 개수를 각각 6개 사용하였습니다.\\n[이미지: ]\\n위의 그림은 인코더와 디코더가 6개씩 존재하는 트랜스포머의 구조를 보여줍니다. 이 책에서는 인코더와 디코더가 각각 여러 개 쌓여있다는 의미를 사용할 때는 알파벳 s를 뒤에 붙여 encoders, decoders라고 표현하겠습니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림은 인코더로부터 정보를 전달받아 디코더가 출력 결과를 만들어내는 트랜스포머 구조를 보여줍니다. 디코더는 마치 기존의 seq2seq 구조처럼 시작 심볼 <sos>를 입력으로 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행합니다. 이는 RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음을 보여줍니다.\\n트랜스포머의 내부 구조를 조금씩 확대해가는 방식으로 트랜스포머를 이해해봅시다. 우선 인코더와 디코더의 구조를 이해하기 전에 트랜스포머의 입력에 대해서 이해해보겠습니다. 트랜스포머의 인코더와 디코더는 단순히 각 단어의 임베딩 벡터들을 입력받는 것이 아니라 임베딩 벡터에서 조정된 값을 입력받는데 이에 대해서 알아보기 위해 입력 부분을 확대해보겠습니다.']\n",
      "['트랜스포머의 내부를 이해하기 전 우선 트랜스포머의 입력에 대해서 알아보겠습니다. RNN이 자연어 처리에서 유용했던 이유는 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보(position information)를 가질 수 있다는 점에 있었습니다.\\n하지만 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있습니다. 트랜스포머는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는데, 이를 포지셔널 인코딩(positional encoding)이라고 합니다.\\n[이미지: ]\\n위의 그림은 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩의 값이 더해지는 것을 보여줍니다. 임베딩 벡터가 인코더의 입력으로 사용되기 전 포지셔널 인코딩값이 더해지는 과정을 시각화하면 아래와 같습니다.\\n[이미지: ]', '[이미지: ]\\n포지셔널 인코딩 값들은 어떤 값이기에 위치 정보를 반영해줄 수 있는 것일까요? 트랜스포머는 위치 정보를 가진 값을 만들기 위해서 아래의 두 개의 함수를 사용합니다.\\n$$PE_{(pos,\\\\ 2i)}=sin(pos/10000^{2i/d_{model}})$$\\n$$PE_{(pos,\\\\ 2i+1)}=cos(pos/10000^{2i/d_{model}})$$\\n사인 함수와 코사인 함수의 그래프를 상기해보면 요동치는 값의 형태를 생각해볼 수 있는데, 트랜스포머는 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해주므로서 단어의 순서 정보를 더하여 줍니다. 그런데 위의 두 함수에는 $pos$, $i$, $d_{model}$ 등의 생소한 변수들이 있습니다. 위의 함수를 이해하기 위해서는 위에서 본 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 임베딩 벡터가 모여 만들어진 문장 행렬과 포지셔널 인코딩 행렬의 덧셈 연산을 통해 이루어진다는 점을 이해해야 합니다.\\n[이미지: ]', '[이미지: ]\\n$pos$는 입력 문장에서의 임베딩 벡터의 위치를 나타내며, $i$는 임베딩 벡터 내의 차원의 인덱스를 의미합니다. 위의 식에 따르면 임베딩 벡터 내의 각 차원의 인덱스가 짝수인 경우에는 사인 함수의 값을 사용하고 홀수인 경우에는 코사인 함수의 값을 사용합니다. 위의 수식에서 $(pos,\\\\ 2i)$일 때는 사인 함수를 사용하고, $(pos,\\\\ 2i+1)$일 때는 코사인 함수를 사용하고 있음을 주목합시다.\\n또한 위의 식에서 $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터입니다. 앞으로 보게 될 트랜스포머의 각종 구조에서 $d_{model}$의 값이 계속해서 등장하는 이유입니다. 임베딩 벡터 또한 $d_{model}$의 차원을 가지는데 위의 그림에서는 마치 4로 표현되었지만 실제 논문에서는 512의 값을 가집니다.', '위와 같은 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존되는데, 예를 들어 각 임베딩 벡터에 포지셔널 인코딩의 값을 더하면 같은 단어라고 하더라도 문장 내의 위치에 따라서 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라집니다. 이에 따라 트랜스포머의 입력은 순서 정보가 고려된 임베딩 벡터가 됩니다. 이를 코드로 구현하면 아래와 같습니다.\\nclass PositionalEncoding(tf.keras.layers.Layer):\\ndef __init__(self, position, d_model):\\nsuper(PositionalEncoding, self).__init__()\\nself.pos_encoding = self.positional_encoding(position, d_model)\\ndef get_angles(self, position, i, d_model):', 'def get_angles(self, position, i, d_model):\\nangles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\\nreturn position * angles\\ndef positional_encoding(self, position, d_model):\\nangle_rads = self.get_angles(\\nposition=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\\ni=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\\nd_model=d_model)\\n# 배열의 짝수 인덱스(2i)에는 사인 함수 적용\\nsines = tf.math.sin(angle_rads[:, 0::2])\\n# 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\\ncosines = tf.math.cos(angle_rads[:, 1::2])', '# 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\\ncosines = tf.math.cos(angle_rads[:, 1::2])\\nangle_rads = np.zeros(angle_rads.shape)\\nangle_rads[:, 0::2] = sines\\nangle_rads[:, 1::2] = cosines\\npos_encoding = tf.constant(angle_rads)\\npos_encoding = pos_encoding[tf.newaxis, ...]\\nprint(pos_encoding.shape)\\nreturn tf.cast(pos_encoding, tf.float32)\\ndef call(self, inputs):\\nreturn inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]', \"def call(self, inputs):\\nreturn inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\\n50 × 128의 크기를 가지는 포지셔널 인코딩 행렬을 시각화하여 어떤 형태를 가지는지 확인해봅시다. 이는 입력 문장의 단어가 50개이면서, 각 단어가 128차원의 임베딩 벡터를 가질 때 사용할 수 있는 행렬입니다.\\n# 문장의 길이 50, 임베딩 벡터의 차원 128\\nsample_pos_encoding = PositionalEncoding(50, 128)\\nplt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\\nplt.xlabel('Depth')\\nplt.xlim((0, 128))\\nplt.ylabel('Position')\\nplt.colorbar()\\nplt.show()\\n(1, 50, 128)\\n[이미지: ]\"]\n",
      "['트랜스포머에서 사용되는 세 가지의 어텐션에 대해서 간단히 정리해봅시다. 지금은 큰 그림을 이해하는 것에만 집중합니다.\\n[이미지: ]\\n첫번째 그림인 셀프 어텐션은 인코더에서 이루어지지만, 두번째 그림인 셀프 어텐션과 세번째 그림인 인코더-디코더 어텐션은 디코더에서 이루어집니다. 셀프 어텐션은 본질적으로 Query, Key, Value가 동일한 경우를 말합니다. 반면, 세번째 그림 인코더-디코더 어텐션에서는 Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터이므로 셀프 어텐션이라고 부르지 않습니다.\\n주의할 점은 여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아니라 벡터의 출처가 같다는 의미입니다.\\n정리하면 다음과 같습니다.\\n인코더의 셀프 어텐션 : Query = Key = Value\\n디코더의 마스크드 셀프 어텐션 : Query = Key = Value', \"정리하면 다음과 같습니다.\\n인코더의 셀프 어텐션 : Query = Key = Value\\n디코더의 마스크드 셀프 어텐션 : Query = Key = Value\\n디코더의 인코더-디코더 어텐션 : Query : 디코더 벡터 / Key = Value : 인코더 벡터\\n[이미지: ]\\n위 그림은 트랜스포머의 아키텍처에서 세 가지 어텐션이 각각 어디에서 이루어지는지를 보여줍니다. 세 개의 어텐션에 추가적으로 '멀티 헤드'라는 이름이 붙어있습니다. 뒤에서 설명하겠지만 트랜스포머가 어텐션을 병렬적으로 수행하는 방법을 의미합니다.\"]\n",
      "['인코더의 구조에 대해서 알아보겠습니다.\\n[이미지: ]\\n트랜스포머는 하이퍼파라미터인 $\\\\text{num_layers}$ 개수의 인코더 층을 쌓습니다. 논문에서는 총 6개의 인코더 층을 사용하였습니다. 인코더를 하나의 층이라는 개념으로 생각한다면, 하나의 인코더 층은 크게 총 2개의 서브층(sublayer)으로 나뉘어집니다. 셀프 어텐션과 피드 포워드 신경망입니다. 위의 그림에서는 멀티 헤드 셀프 어텐션과 포지션 와이즈 피드 포워드 신경망이라고 적혀있지만, 멀티 헤드 셀프 어텐션은 셀프 어텐션을 병렬적으로 사용하였다는 의미고, 포지션 와이즈 피드 포워드 신경망은 우리가 알고있는 일반적인 피드 포워드 신경망입니다. 우선 셀프 어텐션에 대해서 알아봅시다.']\n",
      "[\"트랜스포머에서는 셀프 어텐션이라는 어텐션 기법이 등장하는데 앞서 배웠던 어텐션 함수에 대해서 복습하고, 셀프 어텐션이 앞서 배웠던 어텐션과 무엇이 다른지 이해해보겠습니다.\\n1) 셀프 어텐션의 의미와 이점\\n어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구합니다. 그리고 구해낸 이 유사도를 가중치로 하여 키와 맵핑되어있는 각각의 '값(Value)'에 반영해줍니다. 그리고 유사도가 반영된 '값(Value)'을 모두 가중합하여 리턴합니다.\\n[이미지: ]\\n여기까지는 앞서 배운 어텐션의 개념입니다. 그런데 어텐션 중에서는 셀프 어텐션(self-attention)이라는 것이 있습니다. 어텐션을 자기 자신에게 수행한다는 의미입니다. 앞서 배운 seq2seq에서 어텐션을 사용할 경우의 Q, K, V의 정의를 다시 생각해봅시다.\\nQ = Query : t 시점의 디코더 셀에서의 은닉 상태\\nK = Keys : 모든 시점의 인코더 셀의 은닉 상태들\", 'Q = Query : t 시점의 디코더 셀에서의 은닉 상태\\nK = Keys : 모든 시점의 인코더 셀의 은닉 상태들\\nV = Values : 모든 시점의 인코더 셀의 은닉 상태들\\n사실 t 시점이라는 것은 계속 변화하면서 반복적으로 쿼리를 수행하므로 결국 전체 시점에 대해서 일반화를 할 수도 있습니다.\\nQ = Querys : 모든 시점의 디코더 셀에서의 은닉 상태들\\nK = Keys : 모든 시점의 인코더 셀의 은닉 상태들\\nV = Values : 모든 시점의 인코더 셀의 은닉 상태들\\n이처럼 기존에는 디코더 셀의 은닉 상태가 Q이고 인코더 셀의 은닉 상태가 K라는 점에서 Q와 K가 서로 다른 값을 가지고 있었습니다. 그런데 셀프 어텐션에서는 Q, K, V가 전부 동일합니다. 트랜스포머의 셀프 어텐션에서의 Q, K, V는 아래와 같습니다.\\nQ : 입력 문장의 모든 단어 벡터들\\nK : 입력 문장의 모든 단어 벡터들\\nV : 입력 문장의 모든 단어 벡터들', \"Q : 입력 문장의 모든 단어 벡터들\\nK : 입력 문장의 모든 단어 벡터들\\nV : 입력 문장의 모든 단어 벡터들\\n셀프 어텐션에 대한 구체적인 사항을 배우기 전에 셀프 어텐션을 통해 얻을 수 있는 대표적인 효과에 대해서 이해해봅시다.\\n[이미지: ]\\n위의 그림은 트랜스포머에 대한 구글 AI 블로그 포스트에서 가져왔습니다. 위의 예시 문장을 번역하면 '그 동물은 길을 건너지 않았다. 왜냐하면 그것은 너무 피곤하였기 때문이다.' 라는 의미가 됩니다. 그런데 여기서 그것(it)에 해당하는 것은 과연 길(street)일까요? 동물(animal)일까요? 우리는 피곤한 주체가 동물이라는 것을 아주 쉽게 알 수 있지만 기계는 그렇지 않습니다. 하지만 셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구하므로서 그것(it)이 동물(animal)과 연관되었을 확률이 높다는 것을 찾아냅니다.\\n트랜스포머에서의 셀프 어텐션의 동작 메커니즘을 알아봅시다.\\n2) Q, K, V 벡터 얻기\", '트랜스포머에서의 셀프 어텐션의 동작 메커니즘을 알아봅시다.\\n2) Q, K, V 벡터 얻기\\n앞서 셀프 어텐션은 입력 문장의 단어 벡터들을 가지고 수행한다고 하였는데, 사실 셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 셀프 어텐션을 수행하는 것이 아니라 우선 각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거칩니다. 이때 이 Q벡터, K벡터, V벡터들은 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들보다 더 작은 차원을 가지는데, 논문에서는 $d_{model}$=512의 차원을 가졌던 각 단어 벡터들을 64의 차원을 가지는 Q벡터, K벡터, V벡터로 변환하였습니다.', '64라는 값은 트랜스포머의 또 다른 하이퍼파라미터인 $\\\\text{num_heads}$로 인해 결정되는데, 트랜스포머는 $d_{model}$을 $\\\\text{num_heads}$로 나눈 값을 각 Q벡터, K벡터, V벡터의 차원으로 결정합니다. 논문에서는 $\\\\text{num_heads}$를 8로하였습니다. 그림을 통해 이해해봅시다. 예를 들어 여기서 사용하고 있는 예문 중 student라는 단어 벡터를 Q, K, V의 벡터로 변환하는 과정을 보겠습니다.\\n[이미지: ]', '[이미지: ]\\n기존의 벡터로부터 더 작은 벡터는 가중치 행렬을 곱하므로서 완성됩니다. 각 가중치 행렬은 $d_{model} × (d_{model}\\\\text{/num_heads})$의 크기를 가집니다.  이 가중치 행렬은 훈련 과정에서 학습됩니다. 즉, 논문과 같이 $d_{model}$=512이고 $\\\\text{num_heads}$=8라면, 각 벡터에 3개의 서로 다른 가중치 행렬을 곱하고 64의 크기를 가지는 Q, K, V 벡터를 얻어냅니다. 위의 그림은 단어 벡터 중 student 벡터로부터 Q, K, V 벡터를 얻어내는 모습을 보여줍니다. 모든 단어 벡터에 위와 같은 과정을 거치면 I, am, a, student는 각각의 Q, K, V 벡터를 얻습니다.\\n3) 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)', '3) 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)\\nQ, K, V 벡터를 얻었다면 지금부터는 기존에 배운 어텐션 메커니즘과 동일합니다. 각 Q벡터는 모든 K벡터에 대해서 어텐션 스코어를 구하고, 어텐션 분포를 구한 뒤에 이를 사용하여 모든 V벡터를 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구하게 됩니다. 그리고 이를 모든 Q벡터에 대해서 반복합니다.', '그런데 앞서 어텐션 챕터에서 어텐션 함수의 종류는 다양하다고 언급한 바 있습니다. 트랜스포머에서는 어텐션 챕터에 사용했던 내적만을 사용하는 어텐션 함수 $score(q, k)=q⋅k$가 아니라 여기에 특정값으로 나눠준 어텐션 함수인 $score(q, k)=q⋅k/\\\\sqrt{n}$를 사용합니다. 이러한 함수를 사용하는 어텐션을 어텐션 챕터에서 배운 닷-프로덕트 어텐션(dot-product attention)에서 값을 스케일링하는 것을 추가하였다고 하여 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)이라고 합니다. 그림을 통해 이해해봅시다.\\n[이미지: ]', '[이미지: ]\\n우선 단어 I에 대한 Q벡터를 기준으로 설명해보겠습니다. 지금부터 설명하는 과정은 am에 대한 Q벡터, a에 대한 Q벡터, student에 대한 Q벡터에 대해서도 모두 동일한 과정을 거칩니다. 위의 그림은 단어 I에 대한 Q벡터가 모든 K벡터에 대해서 어텐션 스코어를 구하는 것을 보여줍니다. 위의 128과 32는 저자가 임의로 가정한 수치로 신경쓰지 않아도 좋습니다.\\n위의 그림에서 어텐션 스코어는 각각 단어 I가 단어 I, am, a, student와 얼마나 연관되어 있는지를 보여주는 수치입니다. 트랜스포머에서는 두 벡터의 내적값을 스케일링하는 값으로 K벡터의 차원을 나타내는 $d_{k}$에 루트를 씌운 $\\\\sqrt{d_{k}}$ 사용하는 것을 택했습니다. 앞서 언급하였듯이 논문에서 $d_{k}$는 $d_{model}\\\\text{/num_heads}$라는 식에 따라서 64의 값을 가지므로 $\\\\sqrt{d_{k}}$는 8의 값을 가집니다.\\n[이미지: ]', '[이미지: ]\\n이제 어텐션 스코어에 소프트맥스 함수를 사용하여 어텐션 분포(Attention Distribution)을 구하고, 각 V벡터와 가중합하여 어텐션 값(Attention Value)을 구합니다. 이를 단어 I에 대한 어텐션 값 또는 단어 I에 대한 컨텍스트 벡터(context vector)라고도 할 수 있습니다. am에 대한 Q벡터, a에 대 Q벡터, student에 대한 Q벡터에 대해서도 모두 동일한 과정을 반복하여 각각에 대한 어텐션 값을 구합니다. 그런데 한 가지 의문이 남습니다. 굳이 이렇게 각 Q벡터마다 일일히 따로 연산할 필요가 있을까요?\\n4) 행렬 연산으로 일괄 처리하기', '4) 행렬 연산으로 일괄 처리하기\\n사실 각 단어에 대한 Q, K, V 벡터를 구하고 스케일드 닷-프로덕트 어텐션을 수행하였던 위의 과정들은 벡터 연산이 아니라 행렬 연산을 사용하면 일괄 계산이 가능합니다. 지금까지 벡터 연산으로 설명하였던 이유는 이해를 돕기 위한 과정이고, 실제로는 행렬 연산으로 구현됩니다. 위의 과정을 벡터가 아닌 행렬 연산으로 이해해봅시다. 우선, 각 단어 벡터마다 일일히 가중치 행렬을 곱하는 것이 아니라 문장 행렬에 가중치 행렬을 곱하여 Q행렬, K행렬, V행렬을 구합니다.\\n[이미지: ]\\n행렬 연산을 통해 어텐션 스코어는 어떻게 구할 수 있을까요? 여기서 Q행렬을 K행렬을 전치한 행렬과 곱해준다고 해봅시다. 이렇게 되면 각각의 단어의 Q벡터와 K벡터의 내적이 각 행렬의 원소가 되는 행렬이 결과로 나옵니다.\\n[이미지: ]', '[이미지: ]\\n다시 말해 위의 그림의 결과 행렬의 값에 전체적으로 $\\\\sqrt{d_{k}}$를 나누어주면 이는 각 행과 열이 어텐션 스코어 값을 가지는 행렬이 됩니다. 예를 들어 I 행과 student 열의 값은 I의 Q벡터와 student의 K벡터의 어텐션 스코어 값입니다. 위 행렬을 어텐션 스코어 행렬이라 합시다. 어텐션 스코어 행렬을 구하였다면 남은 것은 어텐션 분포를 구하고, 이를 사용하여 모든 단어에 대한 어텐션 값을 구하는 일입니다. 이는 간단하게 어텐션 스코어 행렬에 소프트맥스 함수를 사용하고, V행렬을 곱하는 것으로 해결됩니다. 이렇게 되면 각 단어의 어텐션 값을 모두 가지는 어텐션 값 행렬이 결과로 나옵니다.\\n[이미지: ]\\n위의 그림은 행렬 연산을 통해 모든 값이 일괄 계산되는 과정을 식으로 보여줍니다. 해당 식은 실제 트랜스포머 논문에 기재된 아래의 수식과 정확하게 일치하는 식입니다.', '위의 그림은 행렬 연산을 통해 모든 값이 일괄 계산되는 과정을 식으로 보여줍니다. 해당 식은 실제 트랜스포머 논문에 기재된 아래의 수식과 정확하게 일치하는 식입니다.\\n$$Attention(Q, K, V) = softmax({QK^T\\\\over{\\\\sqrt{d_k}}})V$$\\n위의 행렬 연산에 사용된 행렬의 크기를 모두 정리해봅시다. 우선 입력 문장의 길이를 seq_len이라고 해봅시다. 그렇다면 문장 행렬의 크기는 $(\\\\text{seq_len},\\\\ d_{model})$입니다. 여기에 3개의 가중치 행렬을 곱해서 Q, K, V 행렬을 만들어야 합니다.', '우선 행렬의 크기를 정의하기 위해 행렬의 각 행에 해당되는 Q벡터와 K벡터의 차원을 $d_{k}$라고 하고, V벡터의 차원을 $d_{v}$라고 해봅시다. 그렇다면 Q행렬과 K행렬의 크기는 $(\\\\text{seq_len},\\\\ d_{k})$이며, V행렬의 크기는 $(\\\\text{seq_len},\\\\ d_{v})$가 되어야 합니다. 그렇다면 문장 행렬과 Q, K, V 행렬의 크기로부터 가중치 행렬의 크기 추정이 가능합니다. $W^{Q}$와 $W^{K}$는 $(d_{model},\\\\ d_{k})$의 크기를 가지며, $W^{V}$는 $(d_{model},\\\\ d_{v})$의 크기를 가집니다. 단, 논문에서는 $d_{k}$와 $d_{v}$의 차원은 $d_{model}\\\\text{/num_heads}$와 같습니다. 즉, $d_{model}\\\\text{/num_heads}= d_{k}=d_{v}$입니다.', '결과적으로 $softmax({QK^T\\\\over{\\\\sqrt{d_k}}})V$ 식을 적용하여 나오는 어텐션 값 행렬 $a$의 크기는 $(\\\\text{seq_len},\\\\ d_{v})$이 됩니다.\\n코드로 작성하면 아래와 같습니다.\\n5) 스케일드 닷-프로덕트 어텐션 구현하기\\ndef scaled_dot_product_attention(query, key, value, mask):\\n# query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\\n# key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\\n# value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\\n# padding_mask : (batch_size, 1, 1, key의 문장 길이)\\n# Q와 K의 곱. 어텐션 스코어 행렬.', '# padding_mask : (batch_size, 1, 1, key의 문장 길이)\\n# Q와 K의 곱. 어텐션 스코어 행렬.\\nmatmul_qk = tf.matmul(query, key, transpose_b=True)\\n# 스케일링\\n# dk의 루트값으로 나눠준다.\\ndepth = tf.cast(tf.shape(key)[-1], tf.float32)\\nlogits = matmul_qk / tf.math.sqrt(depth)\\n# 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\\n# 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\\nif mask is not None:\\nlogits += (mask * -1e9)\\n# 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\\n# attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)', '# attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\\nattention_weights = tf.nn.softmax(logits, axis=-1)\\n# output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\\noutput = tf.matmul(attention_weights, value)\\nreturn output, attention_weights\\nQ행렬과 K행렬을 전치한 행렬을 곱하고, 소프트맥스 함수를 사용하여 어텐션 분포 행렬을 얻은 뒤에 V행렬과 곱합니다. 코드에서 mask가 사용되는 if문은 아직 배우지 않은 내용으로 지금은 무시합니다.', 'scaled_dot_product_attention 함수가 정상 작동하는지 테스트를 해보겠습니다. 우선 temp_q, temp_k, temp_v라는 임의의 Query, Key, Value 행렬을 만들고, 이를 scaled_dot_product_attention 함수에 입력으로 넣어 함수가 리턴하는 값을 출력해보겠습니다.\\n# 임의의 Query, Key, Value인 Q, K, V 행렬 생성\\nnp.set_printoptions(suppress=True)\\ntemp_k = tf.constant([[10,0,0],\\n[0,10,0],\\n[0,0,10],\\n[0,0,10]], dtype=tf.float32)  # (4, 3)\\ntemp_v = tf.constant([[   1,0],\\n[  10,0],\\n[ 100,5],\\n[1000,6]], dtype=tf.float32)  # (4, 2)\\ntemp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)', 'temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\\n여기서 주목할 점은 Query에 해당하는 temp_q의 값 [0, 10, 0]은 Key에 해당하는 temp_k의 두번째 값 [0, 10, 0]과 일치한다는 점입니다. 그렇다면 어텐션 분포와 어텐션 값은 과연 어떤 값이 나올까요?\\n# 함수 실행\\ntemp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\\nprint(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\\nprint(temp_out) # 어텐션 값\\ntf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\\ntf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)', 'tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\\nQuery는 4개의 Key값 중 두번째 값과 일치하므로 어텐션 분포는 [0, 1, 0, 0]의 값을 가지며 결과적으로 Value의 두번째 값인 [10, 0]이 출력되는 것을 확인할 수 있습니다. 이번에는 Query의 값만 다른 값으로 바꿔보고 함수를 실행해봅시다. 이번에 사용할 Query값 [0, 0, 10]은 Key의 세번째 값과, 네번째 값 두 개의 값 모두와 일치하는 값입니다.\\ntemp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)\\ntemp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\\nprint(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\\nprint(temp_out) # 어텐션 값', 'print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\\nprint(temp_out) # 어텐션 값\\ntf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\\ntf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\\nQuery의 값은 Key의 세번째 값과 네번째 값 두 개의 값과 모두 유사하다는 의미에서 어텐션 분포는 [0, 0, 0.5, 0.5]의 값을 가집니다. 결과적으로 나오는 값 [550, 5.5]는 Value의 세번째 값 [100, 5]에 0.5를 곱한 값과 네번째 값 [1000, 6]에 0.5를 곱한 값의 원소별 합입니다. 이번에는 하나가 아닌 3개의 Query의 값을 함수의 입력으로 사용해보겠습니다.\\ntemp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)', 'temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\\ntemp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\\nprint(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\\nprint(temp_out) # 어텐션 값\\ntf.Tensor(\\n[[0.  0.  0.5 0.5]\\n[0.  1.  0.  0. ]\\n[0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\\ntf.Tensor(\\n[[550.    5.5]\\n[ 10.    0. ]\\n[  5.5   0. ]], shape=(3, 2), dtype=float32)\\n6) 멀티 헤드 어텐션(Multi-head Attention)', '[ 10.    0. ]\\n[  5.5   0. ]], shape=(3, 2), dtype=float32)\\n6) 멀티 헤드 어텐션(Multi-head Attention)\\n앞서 배운 어텐션에서는 $d_{model}$의 차원을 가진 단어 벡터를 $\\\\text{num_heads}$로 나눈 차원을 가지는 Q, K, V벡터로 바꾸고 어텐션을 수행하였습니다. 논문 기준으로는 512의 차원의 각 단어 벡터를 8로 나누어 64차원의 Q, K, V 벡터로 바꾸어서 어텐션을 수행한 셈인데, 이제 $\\\\text{num_heads}$의 의미와 왜 $d_{model}$의 차원을 가진 단어 벡터를 가지고 어텐션을 하지 않고 차원을 축소시킨 벡터로 어텐션을 수행하였는지 이해해보겠습니다.\\n[이미지: ]', '[이미지: ]\\n트랜스포머 연구진은 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이라고 판단하였습니다. 그래서 $d_{model}$의 차원을 $\\\\text{num_heads}$개로 나누어 $d_{model}\\\\text{/num_heads}$의 차원을 가지는 Q, K, V에 대해서 $\\\\text{num_heads}$개의 병렬 어텐션을 수행합니다. 논문에서는 하이퍼파라미터인 $\\\\text{num_heads}$의 값을 8로 지정하였고, 8개의 병렬 어텐션이 이루어지게 됩니다. 다시 말해 위에서 설명한 어텐션이 8개로 병렬로 이루어지게 되는데, 이때 각각의 어텐션 값 행렬을 어텐션 헤드라고 부릅니다. 이때 가중치 행렬 $W^{Q}, W^{K}, W^{V}$의 값은 8개의 어텐션 헤드마다 전부 다릅니다.', '병렬 어텐션으로 얻을 수 있는 효과는 무엇일까요? 그리스로마신화에는 머리가 여러 개인 괴물 히드라나 케로베로스가 나옵니다. 이 괴물들의 특징은 머리가 여러 개이기 때문에 여러 시점에서 상대방을 볼 수 있다는 겁니다. 이렇게 되면 시각에서 놓치는 게 별로 없을테니까 이런 괴물들에게 기습을 하는 것이 굉장히 힘이 들겁니다. 멀티 헤드 어텐션도 똑같습니다. 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집하겠다는 겁니다.', \"예를 들어보겠습니다. 앞서 사용한 예문 '그 동물은 길을 건너지 않았다. 왜냐하면 그것은 너무 피곤하였기 때문이다.'를 상기해봅시다. 단어 그것(it)이 쿼리였다고 해봅시다. 즉, it에 대한 Q벡터로부터 다른 단어와의 연관도를 구하였을 때 첫번째 어텐션 헤드는 '그것(it)'과 '동물(animal)'의 연관도를 높게 본다면, 두번째 어텐션 헤드는 '그것(it)'과 '피곤하였기 때문이다(tired)'의 연관도를 높게 볼 수 있습니다. 각 어텐션 헤드는 전부 다른 시각에서 보고있기 때문입니다.\\n[이미지: ]\\n병렬 어텐션을 모두 수행하였다면 모든 어텐션 헤드를 연결(concatenate)합니다. 모두 연결된 어텐션 헤드 행렬의 크기는 $(\\\\text{seq_len},\\\\ d_{model})$가 됩니다.\", '지금까지 그림에서는 책의 지면상의 한계로 4차원을 $d_{model}$=512로 표현하고, 2차원을 $d_{v}$=64로 표현해왔기 때문에 위의 그림의 행렬의 크기에 혼동의 있을 수 있으나 8개의 어텐션 헤드의 연결(concatenate) 과정의 이해를 위해 이번 행렬만 예외로 위와 같이 $d_{model}$의 크기를 $d_{v}$의 8배인 16차원으로 표현하였습니다. 아래의 그림에서는 다시 $d_{model}$를 4차원으로 표현합니다.\\n[이미지: ]\\n어텐션 헤드를 모두 연결한 행렬은 또 다른 가중치 행렬 $W^{o}$을 곱하게 되는데, 이렇게 나온 결과 행렬이 멀티-헤드 어텐션의 최종 결과물입니다. 위의 그림은 어텐션 헤드를 모두 연결한 행렬이 가중치 행렬 $W^{o}$과 곱해지는 과정을 보여줍니다. 이때 결과물인 멀티-헤드 어텐션 행렬은 인코더의 입력이었던 문장 행렬의 $(\\\\text{seq_len},\\\\ d_{model})$ 크기와 동일합니다.', '다시 말해 인코더의 첫번째 서브층인 멀티-헤드 어텐션 단계를 끝마쳤을 때, 인코더의 입력으로 들어왔던 행렬의 크기가 아직 유지되고 있음을 기억해둡시다. 첫번째 서브층인 멀티-헤드 어텐션과 두번째 서브층인 포지션 와이즈 피드 포워드 신경망을 지나면서 인코더의 입력으로 들어올 때의 행렬의 크기는 계속 유지되어야 합니다. 트랜스포머는 동일한 구조의 인코더를 쌓은 구조입니다. 논문 기준으로는 인코더가 총 6개입니다. 인코더에서의 입력의 크기가 출력에서도 동일 크기로 계속 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있습니다.\\n7) 멀티 헤드 어텐션(Multi-head Attention) 구현하기', '7) 멀티 헤드 어텐션(Multi-head Attention) 구현하기\\n멀티 헤드 어텐션에서는 크게 두 종류의 가중치 행렬이 나왔습니다. Q, K, V 행렬을 만들기 위한 가중치 행렬인 WQ, WK, WV 행렬과 바로 어텐션 헤드들을 연결(concatenation) 후에 곱해주는 WO 행렬입니다. 가중치 행렬을 곱하는 것을 구현 상에서는 입력을 전결합층. 즉, 밀집층(Dense layer)을 지나게 하여 구현합니다. 케라스 코드 상으로 지금까지 사용해왔던 Dense()에 해당됩니다.\\nDense(units)\\n멀티 헤드 어텐션의 구현은 크게 다섯 가지 파트로 구성됩니다.\\nWQ, WK, WV에 해당하는 d_model 크기의 밀집층(Dense layer)을 지나게한다.\\n지정된 헤드 수(num_heads)만큼 나눈다(split).\\n스케일드 닷 프로덕트 어텐션.\\n나눠졌던 헤드들을 연결(concatenatetion)한다.\\nWO에 해당하는 밀집층을 지나게 한다.', '스케일드 닷 프로덕트 어텐션.\\n나눠졌던 헤드들을 연결(concatenatetion)한다.\\nWO에 해당하는 밀집층을 지나게 한다.\\n이론으로 설명할 때보다 심플하게 구성되었는데 결국 근본적으로 동일한 내용입니다.\\nclass MultiHeadAttention(tf.keras.layers.Layer):\\ndef __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\\nsuper(MultiHeadAttention, self).__init__(name=name)\\nself.num_heads = num_heads\\nself.d_model = d_model\\nassert d_model % self.num_heads == 0\\n# d_model을 num_heads로 나눈 값.\\n# 논문 기준 : 64\\nself.depth = d_model // self.num_heads\\n# WQ, WK, WV에 해당하는 밀집층 정의', '# 논문 기준 : 64\\nself.depth = d_model // self.num_heads\\n# WQ, WK, WV에 해당하는 밀집층 정의\\nself.query_dense = tf.keras.layers.Dense(units=d_model)\\nself.key_dense = tf.keras.layers.Dense(units=d_model)\\nself.value_dense = tf.keras.layers.Dense(units=d_model)\\n# WO에 해당하는 밀집층 정의\\nself.dense = tf.keras.layers.Dense(units=d_model)\\n# num_heads 개수만큼 q, k, v를 split하는 함수\\ndef split_heads(self, inputs, batch_size):\\ninputs = tf.reshape(\\ninputs, shape=(batch_size, -1, self.num_heads, self.depth))', \"inputs = tf.reshape(\\ninputs, shape=(batch_size, -1, self.num_heads, self.depth))\\nreturn tf.transpose(inputs, perm=[0, 2, 1, 3])\\ndef call(self, inputs):\\nquery, key, value, mask = inputs['query'], inputs['key'], inputs[\\n'value'], inputs['mask']\\nbatch_size = tf.shape(query)[0]\\n# 1. WQ, WK, WV에 해당하는 밀집층 지나기\\n# q : (batch_size, query의 문장 길이, d_model)\\n# k : (batch_size, key의 문장 길이, d_model)\\n# v : (batch_size, value의 문장 길이, d_model)\\n# 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\", '# 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\\nquery = self.query_dense(query)\\nkey = self.key_dense(key)\\nvalue = self.value_dense(value)\\n# 2. 헤드 나누기\\n# q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\\n# k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\\n# v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\\nquery = self.split_heads(query, batch_size)\\nkey = self.split_heads(key, batch_size)\\nvalue = self.split_heads(value, batch_size)', 'key = self.split_heads(key, batch_size)\\nvalue = self.split_heads(value, batch_size)\\n# 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\\n# (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\\nscaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\\n# (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\\nscaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\\n# 4. 헤드 연결(concatenate)하기\\n# (batch_size, query의 문장 길이, d_model)\\nconcat_attention = tf.reshape(scaled_attention,', '# (batch_size, query의 문장 길이, d_model)\\nconcat_attention = tf.reshape(scaled_attention,\\n(batch_size, -1, self.d_model))\\n# 5. WO에 해당하는 밀집층 지나기\\n# (batch_size, query의 문장 길이, d_model)\\noutputs = self.dense(concat_attention)\\nreturn outputs\\n멀티 헤드 어텐션 함수를 구현해보았습니다. 한 가지만 더 설명하고, 두번째 서브층에 대한 설명으로 넘어가봅시다.\\n8) 패딩 마스크(Padding Mask)\\n아직 설명하지 않은 내용이 있습니다. 앞서 구현한 스케일드 닷 프로덕트 어텐션 함수 내부를 보면 mask라는 값을 인자로 받아서, 이 mask값에다가 -1e9라는 아주 작은 음수값을 곱한 후 어텐션 스코어 행렬에 더해주고 있습니다. 이 연산의 정체는 무엇일까요?', 'def scaled_dot_product_attention(query, key, value, mask):\\n... 중략 ...\\nlogits += (mask * -1e9) # 어텐션 스코어 행렬인 logits에 mask*-1e9 값을 더해주고 있다.\\n... 중략 ...\\n이는 입력 문장에 <PAD> 토큰이 있을 경우 어텐션에서 사실상 제외하기 위한 연산입니다. 예를 들어 <PAD>가 포함된 입력 문장의 셀프 어텐션의 예제를 봅시다. 이에 대해서 어텐션을 수행하고 어텐션 스코어 행렬을 얻는 과정은 다음과 같습니다.\\n[이미지: ]', '[이미지: ]\\n그런데 사실 단어 <PAD>의 경우에는 실질적인 의미를 가진 단어가 아닙니다. 그래서 트랜스포머에서는 Key의 경우에 <PAD> 토큰이 존재한다면 이에 대해서는 유사도를 구하지 않도록 마스킹(Masking)을 해주기로 했습니다. 여기서 마스킹이란 어텐션에서 제외하기 위해 값을 가린다는 의미입니다. 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query이고, 열에 해당하는 문장은 Key입니다. 그리고 Key에 <PAD>가 있는 경우에는 해당 열 전체를 마스킹을 해줍니다.\\n[이미지: ]', '[이미지: ]\\n마스킹을 하는 방법은 어텐션 스코어 행렬의 마스킹 위치에 매우 작은 음수값을 넣어주는 것입니다. 여기서 매우 작은 음수값이라는 것은 -1,000,000,000과 같은 -무한대에 가까운 수라는 의미입니다. 현재 어텐션 스코어 함수는 소프트맥스 함수를 지나지 않은 상태입니다. 앞서 배운 연산 순서라면 어텐션 스코어 함수는 소프트맥스 함수를 지나고, 그 후 Value 행렬과 곱해지게 됩니다. 그런데 현재 마스킹 위치에 매우 작은 음수 값이 들어가 있으므로 어텐션 스코어 행렬이 소프트맥스 함수를 지난 후에는 해당 위치의 값은 0이 되어 단어 간 유사도를 구하는 일에 <PAD> 토큰이 반영되지 않게 됩니다.\\n[이미지: ]\\n위 그림은 소프트맥스 함수를 지난 후를 가정하고 있습니다. 소프트맥스 함수를 지나면 각 행의 어텐션 가중치의 총 합은 1이 되는데, 단어 <PAD>의 경우에는 0이 되어 어떤 유의미한 값을 가지고 있지 않습니다.', '패딩 마스크를 구현하는 방법은 입력된 정수 시퀀스에서 패딩 토큰의 인덱스인지, 아닌지를 판별하는 함수를 구현하는 것입니다. 아래의 함수는 정수 시퀀스에서 0인 경우에는 1로 변환하고, 그렇지 않은 경우에는 0으로 변환하는 함수입니다.\\ndef create_padding_mask(x):\\nmask = tf.cast(tf.math.equal(x, 0), tf.float32)\\n# (batch_size, 1, 1, key의 문장 길이)\\nreturn mask[:, tf.newaxis, tf.newaxis, :]\\n임의의 정수 시퀀스 입력을 넣어서 어떻게 변환되는지 보겠습니다.\\nprint(create_padding_mask(tf.constant([[1, 21, 777, 0, 0]])))\\ntf.Tensor([[[[0. 0. 0. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)', 'tf.Tensor([[[[0. 0. 0. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)\\n위 벡터를 통해서 1의 값을 가진 위치의 열을 어텐션 스코어 행렬에서 마스킹하는 용도로 사용할 수 있습니다. 위 벡터를 스케일드 닷 프로덕트 어텐션의 인자로 전달하면, 스케일드 닷 프로덕트 어텐션에서는 위 벡터에다가 매우 작은 음수값인 -1e9를 곱하고, 이를 행렬에 더해주어 해당 열을 전부 마스킹합니다.\\n첫번째 서브층인 멀티 헤드 어텐션을 구현해보았습니다. 앞서 인코더는 두 개의 서브 서브층(sublayer)으로 나뉘어진다고 언급한 적이 있는데, 두번째 서브층인 포지션-와이즈 피드 포워드 신경망에 대해서 알아보겠습니다.']\n",
      "['지금은 인코더를 설명하고 있지만, 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층입니다. 포지션-와이즈 FFNN는 쉽게 말하면 완전 연결 FFNN(Fully-connected FFNN)이라고 해석할 수 있습니다. 앞서 인공 신경망은 결국 벡터와 행렬 연산으로 표현될 수 있음을 배웠습니다. 아래는 포지션 와이즈 FFNN의 수식을 보여줍니다.\\n$FFNN(x) = MAX(0, x{W_{1}} + b_{1}){W_2} + b_2$\\n식을 그림으로 표현하면 아래와 같습니다.\\n[이미지: ]', '$FFNN(x) = MAX(0, x{W_{1}} + b_{1}){W_2} + b_2$\\n식을 그림으로 표현하면 아래와 같습니다.\\n[이미지: ]\\n여기서 $x$는 앞서 멀티 헤드 어텐션의 결과로 나온 $(\\\\text{seq_len},\\\\ d_{model})$의 크기를 가지는 행렬을 말합니다. 가중치 행렬 $W_{1}$은 $(d_{model},\\\\ d_{ff})$의 크기를 가지고, 가중치 행렬 $W_{2}$은 $(d_{ff},\\\\ d_{model})$의 크기를 가집니다. 논문에서 은닉층의 크기인 $d_{ff}$는 앞서 하이퍼파라미터를 정의할 때 언급했듯이 2,048의 크기를 가집니다.\\n여기서 매개변수 $W_{1}$, $b_{1}$, $W_{2}$, $b_{2}$는 하나의 인코더 층 내에서는 다른 문장, 다른 단어들마다 정확하게 동일하게 사용됩니다. 하지만 인코더 층마다는 다른 값을 가집니다.\\n[이미지: ]', \"[이미지: ]\\n위의 그림에서 좌측은 인코더의 입력을 벡터 단위로 봤을 때, 각 벡터들이 멀티 헤드 어텐션 층이라는 인코더 내 첫번째 서브 층을 지나 FFNN을 통과하는 것을 보여줍니다. 이는 두번째 서브층인 Position-wise FFNN을 의미합니다. 물론, 실제로는 그림의 우측과 같이 행렬로 연산되는데, 두번째 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(\\\\text{seq_len},\\\\ d_{model})$의 크기가 보존되고 있습니다. 하나의 인코더 층을 지난 이 행렬은 다음 인코더 층으로 전달되고, 다음 층에서도 동일한 인코더 연산이 반복됩니다.\\n이를 구현하면 다음과 같습니다.\\n# 다음의 코드는 인코더와 디코더 내부에서 사용할 예정입니다.\\noutputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\", \"outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\\noutputs = tf.keras.layers.Dense(units=d_model)(outputs)\"]\n",
      "['[이미지: ]\\n인코더의 두 개의 서브층에 대해서 이해하였다면 인코더에 대한 설명은 거의 끝났습니다! 트랜스포머에서는 이러한 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 기법이 있는데, 바로 Add & Norm입니다. 더 정확히는 잔차 연결(residual connection)과 층 정규화(layer normalization)를 의미합니다.\\n위의 그림은 앞서 Position-wise FFNN를 설명할 때 사용한 앞선 그림에서 화살표와 Add & Norm(잔차 연결과 정규화 과정)을 추가한 그림입니다. 추가된 화살표들은 서브층 이전의 입력에서 시작되어 서브층의 출력 부분을 향하고 있는 것에 주목합시다. 추가된 화살표가 어떤 의미를 갖고 있는지는 잔차 연결과 층 정규화를 배우고 나면 이해할 수 있습니다.\\n1) 잔차 연결(Residual connection)\\n잔차 연결(residual connection)의 의미를 이해하기 위해서 어떤 함수 $H(x)$에 대한 이야기를 해보겠습니다.', '잔차 연결(residual connection)의 의미를 이해하기 위해서 어떤 함수 $H(x)$에 대한 이야기를 해보겠습니다.\\n[이미지: ]\\n위 그림은 입력 $x$와 $x$에 대한 어떤 함수 $F(x)$의 값을 더한 함수 $H(x)$의 구조를 보여줍니다. 어떤 함수 $F(x)$가 트랜스포머에서는 서브층에 해당됩니다. 다시 말해 잔차 연결은 서브층의 입력과 출력을 더하는 것을 말합니다. 앞서 언급했듯이 트랜스포머에서 서브층의 입력과 출력은 동일한 차원을 갖고 있으므로, 서브층의 입력과 서브층의 출력은 덧셈 연산을 할 수 있습니다. 이것이 바로 위의 인코더 그림에서 각 화살표가 서브층의 입력에서 출력으로 향하도록 그려졌던 이유입니다. 잔차 연결은 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법입니다.\\n이를 식으로 표현하면 $x+Sublayer(x)$입니다.\\n가령, 서브층이 멀티 헤드 어텐션이었다면 잔차 연결 연산은 다음과 같습니다.', '이를 식으로 표현하면 $x+Sublayer(x)$입니다.\\n가령, 서브층이 멀티 헤드 어텐션이었다면 잔차 연결 연산은 다음과 같습니다.\\n$H(x) = x+Multi-head\\\\ Attention(x)$\\n[이미지: ]\\n위 그림은 멀티 헤드 어텐션의 입력과 멀티 헤드 어텐션의 결과가 더해지는 과정을 보여줍니다.\\n관련 논문 : https://arxiv.org/pdf/1512.03385.pdf\\n2) 층 정규화(Layer Normalization)\\n잔차 연결을 거친 결과는 이어서 층 정규화 과정을 거치게됩니다. 잔차 연결의 입력을 $x$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 $LN$이라고 하였을 때, 잔차 연결 후 층 정규화 연산을 수식으로 표현하자면 다음과 같습니다.\\n$LN = LayerNorm(x+Sublayer(x))$', '$LN = LayerNorm(x+Sublayer(x))$\\n층 정규화를 하는 과정에 대해서 이해해봅시다. 층 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 돕습니다. 여기서 텐서의 마지막 차원이란 것은 트랜스포머에서는 $d_{model}$ 차원을 의미합니다. 아래 그림은 $d_{model}$ 차원의 방향을 화살표로 표현하였습니다.\\n[이미지: ]\\n층 정규화를 위해서 우선, 화살표 방향으로 각각 평균 $μ$과 분산 $σ^{2}$을 구합니다. 각 화살표 방향의 벡터를 $x_{i}$라고 명명해봅시다.\\n[이미지: ]\\n층 정규화를 수행한 후에는 벡터 $x_{i}$는 $ln_{i}$라는 벡터로 정규화가 됩니다.\\n$ln_{i} = LayerNorm(x_{i})$', '[이미지: ]\\n층 정규화를 수행한 후에는 벡터 $x_{i}$는 $ln_{i}$라는 벡터로 정규화가 됩니다.\\n$ln_{i} = LayerNorm(x_{i})$\\n층 정규화의 수식을 알아봅시다. 여기서는 층 정규화를 두 가지 과정으로 나누어서 설명하겠습니다. 첫번째는 평균과 분산을 통한 정규화, 두번째는 감마와 베타를 도입하는 것입니다. 우선, 평균과 분산을 통해 벡터 $x_{i}$를 정규화 해줍니다. $x_{i}$는 벡터인 반면, 평균 $μ_{i}$과 분산 $σ^{2}_{i}$은 스칼라입니다. 벡터 $x_{i}$의 각 차원을 $k$라고 하였을 때, $x_{i,k}$는 다음의 수식과 같이 정규화 할 수 있습니다. 다시 말해 벡터 $x_{i}$의 각 $k$차원의 값이 다음과 같이 정규화 되는 것입니다.\\n$$\\\\hat{x}_{i, k} = \\\\frac{x_{i, k}-μ_{i}}{\\\\sqrt{σ^{2}_{i}+\\\\epsilon}}$$\\n$ϵ$(입실론)은 분모가 0이 되는 것을 방지하는 값입니다.', '$ϵ$(입실론)은 분모가 0이 되는 것을 방지하는 값입니다.\\n이제 $γ$(감마)와 $β$(베타)라는 벡터를 준비합니다. 단, 이들의 초기값은 각각 1과 0입니다.\\n[이미지: ]\\n$γ$와 $β$를 도입한 층 정규화의 최종 수식은 다음과 같으며 $γ$와 $β$는 학습 가능한 파라미터입니다.\\n$$ln_{i} = γ\\\\hat{x}_{i}+β = LayerNorm(x_{i})$$\\n관련 논문 : https://arxiv.org/pdf/1607.06450.pdf\\n케라스에서는 층 정규화를 위한 LayerNormalization()를 제공하고 있으므로 이를 가져와 사용합니다.']\n",
      "['지금까지 배운 내용을 바탕으로 인코더를 구현한 코드는 다음과 같습니다. 인코더의 입력으로 들어가는 문장에는 패딩이 있을 수 있으므로, 어텐션 시 패딩 토큰을 제외하도록 패딩 마스크를 사용합니다. 이는 MultiHeadAttention 함수의 mask의 인자값으로 padding_mask가 사용되는 이유입니다. 인코더는 총 두 개의 서브층으로 이루어지는데, 멀티 헤드 어텐션과 피드 포워드 신경망입니다. 각 서브층 이후에는 드롭 아웃, 잔차 연결과 층 정규화가 수행됩니다.\\ndef encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\\ninputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\\n# 인코더는 패딩 마스크 사용\\npadding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")', '# 인코더는 패딩 마스크 사용\\npadding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\\n# 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\\nattention = MultiHeadAttention(\\nd_model, num_heads, name=\"attention\")({\\n\\'query\\': inputs, \\'key\\': inputs, \\'value\\': inputs, # Q = K = V\\n\\'mask\\': padding_mask # 패딩 마스크 사용\\n})\\n# 드롭아웃 + 잔차 연결과 층 정규화\\nattention = tf.keras.layers.Dropout(rate=dropout)(attention)\\nattention = tf.keras.layers.LayerNormalization(\\nepsilon=1e-6)(inputs + attention)\\n# 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)', \"epsilon=1e-6)(inputs + attention)\\n# 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\\noutputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\\noutputs = tf.keras.layers.Dense(units=d_model)(outputs)\\n# 드롭아웃 + 잔차 연결과 층 정규화\\noutputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\\noutputs = tf.keras.layers.LayerNormalization(\\nepsilon=1e-6)(attention + outputs)\\nreturn tf.keras.Model(\\ninputs=[inputs, padding_mask], outputs=outputs, name=name)\", 'return tf.keras.Model(\\ninputs=[inputs, padding_mask], outputs=outputs, name=name)\\n위 코드는 하나의 인코더 블록. 즉, 하나의 인코더 층을 구현하는 코드입니다. 실제로 트랜스포머는 num_layers 개수만큼의 인코더 층을 사용하므로 이를 여러번 쌓는 코드를 별도 구현해줄 필요가 있습니다.']\n",
      "['지금까지 인코더 층의 내부 아키텍처에 대해서 이해해보았습니다. 이러한 인코더 층을 num_layers개만큼 쌓고, 마지막 인코더 층에서 얻는 (seq_len, d_model) 크기의 행렬을 디코더로 보내주므로서 트랜스포머 인코더의 인코딩 연산이 끝나게 됩니다. 아래의 코드는 인코더 층을 num_layers개만큼 쌓는 코드입니다.\\ndef encoder(vocab_size, num_layers, dff,\\nd_model, num_heads, dropout,\\nname=\"encoder\"):\\ninputs = tf.keras.Input(shape=(None,), name=\"inputs\")\\n# 인코더는 패딩 마스크 사용\\npadding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\\n# 포지셔널 인코딩 + 드롭아웃\\nembeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)', '# 포지셔널 인코딩 + 드롭아웃\\nembeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\\nembeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\\nembeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\\noutputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\\n# 인코더를 num_layers개 쌓기\\nfor i in range(num_layers):\\noutputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\\ndropout=dropout, name=\"encoder_layer_{}\".format(i),\\n)([outputs, padding_mask])\\nreturn tf.keras.Model(', ')([outputs, padding_mask])\\nreturn tf.keras.Model(\\ninputs=[inputs, padding_mask], outputs=outputs, name=name)']\n",
      "['[이미지: ]\\n지금까지 인코더에 대해서 정리해보았습니다. 이렇게 구현된 인코더는 총 $\\\\text{num_layers}$만큼의 층 연산을 순차적으로 한 후에 마지막 층의 인코더의 출력을 디코더에게 전달합니다. 인코더 연산이 끝났으면 디코더 연산이 시작되어 디코더 또한 $\\\\text{num_layers}$만큼의 연산을 하는데, 이때마다 인코더가 보낸 출력을 각 디코더 층 연산에 사용합니다. 디코더에 대해서 이해해봅시다.']\n",
      "['[이미지: ]\\n위 그림과 같이 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됩니다. 트랜스포머 또한 seq2seq와 마찬가지로 교사 강요(Teacher Forcing)을 사용하여 훈련되므로 학습 과정에서 디코더는 번역할 문장에 해당되는 <sos> je suis étudiant의 문장 행렬을 한 번에 입력받습니다. 그리고 디코더는 이 문장 행렬로부터 각 시점의 단어를 예측하도록 훈련됩니다.', \"여기서 문제가 있습니다. seq2seq의 디코더에 사용되는 RNN 계열의 신경망은 입력 단어를 매 시점마다 순차적으로 입력받으므로 다음 단어 예측에 현재 시점을 포함한 이전 시점에 입력된 단어들만 참고할 수 있습니다. 반면, 트랜스포머는 문장 행렬로 입력을 한 번에 받으므로 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 미래 시점의 단어까지도 참고할 수 있는 현상이 발생합니다. 가령, suis를 예측해야 하는 시점이라고 해봅시다. RNN 계열의 seq2seq의 디코더라면 현재까지 디코더에 입력된 단어는 <sos>와 je뿐일 것입니다. 반면, 트랜스포머는 이미 문장 행렬로 <sos> je suis étudiant를 입력받았습니다.\\n이를 위해 트랜스포머의 디코더에서는 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크(look-ahead mask)를 도입했습니다. 직역하면 '미리보기에 대한 마스크'입니다.\\n[이미지: ]\", '[이미지: ]\\n룩-어헤드 마스크(look-ahead mask)는 디코더의 첫번째 서브층에서 이루어집니다. 디코더의 첫번째 서브층인 멀티 헤드 셀프 어텐션 층은 인코더의 첫번째 서브층인 멀티 헤드 셀프 어텐션 층과 동일한 연산을 수행합니다. 오직 다른 점은 어텐션 스코어 행렬에서 마스킹을 적용한다는 점만 다릅니다. 우선 다음과 같이 셀프 어텐션을 통해 어텐션 스코어 행렬을 얻습니다.\\n[이미지: ]\\n이제 자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 다음과 같이 마스킹합니다.\\n[이미지: ]\\n마스킹 된 후의 어텐션 스코어 행렬의 각 행을 보면 자기 자신과 그 이전 단어들만을 참고할 수 있음을 볼 수 있습니다. 그 외에는 근본적으로 셀프 어텐션이라는 점과, 멀티 헤드 어텐션을 수행한다는 점에서 인코더의 첫번째 서브층과 같습니다.', '룩-어헤드 마스크의 구현에 대해 알아봅시다. 룩-어헤드 마스크는 패딩 마스크와 마찬가지로 앞서 구현한 스케일드 닷 프로덕트 어텐션 함수에 mask라는 인자로 전달됩니다. 패딩 마스킹을 써야하는 경우에는 스케일드 닷 프로덕트 어텐션 함수에 패딩 마스크를 전달하고, 룩-어헤드 마스킹을 써야하는 경우에는 스케일드 닷 프로덕트 어텐션 함수에 룩-어헤드 마스크를 전달합니다.\\n# 스케일드 닷 프로덕트 어텐션 함수를 다시 복습해봅시다.\\ndef scaled_dot_product_attention(query, key, value, mask):\\n... 중략 ...\\nlogits += (mask * -1e9) # 어텐션 스코어 행렬인 logits에 mask*-1e9 값을 더해주고 있다.\\n... 중략 ...', '... 중략 ...\\nlogits += (mask * -1e9) # 어텐션 스코어 행렬인 logits에 mask*-1e9 값을 더해주고 있다.\\n... 중략 ...\\n트랜스포머에는 총 세 가지 어텐션이 존재하며, 모두 멀티 헤드 어텐션을 수행하고, 멀티 헤드 어텐션 함수 내부에서 스케일드 닷 프로덕트 어텐션 함수를 호출하는데 각 어텐션 시 함수에 전달하는 마스킹은 다음과 같습니다.\\n인코더의 셀프 어텐션 : 패딩 마스크를 전달\\n디코더의 첫번째 서브층인 마스크드 셀프 어텐션 : 룩-어헤드 마스크를 전달 <-- 지금 설명하고 있음.\\n디코더의 두번째 서브층인 인코더-디코더 어텐션 : 패딩 마스크를 전달\\n이때 룩-어헤드 마스크를 한다고해서 패딩 마스크가 불필요한 것이 아니므로 룩-어헤드 마스크는 패딩 마스크를 포함하도록 구현합니다. 룩-어헤드 마스크를 구현하는 방법은 패딩 마스크 때와 마찬가지로 마스킹을 하고자 하는 위치에는 1을, 마스킹을 하지 않는 위치에는 0을 리턴하도록 합니다.', '# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\\ndef create_look_ahead_mask(x):\\nseq_len = tf.shape(x)[1]\\nlook_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\\npadding_mask = create_padding_mask(x) # 패딩 마스크도 포함\\nreturn tf.maximum(look_ahead_mask, padding_mask)\\n임의의 정수 시퀀스 입력을 넣어서 결과를 봅시다. 패딩 마스크를 테스트 하기위해 세번째 위치에 정수 0을 넣었습니다.\\nprint(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))\\ntf.Tensor(\\n[[[[0. 1. 1. 1. 1.]\\n[0. 0. 1. 1. 1.]\\n[0. 0. 1. 1. 1.]\\n[0. 0. 1. 0. 1.]', 'tf.Tensor(\\n[[[[0. 1. 1. 1. 1.]\\n[0. 0. 1. 1. 1.]\\n[0. 0. 1. 1. 1.]\\n[0. 0. 1. 0. 1.]\\n[0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\\n룩-어헤드 마스크이므로 삼각형 모양의 마스킹이 형성되면서 패딩 마스크가 포함되어져 있는 세번째 열이 마스킹됩니다.']\n",
      "['디코더의 두번째 서브층에 대해서 이해해봅시다. 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서는 이전의 어텐션들(인코더와 디코더의 첫번째 서브층)과는 공통점이 있으나 이번에는 셀프 어텐션이 아닙니다.\\n셀프 어텐션은 Query, Key, Value가 같은 경우를 말하는데, 인코더-디코더 어텐션은 Query가 디코더인 행렬인 반면, Key와 Value는 인코더 행렬이기 때문입니다. 다시 한 번 각 서브층에서의 Q, K, V의 관계를 정리해봅시다.\\n인코더의 첫번째 서브층 : Query = Key = Value\\n디코더의 첫번째 서브층 : Query = Key = Value\\n디코더의 두번째 서브층 : Query : 디코더 행렬 / Key = Value : 인코더 행렬\\n디코더의 두번째 서브층을 확대해보면, 다음과 같이 인코더로부터 두 개의 화살표가 그려져 있습니다.\\n[이미지: ]', '디코더의 두번째 서브층을 확대해보면, 다음과 같이 인코더로부터 두 개의 화살표가 그려져 있습니다.\\n[이미지: ]\\n두 개의 화살표는 각각 Key와 Value를 의미하며 이는 인코더의 마지막 층에서 온 행렬로부터 얻습니다. 반면 Query는 디코더의 첫번째 서브층의 결과 행렬로부터 얻는다는 점이 다릅니다. Query가 디코더 행렬, Key가 인코더 행렬일 때, 어텐션 스코어 행렬을 구하는 과정은 다음과 같습니다.\\n[이미지: ]\\n그 외에 멀티 헤드 어텐션을 수행하는 과정은 다른 어텐션들과 같습니다.']\n",
      "['디코더는 총 세 개의 서브층으로 구성됩니다. 첫번째와 두번째 서브층 모두 멀티 헤드 어텐션이지만, 첫번째 서브층은 mask의 인자값으로 look_ahead_mask가 들어가는 반면, 두번째 서브층은 mask의 인자값으로 padding_mask가 들어가는 것을 확인할 수 있습니다. 이는 첫번째 서브층은 마스크드 셀프 어텐션을 수행하기 때문입니다. 세 개의 서브층 모두 서브층 연산 후에는 드롭 아웃, 잔차 연결, 층 정규화가 수행되는 것을 확인할 수 있습니다.\\ndef decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\\ninputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\\nenc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\\n# 룩어헤드 마스크(첫번째 서브층)', 'enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\\n# 룩어헤드 마스크(첫번째 서브층)\\nlook_ahead_mask = tf.keras.Input(\\nshape=(1, None, None), name=\"look_ahead_mask\")\\n# 패딩 마스크(두번째 서브층)\\npadding_mask = tf.keras.Input(shape=(1, 1, None), name=\\'padding_mask\\')\\n# 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\\nattention1 = MultiHeadAttention(\\nd_model, num_heads, name=\"attention_1\")(inputs={\\n\\'query\\': inputs, \\'key\\': inputs, \\'value\\': inputs, # Q = K = V\\n\\'mask\\': look_ahead_mask # 룩어헤드 마스크\\n})\\n# 잔차 연결과 층 정규화', '\\'mask\\': look_ahead_mask # 룩어헤드 마스크\\n})\\n# 잔차 연결과 층 정규화\\nattention1 = tf.keras.layers.LayerNormalization(\\nepsilon=1e-6)(attention1 + inputs)\\n# 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\\nattention2 = MultiHeadAttention(\\nd_model, num_heads, name=\"attention_2\")(inputs={\\n\\'query\\': attention1, \\'key\\': enc_outputs, \\'value\\': enc_outputs, # Q != K = V\\n\\'mask\\': padding_mask # 패딩 마스크\\n})\\n# 드롭아웃 + 잔차 연결과 층 정규화\\nattention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\\nattention2 = tf.keras.layers.LayerNormalization(', \"attention2 = tf.keras.layers.LayerNormalization(\\nepsilon=1e-6)(attention2 + attention1)\\n# 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\\noutputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\\noutputs = tf.keras.layers.Dense(units=d_model)(outputs)\\n# 드롭아웃 + 잔차 연결과 층 정규화\\noutputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\\noutputs = tf.keras.layers.LayerNormalization(\\nepsilon=1e-6)(outputs + attention2)\\nreturn tf.keras.Model(\\ninputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\", 'return tf.keras.Model(\\ninputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\\noutputs=outputs,\\nname=name)\\n인코더와 마찬가지로 디코더도 num_layers개만큼 쌓는 코드가 필요합니다.']\n",
      "[\"포지셔널 인코딩 후 디코더 층을 num_layers의 개수만큼 쌓는 코드입니다.\\ndef decoder(vocab_size, num_layers, dff,\\nd_model, num_heads, dropout,\\nname='decoder'):\\ninputs = tf.keras.Input(shape=(None,), name='inputs')\\nenc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\\n# 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\\nlook_ahead_mask = tf.keras.Input(\\nshape=(1, None, None), name='look_ahead_mask')\\npadding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\\n# 포지셔널 인코딩 + 드롭아웃\", \"padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\\n# 포지셔널 인코딩 + 드롭아웃\\nembeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\\nembeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\\nembeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\\noutputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\\n# 디코더를 num_layers개 쌓기\\nfor i in range(num_layers):\\noutputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\", \"for i in range(num_layers):\\noutputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\\ndropout=dropout, name='decoder_layer_{}'.format(i),\\n)(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\\nreturn tf.keras.Model(\\ninputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\\noutputs=outputs,\\nname=name)\"]\n",
      "['지금까지 구현한 인코더와 디코더 함수를 조합하여 트랜스포머를 조립할 차례입니다. 인코더의 출력은 디코더에서 인코더-디코더 어텐션에서 사용되기 위해 디코더로 전달해줍니다. 그리고 디코더의 끝단에는 다중 클래스 분류 문제를 풀 수 있도록, vocab_size 만큼의 뉴런을 가지는 출력층을 추가해줍니다.\\ndef transformer(vocab_size, num_layers, dff,\\nd_model, num_heads, dropout,\\nname=\"transformer\"):\\n# 인코더의 입력\\ninputs = tf.keras.Input(shape=(None,), name=\"inputs\")\\n# 디코더의 입력\\ndec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\\n# 인코더의 패딩 마스크\\nenc_padding_mask = tf.keras.layers.Lambda(', \"# 인코더의 패딩 마스크\\nenc_padding_mask = tf.keras.layers.Lambda(\\ncreate_padding_mask, output_shape=(1, 1, None),\\nname='enc_padding_mask')(inputs)\\n# 디코더의 룩어헤드 마스크(첫번째 서브층)\\nlook_ahead_mask = tf.keras.layers.Lambda(\\ncreate_look_ahead_mask, output_shape=(1, None, None),\\nname='look_ahead_mask')(dec_inputs)\\n# 디코더의 패딩 마스크(두번째 서브층)\\ndec_padding_mask = tf.keras.layers.Lambda(\\ncreate_padding_mask, output_shape=(1, 1, None),\\nname='dec_padding_mask')(inputs)\\n# 인코더의 출력은 enc_outputs. 디코더로 전달된다.\", \"name='dec_padding_mask')(inputs)\\n# 인코더의 출력은 enc_outputs. 디코더로 전달된다.\\nenc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\\nd_model=d_model, num_heads=num_heads, dropout=dropout,\\n)(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\\n# 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\\ndec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\\nd_model=d_model, num_heads=num_heads, dropout=dropout,\", 'd_model=d_model, num_heads=num_heads, dropout=dropout,\\n)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\\n# 다음 단어 예측을 위한 출력층\\noutputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\\nreturn tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)']\n",
      "['트랜스포머의 하이퍼파라미터를 임의로 정하고 모델을 만들어봅시다. 현재 훈련 데이터가 존재하는 것은 아니지만, 단어 집합의 크기는 임의로 9,000으로 정합니다. 단어 집합의 크기로부터 룩업 테이블을 수행할 임베딩 테이블과 포지셔널 인코딩 행렬의 행의 크기를 결정할 수 있습니다.\\n논문에서 제시한 것과는 다르게 하이퍼파라미터를 정해보겠습니다. 인코더와 디코더의 층의 개수 $\\\\text{num_layers}$는 4개, 인코더와 디코더의 포지션 와이즈 피드 포워드 신경망의 은닉층 $d_{ff}$은 128, 인코더와 디코더의 입, 출력의 차원 $d_{model}$은 128, 멀티-헤드 어텐션에서 병렬적으로 사용할 헤드의 수 $\\\\text{num_heads}$는 4로 정했습니다. 128/4의 값인 32가 $d_{v}$의 값이 되겠습니다.\\nsmall_transformer = transformer(\\nvocab_size = 9000,\\nnum_layers = 4,\\ndff = 512,', 'small_transformer = transformer(\\nvocab_size = 9000,\\nnum_layers = 4,\\ndff = 512,\\nd_model = 128,\\nnum_heads = 4,\\ndropout = 0.3,\\nname=\"small_transformer\")\\ntf.keras.utils.plot_model(\\nsmall_transformer, to_file=\\'small_transformer.png\\', show_shapes=True)']\n",
      "[\"다중 클래스 분류 문제를 풀 예정이므로 크로스 엔트로피 함수를 손실 함수로 정의합니다.\\ndef loss_function(y_true, y_pred):\\ny_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\\nloss = tf.keras.losses.SparseCategoricalCrossentropy(\\nfrom_logits=True, reduction='none')(y_true, y_pred)\\nmask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\\nloss = tf.multiply(loss, mask)\\nreturn tf.reduce_mean(loss)\"]\n",
      "['학습률 스케줄러(Learning rate Scheduler)는 미리 학습 일정을 정해두고 그 일정에 따라 학습률이 조정되는 방법입니다. 트랜스포머의 경우 사용자가 정한 단계까지는 학습률을 증가시켰다가 단계에 이르면 학습률을 점차적으로 떨어트리는 방식을 사용합니다. 좀 더 구체적으로 봅시다. step_num(단계)이란 옵티마이저가 매개변수를 업데이트 하는 한 번의 진행 횟수를 의미합니다. 트랜스포머에서는 warmup_steps이라는 변수를 정하고 step_num이 warmup_steps보다 작을 경우는 학습률을 선형적으로 증가 시키고, step_num이 warmup_steps에 도달하게 되면 학습률을 step_num의 역제곱근에 따라서 감소시킵니다. 이를 식으로 표현하면 아래와 같습니다. warmup_steps의 값으로는 4,000을 사용하였습니다.\\n$$', '$$\\n\\\\Large{lrate = d_{model}^{-0.5} × min(\\\\text{step_num}^{-0.5},\\\\ \\\\text{step_num} × \\\\text{warmup_steps}^{-1.5})}\\n$$\\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\\ndef __init__(self, d_model, warmup_steps=4000):\\nsuper(CustomSchedule, self).__init__()\\nself.d_model = d_model\\nself.d_model = tf.cast(self.d_model, tf.float32)\\nself.warmup_steps = warmup_steps\\ndef __call__(self, step):\\nstep = tf.cast(step, tf.float32)\\narg1 = tf.math.rsqrt(step)', 'def __call__(self, step):\\nstep = tf.cast(step, tf.float32)\\narg1 = tf.math.rsqrt(step)\\narg2 = step * (self.warmup_steps**-1.5)\\nreturn tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\\n학습률의 변화를 시각화해봅시다.\\nsample_learning_rate = CustomSchedule(d_model=128)\\nplt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\\nplt.ylabel(\"Learning Rate\")\\nplt.xlabel(\"Train Step\")\\nText(0.5, 0, \\'Train Step\\')\\n[이미지: ]\\n여기서 구현한 트랜스포머 모델을 바탕으로 다음 실습에서 한국어 챗봇을 만들어보겠습니다.', \"Text(0.5, 0, 'Train Step')\\n[이미지: ]\\n여기서 구현한 트랜스포머 모델을 바탕으로 다음 실습에서 한국어 챗봇을 만들어보겠습니다.\\n논문에서는 언급되지 않은 트랜스포머의 구현 이야기가 궁금하시다면 아래 링크를 참고하세요.\\n링크 : https://tunz.kr/post/4\\n==================================================\\n--- 16-02 트랜스포머를 이용한 한국어 챗봇(Transformer Chatbot Tutorial) ---\\n```\\nInput: 게임하자\\nOutput: 게임하세요 !\\n```앞서 구현한 트랜스포머 코드를 사용하여 일상 대화 챗봇을 구현합니다. 이번 실습은 바로 이전의 트랜스포머 실습 코드를 모두 실행하였다고 가정하므로 이전 트랜스포머 실습을 진행 후에 이어서 진행해야 합니다.\\n트랜스포머 챗봇 전체 코드는 아래의 링크에 공유합니다.\", '트랜스포머 챗봇 전체 코드는 아래의 링크에 공유합니다.\\n깃허브 링크 : https://github.com/ukairia777/tensorflow-transformer']\n",
      "['import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport re\\nimport urllib.request\\nimport time\\nimport tensorflow_datasets as tfds\\nimport tensorflow as tf\\n챗봇 데이터를 로드하여 상위 5개의 샘플을 출력해봅시다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\\ntrain_data = pd.read_csv(\\'ChatBotData.csv\\')\\ntrain_data.head()\\n[이미지: ]\\n이 데이터는 질문(Q)과 대답(A)의 쌍으로 이루어진 데이터입니다.\\nprint(\\'챗봇 샘플의 개수 :\\', len(train_data))', \"[이미지: ]\\n이 데이터는 질문(Q)과 대답(A)의 쌍으로 이루어진 데이터입니다.\\nprint('챗봇 샘플의 개수 :', len(train_data))\\n챗봇 샘플의 개수 : 11823\\n총 샘플의 개수는 11,823개입니다. 불필요한 Null 값이 있는지 확인해봅시다.\\nprint(train_data.isnull().sum())\\nQ        0\\nA        0\\nlabel    0\\ndtype: int64\\nNull 값은 별도로 존재하지 않습니다. 이번 실습에서는 토큰화를 위해 형태소 분석기를 사용하지 않고, 다른 방법인 학습 기반의 토크나이저를 사용할 것입니다. 그래서 원 데이터에서 ?, ., !와 같은 구두점을 미리 처리해두어야 하는데, 구두점들을 단순히 제거할 수도 있겠지만, 여기서는 구두점 앞에 공백. 즉, 띄어쓰기를 추가하여 다른 문자들과 구분하겠습니다.\", '가령, \\'12시 땡!\\' 이라는 문장이 있다면 \\'12시 땡 !\\'으로 땡과 !사이에 공백을 추가합니다. 이는 정규 표현식을 사용하여 가능합니다. 이 전처리는 질문 데이터와 답변 데이터 모두에 적용해줍니다.\\nquestions = []\\nfor sentence in train_data[\\'Q\\']:\\n# 구두점에 대해서 띄어쓰기\\n# ex) 12시 땡! -> 12시 땡 !\\nsentence = re.sub(r\"([?.!,])\", r\" \\\\1 \", sentence)\\nsentence = sentence.strip()\\nquestions.append(sentence)\\nanswers = []\\nfor sentence in train_data[\\'A\\']:\\n# 구두점에 대해서 띄어쓰기\\n# ex) 12시 땡! -> 12시 땡 !\\nsentence = re.sub(r\"([?.!,])\", r\" \\\\1 \", sentence)\\nsentence = sentence.strip()\\nanswers.append(sentence)', \"sentence = sentence.strip()\\nanswers.append(sentence)\\n질문과 대답에 대해서 상위 5개만 출력하여 구두점들이 띄어쓰기를 통해 분리되었는지 확인해봅시다.\\nprint(questions[:5])\\nprint(answers[:5])\\n['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\\n['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\\n'하루가 또 가네요 .'와 같이 구두점 앞에 띄어쓰기가 추가되어 분리된 것을 확인할 수 있습니다.\"]\n",
      "['앞서 서브워드 토크나이저 챕터에서 배웠던 서브워드텍스트인코더를 사용해봅시다. 자주 사용되는 서브워드 단위로 토큰을 분리하는 토크나이저로 학습 데이터로부터 학습하여 서브워드로 구성된 단어 집합을 생성합니다.\\n# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성\\ntokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\\nquestions + answers, target_vocab_size=2**13)\\n단어 집합이 생성되었습니다. 그런데 seq2seq 챕터에서 배웠던 것처럼 인코더-디코더 모델 계열에는 디코더의 입력으로 사용할 시작을 의미하는 시작 토큰 SOS와 종료 토큰 EOS 또한 존재합니다. 해당 토큰들도 단어 집합에 포함시킬 필요가 있으므로 이 두 토큰에 정수를 부여해줍니다.\\n# 시작 토큰과 종료 토큰에 대한 정수 부여.', \"# 시작 토큰과 종료 토큰에 대한 정수 부여.\\nSTART_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\\n# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\\nVOCAB_SIZE = tokenizer.vocab_size + 2\\n시작 토큰과 종료 토큰을 추가해주었으나 단어 집합의 크기도 +2를 해줍니다.\\n시작 토큰의 번호와 종료 토큰의 번호, 그리고 단어 집합의 크기를 출력해봅시다.\\nprint('시작 토큰 번호 :',START_TOKEN)\\nprint('종료 토큰 번호 :',END_TOKEN)\\nprint('단어 집합의 크기 :',VOCAB_SIZE)\\n시작 토큰 번호 : [8178]\\n종료 토큰 번호 : [8179]\\n단어 집합의 크기 : 8180\\n패딩에 사용될 0번 토큰부터 마지막 토큰인 8,179번 토큰까지의 개수를 카운트하면 단어 집합의 크기는 8,180개입니다.\"]\n",
      "[\"단어 집합을 생성한 후에는 서브워드텍스트인코더의 토크나이저로 정수 인코딩을 진행할 수 있습니다. 이는 토크나이저의 .encode()를 사용하여 가능합니다. 우선 임의로 선택한 20번 질문 샘플. 즉, questions[20]을 가지고 정수 인코딩을 진행해봅시다.\\n# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\\nprint('임의의 질문 샘플을 정수 인코딩 : {}'.format(tokenizer.encode(questions[20])))\\n임의의 질문 샘플을 정수 인코딩 : [5766, 611, 3509, 141, 685, 3747, 849]\\n임의의 질문 문장이 정수 시퀀스로 변환되었습니다. 반대로 정수 인코딩 된 결과는 다시 decode()를 사용하여 기존의 텍스트 시퀀스로 복원할 수 있습니다. 20번 질문 샘플을 가지고 정수 인코딩하고, 다시 이를 디코딩하는 과정은 다음과 같습니다.\", \"# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\\n# 임의의 입력 문장을 sample_string에 저장\\nsample_string = questions[20]\\n# encode() : 텍스트 시퀀스 --> 정수 시퀀스\\ntokenized_string = tokenizer.encode(sample_string)\\nprint ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\\n# decode() : 정수 시퀀스 --> 텍스트 시퀀스\\noriginal_string = tokenizer.decode(tokenized_string)\\nprint ('기존 문장: {}'.format(original_string))\\n정수 인코딩 후의 문장 [5766, 611, 3509, 141, 685, 3747, 849]\\n기존 문장: 가스비 비싼데 감기 걸리겠어\", \"정수 인코딩 후의 문장 [5766, 611, 3509, 141, 685, 3747, 849]\\n기존 문장: 가스비 비싼데 감기 걸리겠어\\n정수 인코딩 된 문장을 .decode()을 하면 자동으로 서브워드들까지 다시 붙여서 기존 단어로 복원해줍니다. 가령, 정수 인코딩 문장을 보면 정수가 7개인데 기존 문장의 띄어쓰기 단위인 어절은 4개밖에 존재하지 않습니다. 이는 '가스비'나 '비싼데'라는 한 어절이 정수 인코딩 후에는 두 개 이상의 정수일 수 있다는 겁니다. 각 정수가 어떤 서브워드로 맵핑되는지 출력해봅시다.\\n# 각 정수는 각 단어와 어떻게 mapping되는지 병렬로 출력\\n# 서브워드텍스트인코더는 의미있는 단위의 서브워드로 토크나이징한다. 띄어쓰기 단위 X 형태소 분석 단위 X\\nfor ts in tokenized_string:\\nprint ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\\n5766 ----> 가스\\n611 ----> 비\", \"print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\\n5766 ----> 가스\\n611 ----> 비\\n3509 ----> 비싼\\n141 ----> 데\\n685 ----> 감기\\n3747 ----> 걸리\\n849 ----> 겠어\\n샘플 1개를 가지고 정수 인코딩과 디코딩을 수행해보았습니다. 이번에는 전체 데이터에 대해서 정수 인코딩과 패딩을 진행합니다. 이를 위한 함수로 tokenize_and_filter()를 만듭니다. 여기서는 임의로 패딩의 길이는 40으로 정했습니다.\\n# 최대 길이를 40으로 정의\\nMAX_LENGTH = 40\\n# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\\ndef tokenize_and_filter(inputs, outputs):\\ntokenized_inputs, tokenized_outputs = [], []\\nfor (sentence1, sentence2) in zip(inputs, outputs):\", \"tokenized_inputs, tokenized_outputs = [], []\\nfor (sentence1, sentence2) in zip(inputs, outputs):\\n# encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\\nsentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\\nsentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\\ntokenized_inputs.append(sentence1)\\ntokenized_outputs.append(sentence2)\\n# 패딩\\ntokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\\ntokenized_inputs, maxlen=MAX_LENGTH, padding='post')\", \"tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\\ntokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\\ntokenized_outputs, maxlen=MAX_LENGTH, padding='post')\\nreturn tokenized_inputs, tokenized_outputs\\nquestions, answers = tokenize_and_filter(questions, answers)\\n정수 인코딩과 패딩이 진행된 후의 데이터의 크기를 확인해봅시다.\\nprint('질문 데이터의 크기(shape) :', questions.shape)\\nprint('답변 데이터의 크기(shape) :', answers.shape)\\n질문 데이터의 크기(shape) : (11823, 40)\\n답변 데이터의 크기(shape) : (11823, 40)\", '질문 데이터의 크기(shape) : (11823, 40)\\n답변 데이터의 크기(shape) : (11823, 40)\\n질문과 답변 데이터의 모든 문장이 모두 길이 40으로 변환되었습니다. 임의로 0번 샘플을 출력해봅시다.\\n# 0번 샘플을 임의로 출력\\nprint(questions[0])\\nprint(answers[0])\\n[8178 7915 4207 3060   41 8179    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0]\\n[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0', '[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0]\\n길이 40을 맞추기 위해 뒤에 0이 패딩된 것을 확인할 수 있습니다.']\n",
      "[\"tf.data.Dataset을 사용하여 데이터를 배치 단위로 불러올 수 있습니다.\\n# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\\n# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\\nBATCH_SIZE = 64\\nBUFFER_SIZE = 20000\\n# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\\ndataset = tf.data.Dataset.from_tensor_slices((\\n{\\n'inputs': questions,\\n'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\\n},\\n{\\n'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\\n},\\n))\\ndataset = dataset.cache()\", \"},\\n{\\n'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\\n},\\n))\\ndataset = dataset.cache()\\ndataset = dataset.shuffle(BUFFER_SIZE)\\ndataset = dataset.batch(BATCH_SIZE)\\ndataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\\n# 임의의 샘플에 대해서 [:, :-1]과 [:, 1:]이 어떤 의미를 가지는지 테스트해본다.\\nprint(answers[0]) # 기존 샘플\\nprint(answers[:1][:, :-1]) # 마지막 패딩 토큰 제거하면서 길이가 39가 된다.\\nprint(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다.\", 'print(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다.\\n[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0]\\n[[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0]]', '0    0    0    0    0    0    0    0    0    0    0]]\\n[[3844   74 7894    1 8179    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0    0    0    0\\n0    0    0    0    0    0    0    0    0    0    0]]']\n",
      "['이제 트랜스포머를 만들어봅시다. 하이퍼파라미터를 조정하여 실제 논문의 트랜스포머보다는 작은 모델을 만듭니다.\\n여기서 선택한 주요 하이퍼파라미터의 값은 다음과 같습니다.\\n$d_{model}$ = 256\\n$\\\\text{num_layers}$ = 2\\n$\\\\text{num_heads}$ = 8\\n$d_{ff}$ = 512\\ntf.keras.backend.clear_session()\\n# 하이퍼파라미터\\nD_MODEL = 256\\nNUM_LAYERS = 2\\nNUM_HEADS = 8\\nDFF = 512\\nDROPOUT = 0.1\\nmodel = transformer(\\nvocab_size=VOCAB_SIZE,\\nnum_layers=NUM_LAYERS,\\ndff=DFF,\\nd_model=D_MODEL,\\nnum_heads=NUM_HEADS,\\ndropout=DROPOUT)\\n학습률과 옵티마이저를 정의하고 모델을 컴파일합니다.\\nlearning_rate = CustomSchedule(D_MODEL)', 'dropout=DROPOUT)\\n학습률과 옵티마이저를 정의하고 모델을 컴파일합니다.\\nlearning_rate = CustomSchedule(D_MODEL)\\noptimizer = tf.keras.optimizers.Adam(\\nlearning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\\ndef accuracy(y_true, y_pred):\\n# 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\\ny_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\\nreturn tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\\nmodel.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\\n총 50회 모델을 학습합니다.\\nEPOCHS = 50', '총 50회 모델을 학습합니다.\\nEPOCHS = 50\\nmodel.fit(dataset, epochs=EPOCHS)']\n",
      "['챗봇을 평가하기 위한 세 개의 함수를 구현합니다. predict 함수에서 evaluate 함수를 호출하고 evaluate 함수에서 preprocess_sentence 함수를 호출하는 구조입니다. 사용자의 입력이 파이썬의 문자열로 입력되면 preprocess_sentence 함수에서 문자열에 대한 전처리를 수행합니다. 해당 전처리는 학습 전 질문 데이터와 답변 데이터에서 했던 전처리와 동일한 전처리입니다. 전처리가 진행된 문자열에 대해서 evaluate 함수는 트랜스포머 모델에 전처리가 진행된 사용자의 입력을 전달하고, 디코더를 통해 계속해서 현재 시점의 예측. 다시 말해 챗봇의 대답에 해당하는 단어를 순차적으로 예측합니다. 여기서 예측된 단어들은 문자열이 아니라 정수인 상태이므로 evaluate 함수가 리턴하는 것은 결과적으로 정수 시퀀스입니다', '. 여기서 예측된 단어들은 문자열이 아니라 정수인 상태이므로 evaluate 함수가 리턴하는 것은 결과적으로 정수 시퀀스입니다. predict 함수는 evaluate 함수로부터 전달받은 챗봇의 대답에 해당하는 정수 시퀀스를 문자열로 다시 디코딩을 하고 사용자에게 챗봇의 대답을 출력합니다.', 'def preprocess_sentence(sentence):\\n# 단어와 구두점 사이에 공백 추가.\\n# ex) 12시 땡! -> 12시 땡 !\\nsentence = re.sub(r\"([?.!,])\", r\" \\\\1 \", sentence)\\nsentence = sentence.strip()\\nreturn sentence\\ndef evaluate(sentence):\\n# 입력 문장에 대한 전처리\\nsentence = preprocess_sentence(sentence)\\n# 입력 문장에 시작 토큰과 종료 토큰을 추가\\nsentence = tf.expand_dims(\\nSTART_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\\noutput = tf.expand_dims(START_TOKEN, 0)\\n# 디코더의 예측 시작\\nfor i in range(MAX_LENGTH):', 'output = tf.expand_dims(START_TOKEN, 0)\\n# 디코더의 예측 시작\\nfor i in range(MAX_LENGTH):\\npredictions = model(inputs=[sentence, output], training=False)\\n# 현재 시점의 예측 단어를 받아온다.\\npredictions = predictions[:, -1:, :]\\npredicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\\n# 만약 현재 시점의 예측 단어가 종료 토큰이라면 예측을 중단\\nif tf.equal(predicted_id, END_TOKEN[0]):\\nbreak\\n# 현재 시점의 예측 단어를 output(출력)에 연결한다.\\n# output은 for문의 다음 루프에서 디코더의 입력이 된다.\\noutput = tf.concat([output, predicted_id], axis=-1)', \"# output은 for문의 다음 루프에서 디코더의 입력이 된다.\\noutput = tf.concat([output, predicted_id], axis=-1)\\n# 단어 예측이 모두 끝났다면 output을 리턴.\\nreturn tf.squeeze(output, axis=0)\\ndef predict(sentence):\\nprediction = evaluate(sentence)\\n# prediction == 디코더가 리턴한 챗봇의 대답에 해당하는 정수 시퀀스\\n# tokenizer.decode()를 통해 정수 시퀀스를 문자열로 디코딩.\\npredicted_sentence = tokenizer.decode(\\n[i for i in prediction if i < tokenizer.vocab_size])\\nprint('Input: {}'.format(sentence))\\nprint('Output: {}'.format(predicted_sentence))\\nreturn predicted_sentence\", 'print(\\'Output: {}\\'.format(predicted_sentence))\\nreturn predicted_sentence\\n학습된 트랜스포머에 임의로 생각나는 말들을 적어보았습니다.\\noutput = predict(\"영화 볼래?\")\\nInput: 영화 볼래?\\nOutput: 최신 영화가 좋을 것 같아요 .\\noutput = predict(\"고민이 있어\")\\nInput: 고민이 있어\\nOutput: 생각을 종이에 끄젹여여 보는게 도움이 될 수도 있어요 .\\noutput = predict(\"너무 화가나\")\\nInput: 너무 화가나\\nOutput: 그럴수록 당신이 힘들 거예요 .\\noutput = predict(\"카페갈래?\")\\nInput: 카페갈래?\\nOutput: 카페 가서 차 마셔도 돼요 .\\noutput = predict(\"게임하고싶당\")\\nInput: 게임하고싶당\\nOutput: 저도요 !\\noutput = predict(\"게임하자\")\\nInput: 게임하자\\nOutput: 게임하세요 !', 'Input: 게임하고싶당\\nOutput: 저도요 !\\noutput = predict(\"게임하자\")\\nInput: 게임하자\\nOutput: 게임하세요 !\\n간단한 대화인만큼 그럭저럭 괜찮은 답변을 합니다. 데이터가 더 많다면, 더 다양한 대답을 할 수 있는 챗봇을 만들 수 있습니다.\\n==================================================\\n--- 16-03 셀프 어텐션을 이용한 텍스트 분류(Multi-head Self Attention for Text Classification) ---\\n```\\n테스트 정확도: 0.8695\\n```트랜스포머는 RNN 계열의 seq2seq를 대체하기 위해서 등장했습니다. 그리고 트랜스포머의 인코더는 RNN 인코더를, 트랜스포머의 디코더는 RNN 디코더를 대체할 수 있었습니다.', '트랜스포머의 인코더는 셀프 어텐션이라는 메커니즘을 통해 문장을 이해합니다. RNN과 그 동작 방식은 다르지만, RNN이 텍스트 분류나 개체명 인식과 같은 다양한 자연어 처리 태스크에 쓰일 수 있다면 트랜스포머의 인코더 또한 가능할 것입니다.\\n실제로 트랜스포머의 인코더는 다양한 분야의 자연어 처리 태스크에서 사용될 수 있었고, 이 아이디어는 후에 배우게 될 BERT라는 모델로 이어지게 됩니다. 이번 챕터에서는 트랜스포머의 인코더를 사용하여 텍스트 분류를 수행합니다.']\n",
      "['우선 트랜스포머의 인코더의 첫번째 서브층인 멀티 헤드 어텐션층을 클래스로 구현합니다.\\nimport tensorflow as tf\\nclass MultiHeadAttention(tf.keras.layers.Layer):\\ndef __init__(self, embedding_dim, num_heads=8):\\nsuper(MultiHeadAttention, self).__init__()\\nself.embedding_dim = embedding_dim # d_model\\nself.num_heads = num_heads\\nassert embedding_dim % self.num_heads == 0\\nself.projection_dim = embedding_dim // num_heads\\nself.query_dense = tf.keras.layers.Dense(embedding_dim)\\nself.key_dense = tf.keras.layers.Dense(embedding_dim)', 'self.key_dense = tf.keras.layers.Dense(embedding_dim)\\nself.value_dense = tf.keras.layers.Dense(embedding_dim)\\nself.dense = tf.keras.layers.Dense(embedding_dim)\\ndef scaled_dot_product_attention(self, query, key, value):\\nmatmul_qk = tf.matmul(query, key, transpose_b=True)\\ndepth = tf.cast(tf.shape(key)[-1], tf.float32)\\nlogits = matmul_qk / tf.math.sqrt(depth)\\nattention_weights = tf.nn.softmax(logits, axis=-1)\\noutput = tf.matmul(attention_weights, value)\\nreturn output, attention_weights', 'output = tf.matmul(attention_weights, value)\\nreturn output, attention_weights\\ndef split_heads(self, x, batch_size):\\nx = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\\nreturn tf.transpose(x, perm=[0, 2, 1, 3])\\ndef call(self, inputs):\\n# x.shape = [batch_size, seq_len, embedding_dim]\\nbatch_size = tf.shape(inputs)[0]\\n# (batch_size, seq_len, embedding_dim)\\nquery = self.query_dense(inputs)\\nkey = self.key_dense(inputs)\\nvalue = self.value_dense(inputs)', 'query = self.query_dense(inputs)\\nkey = self.key_dense(inputs)\\nvalue = self.value_dense(inputs)\\n# (batch_size, num_heads, seq_len, projection_dim)\\nquery = self.split_heads(query, batch_size)\\nkey = self.split_heads(key, batch_size)\\nvalue = self.split_heads(value, batch_size)\\nscaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\\n# (batch_size, seq_len, num_heads, projection_dim)\\nscaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])', 'scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\\n# (batch_size, seq_len, embedding_dim)\\nconcat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\\noutputs = self.dense(concat_attention)\\nreturn outputs']\n",
      "['멀티 헤드 어텐션에 두번째 서브층인 포지션 와이즈 피드 포워드 신경망을 추가하여 인코더 클래스를 설계합니다.\\nclass TransformerBlock(tf.keras.layers.Layer):\\ndef __init__(self, embedding_dim, num_heads, dff, rate=0.1):\\nsuper(TransformerBlock, self).__init__()\\nself.att = MultiHeadAttention(embedding_dim, num_heads)\\nself.ffn = tf.keras.Sequential(\\n[tf.keras.layers.Dense(dff, activation=\"relu\"),\\ntf.keras.layers.Dense(embedding_dim),]\\n)\\nself.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)', ')\\nself.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\\nself.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\\nself.dropout1 = tf.keras.layers.Dropout(rate)\\nself.dropout2 = tf.keras.layers.Dropout(rate)\\ndef call(self, inputs, training):\\nattn_output = self.att(inputs) # 첫번째 서브층 : 멀티 헤드 어텐션\\nattn_output = self.dropout1(attn_output, training=training)\\nout1 = self.layernorm1(inputs + attn_output) # Add & Norm\\nffn_output = self.ffn(out1) # 두번째 서브층 : 포지션 와이즈 피드 포워드 신경망', 'ffn_output = self.ffn(out1) # 두번째 서브층 : 포지션 와이즈 피드 포워드 신경망\\nffn_output = self.dropout2(ffn_output, training=training)\\nreturn self.layernorm2(out1 + ffn_output) # Add & Norm']\n",
      "['앞서 트랜스포머를 설명할 때는 포지셔널 인코딩을 사용하였지만, 이번에는 위치 정보 자체를 학습을 하도록 하는 포지션 임베딩이라는 방법을 사용합니다. 이는 뒤에서 배우게 될 BERT에서 사용하는 방법이기도 합니다. 포지션 임베딩은 임베딩 층(Embedding layer)를 사용하되, 위치 벡터를 학습하도록 하므로 임베딩 층의 첫번째 인자로 단어 집합의 크기가 아니라 문장의 최대 길이를 넣어줍니다.\\nclass TokenAndPositionEmbedding(tf.keras.layers.Layer):\\ndef __init__(self, max_len, vocab_size, embedding_dim):\\nsuper(TokenAndPositionEmbedding, self).__init__()\\nself.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)', 'self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\\nself.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)\\ndef call(self, x):\\nmax_len = tf.shape(x)[-1]\\npositions = tf.range(start=0, limit=max_len, delta=1)\\npositions = self.pos_emb(positions)\\nx = self.token_emb(x)\\nreturn x + positions']\n",
      "[\"vocab_size = 20000  # 빈도수 상위 2만개의 단어만 사용\\nmax_len = 200  # 문장의 최대 길이\\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\\nprint('훈련용 리뷰 개수 : {}'.format(len(X_train)))\\nprint('테스트용 리뷰 개수 : {}'.format(len(X_test)))\\n훈련용 리뷰 개수 : 25000\\n테스트용 리뷰 개수 : 25000\\nX_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\"]\n",
      "['embedding_dim = 32  # 각 단어의 임베딩 벡터의 차원\\nnum_heads = 2  # 어텐션 헤드의 수\\ndff = 32  # 포지션 와이즈 피드 포워드 신경망의 은닉층의 크기\\ninputs = tf.keras.layers.Input(shape=(max_len,))\\nembedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)\\nx = embedding_layer(inputs)\\ntransformer_block = TransformerBlock(embedding_dim, num_heads, dff)\\nx = transformer_block(x)\\nx = tf.keras.layers.GlobalAveragePooling1D()(x)\\nx = tf.keras.layers.Dropout(0.1)(x)\\nx = tf.keras.layers.Dense(20, activation=\"relu\")(x)', 'x = tf.keras.layers.Dropout(0.1)(x)\\nx = tf.keras.layers.Dense(20, activation=\"relu\")(x)\\nx = tf.keras.layers.Dropout(0.1)(x)\\noutputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\\nmodel.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test))\\nprint(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\\n테스트 정확도: 0.8695', 'print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\\n테스트 정확도: 0.8695\\n==================================================\\n--- 17. BERT(Bidirectional Encoder Representations from Transformers) ---\\n마지막 편집일시 : 2022년 1월 2일 2:51 오후\\n==================================================\\n--- 17-01 NLP에서의 사전 훈련(Pre-training) ---\\n```\\n\"사전 훈련된 단어 임베딩이 모든 NLP 실무자의 도구 상자에서 사전 훈련된 언어 모델로 대체되는 것은 시간 문제이다.\"\\n```2018년 딥 러닝 연구원 세바스찬 루더는 사전 훈련된 언어 모델의 약진을 보며 다음과 같은 말을 했습니다.', '```2018년 딥 러닝 연구원 세바스찬 루더는 사전 훈련된 언어 모델의 약진을 보며 다음과 같은 말을 했습니다.\\n\"사전 훈련된 단어 임베딩이 모든 NLP 실무자의 도구 상자에서 사전 훈련된 언어 모델로 대체되는 것은 시간 문제이다.\"\\nBERT(Bidirectional Encoder Representations from Transformers)와 같은 트랜스포머 계열의 모델들이 자연어 처리를 지배했던 19년과 20년을 회고하면 이 말은 이미 현실이 되었습니다. BERT를 배우기에 앞서 워드 임베딩에서부터 ELMo, 그리고 트랜스포머에 이르기까지 자연어 처리가 발전되어온 흐름을 정리해봅시다.']\n",
      "['앞서 Word2Vec, FastText, GloVe와 같은 워드 임베딩 방법론들을 설명했습니다. 어떤 태스크를 수행할 때, 임베딩을 사용하는 방법으로는 크게 두 가지가 있습니다. 임베딩 층(Embedding layer)을 랜덤 초기화하여 처음부터 학습하는 방법. 그리고 방대한 데이터로 Word2Vec 등과 같은 임베딩 알고리즘으로 사전에 학습된 임베딩 벡터들을 가져와 사용하는 방법입니다. 만약, 태스크에 사용하기 위한 데이터가 적다면, 사전 훈련된 임베딩을 사용하면 성능 향상을 기대해볼 수 있었습니다.', \"그런데 이 두 가지 방법 모두 하나의 단어가 하나의 벡터값으로 맵핑되므로, 문맥을 고려하지 못 하여 다의어나 동음이의어를 구분하지 못하는 문제점이 있습니다. 한국어에는 '사과'라는 단어가 존재하는데 이 '사과'는 용서를 빈다는 의미로도 쓰이지만, 먹는 과일의 의미로도 사용됩니다. 그러나 임베딩 벡터는 '사과'라는 벡터에 하나의 벡터값을 맵핑하므로 이 두 가지 의미를 구분할 수 없었습니다. 이 한계는 사전 훈련된 언어 모델을 사용하므로서 극복할 수 있었으며 아래에서 언급할 ELMo나 BERT 등이 이러한 문제의 해결책입니다.\"]\n",
      "['[이미지: ]', \"2015년 구글은 'Semi-supervised Sequence Learning'라는 논문에서 LSTM 언어 모델을 학습하고나서 이렇게 학습한 LSTM을 텍스트 분류에 추가 학습하는 방법을 보였습니다. 이 방법은 우선 LSTM 언어 모델을 학습합니다. 언어 모델은 주어진 텍스트로부터 이전 단어들로부터 다음 단어를 예측하도록 학습하므로 기본적으로 별도의 레이블이 부착되지 않은 텍스트 데이터로도 학습 가능합니다. 사전 훈련된 워드 임베딩과 마찬가지로 사전 훈련된 언어 모델의 강점은 학습 전 사람이 별도 레이블을 지정해줄 필요가 없다는 점입니다. 그리고 이렇게 레이블이 없는 데이터로 학습된 LSTM과 가중치가 랜덤으로 초기화 된 LSTM 두 가지를 두고, 텍스트 분류와 같은 문제를 학습하여 사전 훈련된 언어 모델을 사용한 전자의 경우가 더 좋은 성능을 얻을 수 있다는 가능성을 보였습니다\", '. 방대한 텍스트로 LSTM 언어 모델을 학습해두고, 언어 모델을 다른 태스크에서 높은 성능을 얻기 위해 사용하는 방법으로 이전에 설명한 ELMo와 같은 아이디어도 존재합니다.', '[이미지: ]\\nELMo는 순방향 언어 모델과 역방향 언어 모델을 각각 따로 학습시킨 후에, 이렇게 사전 학습된 언어 모델로부터 임베딩 값을 얻는다는 아이디어였습니다. 이러한 임베딩은 문맥에 따라서 임베딩 벡터값이 달라지므로, 기존 워드 임베딩인 Word2Vec이나 GloVe 등이 다의어를 구분할 수 없었던 문제점을 해결할 수 있었습니다. 이어 언어 모델은 RNN 계열의 신경망에서 탈피하기 시작합니다. 트랜스포머가 번역기와 같은 인코더-디코더 구조에서 LSTM을 뛰어넘는 좋은 성능을 얻자, LSTM이 아닌 트랜스포머로 사전 훈련된 언어 모델을 학습하는 시도가 등장했습니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림에서 Trm은 트랜스포머(Transformer)의 약자입니다. 트랜스포머의 디코더는 LSTM 언어 모델처럼 순차적으로 이전 단어들로부터 다음 단어를 예측합니다. Open AI는 트랜스포머 디코더로 총 12개의 층을 쌓은 후에 방대한 텍스트 데이터를 학습시킨 언어 모델  GPT-1을 만들었습니다. Open AI는 GPT-1에 여러 다양한 태스크를 위해 추가 학습을 진행하였을 때, 다양한 태스크에서 높은 성능을 얻을 수 있음을 입증했습니다. NLP의 주요 트렌드는 사전 훈련된 언어 모델을 만들고 이를 특정 태스크에 추가 학습시켜 해당 태스크에서 높은 성능을 얻는 것으로 접어들었고, 언어 모델의 학습 방법에 변화를 주는 모델들이 등장했습니다.\\n[이미지: ]', '[이미지: ]\\n위의 좌측 그림에 있는 단방향 언어모델은 지금까지 배운 전형적인 언어 모델입니다. 시작 토큰 <SOS>가 들어가면, 다음 단어 I를 예측하고 그리고 그 다음 단어 am을 예측합니다. 반면, 우측에 있는 양방향 언어 모델은 지금까지 본 적 없던 형태의 언어 모델입니다. 실제로 이렇게 구현하는 경우는 거의 없는데 그 이유가 무엇일까요? 가령, 양방향 LSTM을 이용해서 우측과 같은 언어 모델을 만들었다고 해봅시다. 초록색 LSTM 셀은 순방향 언어 모델로 <sos>를 입력받아 I를 예측하고, 그 후에 am을 예측합니다. 그런데 am을 예측할 때, 출력층은 주황색 LSTM 셀인 역방향 언어 모델의 정보도 함께 받고있습니다. 그런데 am을 예측하는 시점에서 역방향 언어 모델이 이미 관측한 단어는 a, am, I 이렇게 3개의 단어입니다. 이미 예측해야하는 단어를 역방향 언어 모델을 통해 미리 관측한 셈이므로 언어 모델은 일반적으로 양방향으로 구현하지 않습니다.', '하지만 언어의 문맥이라는 것은 실제로는 양방향입니다. 텍스트 분류나 개체명 인식 등에서 양방향 LSTM을 사용하여 모델을 구현해서 좋은 성능을 얻을 수 있었던 것을 상기해봅시다. 하지만 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로 인해 위의 그림과 같은 양방향 언어 모델을 사용할 수 없으므로, 그 대안으로 ELMo에서는 순방향과 역방향이라는 두 개의 단방향 언어 모델을 따로 준비하여 학습하는 방법을 사용했던 것입니다. 이와 같이 기존 언어 모델로는 양방향 구조를 도입할 수 없으므로, 양방향 구조를 도입하기 위해서 2018년에는 새로운 구조의 언어 모델이 탄생했는데 바로 마스크드 언어 모델입니다.']\n",
      "[\"마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 마스킹(Masking)합니다. 여기서 마스킹이란 원래의 단어가 무엇이었는지 모르게 한다는 뜻입니다. 그리고 인공 신경망에게 이렇게 마스킹 된 단어들을(Masked words) 예측하도록 합니다. 문장 중간에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 식입니다. 우리가 영어 시험을 볼 때 종종 마주하는 빈칸 채우기 문제에 비유할 수 있습니다. 예를 들어 '나는 [MASK]에 가서 그곳에서 빵과 [MASK]를 샀다'를 주고 [MASK]에 들어갈 단어를 맞추게 합니다. 이 언어 모델에 대해서는 이번 챕터의 주제인 BERT를 설명하며 더 자세히 알아보겠습니다.\\n==================================================\\n--- 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT) ---\\n```\", '--- 17-02 버트(Bidirectional Encoder Representations from Transformers, BERT) ---\\n```\\n[SEP]\\n```\\n트랜스포머 챕터에 대한 사전 이해가 필요합니다.\\n[이미지: ]\\nBERT(Bidirectional Encoder Representations from Transformers)는 2018년에 구글이 공개한 사전 훈련된 모델입니다. BERT라는 이름은 세서미 스트리트라는 미국 인형극의 케릭터 이름이기도 한데, 앞서 소개한 임베딩 방법론인 ELMo와 마찬가지로 세서미 스트리트의 케릭터 이름을 따온 것이기도 합니다.\\nBERT는 2018년에 공개되어 등장과 동시에 수많은 NLP 태스크에서 최고 성능을 보여주면서 명실공히 NLP의 한 획을 그은 모델로 평가받고 있습니다. 이번 챕터에서는 BERT의 구조에 대해 이해하고, 이어서 BERT를 실습해보겠습니다.', '간단한 BERT 실습을 위해 아래와 같이 transformers라는 패키지를 설치하겠습니다.\\npip install transformers']\n",
      "['[이미지: ]\\nBERT는 이전 챕터에서 배웠던 트랜스포머를 이용하여 구현되었으며, 위키피디아(25억 단어)와 BooksCorpus(8억 단어)와 같은 레이블이 없는 텍스트 데이터로 사전 훈련된 언어 모델입니다.\\nBERT가 높은 성능을 얻을 수 있었던 것은, 레이블이 없는 방대한 데이터로 사전 훈련된 모델을 가지고, 레이블이 있는 다른 작업(Task)에서 추가 훈련과 함께 하이퍼파라미터를 재조정하여 이 모델을 사용하면 성능이 높게 나오는 기존의 사례들을 참고하였기 때문입니다. 다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 파인 튜닝(Fine-tuning)이라고 합니다.', '위의 그림은 BERT의 파인 튜닝 사례를 보여줍니다. 우리가 하고 싶은 태스크가 스팸 메일 분류라고 하였을 때, 이미 위키피디아 등으로 사전 학습된 BERT 위에 분류를 위한 신경망을 한 층 추가합니다. 이 경우, 비유하자면 BERT가 언어 모델 사전 학습 과정에서 얻은 지식을 활용할 수 있으므로 스팸 메일 분류에서 보다 더 좋은 성능을 얻을 수 있습니다.\\n이전에 언급한 ELMo나 OpenAI GPT-1 등이 이러한 파인 튜닝 사례의 대표적인 예입니다.']\n",
      "['[이미지: ]\\nBERT의 기본 구조는 트랜스포머의 인코더를 쌓아올린 구조입니다. Base 버전에서는 총 12개를 쌓았으며, Large 버전에서는 총 24개를 쌓았습니다. 그 외에도 Large 버전은 Base 버전보다 d_model의 크기나 셀프 어텐션 헤드(Self Attention Heads)의 수가 더 큽니다. 트랜스포머 인코더 층의 수를 L, d_model의 크기를 D, 셀프 어텐션 헤드의 수를 A라고 하였을 때 각각의 크기는 다음과 같습니다.\\nBERT-Base : L=12, D=768, A=12 : 110M개의 파라미터\\nBERT-Large : L=24, D=1024, A=16 : 340M개의 파라미터', 'BERT-Base : L=12, D=768, A=12 : 110M개의 파라미터\\nBERT-Large : L=24, D=1024, A=16 : 340M개의 파라미터\\n초기 트랜스포머 모델(https://wikidocs.net/31379)이 L=6, D=512, A=8이었다는 것과 비교하면 Base 또한 초기 트랜스포머보다는 큰 네트워크임을 알 수 있습니다. 여기서 BERT-base는 BERT보다 앞서 등장한 Open AI GPT-1과 하이퍼파라미터가 동일한데, 이는 BERT 연구진이 직접적으로 GPT-1과 성능을 비교하기 위해서 GPT-1과 동등한 크기로 BERT-Base를 설계하였기 때문입니다. 반면, BERT-Large는 BERT의 최대 성능을 보여주기 위해 만들어진 모델입니다. BERT가 세운 기록들은 대부분 BERT-Large를 통해 이루어졌습니다.\\n이 글에서는 앞으로 편의를 위해 BERT-BASE를 기준으로 설명합니다.']\n",
      "['BERT는 ELMo나 GPT-1과 마찬가지로 문맥을 반영한 임베딩(Contextual Embedding)을 사용하고 있습니다.\\n[이미지: ]\\nBERT의 입력은 앞서 배운 딥 러닝 모델들과 마찬가지로 임베딩 층(Embedding layer)를 지난 임베딩 벡터들입니다. d_model을 768로 정의하였으므로, 모든 단어들은 768차원의 임베딩 벡터가 되어 BERT의 입력으로 사용됩니다. BERT는 내부적인 연산을 거친 후, 동일하게 각 단어에 대해서 768차원의 벡터를 출력합니다. 위의 그림에서는 BERT가 각 768차원의 [CLS], I, love, you라는 4개의 벡터를 입력받아서(입력 임베딩) 동일하게 768차원의 4개의 벡터를 출력하는 모습(출력 임베딩)을 보여줍니다.\\n[이미지: ]', '[이미지: ]\\nBERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됩니다. 위의 좌측 그림에서 [CLS]라는 벡터는 BERT의 초기 입력으로 사용되었을 입력 임베딩 당시에는 단순히 임베딩 층(embedding layer)를 지난 임베딩 벡터였지만, BERT를 지나고 나서는 [CLS], I, love, you라는 모든 단어 벡터들을 모두 참고한 후에 문맥 정보를 가진 벡터가 됩니다. 위의 좌측 그림에서는 모든 단어를 참고하고 있다는 것을 점선의 화살표로 표현하였습니다.\\n이는 [CLS]라는 단어 벡터 뿐만 아니라 다른 벡터들도 전부 마찬가지입니다. 가령, 우측의 그림에서 출력 임베딩 단계의 love를 보면 BERT의 입력이었던 모든 단어들인 [CLS], I, love, you를 참고하고 있습니다.\\n[이미지: ]', \"[이미지: ]\\n하나의 단어가 모든 단어를 참고하는 연산은 사실 BERT의 12개의 층에서 전부 이루어지는 연산입니다. 그리고 이를 12개의 층을 지난 후에 최종적으로 출력 임베딩을 얻게되는 것입니다. 가령, 위의 그림은 BERT의 첫번째 층에 입력된 각 단어가 모든 단어를 참고한 후에 출력되는 과정을 화살표로 표현하였습니다. BERT의 첫번째 층의 출력 임베딩은 BERT의 두번째 층에서는 입력 임베딩이 됩니다.\\n[이미지: ]\\n그렇다면 BERT는 어떻게 모든 단어들을 참고하여 문맥을 반영한 출력 임베딩을 얻게 되는 것일까요? 사실 이에 대한 해답을 여러분은 이미 알고있는데, 바로 '셀프 어텐션'입니다. BERT는 기본적으로 트랜스포머 인코더를 12번 쌓은 것이므로 내부적으로 각 층마다 멀티 헤드 셀프 어텐션과 포지션 와이즈 피드 포워드 신경망을 수행하고 있습니다.\"]\n",
      "['BERT는 단어보다 더 작은 단위로 쪼개는 서브워드 토크나이저를 사용합니다. BERT가 사용한 토크나이저는 WordPiece 토크나이저로 서브워드 토크나이저 챕터에서 공부한 바이트 페어 인코딩(Byte Pair Encoding, BPE)의 유사 알고리즘입니다. 동작 방식은 BPE와 조금 다르지만, 글자로부터 서브워드들을 병합해가는 방식으로 최종 단어 집합(Vocabulary)을 만드는 것은 BPE와 유사합니다.\\n서브워드 토크나이저는 기본적으로 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어의 경우에는 더 작은 단위인 서브워드로 분리되어 서브워드들이 단어 집합에 추가된다는 아이디어를 갖고있습니다. 이렇게 단어 집합이 만들어지고 나면, 이 단어 집합을 기반으로 토큰화를 수행합니다. 이는 대표적인 서브워드 토크나이저 패키지인 SentencePiece 실습을 통해 이해한 내용입니다. BERT의 서브워드 토크나이저도 이와 마찬가지로 동작합니다.', 'BERT에서 토큰화를 수행하는 방식은 다음과 같습니다.\\n준비물 : 이미 훈련 데이터로부터 만들어진 단어 집합']\n",
      "['=> 해당 토큰을 분리하지 않는다.']\n",
      "['=> 해당 토큰을 서브워드로 분리한다.\\n=> 해당 토큰의 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 \"##\"를 붙인 것을 토큰으로 한다.', '예를 들어 embeddings이라는 단어가 입력으로 들어왔을 때, BERT의 단어 집합에 해당 단어가 존재하지 않았다고 해봅시다. 만약, 서브워드 토크나이저가 아닌 토크나이저라면 여기서 OOV 문제가 발생합니다. 하지만 서브워드 토크나이저의 경우에는 해당 단어가 단어 집합에 존재하지 않았다고 해서, 서브워드 또한 존재하지 않는다는 의미는 아니므로 해당 단어를 더 쪼개려고 시도합니다. 만약, BERT의 단어 집합에 em, ##bed, ##ding, #s라는 서브 워드들이 존재한다면, embeddings는 em, ##bed, ##ding, #s로 분리됩니다. 여기서 ##은 이 서브워드들은 단어의 중간부터 등장하는 서브워드라는 것을 알려주기 위해 단어 집합 생성 시 표시해둔 기호입니다. 이런 표시가 있어야만 em, ##bed, ##ding, #s를 다시 손쉽게 embeddings로 복원할 수 있을 것입니다.', '실습을 통해서 이해해봅시다. transformers라는 패키지를 사용하여 BERT 토크나이저를 사용할 수 있습니다.\\nimport pandas as pd\\nfrom transformers import BertTokenizer\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Bert-base의 토크나이저\\n\\'Here is the sentence I want embeddings for.\\'라는 문장을 BERT의 토크나이저가 어떻게 토큰화하는지 봅시다.\\nresult = tokenizer.tokenize(\\'Here is the sentence I want embeddings for.\\')\\nprint(result)\\n[\\'here\\', \\'is\\', \\'the\\', \\'sentence\\', \\'i\\', \\'want\\', \\'em\\', \\'##bed\\', \\'##ding\\', \\'##s\\', \\'for\\', \\'.\\']', \"['here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.']\\nembeddings라는 단어는 단어 집합에 존재하지 않으므로 em, ##bed, ##ding, #s로 분리됩니다. 실제로 BERT의 단어 집합에 특정 단어가 있는지 조회하려면 .vocab[]을 통해서 가능합니다. 우선 단어 'here'을 조회해봅시다.\\nprint(tokenizer.vocab['here'])\\n2182\\n정수 2182가 출력됩니다. 이는 단어 here이 정수 인코딩을 위해서 단어 집합 내부적으로 2182라는 정수로 맵핑되어져 있다는 의미입니다. 단어 embeddings를 조회해봅시다.\\nprint(tokenizer.vocab['embeddings'])\\nKeyError: 'embeddings'\", \"print(tokenizer.vocab['embeddings'])\\nKeyError: 'embeddings'\\n해당 단어가 존재하지 않는다는 의미에서 KeyError가 발생합니다. 반면, 단어 em, ##bed, ##ing, ##s는 모두 단어 집합에 존재합니다.\\nprint(tokenizer.vocab['em'])\\n7861\\nprint(tokenizer.vocab['##bed'])\\n8270\\nprint(tokenizer.vocab['##ding'])\\n4667\\nprint(tokenizer.vocab['##s'])\\n2015\\n이번에는 BERT의 단어 집합 전체를 불러와서 살펴보겠습니다. BERT의 단어 집합을 vocabulary.txt에 저장합니다.\\n# BERT의 단어 집합을 vocabulary.txt에 저장\\nwith open('vocabulary.txt', 'w') as f:\\nfor token in tokenizer.vocab.keys():\\nf.write(token + '\\\\n')\", \"with open('vocabulary.txt', 'w') as f:\\nfor token in tokenizer.vocab.keys():\\nf.write(token + '\\\\n')\\nvocabulary.txt 파일을 직접 열어서 살펴볼 수도 있겠지만, 데이터프레임 형태로 저장해서 확인할 수도 있습니다.\\ndf = pd.read_fwf('vocabulary.txt', header=None)\\ndf\\n[이미지: ]\\nprint('단어 집합의 크기 :',len(df))\\n단어 집합의 크기 : 30522\\nBERT의 단어 집합의 크기는 30,522입니다. 데이터프레임의 인덱스가 해당 단어와 맵핑된 정수이므로 정수로부터 맵핑된 단어를 확인할 수 있습니다. 가령, 4667번 단어는 ##ding입니다.\\ndf.loc[4667].values[0]\\n##ding\\n참고로 BERT에서 사용되는 특별 토큰들과 그와 맵핑되는 정수는 다음과 같습니다.\\n[PAD] - 0\\n[UNK] - 100\\n[CLS] - 101\", '##ding\\n참고로 BERT에서 사용되는 특별 토큰들과 그와 맵핑되는 정수는 다음과 같습니다.\\n[PAD] - 0\\n[UNK] - 100\\n[CLS] - 101\\n[SEP] - 102\\n[MASK] - 103\\n임의로 102번 토큰만 출력해봅시다.\\ndf.loc[102].values[0]\\n[SEP]\\n이 특별 토큰들이 어떤 의미를 가지는지에 대해서는 뒤에서 언급하겠습니다.']\n",
      "['트랜스포머에서는 포지셔널 인코딩(Positional Encoding)이라는 방법을 통해서 단어의 위치 정보를 표현했습니다. 포지셔널 인코딩은 사인 함수와 코사인 함수를 사용하여 위치에 따라 다른 값을 가지는 행렬을 만들어 이를 단어 벡터들과 더하는 방법입니다. BERT에서는 이와 유사하지만, 위치 정보를 사인 함수와 코사인 함수로 만드는 것이 아닌 학습을 통해서 얻는 포지션 임베딩(Position Embedding)이라는 방법을 사용합니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림은 포지션 임베딩을 사용하는 방법을 보여줍니다. 우선, 위의 그림에서 WordPiece Embedding은 우리가 이미 알고 있는 단어 임베딩으로 실질적인 입력입니다. 그리고 이 입력에 포지션 임베딩을 통해서 위치 정보를 더해주어야 합니다. 포지션 임베딩의 아이디어는 굉장히 간단한데, 위치 정보를 위한 임베딩 층(Embedding layer)을 하나 더 사용합니다. 가령, 문장의 길이가 4라면 4개의 포지션 임베딩 벡터를 학습시킵니다. 그리고 BERT의 입력마다 다음과 같이 포지션 임베딩 벡터를 더해주는 것입니다.\\n첫번째 단어의 임베딩 벡터 + 0번 포지션 임베딩 벡터\\n두번째 단어의 임베딩 벡터 + 1번 포지션 임베딩 벡터\\n세번째 단어의 임베딩 벡터 + 2번 포지션 임베딩 벡터\\n네번째 단어의 임베딩 벡터 + 3번 포지션 임베딩 벡터', '두번째 단어의 임베딩 벡터 + 1번 포지션 임베딩 벡터\\n세번째 단어의 임베딩 벡터 + 2번 포지션 임베딩 벡터\\n네번째 단어의 임베딩 벡터 + 3번 포지션 임베딩 벡터\\n실제 BERT에서는 문장의 최대 길이를 512로 하고 있으므로, 총 512개의 포지션 임베딩 벡터가 학습됩니다. 결론적으로 현재 설명한 내용을 기준으로는 BERT에서는 총 두 개의 임베딩 층이 사용됩니다. 단어 집합의 크기가 30,522개인 단어 벡터를 위한 임베딩 층과 문장의 최대 길이가 512이므로 512개의 포지션 벡터를 위한 임베딩 층입니다.\\n사실 BERT는 세그먼트 임베딩(Segment Embedding)이라는 1개의 임베딩 층을 더 사용합니다. 이에 대해서는 뒤에 언급합니다.']\n",
      "['[이미지: ]\\n위의 그림은 BERT의 논문에 첨부된 그림으로 ELMo와 GPT-1, 그리고 BERT의 구조적인 차이를 보여줍니다. 가장 우측 그림의 ELMo는 정방향 LSTM과 역방향 LSTM을 각각 훈련시키는 방식으로 양방향 언어 모델을 만들었습니다. 가운데 그림의 GPT-1은 트랜스포머의 디코더를 이전 단어들로부터 다음 단어를 예측하는 방식으로 단방향 언어 모델을 만들었습니다. Trm은 트랜스포머를 의미합니다. 단방향(→)으로 설계된 Open AI GPT와 달리 가장 좌측 그림의 BERT는 화살표가 양방향으로 뻗어나가는 모습을 보여줍니다. 이는 마스크드 언어 모델(Masked Language Model)을 통해 양방향성을 얻었기 때문입니다. BERT의 사전 훈련 방법은 크게 두 가지로 나뉩니다. 첫번째는 마스크드 언어 모델이고, 두번째는 다음 문장 예측(Next sentence prediction, NSP)입니다.', \"논문에 따르면 BERT는 BookCorpus(8억 단어)와 위키피디아(25억 단어)로 학습되었습니다.\\n1) 마스크드 언어 모델(Masked Language Model, MLM)\\nBERT는 사전 훈련을 위해서 인공 신경망의 입력으로 들어가는 입력 텍스트의 15%의 단어를 랜덤으로 마스킹(Masking)합니다. 그리고 인공 신경망에게 이 가려진 단어들을(Masked words) 예측하도록 합니다. 중간에 단어들에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 식입니다. 예를 들어 '나는 [MASK]에 가서 그곳에서 빵과 [MASK]를 샀다'를 주고 '슈퍼'와 '우유'를 맞추게 합니다.\\n더 정확히는 전부 [MASK]로 변경하지는 않고, 랜덤으로 선택된 15%의 단어들은 다시 다음과 같은 비율로 규칙이 적용됩니다.\\n80%의 단어들은 [MASK]로 변경한다.\\nEx) The man went to the store → The man went to the [MASK]\", '80%의 단어들은 [MASK]로 변경한다.\\nEx) The man went to the store → The man went to the [MASK]\\n10%의 단어들은 랜덤으로 단어가 변경된다.\\nEx) The man went to the store → The man went to the dog\\n10%의 단어들은 동일하게 둔다.\\nEx) The man went to the store → The man went to the store\\n이렇게 하는 이유는 [MASK]만 사용할 경우에는 [MASK] 토큰이 파인 튜닝 단계에서는 나타나지 않으므로 사전 학습 단계와 파인 튜닝 단계에서의 불일치가 발생하는 문제가 있습니다. 이 문제을 완화하기 위해서 랜덤으로 선택된 15%의 단어들의 모든 토큰을 [MASK]로 사용하지 않습니다. 이를 전체 단어 관점에서 그래프를 통해 정리하면 다음과 같습니다.\\n[이미지: ]', '[이미지: ]\\n전체 단어의 85%는 마스크드 언어 모델의 학습에 사용되지 않습니다. 마스크드 언어 모델의 학습에 사용되는 단어는 전체 단어의 15%입니다. 학습에 사용되는 12%는 [MASK]로 변경 후에 원래 단어를 예측합니다. 1.5%는 랜덤으로 단어가 변경된 후에 원래 단어를 예측합니다. 1.5%는 단어가 변경되지는 않았지만, BERT는 이 단어가 변경된 단어인지 원래 단어가 맞는지는 알 수 없습니다. 이 경우에도 BERT는 원래 단어가 무엇인지를 예측하도록 합니다.', \"예시를 통해 이해해봅시다. 'My dog is cute. he likes playing'이라는 문장에 대해서 마스크드 언어 모델을 학습하고자 합니다. 약간의 전처리와 BERT의 서브워드 토크나이저에 의해 이 문장은 ['my', 'dog', 'is' 'cute', 'he', 'likes', 'play', '##ing']로 토큰화가 되어 BERT의 입력으로 사용됩니다. 그리고 언어 모델 학습을 위해서 다음과 같이 데이터가 변경되었다고 가정해봅시다.\\n'dog' 토큰은 [MASK]로 변경되었습니다.\\n[이미지: ]\", \"'dog' 토큰은 [MASK]로 변경되었습니다.\\n[이미지: ]\\n위 그림은 'dog' 토큰이 [MASK]로 변경되어서 BERT 모델이 원래 단어를 맞추려고 하는 모습을 보여줍니다. 여기서 출력층에 있는 다른 위치의 벡터들은 예측과 학습에 사용되지 않고, 오직 'dog' 위치의 출력층의 벡터만이 사용됩니다. 구체적으로는 BERT의 손실 함수에서 다른 위치에서의 예측은 무시합니다. 출력층에서는 예측을 위해 단어 집합의 크기만큼의 밀집층(Dense layer)에 소프트맥스 함수가 사용된 1개의 층을 사용하여 원래 단어가 무엇인지를 맞추게 됩니다. 그런데 만약 'dog'만 변경된 것이 아니라 다음과 같이 데이터셋이 변경되었다면 어떨까요? 이번에는 세 가지 유형 모두에 대해서 가정해봅시다.\\n'dog' 토큰은 [MASK]로 변경되었습니다.\\n'he'는 랜덤 단어 'king'으로 변경되었습니다.\\n'play'는 변경되진 않았지만 예측에 사용됩니다.\\n[이미지: ]\", \"'dog' 토큰은 [MASK]로 변경되었습니다.\\n'he'는 랜덤 단어 'king'으로 변경되었습니다.\\n'play'는 변경되진 않았지만 예측에 사용됩니다.\\n[이미지: ]\\nBERT는 랜덤 단어 'king'으로 변경된 토큰에 대해서도 원래 단어가 무엇인지, 변경되지 않은 단어 'play'에 대해서도 원래 단어가 무엇인지를 예측해야 합니다. 'play'는 변경되지 않았지만 BERT 입장에서는 이것이 변경된 단어인지 아닌지 모르므로 마찬가지로 원래 단어를 예측해야 합니다.  BERT는 마스크드 언어 모델 외에도 다음 문장 예측이라는 또 다른 태스크를 학습합니다.\\n2) 다음 문장 예측(Next Sentence Prediction, NSP)\", '2) 다음 문장 예측(Next Sentence Prediction, NSP)\\nBERT는 두 개의 문장을 준 후에 이 문장이 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련시킵니다. 이를 위해서 50:50 비율로 실제 이어지는 두 개의 문장과 랜덤으로 이어붙인 두 개의 문장을 주고 훈련시킵니다. 이를 각각 Sentence A와 Sentence B라고 하였을 때, 다음의 예는 문장의 연속성을 확인한 경우와 그렇지 않은 경우를 보여줍니다.\\n이어지는 문장의 경우\\nSentence A : The man went to the store.\\nSentence B : He bought a gallon of milk.\\nLabel = IsNextSentence\\n이어지는 문장이 아닌 경우 경우\\nSentence A : The man went to the store.\\nSentence B : dogs are so cute.\\nLabel = NotNextSentence\\n[이미지: ]', 'Sentence B : dogs are so cute.\\nLabel = NotNextSentence\\n[이미지: ]\\nBERT의 입력으로 넣을 때에는 [SEP]라는 특별 토큰을 사용해서 문장을 구분합니다. 첫번째 문장의 끝에 [SEP] 토큰을 넣고, 두번째 문장이 끝나면 역시 [SEP] 토큰을 붙여줍니다. 그리고 이 두 문장이 실제 이어지는 문장인지 아닌지를 [CLS] 토큰의 위치의 출력층에서 이진 분류 문제를 풀도록 합니다. [CLS] 토큰은 BERT가 분류 문제를 풀기 위해 추가된 특별 토큰입니다. 그리고 위의 그림에서 나타난 것과 같이 마스크드 언어 모델과 다음 문장 예측은 따로 학습하는 것이 아닌 loss를 합하여 학습이 동시에 이루어집니다.', 'BERT가 언어 모델 외에도 다음 문장 예측이라는 태스크를 학습하는 이유는 BERT가 풀고자 하는 태스크 중에서는 QA(Question Answering)나 NLI(Natural Language Inference)와 같이 두 문장의 관계를 이해하는 것이 중요한 태스크들이 있기 때문입니다.']\n",
      "['[이미지: ]\\n앞서 언급했듯이 BERT는 QA 등과 같은 두 개의 문장 입력이 필요한 태스크를 풀기도 합니다. 문장 구분을 위해서 BERT는 세그먼트 임베딩이라는 또 다른 임베딩 층(Embedding layer)을 사용합니다. 첫번째 문장에는 Sentence 0 임베딩, 두번째 문장에는 Sentence 1 임베딩을 더해주는 방식이며 임베딩 벡터는 두 개만 사용됩니다.\\n결론적으로 BERT는 총 3개의 임베딩 층이 사용됩니다.\\nWordPiece Embedding : 실질적인 입력이 되는 워드 임베딩. 임베딩 벡터의 종류는 단어 집합의 크기로 30,522개.\\nPosition Embedding : 위치 정보를 학습하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 길이인 512개.\\nSegment Embedding : 두 개의 문장을 구분하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수인 2개.', 'Segment Embedding : 두 개의 문장을 구분하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수인 2개.\\n주의할 점은 많은 문헌에서 BERT가 문장 중간의 [SEP] 토큰과 두 종류의 세그먼트 임베딩을 통해서 두 개의 문장을 구분하여 입력받을 수 있다고 설명하고 있지만, 여기서 BERT에 두 개의 문장이 들어간다는 표현에서의 문장이라는 것은 실제 우리가 알고 있는 문장의 단위는 아닙니다. 예를 들어 QA 문제를 푸는 경우에는 [SEP]와 세그먼트 임베딩을 기준으로 구분되는 [질문(Question), 본문(Paragraph)] 두 종류의 텍스트를 입력받지만, Paragraph 1개는 실제로는 다수의 문장으로 구성될수 있습니다. 다시 말해 [SEP]와 세그먼트 임베딩으로 구분되는 BERT의 입력에서의 두 개의 문장은 실제로는 두 종류의 텍스트, 두 개의 문서일 수 있습니다.', 'BERT가 두 개의 문장을 입력받을 필요가 없는 경우도 있습니다. 예를 들어 네이버 영화 리뷰 분류나 IMDB 리뷰 분류와 같은 감성 분류 태스크에서는 한 개의 문서에 대해서만 분류를 하는 것이므로, 이 경우에는 BERT의 전체 입력에 Sentence 0 임베딩만을 더해줍니다. 아래에서 설명할 1)과 2) 파인 튜닝 유형이 그 예라고 할 수 있겠습니다.']\n",
      "['이번에는 사전 학습 된 BERT에 우리가 풀고자 하는 태스크의 데이터를 추가로 학습 시켜서 테스트하는 단계인 파인 튜닝 단계에 대해서 알아보겠습니다. 실질적으로 태스크에 BERT를 사용하는 단계에 해당됩니다.\\n1) 하나의 텍스트에 대한 텍스트 분류 유형(Single Text Classification)\\n[이미지: ]', '1) 하나의 텍스트에 대한 텍스트 분류 유형(Single Text Classification)\\n[이미지: ]\\nBERT를 사용하는 첫번째 유형은 하나의 문서에 대한 텍스트 분류 유형입니다. 이 유형은 영화 리뷰 감성 분류, 로이터 뉴스 분류 등과 같이 입력된 문서에 대해서 분류를 하는 유형으로 문서의 시작에 [CLS] 라는 토큰을 입력합니다. 앞서 사전 훈련 단계에서 다음 문장 예측을 설명할 때, [CLS] 토큰은 BERT가 분류 문제를 풀기위한 특별 토큰이라고 언급한 바 있습니다. 이는 BERT를 실질적으로 사용하는 단계인 파인 튜닝 단계에서도 마찬가지입니다. 텍스트 분류 문제를 풀기 위해서 [CLS] 토큰의 위치의 출력층에서 밀집층(Dense layer) 또는 같은 이름으로는 완전 연결층(fully-connected layer)이라고 불리는 층들을 추가하여 분류에 대한 예측을 하게됩니다.\\n2) 하나의 텍스트에 대한 태깅 작업(Tagging)\\n[이미지: ]', '2) 하나의 텍스트에 대한 태깅 작업(Tagging)\\n[이미지: ]\\nBERT를 사용하는 두번째 유형은 태깅 작업입니다. 앞서 RNN 계열의 신경망들을 이용해서 풀었던 태스크입니다. 대표적으로 문장의 각 단어에 품사를 태깅하는 품사 태깅 작업과 개체를 태깅하는 개체명 인식 작업이 있습니다. 출력층에서는 입력 텍스트의 각 토큰의 위치에 밀집층을 사용하여 분류에 대한 예측을 하게 됩니다.\\n3) 텍스트의 쌍에 대한 분류 또는 회귀 문제(Text Pair Classification or Regression)\\n[이미지: ]', '3) 텍스트의 쌍에 대한 분류 또는 회귀 문제(Text Pair Classification or Regression)\\n[이미지: ]\\nBERT는 텍스트의 쌍을 입력으로 받는 태스크도 풀 수 있습니다. 텍스트의 쌍을 입력으로 받는 대표적인 태스크로 자연어 추론(Natural language inference)이 있습니다. 자연어 추론 문제란, 두 문장이 주어졌을 때, 하나의 문장이 다른 문장과 논리적으로 어떤 관계에 있는지를 분류하는 것입니다. 유형으로는 모순 관계(contradiction), 함의 관계(entailment), 중립 관계(neutral)가 있습니다.\\n텍스트의 쌍을 입력받는 이러한 태스크의 경우에는 입력 텍스트가 1개가 아니므로, 텍스트 사이에 [SEP] 토큰을 집어넣고, Sentence 0 임베딩과 Sentence 1 임베딩이라는 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분합니다.\\n4) 질의 응답(Question Answering)\\n[이미지: ]', '4) 질의 응답(Question Answering)\\n[이미지: ]\\n텍스트의 쌍을 입력으로 받는 또 다른 태스크로 QA(Question Answering)가 있습니다. BERT로 QA를 풀기 위해서 질문과 본문이라는 두 개의 텍스트의 쌍을 입력합니다. 이 태스크의 대표적인 데이터셋으로 SQuAD(Stanford Question Answering Dataset) v1.1이 있습니다. 이 데이터셋을 푸는 방법은 질문과 본문을 입력받으면, 본문의 일부분을 추출해서 질문에 답변하는 것입니다. 실제로 이 데이터셋은 영어로 되어있지만 한국어로 예시를 들어보겠습니다. \"강우가 떨어지도록 영향을 주는 것은?\" 라는 질문이 주어지고, \"기상학에서 강우는 대기 수증기가 응결되어 중력의 영향을 받고 떨어지는 것을 의미합니다. 강우의 주요 형태는 이슬비, 비, 진눈깨비, 눈, 싸락눈 및 우박이 있습니다.\" 라는 본문이 주어졌다고 해보겠습니다 이 경우, 정답은 \"중력\"이 되어야 합니다.']\n",
      "['훈련 데이터는 위키피디아(25억 단어)와 BooksCorpus(8억 단어) ≈ 33억 단어\\nWordPiece 토크나이저로 토큰화를 수행 후 15% 비율에 대해서 마스크드 언어 모델 학습\\n두 문장 Sentence A와 B의 합한 길이. 즉, 최대 입력의 길이는 512로 제한\\n100만 step 훈련 ≈ (총 합 33억 단어 코퍼스에 대해 40 에포크 학습)\\n옵티마이저 : 아담(Adam)\\n학습률(learning rate) : $10^{−4}$\\n가중치 감소(Weight Decay) : L2 정규화로 0.01 적용\\n드롭 아웃 : 모든 레이어에 대해서 0.1 적용\\n활성화 함수 : relu 함수가 아닌 gelu 함수\\n배치 크기(Batch size) : 256']\n",
      "['[이미지: ]\\nBERT를 실제로 실습하게 되면 어텐션 마스크라는 시퀀스 입력이 추가로 필요합니다. 어텐션 마스크는 BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분할 수 있도록 알려주는 입력입니다. 이 값은 0과 1 두 가지 값을 가지는데, 숫자 1은 해당 토큰은 실제 단어이므로 마스킹을 하지 않는다라는 의미이고, 숫자 0은 해당 토큰은 패딩 토큰이므로 마스킹을 한다는 의미입니다. 위의 그림과 같이 실제 단어의 위치에는 1, 패딩 토큰의 위치에는 0의 값을 가지는 시퀀스를 만들어 BERT의 또 다른 입력으로 사용하면 됩니다.\\n다음 챕터에서 BERT를 이용하여 실제로 이진 분류, 다중 클래스 분류, 개체명 인식, QA(Question Answering)을 풀며 BERT에 대한 파인 튜닝 방법을 이해해봅시다.\\n==================================================', \"==================================================\\n--- 17-03 구글 BERT의 마스크드 언어 모델(Masked Language Model) 실습 ---\\n```\\n[{'score': 0.35730746388435364,\\n'sequence': 'i went to work this morning.',\\n'token': 2147,\\n'token_str': 'work'},\\n{'score': 0.23304426670074463,\\n'sequence': 'i went to bed this morning.',\\n'token': 2793,\\n'token_str': 'bed'},\\n{'score': 0.12845049798488617,\\n'sequence': 'i went to school this morning.',\\n'token': 2082,\\n'token_str': 'school'},\\n{'score': 0.062305748462677,\", \"'token': 2082,\\n'token_str': 'school'},\\n{'score': 0.062305748462677,\\n'sequence': 'i went to sleep this morning.',\\n'token': 3637,\\n'token_str': 'sleep'},\\n{'score': 0.04695260152220726,\\n'sequence': 'i went to class this morning.',\\n'token': 2465,\\n'token_str': 'class'}]\\n```\\n모든 BERT 실습은 Colab에서 진행한다고 가정합니다.\\n사전 학습된 한국어 BERT를 이용하여 마스크드 언어 모델을 실습해봅시다. 이번 실습을 위해서만이 아니라 앞으로 사전 학습된 BERT를 사용할 때는 transformers라는 패키지를 자주 사용하게 됩니다. 실습 환경에 transformers 패키지를 설치해둡시다.\\npip install transformers\"]\n",
      "[\"transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. BERT는 이미 누군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야 합니다. 예를 들어서 A라는 이름의 BERT를 사용하는데, B라는 이름의 BERT의 토크나이저를 사용하면 모델은 텍스트를 제대로 이해할 수 없습니다. A라는 BERT의 토크나이저는 '사과'라는 단어를 36번으로 정수 인코딩하는 반면에, B라는 BERT의 토크나이저는 '사과'라는 단어를 42번으로 정수 인코딩하는 등 단어와 맵핑되는 정수 정보 자체가 다르기 때문입니다.\\nfrom transformers import TFBertForMaskedLM\\nfrom transformers import AutoTokenizer\", 'from transformers import TFBertForMaskedLM\\nfrom transformers import AutoTokenizer\\nTFBertForMaskedLM.from_pretrained(\\'BERT 모델 이름\\')을 넣으면 [MASK]라고 되어있는 단어를 맞추기 위한 마스크드 언어 모델링을 위한 구조로 BERT를 로드합니다. 다시 말해서 BERT를 마스크드 언어 모델 형태로 로드합니다.\\nAutoTokenizer.from_pretrained(\\'모델 이름\\')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다.\\nmodel = TFBertForMaskedLM.from_pretrained(\\'bert-large-uncased\\')\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")']\n",
      "[\"''Soccer is a really fun [MASK]'라는 임의의 문장이 있다고 해봅시다. 이를 마스크드 언어 모델의 입력으로 넣으면, 마스크드 언어 모델은 [MASK]의 위치에 해당하는 단어를 예측합니다. 마스크드 언어 모델의 예측 결과를 보기위해서 bert-large-uncased의 토크나이저를 사용하여 해당 문장을 정수 인코딩해봅시다.\\ninputs = tokenizer('Soccer is a really fun [MASK].', return_tensors='tf')\\n토크나이저로 변환된 결과에서 input_ids를 통해 정수 인코딩 결과를 확인할 수 있습니다.\\nprint(inputs['input_ids'])\\ntf.Tensor([[ 101 4715 2003 1037 2428 4569  103 1012  102]], shape=(1, 9), dtype=int32)\", \"tf.Tensor([[ 101 4715 2003 1037 2428 4569  103 1012  102]], shape=(1, 9), dtype=int32)\\n토크나이저로 변환된 결과에서 token_type_ids를 통해서 문장을 구분하는 세그먼트 인코딩 결과를 확인할 수 있습니다.\\nprint(inputs['token_type_ids'])\\ntf.Tensor([[0 0 0 0 0 0 0 0 0]], shape=(1, 9), dtype=int32)\\n현재의 입력은 문장이 두 개가 아니라 한 개이므로 여기서는 문장 길이만큼의 0 시퀀스를 얻습니다. 만약 문장이 두 개였다면 두번째 문장이 시작되는 구간부터는 1의 시퀀스가 나오게 되지만, 여기서는 해당되지 않습니다.\\n토크나이저로 변환된 결과에서 attention_mask를 통해서 실제 단어와 패딩 토큰을 구분하는 용도인 어텐션 마스크를 확인할 수 있습니다.\\nprint(inputs['attention_mask'])\", \"print(inputs['attention_mask'])\\ntf.Tensor([[1 1 1 1 1 1 1 1 1]], shape=(1, 9), dtype=int32)\\n현재의 입력에서는 패딩이 없으므로 여기서는 문장 길이만큼의 1 시퀀스를 얻습니다. 만약 뒤에 패딩이 있었다면 패딩이 시작되는 구간부터는 0의 시퀀스가 나오게 되지만, 여기서는 해당되지 않습니다. 좀 더 다양한 패턴의 입력은 뒤의 텍스트 분류, 개체명 인식, 질의 응답 실습에서 이어서 보겠습니다.\"]\n",
      "[\"FillMaskPipeline은 모델과 토크나이저를 지정하면 손쉽게 마스크드 언어 모델의 예측 결과를 정리해서 보여줍니다. FillMaskPipeline에 우선 앞서 불러온 모델과 토크나이저를 지정해줍니다.\\nfrom transformers import FillMaskPipeline\\npip = FillMaskPipeline(model=model, tokenizer=tokenizer)\\n이제 입력 문장으로부터 [MASK]의 위치에 들어갈 수 있는 상위 5개의 후보 단어들을 출력해봅시다.\\npip('Soccer is a really fun [MASK].')\\n[{'score': 0.762112021446228,\\n'sequence': 'soccer is a really fun sport.',\\n'token': 4368,\\n'token_str': 'sport'},\\n{'score': 0.2034197747707367,\\n'sequence': 'soccer is a really fun game.',\", \"'token_str': 'sport'},\\n{'score': 0.2034197747707367,\\n'sequence': 'soccer is a really fun game.',\\n'token': 2208,\\n'token_str': 'game'},\\n{'score': 0.012208552099764347,\\n'sequence': 'soccer is a really fun thing.',\\n'token': 2518,\\n'token_str': 'thing'},\\n{'score': 0.0018630230333656073,\\n'sequence': 'soccer is a really fun activity.',\\n'token': 4023,\\n'token_str': 'activity'},\\n{'score': 0.001335485139861703,\\n'sequence': 'soccer is a really fun field.',\\n'token': 2492,\\n'token_str': 'field'}]\", \"'sequence': 'soccer is a really fun field.',\\n'token': 2492,\\n'token_str': 'field'}]\\npip('The Avengers is a really fun [MASK].')\\n[{'score': 0.2562903165817261,\\n'sequence': 'the avengers is a really fun show.',\\n'token': 2265,\\n'token_str': 'show'},\\n{'score': 0.1728411316871643,\\n'sequence': 'the avengers is a really fun movie.',\\n'token': 3185,\\n'token_str': 'movie'},\\n{'score': 0.11107689887285233,\\n'sequence': 'the avengers is a really fun story.',\\n'token': 2466,\\n'token_str': 'story'},\", \"'sequence': 'the avengers is a really fun story.',\\n'token': 2466,\\n'token_str': 'story'},\\n{'score': 0.07248972356319427,\\n'sequence': 'the avengers is a really fun series.',\\n'token': 2186,\\n'token_str': 'series'},\\n{'score': 0.07046619802713394,\\n'sequence': 'the avengers is a really fun film.',\\n'token': 2143,\\n'token_str': 'film'}]\\npip('I went to [MASK] this morning.')\\n[{'score': 0.35730746388435364,\\n'sequence': 'i went to work this morning.',\\n'token': 2147,\\n'token_str': 'work'},\", \"'sequence': 'i went to work this morning.',\\n'token': 2147,\\n'token_str': 'work'},\\n{'score': 0.23304426670074463,\\n'sequence': 'i went to bed this morning.',\\n'token': 2793,\\n'token_str': 'bed'},\\n{'score': 0.12845049798488617,\\n'sequence': 'i went to school this morning.',\\n'token': 2082,\\n'token_str': 'school'},\\n{'score': 0.062305748462677,\\n'sequence': 'i went to sleep this morning.',\\n'token': 3637,\\n'token_str': 'sleep'},\\n{'score': 0.04695260152220726,\\n'sequence': 'i went to class this morning.',\", \"'token_str': 'sleep'},\\n{'score': 0.04695260152220726,\\n'sequence': 'i went to class this morning.',\\n'token': 2465,\\n'token_str': 'class'}]\\n==================================================\\n--- 17-04 한국어 BERT의 마스크드 언어 모델(Masked Language Model) 실습 ---\\n```\\n[{'score': 0.08012567460536957,\\n'sequence': '나는 오늘 아침에 회사 에 출근을 했다.',\\n'token': 3769,\\n'token_str': '회사'},\\n{'score': 0.06124098226428032,\\n'sequence': '나는 오늘 아침에 에 출근을 했다.',\\n'token': 1,\\n'token_str': '[UNK]'},\\n{'score': 0.017486684024333954,\", \"'token': 1,\\n'token_str': '[UNK]'},\\n{'score': 0.017486684024333954,\\n'sequence': '나는 오늘 아침에 공장 에 출근을 했다.',\\n'token': 4345,\\n'token_str': '공장'},\\n{'score': 0.016131816431879997,\\n'sequence': '나는 오늘 아침에 사무실 에 출근을 했다.',\\n'token': 5841,\\n'token_str': '사무실'},\\n{'score': 0.015360789373517036,\\n'sequence': '나는 오늘 아침에 서울 에 출근을 했다.',\\n'token': 3671,\\n'token_str': '서울'}]\\n```\\n모든 BERT 실습은 Colab에서 진행한다고 가정합니다.\", \"'token': 3671,\\n'token_str': '서울'}]\\n```\\n모든 BERT 실습은 Colab에서 진행한다고 가정합니다.\\n사전 학습된 한국어 BERT를 이용하여 마스크드 언어 모델을 실습해봅시다. 이번 실습을 위해서만이 아니라 앞으로 사전 학습된 BERT를 사용할 때는 transformers라는 패키지를 자주 사용하게 됩니다. 실습 환경에 transformers 패키지를 설치해둡시다.\\npip install transformers\"]\n",
      "[\"transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. BERT는 이미 누군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야 합니다. 예를 들어서 A라는 이름의 BERT를 사용하는데, B라는 이름의 BERT의 토크나이저를 사용하면 모델은 텍스트를 제대로 이해할 수 없습니다. A라는 BERT의 토크나이저는 '사과'라는 단어를 36번으로 정수 인코딩하는 반면에, B라는 BERT의 토크나이저는 '사과'라는 단어를 42번으로 정수 인코딩하는 등 단어와 맵핑되는 정수 정보 자체가 다르기 때문입니다.\\nklue/bert-base는 대표적인 한국어 BERT입니다. klue/bert-base의 마스크드 언어 모델과 klue/bert-base의 토크나이저를 로드해봅시다.\\nfrom transformers import TFBertForMaskedLM\\nfrom transformers import AutoTokenizer\", \"from transformers import TFBertForMaskedLM\\nfrom transformers import AutoTokenizer\\nTFBertForMaskedLM.from_pretrained('BERT 모델 이름')을 넣으면 [MASK]라고 되어있는 단어를 맞추기 위한 마스크드 언어 모델링을 위한 구조로 BERT를 로드합니다. 다시 말해서 BERT를 마스크드 언어 모델 형태로 로드합니다. from_pt=True는 해당 모델이 기존에는 텐서플로우가 아니라 파이토치로 학습된 모델이었지만 이를 텐서플로우에서 사용하겠다라는 의미입니다.\\nAutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다.\\nmodel = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)\", 'model = TFBertForMaskedLM.from_pretrained(\\'klue/bert-base\\', from_pt=True)\\ntokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")']\n",
      "[\"'축구는 정말 재미있는 [MASK]다'라는 임의의 문장이 있다고 해봅시다. 이를 마스크드 언어 모델의 입력으로 넣으면, 마스크드 언어 모델은 [MASK]의 위치에 해당하는 단어를 예측합니다. 마스크드 언어 모델의 예측 결과를 보기위해서 klue/bert-base의 토크나이저를 사용하여 해당 문장을 정수 인코딩해봅시다.\\ninputs = tokenizer('축구는 정말 재미있는 [MASK]다.', return_tensors='tf')\\n토크나이저로 변환된 결과에서 input_ids를 통해 정수 인코딩 결과를 확인할 수 있습니다.\\nprint(inputs['input_ids'])\\ntf.Tensor([[   2 4713 2259 3944 6001 2259    4  809   18    3]], shape=(1, 10), dtype=int32)\\n토크나이저로 변환된 결과에서 token_type_ids를 통해서 문장을 구분하는 세그먼트 인코딩 결과를 확인할 수 있습니다.\", \"토크나이저로 변환된 결과에서 token_type_ids를 통해서 문장을 구분하는 세그먼트 인코딩 결과를 확인할 수 있습니다.\\nprint(inputs['token_type_ids'])\\ntf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32)\\n현재의 입력은 문장이 두 개가 아니라 한 개이므로 여기서는 문장 길이만큼의 0 시퀀스를 얻습니다. 만약 문장이 두 개였다면 두번째 문장이 시작되는 구간부터는 1의 시퀀스가 나오게 되지만, 여기서는 해당되지 않습니다.\\n토크나이저로 변환된 결과에서 attention_mask를 통해서 실제 단어와 패딩 토큰을 구분하는 용도인 어텐션 마스크를 확인할 수 있습니다.\\nprint(inputs['attention_mask'])\\ntf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)\", \"print(inputs['attention_mask'])\\ntf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)\\n현재의 입력에서는 패딩이 없으므로 여기서는 문장 길이만큼의 1 시퀀스를 얻습니다. 만약 뒤에 패딩이 있었다면 패딩이 시작되는 구간부터는 0의 시퀀스가 나오게 되지만, 여기서는 해당되지 않습니다. 좀 더 다양한 패턴의 입력은 뒤의 텍스트 분류, 개체명 인식, 질의 응답 실습에서 이어서 보겠습니다.\"]\n",
      "[\"FillMaskPipeline은 모델과 토크나이저를 지정하면 손쉽게 마스크드 언어 모델의 예측 결과를 정리해서 보여줍니다. FillMaskPipeline에 우선 앞서 불러온 모델과 토크나이저를 지정해줍니다.\\nfrom transformers import FillMaskPipeline\\npip = FillMaskPipeline(model=model, tokenizer=tokenizer)\\n이제 입력 문장으로부터 [MASK]의 위치에 들어갈 수 있는 상위 5개의 후보 단어들을 출력해봅시다.\\npip('축구는 정말 재미있는 [MASK]다.')\\n[{'score': 0.8963505625724792,\\n'sequence': '축구는 정말 재미있는 스포츠 다.',\\n'token': 4559,\\n'token_str': '스포츠'},\\n{'score': 0.02595764957368374,\\n'sequence': '축구는 정말 재미있는 거 다.',\\n'token': 568,\\n'token_str': '거'},\", \"{'score': 0.02595764957368374,\\n'sequence': '축구는 정말 재미있는 거 다.',\\n'token': 568,\\n'token_str': '거'},\\n{'score': 0.010033931583166122,\\n'sequence': '축구는 정말 재미있는 경기 다.',\\n'token': 3682,\\n'token_str': '경기'},\\n{'score': 0.007924391888082027,\\n'sequence': '축구는 정말 재미있는 축구 다.',\\n'token': 4713,\\n'token_str': '축구'},\\n{'score': 0.00784421805292368,\\n'sequence': '축구는 정말 재미있는 놀이 다.',\\n'token': 5845,\\n'token_str': '놀이'}]\\npip('어벤져스는 정말 재미있는 [MASK]다.')\\n[{'score': 0.8382411599159241,\\n'sequence': '어벤져스는 정말 재미있는 영화 다.',\", \"pip('어벤져스는 정말 재미있는 [MASK]다.')\\n[{'score': 0.8382411599159241,\\n'sequence': '어벤져스는 정말 재미있는 영화 다.',\\n'token': 3771,\\n'token_str': '영화'},\\n{'score': 0.028275618329644203,\\n'sequence': '어벤져스는 정말 재미있는 거 다.',\\n'token': 568,\\n'token_str': '거'},\\n{'score': 0.017189407721161842,\\n'sequence': '어벤져스는 정말 재미있는 드라마 다.',\\n'token': 4665,\\n'token_str': '드라마'},\\n{'score': 0.014989694580435753,\\n'sequence': '어벤져스는 정말 재미있는 이야기 다.',\\n'token': 3758,\\n'token_str': '이야기'},\\n{'score': 0.009382619522511959,\", \"'token': 3758,\\n'token_str': '이야기'},\\n{'score': 0.009382619522511959,\\n'sequence': '어벤져스는 정말 재미있는 장소 다.',\\n'token': 4938,\\n'token_str': '장소'}]\\npip('나는 오늘 아침에 [MASK]에 출근을 했다.')\\n[{'score': 0.08012567460536957,\\n'sequence': '나는 오늘 아침에 회사 에 출근을 했다.',\\n'token': 3769,\\n'token_str': '회사'},\\n{'score': 0.06124098226428032,\\n'sequence': '나는 오늘 아침에 에 출근을 했다.',\\n'token': 1,\\n'token_str': '[UNK]'},\\n{'score': 0.017486684024333954,\\n'sequence': '나는 오늘 아침에 공장 에 출근을 했다.',\\n'token': 4345,\\n'token_str': '공장'},\", \"'sequence': '나는 오늘 아침에 공장 에 출근을 했다.',\\n'token': 4345,\\n'token_str': '공장'},\\n{'score': 0.016131816431879997,\\n'sequence': '나는 오늘 아침에 사무실 에 출근을 했다.',\\n'token': 5841,\\n'token_str': '사무실'},\\n{'score': 0.015360789373517036,\\n'sequence': '나는 오늘 아침에 서울 에 출근을 했다.',\\n'token': 3671,\\n'token_str': '서울'}]\\n==================================================\\n--- 17-05 구글 BERT의 다음 문장 예측(Next Sentence Prediction) ---\\n```\\n최종 예측 레이블 : [1]\\n```\\n모든 BERT 실습은 Colab에서 진행한다고 가정합니다.\", '```\\n최종 예측 레이블 : [1]\\n```\\n모든 BERT 실습은 Colab에서 진행한다고 가정합니다.\\n사전 학습된 한국어 BERT를 이용하여 다음 문장 예측을 실습해봅시다. 이번 실습을 위해서만이 아니라 앞으로 사전 학습된 BERT를 사용할 때는 transformers라는 패키지를 자주 사용하게 됩니다. 실습 환경에 transformers 패키지를 설치해둡시다.\\npip install transformers']\n",
      "[\"transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. BERT는 이미 누군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야 합니다.\\nimport tensorflow as tf\\nfrom transformers import TFBertForNextSentencePrediction\\nfrom transformers import AutoTokenizer\\nTFBertForNextSentencePrediction.from_pretrained('BERT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드합니다.\\nAutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다.\", \"AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다.\\nmodel = TFBertForNextSentencePrediction.from_pretrained('bert-base-uncased')\\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\"]\n",
      "['다음 문장 예측을 위해 문맥 상으로 실제로 이어지는 두 개의 문장을 준비합니다.\\nprompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\\nnext_sentence = \"pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\"\\n앞서 준비한 bert-base-uncased의 토크나이저를 사용하여 두 개의 문장을 정수 인코딩해봅시다.\\nencoding = tokenizer(prompt, next_sentence, return_tensors=\\'tf\\')\\n토크나이저로 변환된 결과에서 input_ids를 통해 정수 인코딩 결과를 확인할 수 있습니다.', \"토크나이저로 변환된 결과에서 input_ids를 통해 정수 인코딩 결과를 확인할 수 있습니다.\\nprint(encoding['input_ids'])\\n<tf.Tensor: shape=(1, 58), dtype=int32, numpy=\\narray([[  101,  1999,  3304,  1010, 10733,  2366,  1999,  5337, 10906,\\n1010,  2107,  2004,  2012,  1037,  4825,  1010,  2003,  3591,\\n4895, 14540,  6610,  2094,  1012,   102, 10733,  2003,  8828,\\n2007,  1996,  2224,  1997,  1037,  5442,  1998,  9292,  1012,\\n1999, 10017, 10906,  1010,  2174,  1010,  2009,  2003,  3013,\", \"1999, 10017, 10906,  1010,  2174,  1010,  2009,  2003,  3013,\\n2046, 17632,  2015,  2000,  2022,  8828,  2096,  2218,  1999,\\n1996,  2192,  1012,   102]], dtype=int32)>\\n여기서 주의할 점은 여기서 101과 102는 특별 토큰이라는 점입니다. 실제로 해당 토크나이저의 [CLS] 토큰과 [SEP] 토큰의 번호를 출력해봅시다.\\nprint(tokenizer.cls_token, ':', tokenizer.cls_token_id)\\nprint(tokenizer.sep_token, ':' , tokenizer.sep_token_id)\\n[CLS] : 101\\n[SEP] : 102\\n위의 정수 인코딩 결과를 다시 디코딩해보면 현재 입력의 구성을 확인할 수 있습니다.\\nprint(tokenizer.decode(encoding['input_ids'][0]))\", \"위의 정수 인코딩 결과를 다시 디코딩해보면 현재 입력의 구성을 확인할 수 있습니다.\\nprint(tokenizer.decode(encoding['input_ids'][0]))\\n[CLS] in italy, pizza served in formal settings, such as at a restaurant, is presented unsliced. [SEP] pizza is eaten with the use of a knife and fork. in casual settings, however, it is cut into wedges to be eaten while held in the hand. [SEP]\", \"BERT에서 두 개의 문장이 입력으로 들어갈 경우에는 맨 앞에는 [CLS] 토큰이 존재하고, 첫번째 문장이 끝나면 [SEP] 토큰, 그리고 두번째 문장이 종료되었을 때 다시 추가적으로 [SEP] 토큰이 추가됩니다. 토크나이저로 변환된 결과에서 token_type_ids를 통해서 문장을 구분하는 세그먼트 인코딩 결과를 확인할 수 있습니다.\\nprint(encoding['token_type_ids'])\\ntf.Tensor(\\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 58), dtype=int32)\", '1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 58), dtype=int32)\\n0이 연속적으로 등장하다가 어느 순간부터 1이 연속적으로 등장하는데, 이는 [CLS] 토큰의 위치부터 첫번째 문장이 끝나고나서 등장한 [SEP] 토큰까지의 위치에는 0이 등장하고, 다음 두번째 문장부터는 1이 등장하는 것입니다. token_type_ids에서는 0과 1로 두 개의 문장을 구분하고 있습니다.']\n",
      "[\"이제 TFBertForNextSentencePrediction를 통해서 다음 문장을 예측해봅시다. 모델에 입력을 넣으면, 해당 모델은 소프트맥스 함수를 지나기 전의 값인 logits을 리턴합니다. 해당 값을 소프트맥스 함수를 통과시킨 후에 각 레이블에 대한 확률값을 출력해봅시다.\\nlogits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\\nsoftmax = tf.keras.layers.Softmax()\\nprobs = softmax(logits)\\nprint(probs)\\ntf.Tensor([[9.9999714e-01 2.8381858e-06]], shape=(1, 2), dtype=float32)\", \"print(probs)\\ntf.Tensor([[9.9999714e-01 2.8381858e-06]], shape=(1, 2), dtype=float32)\\n0번 인덱스에 대한 확률값이 1번 인덱스에 대한 확률값보다 훨씬 큽니다. 실질적으로 모델이 예측한 레이블은 0이라는 뜻입니다. 이제 두 개의 값 중 더 큰 값을 모델의 예측값으로 판단하도록 더 큰 확률값을 가진 인덱스를 리턴하도록 합니다.\\nprint('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())\\n최종 예측 레이블 : [0]\\n최종 예측 레이블은 0입니다. 이는 BERT가 다음 문장 예측을 학습했을 당시에 실질적으로 이어지는 두 개의 문장의 레이블은 0. 이어지지 않는 두 개의 문장의 경우에는 레이블을 1로 두고서 이진 분류로 학습을 하였기 때문입니다. 이번에는 이어지지 않는 두 개의 문장으로 테스트해봅시다. 전체적인 과정은 이전과 같습니다.\\n# 상관없는 두 개의 문장\", '# 상관없는 두 개의 문장\\nprompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\\nnext_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\\nencoding = tokenizer(prompt, next_sentence, return_tensors=\\'tf\\')\\nlogits = model(encoding[\\'input_ids\\'], token_type_ids=encoding[\\'token_type_ids\\'])[0]\\nsoftmax = tf.keras.layers.Softmax()\\nprobs = softmax(logits)\\nprint(\\'최종 예측 레이블 :\\', tf.math.argmax(probs, axis=-1).numpy())\\n최종 예측 레이블 : [1]', \"print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())\\n최종 예측 레이블 : [1]\\n==================================================\\n--- 17-06 한국어 BERT의 다음 문장 예측(Next Sentence Prediction) ---\\n```\\n최종 예측 레이블 : [1]\\n```\\n모든 BERT 실습은 Colab에서 진행한다고 가정합니다.\\n사전 학습된 한국어 BERT를 이용하여 다음 문장 예측을 실습해봅시다. 이번 실습을 위해서만이 아니라 앞으로 사전 학습된 BERT를 사용할 때는 transformers라는 패키지를 자주 사용하게 됩니다. 실습 환경에 transformers 패키지를 설치해둡시다.\\npip install transformers\"]\n",
      "[\"transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. BERT는 이미 누군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야 합니다.\\nimport tensorflow as tf\\nfrom transformers import TFBertForNextSentencePrediction\\nfrom transformers import AutoTokenizer\\nTFBertForNextSentencePrediction.from_pretrained('BERT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 BERT 구조를 로드합니다.\\nAutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다.\", 'AutoTokenizer.from_pretrained(\\'모델 이름\\')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다.\\nmodel = TFBertForNextSentencePrediction.from_pretrained(\\'klue/bert-base\\', from_pt=True)\\ntokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")']\n",
      "['이제 TFBertForNextSentencePrediction를 통해서 다음 문장을 예측해봅시다. 모델에 입력을 넣으면, 해당 모델은 소프트맥스 함수를 지나기 전의 값인 logits을 리턴합니다. 해당 값을 소프트맥스 함수를 통과시킨 후 두 개의 값 중 더 큰 값을 모델의 예측값으로 판단하도록 더 큰 확률값을 가진 인덱스를 리턴하도록 합니다.\\n# 이어지는 두 개의 문장\\nprompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\\nnext_sentence = \"여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다.\"\\nencoding = tokenizer(prompt, next_sentence, return_tensors=\\'tf\\')\\nlogits = model(encoding[\\'input_ids\\'], token_type_ids=encoding[\\'token_type_ids\\'])[0]', 'logits = model(encoding[\\'input_ids\\'], token_type_ids=encoding[\\'token_type_ids\\'])[0]\\nsoftmax = tf.keras.layers.Softmax()\\nprobs = softmax(logits)\\nprint(\\'최종 예측 레이블 :\\', tf.math.argmax(probs, axis=-1).numpy())\\n최종 예측 레이블 : [0]\\n최종 예측 레이블은 0입니다. 이는 BERT가 다음 문장 예측을 학습했을 당시에 실질적으로 이어지는 두 개의 문장의 레이블은 0. 이어지지 않는 두 개의 문장의 경우에는 레이블을 1로 두고서 이진 분류로 학습을 하였기 때문입니다. 이번에는 이어지지 않는 두 개의 문장으로 테스트해봅시다.\\n# 상관없는 두 개의 문장\\nprompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\\nnext_sentence = \"극장가서 로맨스 영화를 보고싶어요\"', 'prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\\nnext_sentence = \"극장가서 로맨스 영화를 보고싶어요\"\\nencoding = tokenizer(prompt, next_sentence, return_tensors=\\'tf\\')\\nlogits = model(encoding[\\'input_ids\\'], token_type_ids=encoding[\\'token_type_ids\\'])[0]\\nsoftmax = tf.keras.layers.Softmax()\\nprobs = softmax(logits)\\nprint(\\'최종 예측 레이블 :\\', tf.math.argmax(probs, axis=-1).numpy())\\n최종 예측 레이블 : [1]\\n==================================================\\n--- 17-07 센텐스버트(Sentence BERT, SBERT) ---', '==================================================\\n--- 17-07 센텐스버트(Sentence BERT, SBERT) ---\\nBERT로부터 문장 임베딩을 얻을 수 있는 센텐스버트(Sentence BERT, SBERT)에 대해서 다룹니다.']\n",
      "[\"BERT로부터 문장 벡터를 얻는 방법은 여러가지 방법이 존재하지만 여기서는 총 세 가지에 대해서 언급하겠습니다. 만약 사전 학습된 BERT에 'I love you'라는 문장이 입력된다고 하였을 때, 이 문장에 대한 벡터를 얻는 첫번째 방법은 [CLS] 토큰의 출력 벡터를 문장 벡터로 간주하는 것입니다.\\n[이미지: ]\\n앞서 BERT로 텍스트 분류 문제를 풀 때, [CLS] 토큰의 출력 벡터를 출력층의 입력으로 사용했던 점을 상기해봅시다. 이는 [CLS] 토큰이 입력된 문장에 대한 총체적 표현이라고 간주하고 있기 때문입니다. 다시 말해 [CLS] 토큰 자체를 입력 문장의 벡터로 간주할 수 있습니다.\\n[이미지: ]\", \"[이미지: ]\\n문장 벡터를 얻는 두번째 방법은 [CLS] 토큰만을 사용하는 것이 아니라 BERT의 모든 출력 벡터들을 평균내는 것입니다. 앞서 9챕터에서 단어 벡터들의 평균을 문장 벡터로 간주할 수 있다고 설명한 바 있습니다. 이는 BERT에서도 적용되는데, BERT의 각 단어에 대한 출력 벡터들에 대해서 평균을 내고 이를 문장 벡터로 볼 수 있습니다. 위 그림에서는 출력 벡터들의 평균을 'pooling'이라고 표현했습니다. 이를 평균 풀링(mean pooling)을 하였다고 표현하기도 합니다. 그런데 풀링에는 평균 풀링만 있는 것이 아니라 11챕터의 합성곱 신경망을 다룰 때 설명했던 맥스 풀링(max pooling) 또한 존재합니다. 세번째 방법은 BERT의 각 단어의 출력 벡터들에 대해서 평균 풀링 대신 맥스 풀링을 진행한 벡터를 얻는 것입니다.\\n정리하면 사전 학습된 BERT로부터 문장 벡터를 얻는 방법은 다음과 같이 세 가지가 있습니다.\", '정리하면 사전 학습된 BERT로부터 문장 벡터를 얻는 방법은 다음과 같이 세 가지가 있습니다.\\nBERT의 [CLS] 토큰의 출력 벡터를 문장 벡터로 간주한다.\\nBERT의 모든 단어의 출력 벡터에 대해서 평균 풀링을 수행한 벡터를 문장 벡터로 간주한다.\\nBERT의 모든 단어의 출력 벡터에 대해서 맥스 풀링을 수행한 벡터를 문장 벡터로 간주한다.\\n이때 평균 풀링을 하느냐와 맥스 풀링을 하느냐에 따라서 해당 문장 벡터가 가지는 의미는 다소 다른데, 평균 풀링을 얻은 문장 벡터의 경우에는 모든 단어의 의미를 반영하는 쪽에 가깝다면, 맥스 풀링을 얻은 문장 벡터의 경우에는 중요한 단어의 의미를 반영하는 쪽에 가깝습니다.']\n",
      "['SBERT는 기본적으로 BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델입니다. SBERT는 위에서 언급한 BERT의 문장 임베딩을 응용하여 BERT를 파인 튜닝합니다. SBERT가 어떤 식으로 학습되는지 정리해봅시다.\\n1) 문장 쌍 분류 태스크로 파인 튜닝\\nSBERT를 학습하는 첫번째 방법은 문장 쌍 분류 태스크. 대표적으로는 NLI(Natural Language Inferencing) 문제를 푸는 것입니다. 다음 챕터에서 한국어 버전의 NLI 데이터인 KorNLI 문제를 BERT로 풀어볼 예정입니다. NLI는 두 개의 문장이 주어지면 수반(entailment) 관계인지, 모순(contradiction) 관계인지, 중립(neutral) 관계인지를 맞추는 문제입니다. 다음은 NLI 데이터의 예시입니다.\\n문장 A\\n문장 B\\n레이블\\nA lady sits on a bench that is against a shopping mall.\\nA person sits on the seat.', '문장 A\\n문장 B\\n레이블\\nA lady sits on a bench that is against a shopping mall.\\nA person sits on the seat.\\nEntailment\\nA lady sits on a bench that is against a shopping mall.\\nA woman is sitting against a building.\\nEntailment\\nA lady sits on a bench that is against a shopping mall.\\nNobody is sitting on the bench.\\nContradiction\\nTwo women are embracing while holding to go packages.\\nThe sisters are hugging goodbye while holding to go packages after just eating lunch.\\nNeutral', 'The sisters are hugging goodbye while holding to go packages after just eating lunch.\\nNeutral\\nSBERT는 NLI 데이터를 학습하기 위해 다음과 같은 구조를 가집니다.\\n[이미지: ]\\n우선 문장 A와 문장 B 각각을 BERT의 입력으로 넣고, 앞서 BERT의 문장 임베딩을 얻기위한 방식이라고 언급했던 평균 풀링 또는 맥스 풀링을 통해서 각각에 대한 문장 임베딩 벡터를 얻습니다. 여기서는 이를 각각 u와 v라고 합시다. 그리고 나서 u벡터와 v벡터의 차이 벡터를 구합니다. 이 벡터는 수식으로 표현하면 |u-v|입니다. 그리고 이 세 가지 벡터를 연결(concatenation)합니다. 세미콜론(;)을 연결 기호로 한다면 연결된 벡터의 수식은 다음과 같습니다.\\n$h = (u; v; |u-v|)$\\n만약 BERT의 문장 임베딩 벡터의 차원이 $n$이라면 세 개의 벡터를 연결한 벡터 $h$의 차원은 $3n$이 됩니다.', '$h = (u; v; |u-v|)$\\n만약 BERT의 문장 임베딩 벡터의 차원이 $n$이라면 세 개의 벡터를 연결한 벡터 $h$의 차원은 $3n$이 됩니다.\\n[이미지: ]\\n그리고 이 벡터를 출력층으로 보내 다중 클래스 분류 문제를 풀도록 합니다. 다시 말해 분류하고자 하는 클래스의 개수가 $k$라면, 가중치 행렬 $3n\\\\ ×\\\\ k$의 크기를 가지는 행렬 $W_{y}$을 곱한 후에 소프트맥스 함수를 통과시킨다고도 볼 수 있습니다. 식으로 표현하면 다음과 같습니다.\\n$o = softmax(W_{y}h)$\\n이제 실제값에 해당하는 레이블로부터 오차를 줄이는 방식으로 학습시킵니다.\\n2) 문장 쌍 회귀 태스크로 파인 튜닝', '$o = softmax(W_{y}h)$\\n이제 실제값에 해당하는 레이블로부터 오차를 줄이는 방식으로 학습시킵니다.\\n2) 문장 쌍 회귀 태스크로 파인 튜닝\\nSBERT를 학습하는 두번째 방법은 문장 쌍으로 회귀 문제를 푸는 것으로 대표적으로 STS(Semantic Textual Similarity) 문제를 푸는 경우입니다. STS란 두 개의 문장으로부터 의미적 유사성을 구하는 문제를 말합니다. 다음은 STS 데이터의 예시입니다. 여기서 레이블은 두 문장의 유사도로 범위값은 0~5입니다.\\n문장 A\\n문장 B\\n레이블\\nA plane is taking off.\\nAn air plane is taking off.\\n5.00\\nA man is playing a large flute.\\nA man is playing a flute.\\n3.80\\nA man is spreading shreded cheese on a pizza.', 'A man is playing a flute.\\n3.80\\nA man is spreading shreded cheese on a pizza.\\nA man is spreading shredded cheese on an uncoo...\\n3.80\\nThree men are playing chess.\\nTwo men are playing chess.\\n2.60\\nA man is playing the cello.\\nA man seated is playing the cello.\\n4.25\\n참고) 한국어 버전의 STS 데이터셋인 KorSTS 데이터셋도 있으므로 아래 링크를 참고하세요.\\n링크 : https://github.com/kakaobrain/KorNLUDatasets\\nSBERT는 STS 데이터를 학습하기 위해 다음과 같은 구조를 가집니다.\\n[이미지: ]', '링크 : https://github.com/kakaobrain/KorNLUDatasets\\nSBERT는 STS 데이터를 학습하기 위해 다음과 같은 구조를 가집니다.\\n[이미지: ]\\n문장 A와 문장 B 각각을 BERT의 입력으로 넣고, 평균 풀링 또는 맥스 풀링을 통해서 각각에 대한 문장 임베딩 벡터를 얻습니다. 이를 각각 u와 v라고 하였을 때 이 두 벡터의 코사인 유사도를 구합니다. 그리고 해당 유사도와 레이블 유사도와의 평균 제곱 오차(Mean Squared Error, MSE)를 최소화하는 방식으로 학습합니다. 코사인 유사도의 값의 범위는 -1과 1사이므로 위 데이터와 같이 레이블 스코어의 범위가 0~5점이라면 학습 전 해당 레이블들의 값들을 5로 나누어 값의 범위를 줄인 후 학습할 수 있습니다.', '선택에 따라서 1) 문장 쌍 분류 태스크로만 파인 튜닝 할 수도 있고, 2) 문장 쌍 회귀 태스크로만 파인 튜닝 할 수도 있으며 1)을 학습한 후에 2)를 학습하는 전략을 세울 수도 있습니다. 뒤에서 파인 튜닝된 SBERT를 이용하여 한국어 챗봇을 구현하는 실습을 진행해보겠습니다.\\n==================================================\\n--- 18. 실전! BERT 실습하기 ---\\n```\\n이번 18챕터의 모든 실습은 구글의 Colab에서 진행한다고 가정합니다.', '--- 18. 실전! BERT 실습하기 ---\\n```\\n이번 18챕터의 모든 실습은 구글의 Colab에서 진행한다고 가정합니다.\\n```이전 챕터에서 배웠던 BERT의 이론적 지식을 바탕으로 BERT로 풀 수 있는 다양한 문제들을 풀어봅시다. 기본적으로 transformers 패키지를 사용하여 트랜스포머 계열들의 모델을 사용하는 방법은 이번 챕터에서 배우게 될 BERT의 사용법과 크게 다르지 않습니다. 이번 챕터에서 학습한 내용을 바탕으로 Huggingface의 transformers 공식 문서 링크를 참고한다면, 풀고자 하는 문제에 맞는 최고 성능의 모델들을 손쉽게 사용할 수 있습니다.\\nHuggingface의 공식 문서 링크 : https://huggingface.co/docs/transformers/index\\n이번 18챕터의 모든 실습은 구글의 Colab에서 진행한다고 가정합니다.', \"이번 18챕터의 모든 실습은 구글의 Colab에서 진행한다고 가정합니다.\\n다른 챕터와 마찬가지로 18챕터의 모든 실습 코드들은 저자의 깃허브에 업로드되어져 있으니 참고하세요. 18챕터는 특히 저자의 깃허브를 함께 참고하면서 학습하시는 것을 권장드립니다. e-book에서 지면의 한계상 설명하지 못했던 여러 변형 버전의 코드들도 저자의 깃허브에는 추가되어져 있으므로 반드시 참고하여 실습하시기 바랍니다.\\n저자의 깃허브 : https://github.com/ukairia777/tensorflow-nlp-tutorial\\n==================================================\\n--- 18-01 코랩(Colab)에서 TPU 사용하기 ---\\n```\\nwith strategy.scope():\\nmodel = create_model()\\nmodel.compile(optimizer='adam',\", \"```\\nwith strategy.scope():\\nmodel = create_model()\\nmodel.compile(optimizer='adam',\\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\\nmetrics=['sparse_categorical_accuracy'])\\n```지금까지는 GPU 사용만으로도 모델을 학습하는데 큰 무리가 없었지만, BERT의 경우 지금까지 사용한 모델보다 무거운 편입니다. 다시 말해 학습 속도가 상대적으로 느린 편입니다. 이번 챕터에서는 Colab에서 GPU보다 더 빠른 TPU도 사용해보겠습니다.\"]\n",
      "[\"Colab에서 GPU를 사용하기 위해서 런타임 유형 변경에서 GPU를 선택하였듯이 TPU를 사용하기 위해서는 TPU를 선택합니다.\\nColab에서 런타임 > 런타임 유형 변경 > 하드웨어 가속기에서 'TPU' 선택\\nGPU를 사용할 때는 런타임 유형 변경에서 GPU를 선택하는 것만으로 GPU를 사용할 수 있었지만, TPU의 경우에는 런타임 유형 변경에서 TPU를 선택한다고 바로 사용할 수 있는 것이 아닙니다. 아래에서 안내할 추가적인 코드 설정을 해주지 않으시면 실제로는 TPU를 사용하지 않으므로 아래의 코드 설정을 반드시 해주세요.\"]\n",
      "[\"딥 러닝 모델을 정의하기 전에 아래의 설정을 미리 해주어야 하므로 아래의 코드는 초반부에 실행해줍니다.\\nimport tensorflow as tf\\nimport os\\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\\ntf.config.experimental_connect_to_cluster(resolver)\\ntf.tpu.experimental.initialize_tpu_system(resolver)\"]\n",
      "['tf.distribute.Strategy는 훈련을 여러 GPU 또는 여러 장비, 여러 TPU로 나누어 처리하기 위한 텐서플로 API입니다. 이 API를 사용하면 기존의 모델이나 훈련 코드를 분산처리를 할 수 있습니다. TPU 사용을 위해서도 Strategy를 셋팅해줍니다.\\nstrategy = tf.distribute.TPUStrategy(resolver)']\n",
      "[\"딥 러닝 모델을 컴파일을 할 때도 추가적인 코드가 필요합니다. 모델의 컴파일은 strategy.scope 내에서 이루어져야 합니다. 이는 모델의 층을 쌓는 create_model()와 같은 함수를 만들어 strategy.scope 내에서 해당 함수를 호출하여 모델을 컴파일하는 방식으로 할 수 있습니다. 예시를 들어봅시다.\\n다음과 같이 모델의 층을 쌓는 create_model()라는 함수를 만듭니다.\\ndef create_model():\\nreturn tf.keras.Sequential(\\n[tf.keras.layers.Conv2D(256, 3, activation='relu', input_shape=(28, 28, 1)),\\ntf.keras.layers.Conv2D(256, 3, activation='relu'),\\ntf.keras.layers.Flatten(),\\ntf.keras.layers.Dense(256, activation='relu'),\", \"tf.keras.layers.Flatten(),\\ntf.keras.layers.Dense(256, activation='relu'),\\ntf.keras.layers.Dense(128, activation='relu'),\\ntf.keras.layers.Dense(10)])\\nwith strategy.scope(): 다음에 들여쓰기를 하고, create_model() 함수를 호출하고 모델을 컴파일합니다. 다시 말해 strategy.scope 내에서 모델을 컴파일합니다.\\nwith strategy.scope():\\nmodel = create_model()\\nmodel.compile(optimizer='adam',\\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\\nmetrics=['sparse_categorical_accuracy'])\", \"metrics=['sparse_categorical_accuracy'])\\n이 모델을 fit() 하게되면 해당 모델은 TPU를 사용하며 학습하게 됩니다! 앞으로 Colab에서 TPU를 사용하여 BERT 모델을 파인 튜닝 할 때에도 위 과정과 동일한 과정을 거칩니다. 위의 1~3번까지의 코드를 실행하고, BERT와 커스텀 레이어에 대한 모델의 컴파일을 strategy.scope 내에서 진행합니다.\"]\n",
      "[\"이번 챕터에서 TPU로 실습을 진행한 코드들을 GPU 환경에서 실습하고 싶다면, 위에서 모델의 층을 쌓고, 컴파일하는 코드를 제외한 TPU 진행만을 위한 코드들을 전부 제거해주면 됩니다.\\n==================================================\\n--- 18-02 transformers의 모델 클래스 불러오기 ---\\n```\\nfrom transformers import TFBertForQuestionAnswering\\nmodel = TFBertForQuestionAnswering.from_pretrained('모델 이름')\", \"model = TFBertForQuestionAnswering.from_pretrained('모델 이름')\\n```transformers 라이브러리에서는 각종 태스크에 맞게 BERT 위에 출력층을 추가한 모델 클래스 구현체를 제공하고 있습니다. 아래의 구현체를 사용하면 사용자가 별도의 출력층을 설계할 필요없이 태스크에 맞게 모델을 로드하여 사용할 수 있습니다. 이 책에서는 모델 구조의 이해를 위해 모든 실습에서 출력층을 직접 설계하여 실습을 진행하지만, 이미 모델의 구조를 이해한 상황에서는 아래와 같이 이미 출력층이 설계된 모델들을 사용하는 것이 훨씬 코드 작성이 간편합니다.\\n저자의 깃허브에서 파일명에 'model_from_transformers'가 들어간 파일들이 아래의 구현체들을 사용한 실습 파일입니다.\"]\n",
      "['다 대 일(many-to-one) 유형은 텍스트 분류에 주로 사용되는 모델입니다. \\'18-4. TFBertForSequenceClassification\\' 실습에서는 아래의 모델을 사용해봅니다.\\nfrom transformers import TFBertForSequenceClassification\\nmodel = TFBertForSequenceClassification.from_pretrained(\"모델 이름\", num_labels=분류할 레이블의 개수)']\n",
      "['다 대 다(many-to-one) 유형은 다수의 입력에 대해서 다수의 출력이 필요할 때 사용하는 모델입니다. \\'18-6. 개체명 인식\\'이 해당 모델을 사용하는 대표적인 예시입니다.\\nfrom transformers import TFBertForTokenClassification\\nmodel = TFBertForTokenClassification.from_pretrained(\"모델 이름\", num_labels=분류할 레이블의 개수)']\n",
      "[\"질의 응답(Question Answering) 문제를 풀기 위해 사용하는 모델로 '18-7. 기계 독해' 실습이 해당 모델을 사용할 수 있는 대표적인 예시입니다.\\nfrom transformers import TFBertForQuestionAnswering\\nmodel = TFBertForQuestionAnswering.from_pretrained('모델 이름')\\n==================================================\\n--- 18-03 KoBERT를 이용한 네이버 영화 리뷰 분류하기 ---\\n마지막 편집일시 : 2022년 11월 24일 11:04 오전\\n==================================================\\n--- 18-04 TFBertForSequenceClassification를 이용한 분류 ---\\n마지막 편집일시 : 2024년 8월 20일 4:29 오후\", '--- 18-04 TFBertForSequenceClassification를 이용한 분류 ---\\n마지막 편집일시 : 2024년 8월 20일 4:29 오후\\n==================================================\\n--- 18-05 KoBERT를 이용한 KorNLI 풀어보기 (다중 클래스 분류) ---\\n마지막 편집일시 : 2022년 11월 14일 3:16 오후\\n==================================================\\n--- 18-06 KoBERT를 이용한 개체명 인식(Named Entity Recognition) ---\\n마지막 편집일시 : 2022년 11월 14일 3:16 오후\\n==================================================\\n--- 18-07 KoBERT를 이용한 기계 독해(Machine Reading Comprehension) ---\\n```', '본문 : [CLS] \" 내각과 장관들이 소외되고 대통령비서실의 권한이 너무 크다 \", \" 행보가 비서 본연의 역할을 벗어난다 \" 는 의견이 제기되었다. 대표적인 예가 10차 개헌안 발표이다. 원로 헌법학자인 허영 경희대 석좌교수는 정부의 헌법개정안 준비 과정에 대해 \" 청와대 비서실이 아닌 국무회의 중심으로 이뤄졌어야 했다 \" 고 지적했다.\\'국무회의의 심의를 거쳐야 한다\\'( 제89조 ) 는 헌법 규정에 충실하지 않았다는 것이다. 그러면서 \" 법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다 \" 고 지적했다. 민정수석은 국회의원에 대해 책임지는 법무부 장관도 아니고, 국민에 대해 책임지는 사람도 아니기 때문에 정당성이 없고, 단지 대통령의 신임이 있을 뿐이라는 것이다. 또한 국무총리 선출 방식에 대한 기자의 질문에 \" 문 대통령도 취임 전에 국무총리에게 실질적 권한을 주겠다고 했지만 그러지 못하고 있다. 대통령비서실장만도 못한 권한을 행사하고 있다', '. 대통령비서실장만도 못한 권한을 행사하고 있다. \" 고 답변했다.', '질문 : 국무회의의 심의를 거쳐야 한다는 헌법 제 몇 조의 내용인가?\\n정답 : 제89조\\n예측 : 제89조\\n----------------------------------------', '본문 : [CLS] \" 내각과 장관들이 소외되고 대통령비서실의 권한이 너무 크다 \", \" 행보가 비서 본연의 역할을 벗어난다 \" 는 의견이 제기되었다. 대표적인 예가 10차 개헌안 발표이다. 원로 헌법학자인 허영 경희대 석좌교수는 정부의 헌법개정안 준비 과정에 대해 \" 청와대 비서실이 아닌 국무회의 중심으로 이뤄졌어야 했다 \" 고 지적했다.\\'국무회의의 심의를 거쳐야 한다\\'( 제89조 ) 는 헌법 규정에 충실하지 않았다는 것이다. 그러면서 \" 법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다 \" 고 지적했다. 민정수석은 국회의원에 대해 책임지는 법무부 장관도 아니고, 국민에 대해 책임지는 사람도 아니기 때문에 정당성이 없고, 단지 대통령의 신임이 있을 뿐이라는 것이다. 또한 국무총리 선출 방식에 대한 기자의 질문에 \" 문 대통령도 취임 전에 국무총리에게 실질적 권한을 주겠다고 했지만 그러지 못하고 있다. 대통령비서실장만도 못한 권한을 행사하고 있다', '. 대통령비서실장만도 못한 권한을 행사하고 있다. \" 고 답변했다.', '질문 : 법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다고 지적한 경희대 석좌교수 이름은?\\n정답 : 허영\\n예측 : 허영\\n----------------------------------------', '정답 : 허영\\n예측 : 허영\\n----------------------------------------\\n본문 : [CLS] 알렉산더 메이그스 헤이그 2세 ( 영어 : Alexander Meigs Haig, Jr., 1924년 12월 2일 ~ 2010년 2월 20일 ) 는 미국의 국무 장관을 지낸 미국의 군인, 관료 및 정치인이다. 로널드 레이건 대통령 밑에서 국무장관을 지냈으며, 리처드 닉슨과 제럴드 포드 대통령 밑에서 백악관 비서실장을 지냈다. 또한 그는 미국 군대에서 2번째로 높은 직위인 미국 육군 부참모 총장과 나토 및 미국 군대의 유럽연합군 최고사령관이었다. 한국 전쟁 시절 더글러스 맥아더 유엔군 사령관의 참모로 직접 참전하였으며, 로널드 레이건 정부 출범당시 초대 국무장관직을 맡아 1980년대 대한민국과 미국의 관계를 조율해 왔다. 저서로 회고록 《 경고 : 현실주의, 레이건과 외교 정책 》 ( 1984년 발간 ) 이 있다.', '질문 : 미국 군대 내 두번째로 높은 직위는 무엇인가?\\n정답 : 미국 육군 부참모 총장\\n예측 : 육군 부참모 총장\\n----------------------------------------', '정답 : 미국 육군 부참모 총장\\n예측 : 육군 부참모 총장\\n----------------------------------------\\n본문 : [CLS] 알렉산더 메이그스 헤이그 2세 ( 영어 : Alexander Meigs Haig, Jr., 1924년 12월 2일 ~ 2010년 2월 20일 ) 는 미국의 국무 장관을 지낸 미국의 군인, 관료 및 정치인이다. 로널드 레이건 대통령 밑에서 국무장관을 지냈으며, 리처드 닉슨과 제럴드 포드 대통령 밑에서 백악관 비서실장을 지냈다. 또한 그는 미국 군대에서 2번째로 높은 직위인 미국 육군 부참모 총장과 나토 및 미국 군대의 유럽연합군 최고사령관이었다. 한국 전쟁 시절 더글러스 맥아더 유엔군 사령관의 참모로 직접 참전하였으며, 로널드 레이건 정부 출범당시 초대 국무장관직을 맡아 1980년대 대한민국과 미국의 관계를 조율해 왔다. 저서로 회고록 《 경고 : 현실주의, 레이건과 외교 정책 》 ( 1984년 발간 ) 이 있다.', '질문 : 로널드 레이건 정부 출범 당시 알렉산더 헤이그는 어떤 직책을 맡았는가?\\n정답 : 초대 국무장관직\\n예측 : 국무장관직\\n----------------------------------------', '정답 : 초대 국무장관직\\n예측 : 국무장관직\\n----------------------------------------\\n본문 : [CLS] 알렉산더 메이그스 헤이그 2세 ( 영어 : Alexander Meigs Haig, Jr., 1924년 12월 2일 ~ 2010년 2월 20일 ) 는 미국의 국무 장관을 지낸 미국의 군인, 관료 및 정치인이다. 로널드 레이건 대통령 밑에서 국무장관을 지냈으며, 리처드 닉슨과 제럴드 포드 대통령 밑에서 백악관 비서실장을 지냈다. 또한 그는 미국 군대에서 2번째로 높은 직위인 미국 육군 부참모 총장과 나토 및 미국 군대의 유럽연합군 최고사령관이었다. 한국 전쟁 시절 더글러스 맥아더 유엔군 사령관의 참모로 직접 참전하였으며, 로널드 레이건 정부 출범당시 초대 국무장관직을 맡아 1980년대 대한민국과 미국의 관계를 조율해 왔다. 저서로 회고록 《 경고 : 현실주의, 레이건과 외교 정책 》 ( 1984년 발간 ) 이 있다.', '질문 : 알렉산더 헤이그는 어느 대통령의 밑에서 국무장관을 지냈는가?\\n정답 : 로널드 레이건 대통령\\n예측 : 로널드 레이건\\n----------------------------------------', '정답 : 로널드 레이건 대통령\\n예측 : 로널드 레이건\\n----------------------------------------\\n본문 : [CLS] 알렉산더 메이그스 헤이그 2세 ( 영어 : Alexander Meigs Haig, Jr., 1924년 12월 2일 ~ 2010년 2월 20일 ) 는 미국의 국무 장관을 지낸 미국의 군인, 관료 및 정치인이다. 로널드 레이건 대통령 밑에서 국무장관을 지냈으며, 리처드 닉슨과 제럴드 포드 대통령 밑에서 백악관 비서실장을 지냈다. 또한 그는 미국 군대에서 2번째로 높은 직위인 미국 육군 부참모 총장과 나토 및 미국 군대의 유럽연합군 최고사령관이었다. 한국 전쟁 시절 더글러스 맥아더 유엔군 사령관의 참모로 직접 참전하였으며, 로널드 레이건 정부 출범당시 초대 국무장관직을 맡아 1980년대 대한민국과 미국의 관계를 조율해 왔다. 저서로 회고록 《 경고 : 현실주의, 레이건과 외교 정책 》 ( 1984년 발간 ) 이 있다.', '질문 : 로널드 레이건 대통령 밑에서 일한 국무 장관은 누구인가?\\n정답 : 알렉산더 메이그스 헤이그 2세\\n예측 : 알렉산더 메이그스 헤이그 2세\\n----------------------------------------\\n```KorQuad 1.0 데이터를 학습하여 사용자의 질문으로부터 본문으로부터 정답을 찾아 대답하는 모델을 구현합니다.\\n현업에서는 기계 독해를 통해 챗봇을 구현하기도 합니다.\\n죄송합니다. 해당 자료는 위키독스 웹 사이트에서는 비공개 처리되어\\n구현 코드와 코드에 대한 상세한 설명은 유료 E-book을 구매하시면 확인할 수 있습니다.\\n결과 예시', '본문 : [CLS] \" 내각과 장관들이 소외되고 대통령비서실의 권한이 너무 크다 \", \" 행보가 비서 본연의 역할을 벗어난다 \" 는 의견이 제기되었다. 대표적인 예가 10차 개헌안 발표이다. 원로 헌법학자인 허영 경희대 석좌교수는 정부의 헌법개정안 준비 과정에 대해 \" 청와대 비서실이 아닌 국무회의 중심으로 이뤄졌어야 했다 \" 고 지적했다.\\'국무회의의 심의를 거쳐야 한다\\'( 제89조 ) 는 헌법 규정에 충실하지 않았다는 것이다. 그러면서 \" 법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다 \" 고 지적했다. 민정수석은 국회의원에 대해 책임지는 법무부 장관도 아니고, 국민에 대해 책임지는 사람도 아니기 때문에 정당성이 없고, 단지 대통령의 신임이 있을 뿐이라는 것이다. 또한 국무총리 선출 방식에 대한 기자의 질문에 \" 문 대통령도 취임 전에 국무총리에게 실질적 권한을 주겠다고 했지만 그러지 못하고 있다. 대통령비서실장만도 못한 권한을 행사하고 있다', '. 대통령비서실장만도 못한 권한을 행사하고 있다. \" 고 답변했다.', '질문 : 국무회의의 심의를 거쳐야 한다는 헌법 제 몇 조의 내용인가?\\n정답 : 제89조\\n예측 : 제89조\\n----------------------------------------', '본문 : [CLS] \" 내각과 장관들이 소외되고 대통령비서실의 권한이 너무 크다 \", \" 행보가 비서 본연의 역할을 벗어난다 \" 는 의견이 제기되었다. 대표적인 예가 10차 개헌안 발표이다. 원로 헌법학자인 허영 경희대 석좌교수는 정부의 헌법개정안 준비 과정에 대해 \" 청와대 비서실이 아닌 국무회의 중심으로 이뤄졌어야 했다 \" 고 지적했다.\\'국무회의의 심의를 거쳐야 한다\\'( 제89조 ) 는 헌법 규정에 충실하지 않았다는 것이다. 그러면서 \" 법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다 \" 고 지적했다. 민정수석은 국회의원에 대해 책임지는 법무부 장관도 아니고, 국민에 대해 책임지는 사람도 아니기 때문에 정당성이 없고, 단지 대통령의 신임이 있을 뿐이라는 것이다. 또한 국무총리 선출 방식에 대한 기자의 질문에 \" 문 대통령도 취임 전에 국무총리에게 실질적 권한을 주겠다고 했지만 그러지 못하고 있다. 대통령비서실장만도 못한 권한을 행사하고 있다', '. 대통령비서실장만도 못한 권한을 행사하고 있다. \" 고 답변했다.', '질문 : 법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다고 지적한 경희대 석좌교수 이름은?\\n정답 : 허영\\n예측 : 허영\\n----------------------------------------', '정답 : 허영\\n예측 : 허영\\n----------------------------------------\\n본문 : [CLS] 알렉산더 메이그스 헤이그 2세 ( 영어 : Alexander Meigs Haig, Jr., 1924년 12월 2일 ~ 2010년 2월 20일 ) 는 미국의 국무 장관을 지낸 미국의 군인, 관료 및 정치인이다. 로널드 레이건 대통령 밑에서 국무장관을 지냈으며, 리처드 닉슨과 제럴드 포드 대통령 밑에서 백악관 비서실장을 지냈다. 또한 그는 미국 군대에서 2번째로 높은 직위인 미국 육군 부참모 총장과 나토 및 미국 군대의 유럽연합군 최고사령관이었다. 한국 전쟁 시절 더글러스 맥아더 유엔군 사령관의 참모로 직접 참전하였으며, 로널드 레이건 정부 출범당시 초대 국무장관직을 맡아 1980년대 대한민국과 미국의 관계를 조율해 왔다. 저서로 회고록 《 경고 : 현실주의, 레이건과 외교 정책 》 ( 1984년 발간 ) 이 있다.', '질문 : 미국 군대 내 두번째로 높은 직위는 무엇인가?\\n정답 : 미국 육군 부참모 총장\\n예측 : 육군 부참모 총장\\n----------------------------------------', '정답 : 미국 육군 부참모 총장\\n예측 : 육군 부참모 총장\\n----------------------------------------\\n본문 : [CLS] 알렉산더 메이그스 헤이그 2세 ( 영어 : Alexander Meigs Haig, Jr., 1924년 12월 2일 ~ 2010년 2월 20일 ) 는 미국의 국무 장관을 지낸 미국의 군인, 관료 및 정치인이다. 로널드 레이건 대통령 밑에서 국무장관을 지냈으며, 리처드 닉슨과 제럴드 포드 대통령 밑에서 백악관 비서실장을 지냈다. 또한 그는 미국 군대에서 2번째로 높은 직위인 미국 육군 부참모 총장과 나토 및 미국 군대의 유럽연합군 최고사령관이었다. 한국 전쟁 시절 더글러스 맥아더 유엔군 사령관의 참모로 직접 참전하였으며, 로널드 레이건 정부 출범당시 초대 국무장관직을 맡아 1980년대 대한민국과 미국의 관계를 조율해 왔다. 저서로 회고록 《 경고 : 현실주의, 레이건과 외교 정책 》 ( 1984년 발간 ) 이 있다.', '질문 : 로널드 레이건 정부 출범 당시 알렉산더 헤이그는 어떤 직책을 맡았는가?\\n정답 : 초대 국무장관직\\n예측 : 국무장관직\\n----------------------------------------', '정답 : 초대 국무장관직\\n예측 : 국무장관직\\n----------------------------------------\\n본문 : [CLS] 알렉산더 메이그스 헤이그 2세 ( 영어 : Alexander Meigs Haig, Jr., 1924년 12월 2일 ~ 2010년 2월 20일 ) 는 미국의 국무 장관을 지낸 미국의 군인, 관료 및 정치인이다. 로널드 레이건 대통령 밑에서 국무장관을 지냈으며, 리처드 닉슨과 제럴드 포드 대통령 밑에서 백악관 비서실장을 지냈다. 또한 그는 미국 군대에서 2번째로 높은 직위인 미국 육군 부참모 총장과 나토 및 미국 군대의 유럽연합군 최고사령관이었다. 한국 전쟁 시절 더글러스 맥아더 유엔군 사령관의 참모로 직접 참전하였으며, 로널드 레이건 정부 출범당시 초대 국무장관직을 맡아 1980년대 대한민국과 미국의 관계를 조율해 왔다. 저서로 회고록 《 경고 : 현실주의, 레이건과 외교 정책 》 ( 1984년 발간 ) 이 있다.', '질문 : 알렉산더 헤이그는 어느 대통령의 밑에서 국무장관을 지냈는가?\\n정답 : 로널드 레이건 대통령\\n예측 : 로널드 레이건\\n----------------------------------------', '정답 : 로널드 레이건 대통령\\n예측 : 로널드 레이건\\n----------------------------------------\\n본문 : [CLS] 알렉산더 메이그스 헤이그 2세 ( 영어 : Alexander Meigs Haig, Jr., 1924년 12월 2일 ~ 2010년 2월 20일 ) 는 미국의 국무 장관을 지낸 미국의 군인, 관료 및 정치인이다. 로널드 레이건 대통령 밑에서 국무장관을 지냈으며, 리처드 닉슨과 제럴드 포드 대통령 밑에서 백악관 비서실장을 지냈다. 또한 그는 미국 군대에서 2번째로 높은 직위인 미국 육군 부참모 총장과 나토 및 미국 군대의 유럽연합군 최고사령관이었다. 한국 전쟁 시절 더글러스 맥아더 유엔군 사령관의 참모로 직접 참전하였으며, 로널드 레이건 정부 출범당시 초대 국무장관직을 맡아 1980년대 대한민국과 미국의 관계를 조율해 왔다. 저서로 회고록 《 경고 : 현실주의, 레이건과 외교 정책 》 ( 1984년 발간 ) 이 있다.', '질문 : 로널드 레이건 대통령 밑에서 일한 국무 장관은 누구인가?\\n정답 : 알렉산더 메이그스 헤이그 2세\\n예측 : 알렉산더 메이그스 헤이그 2세\\n----------------------------------------\\n==================================================\\n--- 18-08 BERT의 문장 임베딩(SBERT)을 이용한 한국어 챗봇 ---\\n```\\n그런 사람이 있으면 저 좀 소개시켜주세요.\\n```SBERT를 이용하여 문장(문서) 임베딩을 얻을 수 있는 패키지인 sentence_transformers를 사용하여 쉽고 간단하게 한국어 챗봇을 구현해봅시다. 실습에 앞서 sentence_transformers를 설치합니다.\\npip install sentence_transformers\\n트랜스포머를 이용한 한국어 챗봇 실습에서 사용했던 데이터를 그대로 사용합니다.\\nimport numpy as np\\nimport pandas as pd', '트랜스포머를 이용한 한국어 챗봇 실습에서 사용했던 데이터를 그대로 사용합니다.\\nimport numpy as np\\nimport pandas as pd\\nfrom numpy import dot\\nfrom numpy.linalg import norm\\nimport urllib.request\\nfrom sentence_transformers import SentenceTransformer\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\\ntrain_data = pd.read_csv(\\'ChatBotData.csv\\')\\ntrain_data.head()\\n[이미지: ]\\n문장 임베딩을 얻기 위해서 사전 훈련된 BERT를 로드합니다. 여기서는 한국어도 포함되어 학습된 다국어 모델을 로드합니다.', \"train_data.head()\\n[이미지: ]\\n문장 임베딩을 얻기 위해서 사전 훈련된 BERT를 로드합니다. 여기서는 한국어도 포함되어 학습된 다국어 모델을 로드합니다.\\nmodel = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\\n모델의 이름은 'xlm-r-100langs-bert-base-nli-stsb-mean-tokens'인데 이름이 의미하는 바는 100가지 언어를 지원(한국어 포함)하는 다국어 BERT BASE 모델로 SNLI 데이터를 학습 후 STS-B 데이터로 학습되었으며, 문장 표현을 얻기 위해서는 평균 풀링(mean-tokens)을 사용했다는 의미입니다. 다시 말해서 NLI 데이터를 학습 후에 STS 데이터로 추가 파인 튜닝한 모델이라는 의미입니다.\", \"SentenceTransformer로 로드할 수 있는 다양한 모델에 대한 리스트는 아래의 링크에서 확인 가능합니다.\\n해당 링크에 한국어 버전의 모델들 또한 공개되어 있으니 방문해보세요.\\n링크 : https://huggingface.co/models?library=sentence-transformers\\n데이터에서 모든 질문열. 즉, train_data['Q']에 대해서 문장 임베딩 값을 구한 후 embedding이라는 새로운 열에 저장합니다.\\ntrain_data['embedding'] = train_data.apply(lambda row: model.encode(row.Q), axis = 1)\\n두 개의 벡터로부터 코사인 유사도를 구하는 함수 cos_sim를 정의합니다.\\ndef cos_sim(A, B):\\nreturn dot(A, B)/(norm(A)*norm(B))\", \"두 개의 벡터로부터 코사인 유사도를 구하는 함수 cos_sim를 정의합니다.\\ndef cos_sim(A, B):\\nreturn dot(A, B)/(norm(A)*norm(B))\\nreturn_answer 함수는 임의의 질문이 들어오면 해당 질문의 문장 임베딩 값과 챗봇 데이터의 임베딩 열. 즉, train_data['embedding']에 저장해둔 모든 질문 샘플들의 문장 임베딩 값들을 전부 비교하여 코사인 유사도 값이 가장 높은 질문 샘플을 찾아냅니다. 그리고 해당 질문 샘플과 짝이 되는 답변 샘플을 리턴합니다.\\ndef return_answer(question)\\nembedding = model.encode(question)\\ntrain_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)\", \"train_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)\\nreturn train_data.loc[train_data['score'].idxmax()]['A']\\n이제 챗봇을 테스트해봅시다.\\nreturn_answer('결혼하고싶어')\\n좋은 사람이랑 결혼할 수 있을 거예요.\\nreturn_answer('나랑 커피먹을래?')\\n카페인이 필요한 시간인가 봐요.\\nreturn_answer('반가워')\\n저도 반가워요.\\nreturn_answer('사랑해')\\n상대방에게 전해보세요.\\nreturn_answer('너는 누구니?')\\n저는 위로봇입니다.\\nreturn_answer('너무 짜증나')\\n짜증날 땐 짜장면\\nreturn_answer('화가납니다')\\n화를 참는 연습을 해보세요.\\nreturn_answer('나랑 놀자')\\n지금 그러고 있어요.\\nreturn_answer('나랑 게임하자')\\n같이 놀아요.\", \"화를 참는 연습을 해보세요.\\nreturn_answer('나랑 놀자')\\n지금 그러고 있어요.\\nreturn_answer('나랑 게임하자')\\n같이 놀아요.\\nreturn_answer('출근하기싫어')\\n씻고 푹 쉬세요.\\nreturn_answer('여행가고싶다')\\n이김에 떠나보세요.\\nreturn_answer('너 말 잘한다')\\n그런 사람이 있으면 저 좀 소개시켜주세요.\\n짧은 질문들이지만 대부분의 질문에서 그럴듯한 답변을 하는 모습을 확인할 수 있었습니다.\\n==================================================\\n--- 18-09 벡터 데이터베이스 Faiss를 이용한 임베딩 검색기(Semantic Search) ---\\n```\\nUnderwater Forest Discovered\\ntotal time: 1.069244384765625\\nresults :\\nunderwater loop\\nthriving underwater antarctic garden discovered\", 'results :\\nunderwater loop\\nthriving underwater antarctic garden discovered\\nbaton goes underwater in wa\\nunderwater footage shows inside doomed costa\\nunderwater uluru found off wa coast\\n```시맨틱 검색(Semantic search)은 기존의 키워드 매칭이 아닌 문장의 의미에 초점을 맞춘 정보 검색 시스템을 말합니다. 여기서는 SBERT와 FAISS를 사용하여 간단한 검색 엔진을 구현해봅시다. Faiss는 벡터화 된 데이터를 인덱싱하고 데이터에 대한 효율적인 검색을 수행하기 위해 Facebook AI에서 구축한 C ++ 기반 라이브러리입니다. 우선 필요한 라이브러리를 설치합니다. CPU 환경에서 진행하신다면 faiss-gpu가 아니라 faiss-cpu를 설치하시기 바랍니다\\n!pip install faiss-gpu', '!pip install faiss-gpu\\n!pip install -U sentence-transformers']\n",
      "['import numpy as np\\nimport os\\nimport pandas as pd\\nimport urllib.request\\nimport faiss\\nimport time\\nfrom sentence_transformers import SentenceTransformer\\n여기서는 약 100만개의 뉴스 기사 제목 데이터를 사용합니다. 데이터를 로드하여 리스트 형태로 변환하겠습니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/19.%20Topic%20Modeling%20(LDA%2C%20BERT-Based)/dataset/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\\ndf = pd.read_csv(\"abcnews-date-text.csv\")', 'df = pd.read_csv(\"abcnews-date-text.csv\")\\ndata = df.headline_text.to_list()\\n상위 5개의 샘플을 출력해봅시다.\\n# 상위 5개의 샘플 출력\\ndata[:5]\\n[\\'aba decides against community broadcasting licence\\',\\n\\'act fire witnesses must be aware of defamation\\',\\n\\'a g calls for infrastructure protection summit\\',\\n\\'air nz staff in aust strike for pay rise\\',\\n\\'air nz strike to affect australian travellers\\']\\n샘플 수를 확인해보면 약 108만개가 존재합니다.\\nprint(\\'총 샘플의 개수 :\\', len(data))\\n총 샘플의 개수 : 1082168']\n",
      "[\"모든 샘플에 대해서 SBERT로 임베딩을 진행합니다.\\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\\nencoded_data = model.encode(data)\\nprint('임베딩 된 벡터 수 :', len(encoded_data))\\n임베딩 된 벡터 수 : 1082168\"]\n",
      "[\"인덱스를 정의하고 여기에 데이터를 추가합니다.\\nindex = faiss.IndexIDMap(faiss.IndexFlatIP(768))\\nindex.add_with_ids(encoded_data, np.array(range(0, len(data))))\\nfaiss.write_index(index, 'abc_news')\"]\n",
      "[\"실제 검색을 진행해보고 시간을 측정해봅시다. 여기서는 주어진 쿼리에 대해서 유사도가 높은 상위 5개의 샘플을 추출하겠습니다.\\ndef search(query):\\nt = time.time()\\nquery_vector = model.encode([query])\\nk = 5\\ntop_k = index.search(query_vector, k)\\nprint('total time: {}'.format(time.time() - t))\\nreturn [data[_id] for _id in top_k[1].tolist()[0]]\\nquery = str(input())\\nresults = search(query)\\nprint('results :')\\nfor result in results:\\nprint('\\\\t', result)\\n'Underwater Forest Discovered'라는 임의의 문장을 입력해보겠습니다.\\nUnderwater Forest Discovered\", \"print('\\\\t', result)\\n'Underwater Forest Discovered'라는 임의의 문장을 입력해보겠습니다.\\nUnderwater Forest Discovered\\ntotal time: 1.069244384765625\\nresults :\\nunderwater loop\\nthriving underwater antarctic garden discovered\\nbaton goes underwater in wa\\nunderwater footage shows inside doomed costa\\nunderwater uluru found off wa coast\\n약 108만개의 문서에 대해서 시맨틱 검색을 수행하였음에도 약 1초 내외의 시간밖에 걸리지 않았습니다.\\n==================================================\\n--- 18-10 문서 임베딩 모델(BGE-M3) 파인 튜닝하기 ---\\n```\\n융합된 모델: [[0.938  0.3674]]\", '--- 18-10 문서 임베딩 모델(BGE-M3) 파인 튜닝하기 ---\\n```\\n융합된 모델: [[0.938  0.3674]]\\n```이번 챕터에서는 검색 증강 생성(Retrieval-Augmented Generation, RAG) 등에서 굉장히 많이 사용되고 있는 한국어 SBERT 모델(문장 임베딩 모델 or 문서 임베딩 모델)인 BGE-M3를 커스텀 데이터로 파인 튜닝하여 검색 성능을 올리는 방법에 대해서 알아보겠습니다.\\n임베딩을 통해 검색기(Retriever)를 사용할 때, 임베딩 모델을 다른 모델을 변경하거나, BM-25 등의 키워드 기반의 검색기를 함께 사용하는 방안들 외에도 아예 여러분들이 가진 데이터로 임베딩 모델을 파인 튜닝하여 검색기의 성능을 높일 수 있습니다.\\n아래 실습은 구글의 Colab에서 진행한다고 가정합니다.\\n!pip install -U FlagEmbedding peft faiss-gpu LM_Cocktail']\n",
      "['우선 학습을 위해 임의의 데이터셋을 준비합니다. 데이터셋의 형식은 반드시 jsonl 형태여야만 합니다. 각 데이터는 \"query\", \"pos\", \"neg\"로 구성되며, 이때 \"query\"는 검색어, \"pos\"는 해당 검색어와 연관된 문서들이 포함된 리스트, \"neg\"는 해당 검색어와 연관되지 않은 문서들이 포함된 리스트입니다.\\n여기서는 저자가 임의로 데이터를 준비했지만, 실제 파인 튜닝 상황에서는 사용자는 실제로 이런 검색어가 입력되었을 때(query), 이런 문서들이 검색되었으면 하며(pos), 이런 문서들은 검색되지 않았으면 한다(neg)에 해당하는 데이터들을 직접 구축해야만 합니다.\\nimport json\\n# 번역된 데이터\\ntranslated_data = [', 'import json\\n# 번역된 데이터\\ntranslated_data = [\\n{\"query\": \"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\", \"pos\": [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\"], \"neg\": [\"4명의 여성이 해변에 앉아 있다.\", \"1996년에 개혁이 있었다.\", \"그녀는 자신의 기록을 정정하기 위해 법정에 가지 않을 것이다.\", \"그 남자는 하와이에 대해 이야기하고 있다.\", \"한 여성이 밖에 서 있다.\", \"전투는 끝났다.\", \"한 무리의 사람들이 배구를 하고 있다.\"]},', '{\"query\": \"한 여성이 높은 절벽 위에서 한 발로 서서 강을 내려다보고 있다.\", \"pos\": [\"한 여성이 절벽 위에 서 있다.\"], \"neg\": [\"한 여성이 의자에 앉아 있다.\", \"조지 부시는 공화당원들에게 최고 고문들의 조언에 반하여 이 어리석은 생각을 고려조차 하지 않겠다고 말했다.\", \"그 가족은 무너지고 있었다.\", \"아무도 회의에 나타나지 않았다\", \"한 소년이 밖에서 모래를 가지고 놀고 있다.\", \"전보를 받자마자 끝났다.\", \"한 아이가 자기 방에서 책을 읽고 있다.\"]},', '{\"query\": \"두 여성이 악기를 연주하고 있다; 한 명은 클라리넷, 다른 한 명은 바이올린을 연주한다.\", \"pos\": [\"몇 사람이 곡을 연주하고 있다.\"], \"neg\": [\"두 여성이 기타와 드럼을 연주하고 있다.\", \"한 남자가 산을 스키를 타고 내려가고 있다.\", \"살인자가 생각했던 때에 치명적인 용량이 투여되지 않았다.\", \"자전거를 타고 있는 사람\", \"그 소녀는 아치길에 기대어 서 있다.\", \"한 무리의 여성들이 소파 오페라를 보고 있다.\", \"사람들은 나이가 들어도 절대 잊지 않는다.\"]},', '{\"query\": \"파란색 탱크톱을 입은 소녀가 앉아서 세 마리의 개를 지켜보고 있다.\", \"pos\": [\"한 소녀가 파란색을 입고 있다.\"], \"neg\": [\"한 소녀가 세 마리의 고양이와 함께 있다.\", \"사람들이 장례 행렬을 지켜보고 있다.\", \"그 아이는 검은색을 입고 있다.\", \"공립학교에서 우리에게 재정은 문제이다.\", \"수영장에 있는 아이들.\", \"폭행당하는 것은 진정시키는 일이다.\", \"나는 18살에 심각한 문제에 직면했다.\"]},', '{\"query\": \"노란 개가 숲길을 따라 달리고 있다.\", \"pos\": [\"개가 달리고 있다\"], \"neg\": [\"고양이가 달리고 있다\", \"스틸은 그녀의 원래 이야기를 지키지 않았다.\", \"이 규칙은 사람들이 자녀 양육비를 내는 것을 막는다.\", \"조끼를 입은 남자가 차 안에 앉아 있다.\", \"검은 옷을 입고 흰색 반다나와 선글라스를 낀 사람이 버스 정류장에서 기다리고 있다.\", \"글로브나 메일 중 어느 쪽도 캐나다의 현재 도로 체계 상태에 대해 언급하지 않았다.\", \"스프링 크릭 시설은 오래되고 구식이다.\"]},', '{\"query\": \"각 단계에서의 필수 활동과 그 활동들과 관련된 중요한 요소들을 설명한다.\", \"pos\": [\"필수 활동에 대한 중요 요소들이 설명되어 있다.\"], \"neg\": [\"중요한 활동들을 설명하지만 그 활동들과 관련된 중요한 요소들에 대한 규정은 없다.\", \"사람들이 항의하기 위해 모여 있다.\", \"주 정부는 당신이 그렇게 하기를 선호할 것이다.\", \"한 소녀가 한 소년 옆에 앉아 있다.\", \"두 남성이 공연하고 있다.\", \"아무도 뛰고 있지 않다\", \"콘라드는 머리를 맞도록 음모를 꾸미고 있었다.\"]},', '{\"query\": \"한 남자가 레스토랑에서 연설을 하고 있다.\", \"pos\": [\"한 사람이 연설을 하고 있다.\"], \"neg\": [\"그 남자는 테이블에 앉아 음식을 먹고 있다.\", \"이것은 확실히 승인이 아니다.\", \"그들은 은퇴 때문에 집을 팔았지, 대출 때문이 아니다.\", \"미주리 주의 인장은 완벽하다.\", \"누군가가 손을 들고 있다.\", \"한 운동선수가 1500미터 수영 경기에 참가하고 있다.\", \"두 남자가 마술 쇼를 보고 있다.\"]},', '{\"query\": \"인디언들이 코트를 입고 음식과 음료를 가지고 모임을 갖고 있다.\", \"pos\": [\"인디언 그룹이 음식과 음료를 가지고 모임을 갖고 있다\"], \"neg\": [\"인디언 그룹이 장례식을 하고 있다\", \"이것은 팔마의 큰 투우장에서 겨울 오후에만 공연된다.\", \"올바른 정보는 법률 서비스 관행과 사법 체계를 강화할 수 있다.\", \"한편, 본토는 인구가 없었다.\", \"두 아이가 자고 있다.\", \"어부가 원숭이를 잡으려고 하고 있다\", \"사람들이 기차 안에 있다\"]},', '{\"query\": \"보라색 머리를 한 여성이 밖에서 자전거를 타고 있다.\", \"pos\": [\"한 여성이 자전거를 타고 있다.\"], \"neg\": [\"한 여성이 공원에서 조깅을 하고 있다.\", \"그 거리는 하얀색으로 칠해진 집들로 가득했다.\", \"한 그룹이 안에서 영화를 보고 있다.\", \"소풍에서 남자들이 스테이크를 자르고 있다\", \"여러 명의 요리사들이 앉아서 음식에 대해 이야기하고 있다.\", \"위원회는 중요한 대안들이 고려되지 않았다고 지적한다.\", \"우리는 장작이 다 떨어져서 불을 위해 소나무 바늘을 사용해야 했다.\"]},', '{\"query\": \"한 남자가 도시 거리에서 인력거로 두 여성을 끌고 있다.\", \"pos\": [\"한 남자가 도시에 있다.\"], \"neg\": [\"한 남자가 비행기 조종사이다.\", \"그것은 지루하고 평범하다.\", \"아침 햇살이 밝게 비치고 따뜻했다.\", \"두 사람이 부두에서 뛰어내렸다.\", \"사람들이 우주선 발사를 보고 있다.\", \"테레사 수녀는 쉬운 선택이다.\", \"원하는 속도로 갈 수 있는 것은 가치가 있다.\"]}\\n]\\n# JSONL 파일로 저장\\nwith open(\\'toy_finetune_data.jsonl\\', \\'w\\', encoding=\\'utf-8\\') as f:\\nfor item in translated_data:\\njson.dump(item, f, ensure_ascii=False)\\nf.write(\\'\\\\n\\')\\nprint(\"데이터가 \\'toy_finetune_data.jsonl\\' 파일로 저장되었습니다.\")\\n데이터가 \\'toy_finetune_data.jsonl\\' 파일로 저장되었습니다.', 'print(\"데이터가 \\'toy_finetune_data.jsonl\\' 파일로 저장되었습니다.\")\\n데이터가 \\'toy_finetune_data.jsonl\\' 파일로 저장되었습니다.\\n위 코드를 실행하면 총 10개의 데이터가 저장되어져 있는 toy_finetune_data.jsonl 파일이 현재 경로에 저장됩니다. 만약, 여러분들이 데이터셋을 구축할 때 사용자가 검색할 것으로 예상되는 검색어에 해당하는 query와 검색이 되었으면 하는 문서에 해당하는 pos는 갖고 있지만, neg는 따로 갖고 있지 않을 수도 있습니다.\\n이 경우에는 우리가 가진 데이터에서 임의로 neg를 랜덤으로 뽑아서 데이터에 넣는 방식을 사용할 수도 있는데, 이를 네거티브 샘플링(Negative Sampling)이라고 합니다. 네거티브 샘플링을 자동으로 수행해주는 코드를 아래에 준비하였으니 참고하시기 바랍니다.']\n",
      "['만약 여러분들의 데이터가 이미 충분히 잘 구축되어져 있다면 아래의 네거티브 샘플링 과정은 생략해도 무방합니다. 그렇지 않다면, 아래의 코드를 수행하여 여러분들의 데이터를 자동으로 최적화하는 과정을 거쳐봅시다.\\n!python -m FlagEmbedding.baai_general_embedding.finetune.hn_mine \\\\\\n--model_name_or_path BAAI/bge-m3 \\\\\\n--input_file toy_finetune_data.jsonl \\\\\\n--output_file toy_finetune_data_minedHN.jsonl \\\\\\n--range_for_sampling 2-200 \\\\\\n--negative_number 15 \\\\\\n--use_gpu_for_searching', '--range_for_sampling 2-200 \\\\\\n--negative_number 15 \\\\\\n--use_gpu_for_searching\\ninput_file: 최적화 하기를 원하는 여러분들이 파인튜닝을 위해 준비한 JSON 데이터입니다. 위 코드를 실행하면 각 query에 대해서 실제로 bge-m3로 유사도가 높은 상위 k개의 문서를 검색하고, 이 상위 k개 문서에서 무작위로 네거티브 샘플링을 수행합니다 (pos 문서는 제외).\\noutput_file: 네거티브 샘플링을 수행하여 여러분들의 데이터를 학습하기에 최적화 시킨 후의 JSON 데이터를 저장할 경로입니다.\\nnegative_number: 샘플링할 네거티브의 수입니다.', 'negative_number: 샘플링할 네거티브의 수입니다.\\nrange_for_sampling: 네거티브를 샘플링할 범위입니다. 예를 들어, 2-100은 상위 2위부터 200위 문서 중에서 negative_number만큼의 네거티브를 샘플링한다는 의미입니다. 네거티브의 난이도를 낮추기 위해 더 큰 값을 설정할 수 있습니다 (예: 상위 60-300위 문서에서 네거티브를 샘플링하려면 60-300으로 설정)\\nuse_gpu_for_searching: 네거티브 검색에 faiss-gpu를 사용할지 여부입니다.\\n코드 실행 후에는 toy_finetune_data_minedHN.jsonl 라는 새로운 파일이 생기게 되며 해당 파일은 우리가 기존에 만들어두었던 toy_finetune_data.jsonl 파일과는 내용이 다릅니다. 예를 들어 변경 전의 첫번째 데이터와 변경 후의 첫번째 데이터를 비교해봅시다.\\n변경 전', '변경 전\\n네거티브 샘플링을 수행하기 전의 toy_finetune_data.jsonl 파일의 첫번째 데이터입니다.\\n{\"query\": \"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\", \"pos\": [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\"], \"neg\": [\"4명의 여성이 해변에 앉아 있다.\", \"1996년에 개혁이 있었다.\", \"그녀는 자신의 기록을 정정하기 위해 법정에 가지 않을 것이다.\", \"그 남자는 하와이에 대해 이야기하고 있다.\", \"한 여성이 밖에 서 있다.\", \"전투는 끝났다.\", \"한 무리의 사람들이 배구를 하고 있다.\"]}\\n변경 후\\n네거티브 샘플링을 수행한 후의 toy_finetune_data_minedHN.jsonl 파일의 첫번째 데이터입니다.', '{\"query\": \"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\", \"pos\": [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\"], \"neg\": [\"고양이가 달리고 있다\", \"그들은 은퇴 때문에 집을 팔았지, 대출 때문이 아니다.\", \"우리는 장작이 다 떨어져서 불을 위해 소나무 바늘을 사용해야 했다.\", \"조끼를 입은 남자가 차 안에 앉아 있다.\", \"공립학교에서 우리에게 재정은 문제이다.\", \"아무도 뛰고 있지 않다\", \"한 아이가 자기 방에서 책을 읽고 있다.\", \"두 사람이 부두에서 뛰어내렸다.\", \"이 규칙은 사람들이 자녀 양육비를 내는 것을 막는다.\", \"두 여성이 기타와 드럼을 연주하고 있다.\", \"1996년에 개혁이 있었다.\", \"사람들은 나이가 들어도 절대 잊지 않는다.\", \"콘라드는 머리를 맞도록 음모를 꾸미고 있었다.\", \"중요한 활동들을 설명하지만 그 활동들과 관련된 중요한 요소들에 대한 규정은 없다.\", \"원하는 속도로 갈 수 있는', '\"콘라드는 머리를 맞도록 음모를 꾸미고 있었다.\", \"중요한 활동들을 설명하지만 그 활동들과 관련된 중요한 요소들에 대한 규정은 없다.\", \"원하는 속도로 갈 수 있는 것은 가치가 있다.\"]}', '우선 네거티브 샘플(neg)의 수가 변경되었습니다. 원본 데이터에서는 각 데이터마다 네거티브 샘플의 수가 일정하지 않았습니다. 변경된 데이터에서는 각 쿼리마다 정확히 15개의 네거티브 샘플이 있습니다. 이는--negative_number 15 옵션 때문입니다.\\n네거티브 샘플의 내용도 변경되었습니다. 원본 데이터의 네거티브 샘플들은 쿼리와 완전히 무관한 내용이 많았습니다. 변경된 데이터의 네거티브 샘플들은 쿼리와 더 관련이 있어 보이는 내용으로 바뀌었습니다.']\n",
      "['이제 준비한 데이터 toy_finetune_data_minedHN.jsonl를 사용하여 한글 성능이 뛰어난 SBERT 모델인 BAAI/bge-m3을 파인 튜닝해봅시다.\\n!torchrun --nproc_per_node 1 \\\\\\n-m FlagEmbedding.baai_general_embedding.finetune.run \\\\\\n--output_dir ./checkpoint \\\\\\n--model_name_or_path BAAI/bge-m3 \\\\\\n--train_data ./toy_finetune_data.jsonl \\\\\\n--learning_rate 1e-5 \\\\\\n--fp16 \\\\\\n--num_train_epochs 5 \\\\\\n--per_device_train_batch_size 1 \\\\\\n--dataloader_drop_last True \\\\\\n--normlized True \\\\\\n--temperature 0.02 \\\\\\n--query_max_len 64 \\\\\\n--passage_max_len 256 \\\\', '--normlized True \\\\\\n--temperature 0.02 \\\\\\n--query_max_len 64 \\\\\\n--passage_max_len 256 \\\\\\n--train_group_size 2 \\\\\\n--negatives_cross_device \\\\\\n--logging_steps 10 \\\\\\n--save_steps 1000 \\\\\\n--query_instruction_for_retrieval \"\"\\nper_device_train_batch_size: 학습 시 배치 크기입니다. 대부분의 경우, 더 큰 배치 크기가 더 강력한 성능을 가져옵니다. GPU 성능에 달려있으며, 여기서는 토이 프로젝트이고 데이터가 소량이므로 1을 선택했습니다.', 'train_group_size: 학습 시 쿼리당 긍정 및 부정 예제의 수입니다. 항상 하나의 긍정 예제가 있으므로, 이 인자는 부정 예제의 수를 제어합니다 (부정 예제 수 = train_group_size - 1). 부정 예제의 수가 데이터의 \"neg\":List[str]에 있는 부정 예제 수보다 크지 않아야 함에 주의하세요. 이 그룹의 부정 예제 외에도, 배치 내 부정 예제도 파인튜닝에 사용됩니다.\\nnegatives_cross_device: 모든 GPU에서 부정 예제를 공유합니다. 이 인자는 부정 예제의 수를 확장합니다.\\nlearning_rate: 모델에 적합한 값을 선택하세요. 대규모/기본/소규모 모델에 대해 각각 1e-5/2e-5/3e-5를 추천합니다.\\ntemperature: 유사도 점수의 분포에 영향을 미칩니다. 권장 값: 0.01-0.1.\\nquery_max_len: 쿼리의 최대 길이입니다. 데이터의 평균 쿼리 길이에 따라 설정해 주세요.', 'query_max_len: 쿼리의 최대 길이입니다. 데이터의 평균 쿼리 길이에 따라 설정해 주세요.\\npassage_max_len: 문장의 최대 길이입니다. 데이터의 평균 문장 길이에 따라 설정해 주세요.\\nquery_instruction_for_retrieval: 쿼리에 대한 지시사항으로, 각 쿼리에 추가됩니다. 아무것도 추가하지 않으려면 \"\"로 설정할 수 있습니다.\\nuse_inbatch_neg: 같은 배치 내의 문장들을 부정 예제로 사용합니다. 기본값은 True입니다.\\nsave_steps: 체크포인트를 저장할 학습 단계 간격을 설정합니다.\\n학습이 끝나면 checkpoint라는 디렉토리에 학습 모델이 저장됩니다.']\n",
      "['학습 데이터에 대해서 학습 전 모델과 학습 후의 모델에 대한 임베딩 유사도 값을 확인해봅시다.\\nfrom FlagEmbedding import FlagModel\\nsentences_1 = [\"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\"]\\nsentences_2 = [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\", \"꽁꽁 얼어붙은 한강 위로 고양이가 걸어가고 있다\"]\\nsentences_1에 있는 문장 \"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\"은 실제로 학습 데이터에 존재했던 데이터로 이번 실험에서 query에 해당하는 문장입니다. 즉, 학습 데이터에 존재했던 유형이므로 파인 튜닝 모델은 이 query에 대해서는 기존 모델보다 유사도가 더 명확하게 나오기를 기대할 수 있습니다.', \"sentences_2에는 sentences_1과 유사도를 구하기 위해 추가한 문장들입니다. 이 문장들은 검색 문서에 해당되며, 첫번째 문장은 실제로 query와 밀접한 문장, 두번째 문장은 query와 밀접하지 않은 문장입니다. 따라서 이상적인 상황에서는 첫번째 문장은 query와 유사도가 높게 나와야 하고, 두번째 문장은 query와 유사도가 낮게 나와야 합니다. 그리고 이 차이가 파인 튜닝 모델에서는 기존 모델보다 더 극명해야만 학습의 효과가 있다고 볼 수 있을 것입니다.\\n실제로 기존 모델과 파인 튜닝 모델에 대해서 query와 첫번째 문장의 유사도, query와 두번째 문장의 유사도를 출력해봅시다.\\n# use_fp16의 값을 True로 사용하면 약간의 성능 저하는 있지만 속도를 더 빠르게 할 수 있습니다.\\nmodel = FlagModel('BAAI/bge-m3', use_fp16=True)\", \"model = FlagModel('BAAI/bge-m3', use_fp16=True)\\nfine_tuned_model = FlagModel('/content/checkpoint', use_fp16=True)\\n# 기존 모델로 각각 임베딩\\nembeddings_1 = model.encode(sentences_1)\\nembeddings_2 = model.encode(sentences_2)\\n# 기존 모델로부터 나온 임베딩으로 유사도 계산\\nsimilarity_from_base_model = embeddings_1 @ embeddings_2.T\\n# 파인 튜닝 모델로 각각 임베딩\\nembeddings_1 = fine_tuned_model.encode(sentences_1)\\nembeddings_2 = fine_tuned_model.encode(sentences_2)\\n# 파인 튜닝 모델로부터 나온 임베딩으로 유사도 계산\", \"embeddings_2 = fine_tuned_model.encode(sentences_2)\\n# 파인 튜닝 모델로부터 나온 임베딩으로 유사도 계산\\nsimilarity_from_fine_tuned_model = embeddings_1 @ embeddings_2.T\\nprint('기존 모델:', similarity_from_base_model)\\nprint('파인 튜닝된 모델:', similarity_from_fine_tuned_model)\\n기존 모델: [[0.9336 0.4077]]\\n파인 튜닝된 모델: [[0.9414 0.3503]]\", '기존 모델: [[0.9336 0.4077]]\\n파인 튜닝된 모델: [[0.9414 0.3503]]\\n기존 모델은 query와 첫번째 문장의 유사도가 0.9336인 반면, 파인 튜닝 모델은 query와 첫번째 문장의 유사도가 0.9414로 두 문장의 유사도를 더 높게 측정하고 있습니다. 반면, 기존 모델은 query와 두번째 문장의 유사도가 0.4077인 반면 파인 튜닝 모델은 query와 두번째 문장의 유사도가 0.3503으로 두 문장의 유사도를 더 낮게 측정하고 있습니다. 이에 따라서 현재 테스트 한 데이터를 기준으로 본다면 파인 튜닝 모델이 학습한 유형의 query에 대해서 유사도를 더 엄밀하게 측정하고 있다고 판단할 수 있습니다.']\n",
      "['기본 BGE 모델을 파인튜닝하면 목표 작업에서의 성능을 향상시킬 수 있지만, 학습한 유형의 도메인을 넘어선 모델의 일반적인 능력이 저하될 수도 있습니다. 예를 들어서 금융 분야의 도메인의 데이터를 학습시키면, 금융 분야에서는 성능이 오를지 몰라도 그 외의 도메인의 데이터에서는 성능 저하가 심각하게 발생하는 경우 등이 있습니다. 이 경우, 파인 튜닝 모델과 기존 모델을 병합하여 일반화 성능을 어느 정도 유지하면서도 파인 튜닝 모델 성능을 가져올 수 있습니다.\\n두 모델 중 어떤 모델의 능력을 더 우선시 하면서 융합할 것인지를 정할 수 있는데, 이는 아래의 코드에서 weights의 값을 조절하여 결정합니다. 예컨대 [0.5, 0.5]는 두 모델의 비중을 50:50으로 가져가는 것을 의미합니다.\\nfrom LM_Cocktail import mix_models, mix_models_with_data', 'from LM_Cocktail import mix_models, mix_models_with_data\\n# 기존 모델과 파인 튜닝 모달을 융합하여 mixed_model_1 디렉토리에 융합 모델을 저장.\\nmodel = mix_models(\\nmodel_names_or_paths=[\"BAAI/bge-m3\", \"/content/checkpoint\"],\\nmodel_type=\\'encoder\\',\\nweights=[0.5, 0.5],  # 가중치를 조절하세요.\\noutput_path=\\'./mixed_model_1\\')\\n위 코드를 실행하면 융합된 모델이 mixed_model_1 디렉토리에 저장됩니다. 저장된 모델을 불러와서 앞서 확인한 예제로부터 유사도 점수를 출력해봅시다.\\nsentences_1 = [\"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\"]\\nsentences_2 = [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\", \"꽁꽁 얼어붙은 한강 위로 고양이가 걸어가고 있다\"]', 'sentences_2 = [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\", \"꽁꽁 얼어붙은 한강 위로 고양이가 걸어가고 있다\"]\\n# Setting use_fp16 to True speeds up computation with a slight performance degradation\\nmodel = FlagModel(\\'./mixed_model_1\\', use_fp16=True)\\nembeddings_1 = model.encode(sentences_1)\\nembeddings_2 = model.encode(sentences_2)\\nsimilarity = embeddings_1 @ embeddings_2.T\\nprint(\\'융합된 모델:\\', similarity)\\n융합된 모델: [[0.938  0.3674]]\\n융합된 모델은 기존 모델과 파인 튜닝 모델의 중간 정도의 성능을 얻는 것을 확인할 수 있습니다.', '융합된 모델: [[0.938  0.3674]]\\n융합된 모델은 기존 모델과 파인 튜닝 모델의 중간 정도의 성능을 얻는 것을 확인할 수 있습니다.\\n여러분들이 성능을 높이고자 하는 검색 데이터를 준비하고, 네거티브 샘플링을 통해 데이터를 최적화 시킨 후 실제 파인 튜닝을 통해서 검색 성능을 높여보시기 바랍니다.\\n==================================================\\n--- 19. GPT(Generative Pre-trained Transformer) ---\\n마지막 편집일시 : 2024년 8월 26일 12:23 오전\\n==================================================\\n--- 19-01 지피티(Generative Pre-trained Transformer, GPT) ---\\n16챕터와 17챕터의 사전 학습을 권장드립니다.', '--- 19-01 지피티(Generative Pre-trained Transformer, GPT) ---\\n16챕터와 17챕터의 사전 학습을 권장드립니다.\\n2022년 OpenAI에서 ChatGPT란 인공지능 모델을 공개하였고, 이어 GPT-4, GPT-4o 등을 연속으로 공개하며 놀라운 성능으로 많은 사람들에게 충격을 불러일으키고 있습니다. OpenAI에서는 앞으로도 GPT(Generative Pre-trained Transformer)란 이름의 모델을 계속해서 공개할 것입니다.\\n이번 챕터에서는 OpenAI에서 공개한 GPT라는 모델의 내부 구조와 아이디어에 대해서 실습합니다. 금일 소개할 GPT의 구조는 GPT-2를 기준으로 합니다. 현재 우리가 알고 있는 ChatGPT와 같은 모델들도 GPT-2의 구조에서 파생된 구조입니다.']\n",
      "['BERT가 트랜스포머의 인코더로 설계된 모델이라면, GPT는 트랜스포머의 디코더로 설계된 모델입니다. Open AI는 2019년에 GPT-1을 공개한 이후로, 2019년 GPT-2, 2020년 GPT-3, 2022년 ChatGPT(GPT 3.5), 2023년에는 GPT-4, 2024년에는 GPT-4o를 공개하며 GPT 시리즈를 발전시켜 왔습니다.\\n[이미지: ]\\nGPT-1\\nGPT-2\\nGPT-3\\nGPT-3.5\\nGPT-4 Models\\n파라미터 개수\\n1억 1700만개\\n15억\\n1,750억\\n?\\n1조 8천억(추정)\\n디코더의 층\\n12\\n48\\n96\\n?\\n?\\n처리 가능한 토큰 개수\\n512\\n1024\\n2048\\n?\\n128,000\\n은닉층의 크기\\n768\\n1600\\n12288\\n?\\n?', '?\\n1조 8천억(추정)\\n디코더의 층\\n12\\n48\\n96\\n?\\n?\\n처리 가능한 토큰 개수\\n512\\n1024\\n2048\\n?\\n128,000\\n은닉층의 크기\\n768\\n1600\\n12288\\n?\\n?\\n위의 표는 Open AI가 GPT 시리즈의 모델 사이즈(모델 파라미터 개수)를 꾸준히 키우고 있음을 보여줍니다. 파라미터 수 기준으로 보면 GPT-3은 GPT-1 대비 1400배, GPT-2 대비 117배나 증가했습니다. 다만, GPT-3.5부터는 모델의 크기를 공식적으로는 공개하고 있지 않은데 GPT-3.5도 최소한 GPT-2보다는 수십 배의 크기를 가지고 있다는 것이 정설입니다.', 'GPT-1은 2018년에 공개된 최초의 모델로, 약 1억 1,700만 개의 파라미터를 가지고 있습니다. 이 모델은 12개의 디코더 층을 가진 트랜스포머 아키텍처를 사용합니다. GPT-1의 가장 큰 성과 중 하나는 제로샷 학습 능력으로, 다양한 자연어 처리 작업에서 초기 성능을 보여 주었습니다. GPT-2는 2019년 후반에 발표된 후속 모델로, 약 15억 개(1.5B)의 파라미터를 보유하며 GPT-1 대비 약 10배 커진 규모로 GPT-1보다 문맥을 이해하는 능력이 개선되었습니다. (B는 10억이라는 의미입니다.)', \"GPT-3는 약 1,750억 개(175B)의 파라미터를 가지며, GPT-2보다 약 100배 더 큽니다. 이 모델은 'Common Crawl' 데이터라 불리는 인터넷에서 수집된 5000억 단어 이상의 데이터 세트를 사용하여 훈련되었습니다. 당시 GPT-3는 모델의 크기(파라미터의 개수)가 충분히 커질 경우, 마치 사람의 지시사항을 따르는 것처럼 보이는 현재의 거대 언어 모델(Large Language Model)의 가능성을 직접적으로 보여준 모델이었습니다. 그리고 2022년 12월. GPT-3의 후속 모델인 ChatGPT가 누구나 쉽게 접근할 수 있는 접근성 높은 Chat UI형태로 공개되면서 전세계적인 관심을 받게되었고, 거대 언어 모델의 시대가 도래하게 됩니다.\"]\n",
      "['언어 모델(Language Model)은 앞서 수차례 언급한 개념이지만, 인공 지능 분야에서 컴퓨터가 사람의 언어를 이해하고 생성할 수 있도록 하는 기술입니다. 이 모델은 대규모의 텍스트 데이터를 통해, 이전 단어들로부터 다음 단어를 예측하는 방식으로 작동합니다. 예를 들어, \"아침에 일어나 창문을 열었더니\"라는 문장이 주어지면, 언어 모델은 가장 그럴 듯한 다음 단어들을 나열하여 \"바람이 상쾌하게 불어왔다\"와 같은 이어지는 문장을 예측하는 모델입니다.\\n언어 모델의 대표적인 예로는 OpenAI의 GPT(Generative Pre-trained Transformer) 시리즈가 있습니다. 많은 입문자들이 착각하는 내용은 언어 모델과 GPT는 동격의 개념이 아니라는 것입니다. (3챕터 언어 모델 참고)', '기본적으로 언어 모델은 이전 단어들로부터 다음 단어를 예측하는 생성 모델이고, GPT는 수많은 언어 모델 중 하나입니다. 예를 들어서 구글의 Gemini, 앤트로픽의 Claude, 네이버의 CLOVA X와 같은 모델들도 언어 모델 중 하나일 것입니다. 이 중 딥 러닝 언어 모델로서 파라미터 개수가 충분히 큰 언어 모델을 우리는 거대 언어 모델(Large Language Model)이라고 부릅니다.']\n",
      "['[이미지: ]\\n앞서 16장에서 트랜스포머(인코더-디코더) 구조를 학습했고, 17장에서는 기존의 인코더-디코더 아키텍처에서 인코더만 분리한 BERT(인코더 Only) 라는 모델을 학습했습니다. 이번 장에서 학습하는 GPT(디코더 Only) 는 기본적으로 트랜스포머 디코더로만 구성되어져 있습니다. GPT는 기본적으로 BERT와는 달리 이전 단어들로부터 다음 단어를 예측하는 모델이기 때문에, 다음 단어를 지속적으로 생성할 수 있어 기본적으로 글쓰기가 가능한 생성 모델입니다. 트랜스포머 디코더 아키텍처를 가지며 다음 단어를 생성하는 이러한 구조는 현재 대부분의 거대 언어 모델이 채택하고 있는 구조이기도 합니다.\\n[이미지: ]', '[이미지: ]\\n위의 GPT-2 아키텍처를 도식화한 그림은 트랜스포머 디코더 층이 16 챕터에서 배웠던 초기 트랜스포머 디코더 층과 크게 다르지 않음을 보여줍니다. 16챕터에서 배웠던 트랜스포머에서의 디코더는 인코더가 같이 존재하는 아키텍처였기 때문에 인코더-디코더 어텐션이 존재했는데 여기서는 해당 부분이 제거되었습니다.\\n[이미지: ]\\n사실 GPT-1, GPT-2, GPT-3는 아키텍처 상으로는 큰 차이를 보이지 않습니다. 예컨대, GPT-1은 16 챕터에서 배웠던 초기 트랜스포머 디코더에서 인코더-디코더 어텐션이 제거된 아키텍처이고, GPT-1에서는 층 정규화(Layer Normalization)가 서브층(Sub layer) 다음에 위치한다면, GPT-2는 서브층(Sub layer)의 입력으로 이동되었습니다. 그 외에는 초기화 방법이나, 단어 집합의 크기, 토큰 길이의 확장(GPT-1이 512였으나 GPT-2에서 1024로 증가) 정도의 차이를 가집니다.', '이는 GPT-2에서 GPT-3로 모델이 다시 한 번 변경되었을 때에도 마찬가지입니다. OpenAI는 GPT-3에서도 특별히 새로운 모델, 아키텍처, 알고리즘을 제안하지 않습니다. 그저 모델이 거대 언어 모델(Large Language Model)이 되면서 새로운 능력들이 발현되기 시작했다는 점이 뚜렷하게 볼 수 있는 차이점입니다.\\nGPT 모델은 최대한 다양하고 대량의 학습 데이터가 많이 확보되면서 파인 튜닝(Fine-tuning)을 하지 않았을 때 사전 학습(Pre-training)만으로도 특정 문제를 풀 수 있는 일반화 모델(General Language Model)로 발전해나가고 있으며, 이러한 기조는 현재의 수많은 거대 언어 모델들에서 여전히 이어지고 있습니다. 이에 대한 자세한 내용은 추후 이어지는 거대 언어 모델 챕터에서 다룹니다.\\n==================================================', \"==================================================\\n--- 19-02 GPT-2를 이용한 문장 생성 ---\\n```\\n'근육이 커지기 위해서는 무엇보다 균형 있는 식사량이 필요하다.\\\\n또, 규칙적인 운동을 통한 영양소 섭취와 규칙적인 운동 등을 통해서 체지방을 줄여나가는 게 중요하다.\\\\n최소 한 달에 1회 이상 운동을 하고 체중이 감소하면 그 효과를 확인할 수는 있지만, 그 효과가 반감되는'\\n```\\n모든 GPT 실습은 Colab에서 진행한다고 가정하고 설명합니다.\\n사전 학습된 한국어 GPT-2를 이용하여 다음 문장 예측을 실습해봅시다. 이번 실습을 위해서만이 아니라 앞으로 사전 학습된 GPT를 사용할 때는 transformers라는 패키지를 자주 사용하게 됩니다. 실습 환경에 transformers 패키지를 설치해둡시다.\\npip install transformers\"]\n",
      "[\"transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. BERT와 마찬가지로 GPT는 이미 누군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야 합니다.\\nimport numpy as np\\nimport random\\nimport tensorflow as tf\\nfrom transformers import AutoTokenizer\\nfrom transformers import TFGPT2LMHeadModel\\nTFGPT2LMHeadModel.from_pretrained('GPT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 GPT 구조를 로드합니다. AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다.\", \"model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\\ntokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\\nGPT가 생성할 문장의 방향성을 알려주기 위해서 시작 문자열을 정해줍니다. 여기서는 '근육이 커지기 위해서는' 이라는 문자열을 주고 GPT에게 이어서 문장을 생성해보라고 해봅시다.\\nsent = '근육이 커지기 위해서는'\\nGPT의 입력으로는 정수 인코딩 된 결과가 입력되어야 하므로 tokenizer.encode()를 통해서 '근육이 커지기 위해서는'이라는 문자열을 정수 시퀀스로 변환해줍니다.\\ninput_ids = tokenizer.encode(sent)\\ninput_ids = tf.convert_to_tensor([input_ids])\\nprint(input_ids)\", 'input_ids = tokenizer.encode(sent)\\ninput_ids = tf.convert_to_tensor([input_ids])\\nprint(input_ids)\\ntf.Tensor([[33245 10114 12748 11357]], shape=(1, 4), dtype=int32)\\n33245 10114 12748 11357라는 5개의 정수 시퀀스를 얻습니다. 해당 정수 시퀀스를 GPT의 입력으로 사용하여 GPT가 이어서 문장을 생성하도록 해봅시다. 주어진 문장으로부터 이어서 문장을 생성하도록 하는 것은 model.generate()를 사용합니다.\\noutput = model.generate(input_ids,\\nmax_length=128,\\nrepetition_penalty=2.0,\\nuse_cache=True)\\noutput_ids = output.numpy().tolist()[0]\\nprint(output_ids)', '[33245, 10114, 12748, 11357, 23879, 39306, 9684, 7884, 10211, 15177, 26421, 387, 17339, 7889, 9908, 15768, 6903, 15386, 8146, 12923, 9228, 18651, 42600, 9564, 17764, 9033, 9199, 14441, 7335, 8704, 12557, 32030, 9510, 18595, 9025, 10571, 25741, 10599, 13229, 9508, 7965, 8425, 33102, 9122, 21240, 9801, 32106, 13579, 12442, 13235, 19430, 8022, 12972, 9566, 11178, 9554, 24873, 7198, 9391, 12486, 8711, 9346, 7071, 36736, 9693, 12006, 9038, 10279, 36122, 9960, 8405, 10826, 18988, 25998, 9292, 7671,', '9346, 7071, 36736, 9693, 12006, 9038, 10279, 36122, 9960, 8405, 10826, 18988, 25998, 9292, 7671, 9465, 7489, 9277, 10137, 9677, 9248, 9912, 12834, 11488, 13417, 7407, 8428, 8137, 9430, 14222, 11356, 10061, 9885, 19265, 9377, 20305, 7991, 9178, 9648, 9133, 10021, 10138, 30315, 21833, 9362, 9301, 9685, 11584, 9447, 42129, 10124, 7532, 17932, 47123, 37544, 9355, 15632, 9124, 10536, 13530, 12204, 9184, 36152, 9673, 9788, 9029, 11764]', '기존의 33245 10114 12748 11357 라는 5개의 정수 시퀀스 뒤에도 여러 정수들이 추가로 생성된 것을 볼 수 있습니다. 정수들이 단순히 나열된 것만으로는 GPT가 실제로 어떤 문장을 생성했는지 알기 어려우니 해당 정수 시퀀스를 한국어로 변환해봅시다. 이 과정은 tokenizer.decode()를 사용하여 가능합니다.\\ntokenizer.decode(output_ids)', \"tokenizer.decode(output_ids)\\n'근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\\\\n특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\\\\n또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\\\\n아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\\\\n운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\\\\n운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\\\\n운동을'\", \"'근육이 커지기 위해서는' 라는 문자열에 이어서 그 뒤에 근육이 커지기 위한이라는 문맥에 맞는듯한 글들을 생성합니다. 물론 GPT가 생성한 문장들은 문맥상 그럴듯해 보이지만 실제 사실 여부와는 다를 수 있으니 이 점은 늘 주의해야 합니다.\"]\n",
      "[\"GPT는 기본적으로 이전 단어들로부터 다음 단어를 예측하는 언어 모델(Language Model)입니다. 위의 실습에서 확인한 바와 같이 '근육이 커지기 위해서는' 이라는 입력을 넣었을 때 GPT는 다음 단어로 '무엇보다' 라는 단어를 예측했었는데요. 실제로는 수많은 후보의 다음 단어들이 있었지만, 그 중 가장 확률이 높은 단어. 즉, Top 1의 단어인 '무엇보다'를 예측한 것입니다. 그렇다면 다음 단어로 또 어떤 후보들이 있었는지 Top 5의 단어들을 뽑아봅시다. model()에다가 '근육이 커지기 위해서는'의 정수 인코딩 된 결과를 입력으로 넣은 후 가장 확률이 높은 Top 5의 단어들을 뽑아냅니다.\\noutput = model(input_ids)\\ntop5 = tf.math.top_k(output.logits[0, -1], k=5)\\n그 후 Top 5의 단어를 한국어로 변환하여 출력해봅시다.\", \"top5 = tf.math.top_k(output.logits[0, -1], k=5)\\n그 후 Top 5의 단어를 한국어로 변환하여 출력해봅시다.\\ntokenizer.convert_ids_to_tokens(top5.indices.numpy())\\n['▁무엇보다', '▁우선', '▁반드시', '▁피부', '▁무엇보다도']\\n'무엇보다' 라는 단어 외에도 '우선', '반드시', '피부', '무엇보다도' 라는 4개의 단어가 높은 확률로 선택될 수 있었음을 보여줍니다.\"]\n",
      "[\"앞서 문장을 생성했을 당시에는 각 시점(time step)마다 가장 확률이 높은 단어를 예측했지만, 이번에는 매 시점마다 Top 5개의 단어들 중에서 랜덤으로 선택하는 방식을 택하여 문장을 생성해봅시다.\\nsent = '근육이 커지기 위해서는'\\ninput_ids = tokenizer.encode(sent)\\nwhile len(input_ids) < 50:\\noutput = model(np.array([input_ids]))\\n# Top 5의 단어들을 추출\\ntop5 = tf.math.top_k(output.logits[0, -1], k=5)\\n# Top 5의 단어들 중 랜덤으로 다음 단어로 선택.\\ntoken_id = random.choice(top5.indices.numpy())\\ninput_ids.append(token_id)\\ntokenizer.decode(input_ids)\", \"input_ids.append(token_id)\\ntokenizer.decode(input_ids)\\n'근육이 커지기 위해서는 무엇보다 균형 있는 식사량이 필요하다.\\\\n또, 규칙적인 운동을 통한 영양소 섭취와 규칙적인 운동 등을 통해서 체지방을 줄여나가는 게 중요하다.\\\\n최소 한 달에 1회 이상 운동을 하고 체중이 감소하면 그 효과를 확인할 수는 있지만, 그 효과가 반감되는'\\n==================================================\\n--- 19-03 GPT-2를 이용한 한국어 챗봇 ---\\n```\\n인공지능에 대한 지식이 필요하겠네요.\\n```\\n모든 GPT 실습은 Colab에서 진행한다고 가정하고 설명합니다.\", '--- 19-03 GPT-2를 이용한 한국어 챗봇 ---\\n```\\n인공지능에 대한 지식이 필요하겠네요.\\n```\\n모든 GPT 실습은 Colab에서 진행한다고 가정하고 설명합니다.\\n사전 학습된 한국어 GPT-2를 이용하여 다음 문장 예측을 실습해봅시다. 이번 실습을 위해서만이 아니라 앞으로 사전 학습된 GPT를 사용할 때는 transformers라는 패키지를 자주 사용하게 됩니다. 실습 환경에 transformers 패키지를 설치해둡시다.']\n",
      "['transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. BERT와 마찬가지로 GPT는 이미 누군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야 합니다.\\nimport tensorflow as tf\\nfrom transformers import AutoTokenizer\\nfrom transformers import TFGPT2LMHeadModel', \"from transformers import AutoTokenizer\\nfrom transformers import TFGPT2LMHeadModel\\nTFGPT2LMHeadModel.from_pretrained('GPT 모델 이름')을 넣으면 두 개의 문장이 이어지는 문장 관계인지 여부를 판단하는 GPT 구조를 로드합니다. AutoTokenizer.from_pretrained('모델 이름')을 넣으면 해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다. 토크나이저를 로드할 때 텍스트 생성을 시작할 때 사용하는 시작 토큰을 bos_token, 텍스트 생성을 종료할 때 사용하는 종료 토큰을 eos_token이라는 이름으로 지정할 수 있습니다. 여기서는 </s>라는 토큰을 시작 토큰이자 종료 토큰으로 사용하고자 합니다.\", \"tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')\\nmodel = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\\nGPT에는 특별한 용도로 사용하기 위해서 할당해놓은 특별 토큰(special token)들이 존재합니다. Ko-GPT2에서 사용되는 특별 토큰들의 정수 그리고 반대로 정수 1, 2, 3, 4는 각각 어떤 토큰들을 의미하는지 확인해봅시다.\\nprint(tokenizer.bos_token_id)\\nprint(tokenizer.eos_token_id)\\nprint(tokenizer.pad_token_id)\\nprint('-' * 10)\\nprint(tokenizer.decode(1))\\nprint(tokenizer.decode(2))\", \"print(tokenizer.pad_token_id)\\nprint('-' * 10)\\nprint(tokenizer.decode(1))\\nprint(tokenizer.decode(2))\\nprint(tokenizer.decode(3))\\nprint(tokenizer.decode(4))\\n1\\n1\\n3\\n----------\\n</s>\\n<usr>\\n<pad>\\n<sys>\", 'print(tokenizer.decode(3))\\nprint(tokenizer.decode(4))\\n1\\n1\\n3\\n----------\\n</s>\\n<usr>\\n<pad>\\n<sys>\\n주로 문장의 시작을 의미하는 용도로 사용되는 BOS 토큰과 주로 문장의 종료를 의미하는 용도로 사용되는 EOS token이 정수로는 몇 번인지 확인하기 위해서 tokenizer.bos_token_id와 tokenizer.eos_token_id를 각각 출력했습니다. 또한 패딩 토큰이 정수로는 몇 번인지 확인하기 위해서 tokenizer.pad_token_id를 출력했습니다. 확인 결과, KoGPT-2에서는 BOS 토큰과 EOS 토큰이 동일한 정수 1로 맵핑되는 것으로 확인되었으며 (이것은 KoGPT-2 제작자들이 이렇게 정한 것입니다.) 패딩 토큰은 정수 3으로 맵핑되는 것을 확인했습니다.', '반대로 정수 1, 2, 3, 4를 단어로 표현할 경우를 출력해보았습니다. 정수 1. 즉, BOS 토큰이기도 하고, EOS 토큰이기도 한 토큰은 </s>로, 패딩 토큰인 정수 3은 <pad>로 출력되는 것을 확인했습니다. 정수 2와 정수 4의 경우에는 <usr>와 <sys>라는 토큰으로 맵핑되어져 있었는데, 이는 GPT로 대화 모델을 만들 경우에 각각 유저(User)와 시스템(System)을 구분하기 위한 용도입니다. 이번 실습에서도 실제로 이 두 토큰을 이용하여 챗봇을 구현할 것이므로 어떠한 의미인지는 뒤에서 실습을 통해 이해해보겠습니다.']\n",
      "['import pandas as pd\\nimport tqdm\\nimport urllib.request\\n챗봇 구현을 위해 챗봇 데이터를 로드합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\\ntrain_data = pd.read_csv(\\'ChatBotData.csv\\')\\nprint(\\'챗봇 데이터의 개수 :\\', len(train_data))\\n챗봇 데이터의 개수 : 11823\\n챗봇 데이터의 개수는 11,823개로 해당 데이터는 앞서 18-08 챕터에서 사용한 데이터와 동일하므로 데이터에 대한 설명은 생략합니다.']\n",
      "[\"챗봇 데이터를 전처리하기 위한 함수 get_chat_data()를 만듭니다.\\ndef get_chat_data():\\nfor question, answer in zip(train_data.Q.to_list(), train_data.A.to_list()):\\nbos_token = [tokenizer.bos_token_id]\\neos_token = [tokenizer.eos_token_id]\\nsent = tokenizer.encode('<usr>' + question + '<sys>' + answer)\\nyield bos_token + sent + eos_token\", \"sent = tokenizer.encode('<usr>' + question + '<sys>' + answer)\\nyield bos_token + sent + eos_token\\n위의 전처리는 <usr> 다음에 사용자의 질문을 부착하고, 그 후 <sys> 다음에 챗봇의 답변을 부착하는 형식으로 전처리를 진행합니다. 그리고 앞 뒤에 시작 토큰(BOS 토큰)과 종료 토큰(EOS 토큰)을 부착하는 전처리입니다. 앞에서 확인했듯이 두 토큰은 동일하며 </s>입니다. 전처리 후에 데이터가 어떻게 변경되는지는 뒤에서 확인합니다. 배치 크기를 32로 하고, 패딩 토큰으로 패딩을 진행합니다. 위에서 확인했듯이 패딩 토큰의 정수는 3입니다.\\nbatch_size = 32\\ndataset = tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32)\\n# 배치 크기만큼 데이터를 구성하되 패딩 토큰으로 패딩을 진행.\", '# 배치 크기만큼 데이터를 구성하되 패딩 토큰으로 패딩을 진행.\\ndataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)\\n첫번째 배치(첫 32개의 데이터 묶음)을 출력해봅시다.\\n# 첫번째 배치 출력\\nfor batch in dataset:\\nprint(batch)\\nbreak\\ntf.Tensor(\\n[[    1     2  9349  7888   739  7318   376     4 12557  6824  9108  9028\\n7098 25856     1     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3]\\n[    1     2  9020  8263  7497 10192 11615  8210  8006     4 12422  8711', '[    1     2  9020  8263  7497 10192 11615  8210  8006     4 12422  8711\\n9535  7483 12521     1     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3]\\n... 중략 ...\\n[    1     2 10464  9136  7380  9071  7513  8711  8210  8006     4  9054\\n7285  9117  7703  7788 11120  8705 14553 10667  8718  7055  7661 25856\\n1     3     3     3     3     3]], shape=(32, 30), dtype=int32)', '1     3     3     3     3     3]], shape=(32, 30), dtype=int32)\\n데이터가 길어 데이터를 중략하였습니다. 32개의 데이터를 보면 길이는 30으로 패딩되었으며 뒤에 채워진 패딩 토큰은 숫자 3이고, 각 데이터의 맨 앞에는 정수 1, 2가 부착되어져 있습니다. 정수 1, 2는 앞에서 확인했던 것과 같이 각각 </s>와 <user>에 해당됩니다. 첫번째 배치는 현재 batch라는 변수에 저장되어져 있으므로 32개의 데이터 중 가장 첫번째 샘플을 출력해봅시다. 이때 이미 정수 인코딩이 진행된 상황이므로 tokenizer.decode()를 통해 정수 인코딩 된 결과를 다시 복원하여 출력합니다.\\n# 첫번째 배치(32개의 샘플) 중 첫번째 샘플 출력\\nprint(tokenizer.decode(batch[0]))', '# 첫번째 배치(32개의 샘플) 중 첫번째 샘플 출력\\nprint(tokenizer.decode(batch[0]))\\n</s><usr> 12시 땡!<sys> 하루가 또 가네요.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n위의 get_chat_data() 함수로 전처리를 진행할 당시의 의도대로 사용자의 질문 앞에 <usr>가 붙어있고, 챗봇의 답변 앞에는 <sys>가 붙어있습니다. 그리고 사용자의 질문과 챗봇의 답변 앞 뒤로는 시작 토큰과 종료 토큰에 해당하는 </s>이 붙어있으며, 그 뒤에는 패딩 토큰인 <pad>가 붙습니다. 이제 학습을 진행해봅시다.']\n",
      "['# 옵티마이저 결정\\nadam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\\n# 전체 데이터의 개수를 배치 크기로 나누면 하나의 에포크에서 실행되는 학습 횟수가 계산됨.\\nsteps = len(train_data) // batch_size + 1\\nEPOCHS = 3\\nfor epoch in range(EPOCHS):\\nepoch_loss = 0\\nfor batch in tqdm.tqdm_notebook(dataset, total=steps):\\nwith tf.GradientTape() as tape:\\nresult = model(batch, labels=batch)\\nloss = result[0]\\nbatch_loss = tf.reduce_mean(loss)\\ngrads = tape.gradient(batch_loss, model.trainable_variables)', \"batch_loss = tf.reduce_mean(loss)\\ngrads = tape.gradient(batch_loss, model.trainable_variables)\\nadam.apply_gradients(zip(grads, model.trainable_variables))\\nepoch_loss += batch_loss / steps\\nprint('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))\\n[Epoch:    1] cost = 2.12707806\\n[Epoch:    2] cost = 1.69811654\\n[Epoch:    3] cost = 1.37535608\\n저자의 경우 총 3에포크 학습을 진행하였습니다.\"]\n",
      "[\"이제 챗봇에게 답변을 얻기 위해서 어떤 전처리를 진행해야 할 지 차근차근 정리해봅시다. 우선 앞서 모델 학습 전에 전처리를 진행했던 것처럼 사용자의 질문의 앞 뒤에 <usr>와 <sys>를 부착합니다. 그 후 해당 문장 앞에 시작 토큰에 해당하는 </s>를 부착합니다.\\ntext = '오늘도 좋은 하루!'\\nsent = '<usr>' + text + '<sys>'\\ninput_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\\ninput_ids = tf.convert_to_tensor([input_ids])\\nprint('정수 인코딩 후 :', input_ids)\\nprint('정수 인코딩을 재복원 :', tokenizer.decode(input_ids[0]))\\n정수 인코딩 후 : tf.Tensor([[    1     2 10070  7235 10586 12557   376     4]], shape=(1, 8), dtype=int32)\", '정수 인코딩을 재복원 : </s><usr> 오늘도 좋은 하루!<sys>\\n현재의 입력은 챗봇이 학습할 당시의 형태(</s><usr>사용자의 질문<sys>챗봇의 답변</s>)에서 챗봇의 답변이 바로 시작되기 전까지의 형태입니다. 기존에 학습하였을 당시에는 <sys> 뒤에 챗봇의 답변이 있었으므로 현재의 입력을 KoGPT-2에 입력으로 사용하면 챗봇은 학습때 봤던 데이터의 형식 그대로 챗봇의 답변을 작성하려는 관성을 갖고 있습니다. 현재의 입력을 모델의 입력으로 넣고 모델이 생성하는 문장을 얻습니다.\\noutput = model.generate(input_ids, max_length=50, early_stopping=True, eos_token_id=tokenizer.eos_token_id)\\ndecoded_sentence = tokenizer.decode(output[0].numpy().tolist())\\nprint(decoded_sentence)', \"decoded_sentence = tokenizer.decode(output[0].numpy().tolist())\\nprint(decoded_sentence)\\n</s><usr> 오늘도 좋은 하루!<sys> 오늘도 좋은 하루네요.</s>\\nKoGPT-2의 출력은 사용자가 넣어준 입력이었던 </s><usr> 오늘도 좋은 하루!<sys>도 포함하여 반환되므로 챗봇의 답변만 확인하기 위해서는 <sys>를 기준으로 분할하여 뒷 부분만 가져와야 합니다.\\nprint(decoded_sentence.split('<sys> ')[1].replace('</s>', ''))\\n오늘도 좋은 하루네요.\\n만약 동일한 질문에도 KoGPT-2의 답변이 랜덤성을 갖기를 원한다면 do_sample=True, top_k=10 이라는 파라미터를 통해서 후보가 되는 단어 10개 중 랜덤으로 선택하여 생성하도록 유도할 수 있습니다.\", \"output = model.generate(input_ids, max_length=50, do_sample=True, top_k=10)\\ndecoded_sentence = tokenizer.decode(output[0].numpy().tolist())\\nprint(decoded_sentence.split('<sys> ')[1].replace('</s>', ''))\\n오늘도 좋은 하루 감사해요.\\n지금까지의 과정을 함수로 만들어서 임의의 입력에 대해서 챗봇의 답변을 출력해봅시다.\\ndef return_answer_by_chatbot(user_text):\\nsent = '<usr>' + user_text + '<sys>'\\ninput_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\\ninput_ids = tf.convert_to_tensor([input_ids])\", \"input_ids = tf.convert_to_tensor([input_ids])\\noutput = model.generate(input_ids, max_length=50, do_sample=True, top_k=20)\\nsentence = tokenizer.decode(output[0].numpy().tolist())\\nchatbot_response = sentence.split('<sys> ')[1].replace('</s>', '')\\nreturn chatbot_response\\nreturn_answer_by_chatbot('안녕! 반가워~')\\n반가워요\\nreturn_answer_by_chatbot('너는 누구야?')\\n저한테 물어보면 저랑 이야기할 수 있어요.\\nreturn_answer_by_chatbot('너무 심심한데 나랑 놀자')\\n같이 놀아요.\\nreturn_answer_by_chatbot('영화 해리포터 재밌어?')\\n영화를 보면 다 알 수 있지 않을까요.\", \"같이 놀아요.\\nreturn_answer_by_chatbot('영화 해리포터 재밌어?')\\n영화를 보면 다 알 수 있지 않을까요.\\nreturn_answer_by_chatbot('너 딥 러닝 잘해?')\\n인공지능에 대한 지식이 필요하겠네요.\\nKoGPT-2는 상대적으로 모델의 크기가 작습니다. 챗봇의 성능을 올리고 싶다면 데이터의 양을 증량하거나 T5-Large와 같은 보다 큰 모델을 시도하시기를 권합니다.\\n==================================================\\n--- 19-04 GPT-2를 이용한 네이버 영화 리뷰 분류 ---\\n```\\n93.78% 확률로 긍정 리뷰입니다.\\n```해당 실습은 Colab에서 TPU를 사용한다고 가정합니다.\\nColab에서 런타임 > 런타임 유형 변경 > 하드웨어 가속기에서 'TPU' 선택\\n만약 GPU 환경에서 아래의 실습 코드를 실습하고 싶다면 TPU 설정 코드들을 제거해주면 됩니다.\", '만약 GPU 환경에서 아래의 실습 코드를 실습하고 싶다면 TPU 설정 코드들을 제거해주면 됩니다.\\npip install transformers']\n",
      "['import pandas as pd\\nimport numpy as np\\nimport urllib.request\\nimport os\\nfrom tqdm import tqdm\\nimport tensorflow as tf\\nfrom transformers import AutoTokenizer, TFGPT2Model\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\n네이버 영화 리뷰 데이터 학습을 위해 훈련 데이터와 테스트 데이터를 다운로드합니다.\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")', 'urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\\ntrain_data = pd.read_table(\\'ratings_train.txt\\')\\ntest_data = pd.read_table(\\'ratings_test.txt\\')\\nprint(\\'훈련용 리뷰 개수 :\\',len(train_data)) # 훈련용 리뷰 개수 출력\\nprint(\\'테스트용 리뷰 개수 :\\',len(test_data)) # 테스트용 리뷰 개수 출력\\n훈련용 리뷰 개수 : 150000\\n테스트용 리뷰 개수 : 50000\\n훈련 데이터와 테스트 데이터의 리뷰 개수는 각각 15만개와 5만개입니다. 중복 데이터와 결측값을 제거합니다.', \"훈련용 리뷰 개수 : 150000\\n테스트용 리뷰 개수 : 50000\\n훈련 데이터와 테스트 데이터의 리뷰 개수는 각각 15만개와 5만개입니다. 중복 데이터와 결측값을 제거합니다.\\ntrain_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\\ntrain_data = train_data.dropna(how='any') # Null 값이 존재하는 행 제거\\nprint('훈련 데이터의 리뷰 수 :',len(train_data))\\n훈련 데이터의 리뷰 수 : 146182\\n훈련 데이터의 총 샘플의 수는 146,182개입니다. 마찬가지로 테스트 데이터에서도 결측값을 제거합니다.\\ntest_data = test_data.dropna(how = 'any')\\nprint('테스트 데이터의 리뷰 수 :',len(test_data))\\n테스트 데이터의 리뷰 수 : 49997\", \"print('테스트 데이터의 리뷰 수 :',len(test_data))\\n테스트 데이터의 리뷰 수 : 49997\\n테스트 데이터에서의 샘플 수는 49,997개입니다.\"]\n",
      "['AutoTokenizer.from_pretrained(\\'모델 이름\\')을 사용하면 모델 이름에 맞는 토크나이저를 자동으로 로드합니다. skt/kogpt2-base-v2의 토크나이저를 로드해봅시다.\\ntokenizer = AutoTokenizer.from_pretrained(\\'skt/kogpt2-base-v2\\', bos_token=\\'</s>\\', eos_token=\\'</s>\\', pad_token=\\'<pad>\\')\\n토크나이저의 tokenize()를 사용하면 토큰화 결과를 얻을 수 있습니다.\\nprint(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\\n[\\'▁보는\\', \\'내\\', \\'내\\', \\'▁그대로\\', \\'▁들어\\', \\'맞\\', \\'는\\', \\'▁예측\\', \\'▁카\\', \\'리스\\', \\'마\\', \\'▁없는\\', \\'▁악\\', \\'역\\']\\n토크나이저의 encode()를 사용하면 정수 인코딩 결과를 얻을 수 있습니다.', '토크나이저의 encode()를 사용하면 정수 인코딩 결과를 얻을 수 있습니다.\\nprint(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\\n[11867, 7071, 7071, 10554, 9359, 7498, 7162, 15305, 9488, 10191, 7487, 9712, 9868, 8031]\\ndecode()를 사용하면 정수 인코딩 결과를 다시 텍스트로 변환합니다.\\ntokenizer.decode(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\\n보는내내 그대로 들어맞는 예측 카리스마 없는 악역\\n패딩 토큰을 확인해보겠습니다.\\nprint(tokenizer.decode(3))\\n<pad>', '보는내내 그대로 들어맞는 예측 카리스마 없는 악역\\n패딩 토큰을 확인해보겠습니다.\\nprint(tokenizer.decode(3))\\n<pad>\\n패딩 토큰은 3번임을 확인했습니다. encode()를 할 때, 최대 길이를 지정하고 해당 길이까지 패딩하는 것도 가능합니다. max_length의 값으로 최대 길이를 지정해주고 pad_to_max_length의 값을 True로 하여 정수 인코딩을 하는 동시에 최대 길이까지 패딩해봅시다.\\nmax_seq_len = 128\\nencoded_result = tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\", max_length=max_seq_len, pad_to_max_length=True)\\nprint(encoded_result)\\nprint(\\'길이 :\\', len(encoded_result))', \"print(encoded_result)\\nprint('길이 :', len(encoded_result))\\n[9034, 13555, 16447, 10584, 389, 9427, 10056, 7898, 8135, 10584, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\\n길이 : 128\", '길이 : 128\\n전체 데이터에 대해서 전처리를 진행해봅시다. 전처리 과정에서 koGPT-2의 시작 토큰/종료 토큰을 리뷰의 앞 뒤에 부착합니다. 앞의 챗봇 구현 실습에서 확인한 바와 같이 해당 토큰은 </s>입니다. 그 후 정해진 최대 길이로 패딩을 진행합니다.\\ndef convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\\ninput_ids, data_labels = [], []\\nfor example, label in tqdm(zip(examples, labels), total=len(examples)):\\nbos_token = [tokenizer.bos_token]\\neos_token = [tokenizer.eos_token]\\ntokens = bos_token + tokenizer.tokenize(example) + eos_token', 'eos_token = [tokenizer.eos_token]\\ntokens = bos_token + tokenizer.tokenize(example) + eos_token\\ninput_id = tokenizer.convert_tokens_to_ids(tokens)\\ninput_id = pad_sequences([input_id], maxlen=max_seq_len, value=tokenizer.pad_token_id, padding=\\'post\\')[0]\\nassert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\\ninput_ids.append(input_id)\\ndata_labels.append(label)\\ninput_ids = np.array(input_ids, dtype=int)', \"input_ids.append(input_id)\\ndata_labels.append(label)\\ninput_ids = np.array(input_ids, dtype=int)\\ndata_labels = np.asarray(data_labels, dtype=np.int32)\\nreturn input_ids, data_labels\\n훈련 데이터에 대해서 진행합니다.\\ntrain_X, train_y = convert_examples_to_features(train_data['document'], train_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)\\n테스트 데이터에 대해서 진행합니다.\\ntest_X, test_y = convert_examples_to_features(test_data['document'], test_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)\", \"훈련 데이터의 첫번째 샘플에 대해서 출력해보겠습니다.\\n# 최대 길이: 128\\ninput_id = train_X[0]\\nlabel = train_y[0]\\nprint('단어에 대한 정수 인코딩 :',input_id)\\nprint('각 인코딩의 길이 :', len(input_id))\\nprint('정수 인코딩 복원 :',tokenizer.decode(input_id))\\nprint('레이블 :',label)\\n단어에 대한 정수 인코딩 : [    1  9050  9267  7700  9705 23971 12870  8262  7055  7098  8084 48213\\n1     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\", '3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3', '3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3]\\n각 인코딩의 길이 : 128', '정수 인코딩 복원 : </s> 아 더빙.', '. 진짜 짜증나네요', '목소리</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><p', 'ad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '레이블 : 0']\n",
      "[\"koGPT-2를 이용해 모델을 구현하기 위해서는 koGPT-2의 출력을 이해할 필요가 있습니다. 우선, 한국어 GPT-2인 skt/kogpt2-base-v2를 로드해봅시다.\\nmodel = TFGPT2Model.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\\nkoGPT-2의 출력을 outpus이라는 변수에 저장합니다. 입력 문장의 길이는 128로 가정합니다.\\nmax_seq_len = 128\\ninput_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\\noutputs = model([input_ids_layer])\\noutputs에는 두 개의 출력이 존재하는데 인덱스 0을 확인해봅시다.\\n# 문장 길이만큼의 출력\\nprint(outputs[0])\", 'outputs에는 두 개의 출력이 존재하는데 인덱스 0을 확인해봅시다.\\n# 문장 길이만큼의 출력\\nprint(outputs[0])\\nKerasTensor(type_spec=TensorSpec(shape=(None, 128, 768), dtype=tf.float32, name=None), name=\\'tfgpt2_model/transformer/Reshape_2:0\\', description=\"created by layer \\'tfgpt2_model\\'\")\\noutputs[0]은 (배치 크기, 128, 768)의 크기를 가지는 텐서입니다. 이는 768차원의 벡터가 128개가 있다는 의미로 문장 길이 개수만큼의 출력을 얻었음을 의미합니다. 텍스트 분류 문제를 풀 경우에는 koGPT-2의 마지막 예측에 해당하는 벡터를 사용해야 합니다.\\n# 마지막 출력 벡터\\nprint(outputs[0][:, -1])', '# 마지막 출력 벡터\\nprint(outputs[0][:, -1])\\nKerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name=\\'tf.__operators__.getitem/strided_slice:0\\', description=\"created by layer \\'tf.__operators__.getitem\\'\")']\n",
      "[\"서브클래싱 구현 방식으로 구현한 텍스트 분류 모델은 다음과 같습니다. GPT의 출력 중 outputs[0][:, -1]. 즉, 마지막 출력 벡터를 시그모이드 함수가 활성화 함수로 설정된 출력층으로 연결합니다.\\nclass TFGPT2ForSequenceClassification(tf.keras.Model):\\ndef __init__(self, model_name):\\nsuper(TFGPT2ForSequenceClassification, self).__init__()\\nself.gpt = TFGPT2Model.from_pretrained(model_name, from_pt=True)\\nself.dropout = tf.keras.layers.Dropout(0.2)\\nself.classifier = tf.keras.layers.Dense(1,\\nkernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\\nactivation='sigmoid',\", \"kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\\nactivation='sigmoid',\\nname='classifier')\\ndef call(self, inputs):\\noutputs = self.gpt(input_ids=inputs)\\ncls_token = outputs[0][:, -1]\\ncls_token = self.dropout(cls_token)\\nprediction = self.classifier(cls_token)\\nreturn prediction\\n이전의 Colab에서의 TPU 사용법을 참고하여 TPU 코드를 작성합니다. 저자의 경우 Colab에서 TPU 사용을 위해 TPU 코드를 추가했습니다. TPU 사용을 원치 않는다면 런타임 유형 변경에서 GPU를 선택하고 해당 코드는 제거해줍시다.\\n# TPU 작동을 위한 코드\", '# TPU 작동을 위한 코드\\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\\'grpc://\\' + os.environ[\\'COLAB_TPU_ADDR\\'])\\ntf.config.experimental_connect_to_cluster(resolver)\\ntf.tpu.experimental.initialize_tpu_system(resolver)\\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\\nTPU를 사용할 것이므로 with strategy.scope():의 범위 안에서 모델을 컴파일합니다. 마찬가지로 TPU 사용을 원치 않는다면 with strategy.scope():는 제거해줍니다.\\nwith strategy.scope():\\nmodel = TFGPT2ForSequenceClassification(\"skt/kogpt2-base-v2\")', 'with strategy.scope():\\nmodel = TFGPT2ForSequenceClassification(\"skt/kogpt2-base-v2\")\\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\\nloss = tf.keras.losses.BinaryCrossentropy()\\nmodel.compile(optimizer=optimizer, loss=loss, metrics = [\\'accuracy\\'])\\n배치 크기는 32로 하고 훈련 데이터의 20%를 검증 데이터로 사용하여 2에포크 학습합니다.\\nmodel.fit(train_X, train_y, epochs=2, batch_size=32, validation_split=0.2)\\n테스트 데이터에 대해서 로스와 정확도를 계산합니다.\\nresults = model.evaluate(test_X, test_y, batch_size=1024)', '테스트 데이터에 대해서 로스와 정확도를 계산합니다.\\nresults = model.evaluate(test_X, test_y, batch_size=1024)\\nprint(\"test loss, test acc: \", results)\\n49/49 [==============================] - 22s 325ms/step - loss: 0.3139 - accuracy: 0.8633\\ntest loss, test acc:  [0.3138848841190338, 0.8632717728614807]\\n약 86.32%의 정확도를 얻습니다.']\n",
      "['def sentiment_predict(new_sentence):\\nbos_token = [tokenizer.bos_token]\\neos_token = [tokenizer.eos_token]\\ntokens = bos_token + tokenizer.tokenize(new_sentence) + eos_token\\ninput_id = tokenizer.convert_tokens_to_ids(tokens)\\ninput_id = pad_sequences([input_id], maxlen=max_seq_len, value=tokenizer.pad_token_id, padding=\\'post\\')[0]\\ninput_id = np.array([input_id])\\nscore = model.predict(input_id)[0][0]\\nif(score > 0.5):\\nprint(\"{:.2f}% 확률로 긍정 리뷰입니다.\\\\n\".format(score * 100))\\nelse:', 'if(score > 0.5):\\nprint(\"{:.2f}% 확률로 긍정 리뷰입니다.\\\\n\".format(score * 100))\\nelse:\\nprint(\"{:.2f}% 확률로 부정 리뷰입니다.\\\\n\".format((1 - score) * 100))\\nsentiment_predict(\"보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에\")\\n80.65% 확률로 부정 리뷰입니다.\\nsentiment_predict(\"스토리는 확실히 실망이였지만 배우들 연기력이 대박이였다 특히 이제훈 연기 정말 ... 이 배우들로 이렇게밖에 만들지 못한 영화는 아쉽지만 배우들 연기력과 사운드는 정말 빛났던 영화. 기대하고 극장에서 보면 많이 실망했겠지만 평점보고 기대없이 집에서 편하게 보면 괜찮아요. 이제훈님 연기력은 최고인 것 같습니다\")\\n88.93% 확률로 긍정 리뷰입니다.', '88.93% 확률로 긍정 리뷰입니다.\\nsentiment_predict(\"남친이 이 영화를 보고 헤어지자고한 영화. 자유롭게 살고 싶다고 한다. 내가 무슨 나비를 잡은 덫마냥 나에겐 다시 보고싶지 않은 영화.\")\\n83.53% 확률로 부정 리뷰입니다.\\nsentiment_predict(\\'이 영화 개꿀잼 ㅋㅋㅋ\\')\\n97.95% 확률로 긍정 리뷰입니다.\\nsentiment_predict(\\'이 영화 핵노잼 ㅠㅠ\\')\\n96.95% 확률로 부정 리뷰입니다.\\nsentiment_predict(\\'이딴게 영화냐 ㅉㅉ\\')\\n97.43% 확률로 부정 리뷰입니다.\\nsentiment_predict(\\'감독 뭐하는 놈이냐?\\')\\n91.85% 확률로 부정 리뷰입니다.\\nsentiment_predict(\\'와 개쩐다 정말 세계관 최강자들의 영화다\\')\\n93.78% 확률로 긍정 리뷰입니다.\\n==================================================', '93.78% 확률로 긍정 리뷰입니다.\\n==================================================\\n--- 19-05 GPT-2를 이용한 KorNLI 분류 ---\\n```\\n5/5 [==============================] - 12s 2s/step - loss: 0.6488 - accuracy: 0.7285\\ntest loss, test acc:  [0.6487572193145752, 0.7285429239273071]\\n```KorNLI 데이터는 카카오 브레인에서 공개한 한국어 벤치마크 데이터셋입니다.\\nKorNLI 데이터셋의 깃허브 : https://github.com/kakaobrain/KorNLUDatasets', \"KorNLI 데이터셋의 깃허브 : https://github.com/kakaobrain/KorNLUDatasets\\nNLI(Natural Language Inferencing)는 두 개의 문장이 주어지고, 두 개의 문장이 수반(entailment) 관계인지, 모순(contradiction) 관계인지, 중립(neutral) 관계인지를 맞추는 문제입니다. 이번에는 한국어 NLI 데이터셋인 KorNLI 데이터셋을 가지고 두 개의 문장을 입력받아서 세 개 중 하나인 관계를 맞추는 다중 클래스 분류 문제를 풀어봅시다. 그리고 BERT가 두 개의 문장을 입력받을 때는 어떤 식으로 전처리를 진행하는지 보겠습니다.\\n해당 실습은 Colab에서 TPU를 사용한다고 가정합니다.\\nColab에서 런타임 > 런타임 유형 변경 > 하드웨어 가속기에서 'TPU' 선택\\n만약 GPU 환경에서 아래의 실습 코드를 실습하고 싶다면 TPU 설정 코드들을 제거해주면 됩니다.\\npip install transformers\", '만약 GPU 환경에서 아래의 실습 코드를 실습하고 싶다면 TPU 설정 코드들을 제거해주면 됩니다.\\npip install transformers\\n초기 데이터의 전처리 과정은 18-05와 유사하므로 해당 챕터를 참고하시기 바랍니다.']\n",
      "['import pandas as pd\\nimport numpy as np\\nimport urllib.request\\nimport os\\nfrom tqdm import tqdm\\nimport tensorflow as tf\\nfrom sklearn import preprocessing\\nfrom transformers import AutoTokenizer, TFGPT2Model\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\n훈련 데이터, 검증 데이터, 테스트 데이터로 사용할 NLI 데이터를 로드합니다.\\n# 훈련 데이터 다운로드', '훈련 데이터, 검증 데이터, 테스트 데이터로 사용할 NLI 데이터를 로드합니다.\\n# 훈련 데이터 다운로드\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/multinli.train.ko.tsv\", filename=\"multinli.train.ko.tsv\")\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/snli_1.0_train.ko.tsv\", filename=\"snli_1.0_train.ko.tsv\")\\n# 검증 데이터 다운로드', '# 검증 데이터 다운로드\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.dev.ko.tsv\", filename=\"xnli.dev.ko.tsv\")\\n# 테스트 데이터 다운로드\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.test.ko.tsv\", filename=\"xnli.test.ko.tsv\")\\n훈련 데이터로 사용할 두 개의 파일과, 검증 데이터와 테스트 데이터 파일을 각각 데이터프레임으로 로드합니다.\\ntrain_snli = pd.read_csv(\"snli_1.0_train.ko.tsv\", sep=\\'\\\\t\\', quoting=3)', 'train_snli = pd.read_csv(\"snli_1.0_train.ko.tsv\", sep=\\'\\\\t\\', quoting=3)\\ntrain_xnli = pd.read_csv(\"multinli.train.ko.tsv\", sep=\\'\\\\t\\', quoting=3)\\nval_data = pd.read_csv(\"xnli.dev.ko.tsv\", sep=\\'\\\\t\\', quoting=3)\\ntest_data = pd.read_csv(\"xnli.test.ko.tsv\", sep=\\'\\\\t\\', quoting=3)\\n훈련 데이터로 사용할 두 개의 데이터프레임을 학습을 위해 하나로 합쳐줍니다. 그리고 데이터를 섞어줍니다.\\n# 결합 후 섞기\\ntrain_data = train_snli.append(train_xnli)\\ntrain_data = train_data.sample(frac=1)', '# 결합 후 섞기\\ntrain_data = train_snli.append(train_xnli)\\ntrain_data = train_data.sample(frac=1)\\n훈련 데이터와 검증 데이터 그리고 테스트 데이터에 대해서 각각 상위 5개의 행을 출력해봅시다. 데이터가 섞일 때는 랜덤이므로 아래의 데이터의 출력 결과는 저자와 다를 수 있습니다.\\ntrain_data.head()\\n[이미지: ]\\nval_data.head()\\n[이미지: ]\\ntest_data.head()\\n[이미지: ]\\n훈련 데이터, 검증 데이터, 테스트 데이터에 중복 샘플이나 결측값이 있는 샘플이 있다면 제거해줍니다. 이는 dropn_na_and_duplciates라는 함수를 사용하여 적용합니다.\\ndef drop_na_and_duplciates(df):\\ndf = df.dropna()\\ndf = df.drop_duplicates()\\ndf = df.reset_index(drop=True)\\nreturn df', \"df = df.dropna()\\ndf = df.drop_duplicates()\\ndf = df.reset_index(drop=True)\\nreturn df\\n# 결측값 및 중복 샘플 제거\\ntrain_data = drop_na_and_duplciates(train_data)\\nval_data = drop_na_and_duplciates(val_data)\\ntest_data = drop_na_and_duplciates(test_data)\\n각 데이터의 샘플 수는 다음과 같습니다.\\nprint('훈련용 샘플 개수 :',len(train_data))\\nprint('검증용 샘플 개수 :',len(val_data))\\nprint('테스트용 샘플 개수 :',len(test_data))\\n훈련용 샘플 개수 : 941814\\n검증용 샘플 개수 : 2490\\n테스트용 샘플 개수 : 5010\"]\n",
      "[\"토큰화를 위해 KoGPT-2 토크나이저를 로드합니다.\\ntokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='<s>', eos_token='</s>', pad_token='<pad>')\\nKoGPT-2의 정수 0, 1, 2, 3, 4는 각각 어떤 토큰을 의미하는지 확인해봅시다.\\nprint(tokenizer.decode(0))\\nprint(tokenizer.decode(1))\\nprint(tokenizer.decode(2))\\nprint(tokenizer.decode(3))\\nprint(tokenizer.decode(4))\\n<s>\\n</s>\\n<usr>\\n<pad>\\n<sys>\\n임의의 샘플을 작성하여 토크나이저의 전처리 결과를 봅시다. 여기서는 최대 길이를 128로 정했습니다.\\nmax_seq_len = 128\\nsent1 = '모든 섬사람들이 알고 있듯이, 가장 멋진 만과 해변들 중 많은 것들은 보트로만 도달할 수 있다.'\", \"max_seq_len = 128\\nsent1 = '모든 섬사람들이 알고 있듯이, 가장 멋진 만과 해변들 중 많은 것들은 보트로만 도달할 수 있다.'\\nsent2 = '보트는 일부 지역으로 가는 유일한 여행 수단이다.'\\nprint('문장1 :',sent1)\\nprint('문장2 :',sent2)\\n문장1 : 모든 섬사람들이 알고 있듯이, 가장 멋진 만과 해변들 중 많은 것들은 보트로만 도달할 수 있다.\\n문장2 : 보트는 일부 지역으로 가는 유일한 여행 수단이다.\", '문장1 : 모든 섬사람들이 알고 있듯이, 가장 멋진 만과 해변들 중 많은 것들은 보트로만 도달할 수 있다.\\n문장2 : 보트는 일부 지역으로 가는 유일한 여행 수단이다.\\n위의 두 개의 문장으로부터 KoGPT-2에 넣을 입력으로 전처리를 진행해봅시다. KoGPT-2가 두 개의 서로 다른 문장임을 인식할 수 있도록 힌트를 줄 필요가 있습니다. 저자는 각 문장의 시작과 끝에 KoGPT-2의 시작 토큰과 종료 토큰을 붙이는 방식을 택했습니다. 그리고 입력이 완전히 끝났다는 것을 알려주기 위해서 <unused0>라는 사용 용도가 정해져 있지 않은 KoGPT-2의 스페셜 토큰을 사용하였습니다. 그 후 배치 연산을 위해 패딩을 해줍니다. KoGPT-2의 패딩 토큰은 정수 3입니다.\\nbos_token = [tokenizer.bos_token]\\neos_token = [tokenizer.eos_token]\\n# 첫번째 문장의 앞과 뒤에 시작 토큰 <s>과 종료 토큰 </s>으로 감싼다.', \"eos_token = [tokenizer.eos_token]\\n# 첫번째 문장의 앞과 뒤에 시작 토큰 <s>과 종료 토큰 </s>으로 감싼다.\\nsent1_tokens = bos_token + tokenizer.tokenize(sent1) + eos_token\\n# 두번째 문장의 앞과 뒤에 시작 토큰 <s>과 종료 토큰 </s>으로 감싼다. 그 후 <unused0>를 붙인다.\\nsent2_tokens = bos_token + tokenizer.tokenize(sent2) + eos_token + ['<unused0>']\\n# 두 개의 문장을 연달아 이어붙인 후 정수 인코딩을 수행한다.\\ntokens = sent1_tokens + sent2_tokens\\ninput_id = tokenizer.convert_tokens_to_ids(tokens)\\nprint('정수 인코딩 전:', tokens)\\nprint('정수 인코딩 후:', input_id)\\n# 최대 길이로 패딩\", \"print('정수 인코딩 전:', tokens)\\nprint('정수 인코딩 후:', input_id)\\n# 최대 길이로 패딩\\ninput_id = pad_sequences([input_id], maxlen=max_seq_len, value=tokenizer.pad_token_id, padding='post')[0]\\nprint('패딩 후:', input_id)\\n정수 인코딩 전: ['<s>', '▁모든', '▁섬', '사람들이', '▁알고', '▁있듯이,', '▁가장', '▁멋진', '▁만과', '▁해변', '들', '▁중', '▁많은', '▁것들은', '▁보', '트로', '만', '▁도달할', '▁수', '▁있다.', '</s>', '<s>', '▁보', '트는', '▁일부', '▁지역으로', '▁가는', '▁유일한', '▁여행', '▁수단이', '다.', '</s>', '<unused0>']\", '정수 인코딩 후: [0, 9548, 9709, 34539, 12487, 42370, 9278, 43719, 34766, 23545, 7285, 9044, 9366, 24860, 9049, 11714, 7489, 48699, 9025, 10960, 1, 0, 9049, 11943, 9616, 14303, 11318, 13382, 12079, 26626, 9016, 1, 9]\\n패딩 후: [    0  9548  9709 34539 12487 42370  9278 43719 34766 23545  7285  9044\\n9366 24860  9049 11714  7489 48699  9025 10960     1     0  9049 11943\\n9616 14303 11318 13382 12079 26626  9016     1     9     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3', '3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3', '3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3]\\n전처리 결과는 위와 같습니다. 각 문장의 시작에는 시작 토큰인 <s>가 붙어있으며 정수로는 0입니다. 각 문장의 끝에는 종료 토큰인 </s>가 붙어있으며 정수로는 1입니다. 이에 따라 두 개의 문장의 앞, 뒤에는 0과 1이 있습니다. 그리고 입력이 완전히 끝나면 <unused0>에 해당하는 정수인 9가 부착됩니다. 그리고 최대 길이를 128로 정하였으므로 128의 길이로 일치시켜주기 위해서 패딩 토큰인 정수 3이 채워집니다.\\n위 과정을 convert_examples_to_features 라는 함수로 만들고, 훈련 데이터의 첫번째 샘플을 가지고 임의로 진행했던 전처리를 훈련 데이터, 검증 데이터, 테스트 데이터에 대해서 모두 진행해봅시다.', \"def convert_examples_to_features(sent_list1, sent_list2, max_seq_len, tokenizer):\\ninput_ids = []\\nfor sent1, sent2 in tqdm(zip(sent_list1, sent_list2), total=len(sent_list1)):\\nbos_token = [tokenizer.bos_token]\\neos_token = [tokenizer.eos_token]\\nsent1_tokens = bos_token + tokenizer.tokenize(sent1) + eos_token\\nsent2_tokens = bos_token + tokenizer.tokenize(sent2) + eos_token + ['<unused0>']\\ntokens = sent1_tokens + sent2_tokens\\ninput_id = tokenizer.convert_tokens_to_ids(tokens)\", 'tokens = sent1_tokens + sent2_tokens\\ninput_id = tokenizer.convert_tokens_to_ids(tokens)\\ninput_id = pad_sequences([input_id], maxlen=max_seq_len, value=tokenizer.pad_token_id, padding=\\'post\\')[0]\\nassert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\\ninput_ids.append(input_id)\\ninput_ids = np.array(input_ids, dtype=int)\\nreturn input_ids\\n훈련 데이터에 대해서 전처리를 진행합니다.', \"input_ids = np.array(input_ids, dtype=int)\\nreturn input_ids\\n훈련 데이터에 대해서 전처리를 진행합니다.\\nX_train = convert_examples_to_features(train_data['sentence1'], train_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)\\n훈련 데이터의 첫번째 샘플에 대한 정수 인코딩, 정수 인코딩을 기존의 문자열로 복원한 결과는 다음과 같습니다. (데이터가 섞일 때는 랜덤이므로 저자와 첫번째 샘플은 다를 수 있습니다.)\\n# 최대 길이: 128\\ninput_id = X_train[0]\\nprint('단어에 대한 정수 인코딩 :',input_id)\\nprint('각 인코딩의 길이 :', len(input_id))\\nprint('정수 인코딩 복원 :',tokenizer.decode(input_id))\", \"print('각 인코딩의 길이 :', len(input_id))\\nprint('정수 인코딩 복원 :',tokenizer.decode(input_id))\\n단어에 대한 정수 인코딩 : [    0  9174 11243  9114 13117  7281  9829  8236 43139  9192 17774  9673\\n11539  9741 17698 11579 20564     1     0  9174 28736 19937 28505  9633\\n8236  9038 10960     1     9     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\", '3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3]\\n각 인코딩의 길이 : 128', '정수 인코딩 복원 : <s> 두 명의 오프로드 폭주족들이 화창한 날 흙 코스에서 경쟁한다.</s><s> 두 남자가 진흙 속을 질주하고', '있다.</s><unused0><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pa', 'd><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', \"검증 데이터에 대해서도 전처리를 진행해봅시다.\\nX_val = convert_examples_to_features(val_data['sentence1'], val_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)\\n검증 데이터의 첫번째 샘플에 대해서 전처리를 진행한 결과는 다음과 같습니다.\\n# 최대 길이: 128\\ninput_id = X_val[0]\\nprint('단어에 대한 정수 인코딩 :',input_id)\\nprint('각 인코딩의 길이 :', len(input_id))\\nprint('정수 인코딩 복원 :',tokenizer.decode(input_id))\\n단어에 대한 정수 인코딩 : [    0  9394  9871  9135  8718 14364 10063  8013 37144  9265 12583  8006\", '단어에 대한 정수 인코딩 : [    0  9394  9871  9135  8718 14364 10063  8013 37144  9265 12583  8006\\n25856   377     1     0  9258 10192  9848 11001 10644 10396 18796 20485\\n37472  9134 35673  9539 18174     1     9     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3', '3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3     3     3     3     3\\n3     3     3     3     3     3     3     3]\\n각 인코딩의 길이 : 128', '정수 인코딩 복원 : <s> 그리고 그가 말했다, \"엄마, 저 왔어요.\"</s><s> 그는 학교 버스가 그를 내려주자마자 엄마에게 전화를', '걸었다.</s><unused0><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><p', 'ad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', \"테스트 데이터에 대해서 전처리를 진행해봅시다.\\nX_test = convert_examples_to_features(test_data['sentence1'], test_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)\\ncontradiction, entailment, neutral과 같이 문자열로 구성된 레이블에 대해서도 정수 인코딩을 진행합니다.\\ntrain_label = train_data['gold_label'].tolist()\\nval_label = val_data['gold_label'].tolist()\\ntest_label = test_data['gold_label'].tolist()\\nidx_encode = preprocessing.LabelEncoder()\\nidx_encode.fit(train_label)\\n# 고유한 정수로 변환\\ny_train = idx_encode.transform(train_label)\", \"idx_encode.fit(train_label)\\n# 고유한 정수로 변환\\ny_train = idx_encode.transform(train_label)\\ny_val = idx_encode.transform(val_label)\\ny_test = idx_encode.transform(test_label)\\nlabel_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\\nidx_label = {value: key for key, value in label_idx.items()}\\nprint('각 레이블과 정수 :', label_idx)\\n각 레이블과 정수 : {'contradiction': 0, 'entailment': 1, 'neutral': 2}\\n레이블을 정수 인코딩을 하기 전과 후를 테스트 데이터에 대해서 상위 5개를 출력하여 비교해봅시다.\", \"레이블을 정수 인코딩을 하기 전과 후를 테스트 데이터에 대해서 상위 5개를 출력하여 비교해봅시다.\\nprint('변환 전 :', test_label[:5])\\nprint('변환 후 :',y_test[:5])\\n변환 전 : ['contradiction', 'entailment', 'neutral', 'neutral', 'entailment']\\n변환 후 : [0 1 2 2 1]\"]\n",
      "[\"koGPT-2를 이용해 모델을 구현하기 위해서는 koGPT-2의 출력을 이해할 필요가 있습니다. 우선, 한국어 GPT-2인 skt/kogpt2-base-v2를 로드해봅시다.\\nmodel = TFGPT2Model.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\\nkoGPT-2의 출력을 outpus이라는 변수에 저장합니다. 입력 문장의 길이는 128로 가정합니다.\\nmax_seq_len = 128\\ninput_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\\noutputs = model([input_ids_layer])\\noutputs에는 두 개의 출력이 존재하는데 인덱스 0을 확인해봅시다.\\n# 문장 길이만큼의 출력\\nprint(outputs[0])\", 'outputs에는 두 개의 출력이 존재하는데 인덱스 0을 확인해봅시다.\\n# 문장 길이만큼의 출력\\nprint(outputs[0])\\nKerasTensor(type_spec=TensorSpec(shape=(None, 128, 768), dtype=tf.float32, name=None), name=\\'tfgpt2_model/transformer/Reshape_2:0\\', description=\"created by layer \\'tfgpt2_model\\'\")\\noutputs[0]은 (배치 크기, 128, 768)의 크기를 가지는 텐서입니다. 이는 768차원의 벡터가 128개가 있다는 의미로 문장 길이 개수만큼의 출력을 얻었음을 의미합니다. 텍스트 분류 문제를 풀 경우에는 koGPT-2의 마지막 예측에 해당하는 벡터를 사용해야 합니다.\\n# 마지막 출력 벡터\\nprint(outputs[0][:, -1])', '# 마지막 출력 벡터\\nprint(outputs[0][:, -1])\\nKerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name=\\'tf.__operators__.getitem/strided_slice:0\\', description=\"created by layer \\'tf.__operators__.getitem\\'\")']\n",
      "[\"서브클래싱 구현 방식으로 구현한 텍스트 분류 모델은 다음과 같습니다. GPT의 출력 중 outputs[0][:, -1]. 즉, 마지막 출력 벡터를 소프트맥스 함수가 활성화 함수로 설정된 출력층으로 연결합니다.\\nclass TFGPT2ForSequenceClassification(tf.keras.Model):\\ndef __init__(self, model_name, num_labels):\\nsuper(TFGPT2ForSequenceClassification, self).__init__()\\nself.gpt = TFGPT2Model.from_pretrained(model_name, from_pt=True)\\nself.classifier = tf.keras.layers.Dense(num_labels,\\nkernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\\nactivation='softmax',\\nname='classifier')\", \"activation='softmax',\\nname='classifier')\\ndef call(self, inputs):\\noutputs = self.gpt(input_ids=inputs)\\ncls_token = outputs[0][:, -1]\\nprediction = self.classifier(cls_token)\\nreturn prediction\\nTPU 코드를 추가하였습니다. 만약, TPU 사용을 원하지 않는다면 런타임 유형을 GPU로 설정하고 해당 코드는 제거하시기 바랍니다.\\n# TPU 작동을 위한 코드\\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\\ntf.config.experimental_connect_to_cluster(resolver)\\ntf.tpu.experimental.initialize_tpu_system(resolver)\", 'tf.tpu.experimental.initialize_tpu_system(resolver)\\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\\nTPU 사용을 위해서는 with strategy.scope(): 코드를 사용한 후 모델을 컴파일 해야합니다. 만약, TPU 사용을 원하지 않는다면 런타임을 GPU로 설정하고, with strategy.scope(): 해당 코드는 제거하시기 바랍니다. 모델은 토크나이저와 동일하게 skt/kogpt2-base-v2를 사용하며 레이블의 수는 3입니다.\\nwith strategy.scope():\\nmodel = TFGPT2ForSequenceClassification(\"skt/kogpt2-base-v2\", num_labels=3)\\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)', 'optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\\nloss = tf.keras.losses.SparseCategoricalCrossentropy()\\nmodel.compile(optimizer=optimizer, loss=loss, metrics = [\\'accuracy\\'])\\n두 번의 에포크를 수행합니다.\\nearly_stopping = EarlyStopping(\\nmonitor=\"val_accuracy\",\\nmin_delta=0.001,\\npatience=2)\\nmodel.fit(\\nX_train, y_train, epochs=2, batch_size=32, validation_data = (X_val, y_val),\\ncallbacks = [early_stopping]\\n)\\nresults = model.evaluate(X_test, y_test, batch_size=1024)', 'callbacks = [early_stopping]\\n)\\nresults = model.evaluate(X_test, y_test, batch_size=1024)\\nprint(\"test loss, test acc: \", results)\\n5/5 [==============================] - 12s 2s/step - loss: 0.6488 - accuracy: 0.7285\\ntest loss, test acc:  [0.6487572193145752, 0.7285429239273071]\\n테스트 데이터에 대한 정확도로 72%를 얻었습니다.\\n==================================================\\n--- 20. 사전 훈련된 인코더-디코더 모델 ---\\n마지막 편집일시 : 2024년 8월 26일 12:24 오전\\n==================================================\\n--- 20-01. 인코더와 디코더 ---', '==================================================\\n--- 20-01. 인코더와 디코더 ---\\n이번 챕터에서는 인코더 구조의 BERT와 디코더 구조의 GPT와 비교되는 또 다른 유형의 모델인 인코더와 디코더 모두가 존재하는 인코더-디코더 구조(출처에 따라서는 Seq2Seq 구조라고 설명)의 BART를 소개합니다. 이번 챕터에서는 인코더-디코더 구조의 모델이 갖고 있는 의의와 BART의 사전 학습 방식에 대해서 다룹니다. BART의 상세 파인 튜닝 방법에 대해서는 다음 실습인 BART를 이용한 한국어 뉴스 요약에서 설명하겠습니다.']\n",
      "['아래의 그림은 16챕터에서 배웠던 트랜스포머의 모습을 보여줍니다.\\n[이미지: ]\\n트랜스포머 구조를 제안한 논문 Attention is All you need에서 제안되었던 초기 트랜스포머는 인코더-디코더 구조를 가지고 있었습니다. 인코더는 일반적으로 자연어 이해(Natural Language Understanding, NLU) 능력이 뛰어나고, 디코더는 자연어 생성(Natural Language Generation, NLG) 능력을 가지고 있습니다. 그리고 이후 트랜스포머 인코더와 디코더는 서로 분리되어 각자의 장점을 극대화시켜 별도의 모델들로 발전하게 되는데 대표적인 모델이 인코더 구조의 BERT와 디코더 구조의 GPT입니다.\\n[이미지: ]', '[이미지: ]\\nBERT의 경우 트랜스포머의 인코더 구조를 가지며 사전 학습(Pre-training) 된 모델입니다. BERT는 양방향의 문맥을 반영하도록 학습되어 자연어 이해 능력이 뛰어나지만, 양방향의 문맥을 반영하여 학습하는 방식은 문장을 생성하는 구조에 적합하지 않아 요약문을 생성, 번역문을 생성, 챗봇의 답변을 생성하는 등의 생성 문제에는 쓰이지 않습니다. 반면, 문장의 임베딩을 생성하거나, 텍스트를 분류하는 작업, 개체명 인식과 같은 작업에서는 비슷한 크기의 디코더 류의 모델에 비해서 좋은 성능을 얻기에 유리한 편입니다.', 'GPT의 경우 트랜스포머의 디코더만을 이용하여 사전 학습(Pre-training) 된 모델로 이전 단어들로부터 다음 단어를 생성하도록 학습된 모델입니다. 이러한 특징으로 인해서 언어 모델(Language Model)의 일종이며, 다음 단어를 생성하도록 학습이 된 만큼 문장을 생성하는 능력을 갖고 있습니다. BERT와는 달리 요약문을 생성, 번역문을 생성, 챗봇의 답변을 생성하는 등의 문제를 풀기에 적합합니다. 그리고 이러한 디코더 류의 모델들이 점점 파라미터 개수가 커지면서 거대 언어 모델(Large Language Model, LLM)의 시대가 되었습니다.', 'BERT와 같은 인코더 모델과 GPT와 같은 디코더 모델은 각자의 장, 단점을 가진 채 모델이 별도로 발전되어져 왔지만 초기 트랜스포머와 같이 인코더와 디코더가 같이 존재하는 구조를 유지한 채 모델을 사전 학습(Pre-training)하는 시도들이 있었습니다. 이러한 인코더-디코더 구조의 대표적인 모델들이 BART와 T5입니다.']\n",
      "['[이미지: ]\\n2023년 개발자 행사 데뷰(DEVIEW)에서 네이버 AI 엔지니어들의 발표인 이해, 생성, 효율: 세 마리 토끼 다 잡는 Seq2Seq HyperCLOVA라는 영상의 제목은 인코더-디코더 계열의 모델인 T5가 갖고 있는 장점을 잘 요약하고 있는 제목입니다. T5가 인코더-디코더 구조. 즉, Seq2Seq 구조이기에 이해와 생성에서 모두 이점을 볼 수 있습니다. 아래는 해당 발표의 영상 주소입니다.\\n이해, 생성, 효율: 세 마리 토끼 다 잡는 Seq2Seq HyperCLOVA: https://www.youtube.com/watch?v=upL76wu1EVQ\\n인코더 모델, 디코더 모델, 그리고 인코더-디코더 모델에 대해서 대략적으로 정리해봤습니다. 이제 인코더-디코더 구조의 사전 학습 모델(Pre-trained Language Model, PLM)인 BART와 T5에 대해서 정리해봅시다.', '==================================================\\n--- 20-02. BART(Bidirectional Auto-Regressive Transformers) ---\\n이번 챕터에서는 인코더-디코더 구조를 가지고 다양한 자연어 이해, 생성 문제에서 뛰어난 성능을 얻은 BART(Bidirectional Auto-Regressive Transformers)에 대해서 알아보겠습니다.']\n",
      "['BART(Bidirectional Auto-Regressive Transformers)는 2020년 페이스북 AI에서 발표한 모델로 인코더-디코더 구조의 가지고 있어 자연어 이해 능력과 자연어 생성 능력을 모두 적절히 겸비한 모델입니다. 앞서 20-01에서 설명한 바와 같이 일반적으로 디코더 모델은 동 크기의 인코더 모델 대비 자연어 이해 문제에서 비교적 떨어지는 성능을 보였습니다.', '하지만 BART의 경우 인코더와 디코더 모두를 가지고 있어 기계 독해나 NLI 문제와 같이 자연어 이해 능력이 중요한 문제에서는 인코더 모델인 RoBERTa(BERT의 개선된 버전)에 뒤지지 않는 성능을 보였으며, 요약, 챗봇, 번역 등과 같은 생성 문제에서도 동 크기의 디코더 계열의 모델 대비 뛰어난 성능을 보였습니다. 이는 인코더-디코더 구조의 모델에 적절하게 사전 학습을 진행한다면, 생성 문제에서의 성능 향상을 이루면서도, 기계 독해와 같은 다양한 자연어 이해 문제에 대한 성능을 떨어트리지 않을 수 있다는 것에 의미를 두고 있습니다.']\n",
      "['[이미지: ]\\n위의 그림은 BERT와 GPT 그리고 BART의 사전 학습 단계에서의 차이를 보여줍니다. BERT는 양방향의 문맥을 반영하여 가려진 단어를 맞추는 식으로 학습하고, GPT는 이전 단어들로부터 다음 단어를 예측하는 방식으로 학습합니다. 반면, BART는 인코더에서 훼손된 문장을 디코더에서 복원하는 방식으로 학습합니다.\\nBART는 인코더-디코더 구조를 가지고 있어 인코더의 입력 문장과 디코더가 생성하는 문장의 길이가 서로 달라도 된다는 특징을 가지고 있으며, BART는 해당 특징을 살려서 사전 학습 시에 총 5개의 방법을 사용합니다.', '기본적으로 이 5가지의 방법은 모두 인코더에 고의적으로 원본 문장을 훼손된 문장을 넣고 디코더에서 원본 문장을 다시 재복원하도록 하여 언어에 대한 이해를 높이는 데에 초점이 맞춰져 있습니다. Token Masking, Text Infilling, Sentence Permutation, Document Rotation, Document Rotation 이 5개의 방법을 차근차근 정리해봅시다.\\n1) Token Masking\\n토큰 마스킹(Token Masking)은 입력 문장에서 임의의 단어 하나를 마스킹하여 입력 문장을 훼손시키고, 디코더가 원래 문장을 정확하게 예측, 복원하도록 학습하는 방식입니다. 이를 통해 모델은 개별 단어가 문맥 속에서 어떻게 기능하는지를 학습하며, 언어의 미묘한 뉘앙스를 포착하는 능력을 개발합니다. 예를 들어 다음과 같이 예시 문장이 있다고 가정해봅시다.\\n\"The children played soccer outside until it got dark.\"', '\"The children played soccer outside until it got dark.\"\\n해당 문장에서 soccer를 마스킹하고 디코더에서 해당 문장을 복원해야 한다면 BART는 인코더에 변형된 문장이 들어가고, 디코더의 레이블은 원본 문장이 들어가서 변형된 문장으로부터 원본 문장을 복원하도록 학습됩니다. 인코더의 입력, 디코더의 레이블, 그리고 실제 예측 과정을 정리해봅시다.\\n인코더 입력과 디코더의 레이블\\n인코더 입력(변형된 문장): \"The children played <mask> outside until it got dark.\"\\n디코더의 레이블(원본 문장): \"The children played soccer outside until it got dark.\"\\n디코더 입력 및 레이블 (각 사이클)\\n기본적으로 트랜스포머 디코더는 한 단어씩 생성합니다. <s>는 문장의 시작을, </s>는 문장의 끝을 나타내는 신호입니다.\\n단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1\\n<s>', '기본적으로 트랜스포머 디코더는 한 단어씩 생성합니다. <s>는 문장의 시작을, </s>는 문장의 끝을 나타내는 신호입니다.\\n단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1\\n<s>\\nThe\\n2\\n<s> The\\nchildren\\n3\\n<s> The children\\nplayed\\n4\\n<s> The children played\\nsoccer\\n마스크된 단어를 예측.\\n5\\n<s> The children played soccer\\noutside\\n6\\n<s> The children played soccer outside\\nuntil\\n7\\n<s> The children played soccer outside until\\nit\\n8\\n<s> The children played soccer outside until it\\ngot\\n9\\n<s> The children played soccer outside until it got\\ndark\\n10', \"got\\n9\\n<s> The children played soccer outside until it got\\ndark\\n10\\n<s> The children played soccer outside until it got dark\\n</s>\\n문장 종료\\n학습 과정에서 단어를 1개씩 생성하며 중간에 마스킹 된 단어도 예측해야 합니다. 마지막 단계에서는 디코더는 문장 종료 토큰 </s>를 생성합니다. 이 토큰은 모델에게 문장이 완성되었음을 알려주는 역할을 합니다. 이 과정을 통해 BART는 마스킹된 단어를 정확히 예측하는 능력뿐만 아니라, 전체 문장의 구조와 흐름을 이해하고 재구성하는 능력을 얻습니다. 또한 문장의 시작(<s>)과 끝(</s>)을 인식하는 능력도 함께 학습하게 됩니다.\\nToken Masking의 주요 포인트\\n마스크된 단어 ('soccer')를 정확히 예측하는 것이 핵심 과제입니다.\\n<s>와 </s> 토큰을 통해 모델은 문장의 시작과 끝을 인식합니다.\", '마스크된 단어 (\\'soccer\\')를 정확히 예측하는 것이 핵심 과제입니다.\\n<s>와 </s> 토큰을 통해 모델은 문장의 시작과 끝을 인식합니다.\\n이 과정을 통해 BART는 전체 문장 구조와 흐름을 이해하고 재구성하는 능력을 개발합니다.\\n2) Text Infilling\\n텍스트 인필링(Text Infilling)은 입력 문장에서 하나 이상의 연속된 단어를 마스킹하여 입력 문장을 훼손시키고, 디코더가 원래 문장을 정확하게 예측, 복원하도록 학습하는 방식입니다. 이를 통해 모델은 문맥에서 빠진 부분을 채우는 능력을 개발하며, 복합적인 문맥 이해와 언어적 예측 능력을 향상시킵니다. 예를 들어 다음과 같이 예시 문장이 있다고 가정해봅시다.\\n\"He goes to school and then to the gym.\"', '\"He goes to school and then to the gym.\"\\n해당 문장에서 \"to school and then\"을 <mask>로 마스킹하고 디코더에서 해당 문장을 복원해야 한다면 BART는 인코더에 변형된 문장이 들어가고, 디코더의 레이블은 원본 문장이 들어갑니다. BART는 변형된 문장으로부터 원본 문장을 복원하도록 학습됩니다. 인코더의 입력, 디코더의 레이블, 그리고 실제 예측 과정을 정리해봅시다.\\n인코더 입력과 디코더의 레이블\\n인코더 입력(변형된 문장): \"He goes <mask> to the gym.\"\\n디코더의 레이블(원본 문장): \"He goes to school and then to the gym.\"\\n디코더 입력 및 레이블 (각 사이클)\\n기본적으로 트랜스포머 디코더는 한 단어씩 생성합니다. <s>는 문장의 시작을, </s>는 문장의 끝을 나타내는 신호입니다.\\n단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1\\n<s>\\nHe\\n2\\n<s> He\\ngoes\\n3', '단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1\\n<s>\\nHe\\n2\\n<s> He\\ngoes\\n3\\n<s> He goes\\nto\\n마스크된 구간 시작\\n4\\n<s> He goes to\\nschool\\n5\\n<s> He goes to school\\nand\\n6\\n<s> He goes to school and\\nthen\\n마스크된 구간 끝\\n7\\n<s> He goes to school and then\\nto\\n8\\n<s> He goes to school and then to\\nthe\\n9\\n<s> He goes to school and then to the\\ngym\\n10\\n<s> He goes to school and then to the gym\\n</s>\\n문장 종료', \"9\\n<s> He goes to school and then to the\\ngym\\n10\\n<s> He goes to school and then to the gym\\n</s>\\n문장 종료\\n학습 과정에서 단어를 1개씩 생성하며 중간에 마스킹 된 여러 단어들을 연속적으로 예측해야 합니다. 마지막 단계에서는 디코더는 문장 종료 토큰 </s>를 생성합니다. 이 토큰은 모델에게 문장이 완성되었음을 알려주는 역할을 합니다. 이 과정을 통해 BART는 마스킹된 연속된 여러 단어를 정확히 예측하는 능력뿐만 아니라, 전체 문장의 구조와 흐름을 이해하고 재구성하는 능력을 얻습니다. 또한 문장의 시작(<s>)과 끝(</s>)을 인식하는 능력도 함께 학습하게 됩니다.\\n주요 포인트\\n연속된 여러 단어('to school and then')를 정확히 예측하는 것이 핵심 과제입니다.\\n모델은 더 넓은 문맥을 이해하고 적절한 구문을 생성하여 자연스러운 문장을 완성해야 합니다.\", '모델은 더 넓은 문맥을 이해하고 적절한 구문을 생성하여 자연스러운 문장을 완성해야 합니다.\\n이 과정을 통해 BART는 더 복잡한 언어 구조와 의미를 이해하고 생성하는 능력을 개발합니다.\\nText Infilling은 Token Masking보다 더 어려운 문제이기 때문에 모델의 문맥 이해 능력과 연속적인 텍스트 생성 능력을 더욱 향상시킵니다.\\n3) Sentence Permutation\\n문장 순서 바꾸기(Sentence Permutation)는 여러 문장으로 구성된 텍스트의 문장 순서를 무작위로 섞고, 디코더가 원래 순서의 문장들을 정확하게 예측, 복원하도록 학습하는 방식입니다. 이를 통해 모델은 문서 내에서 문장 간의 논리적 연결과 흐름을 이해하고, 올바른 순서로 재배열하는 능력을 개발합니다. 예를 들어 다음과 같이 예시 문장들이 있다고 가정해봅시다.\\n\"He left the house. He met his friend. They went to the cinema.\"', '\"He left the house. He met his friend. They went to the cinema.\"\\n해당 문장들의 순서를 무작위로 섞고 디코더에서 원래 순서로 복원해야 한다면 BART는 인코더에 순서가 섞인 문장들이 들어가고, 디코더의 레이블은 원래 순서의 문장들이 들어가서 섞인 순서의 문장들로부터 원래 순서의 문장들을 복원하도록 학습됩니다. 인코더의 입력, 디코더의 레이블, 그리고 실제 예측 과정을 정리해봅시다.\\n인코더 입력과 디코더의 레이블\\n인코더 입력(순서가 섞인 문장들): \"They went to the cinema. He met his friend. He left the house.\"\\n디코더의 레이블(원래 순서의 문장들): \"He left the house. He met his friend. They went to the cinema.\"\\n디코더 입력 및 레이블 (각 사이클)', '디코더 입력 및 레이블 (각 사이클)\\n기본적으로 트랜스포머 디코더는 한 단어씩 생성합니다. <s>는 문장의 시작을, </s>는 문장의 끝을 나타내는 신호입니다.\\n단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1\\n<s>\\nHe\\n첫 번째 문장 시작\\n2\\n<s> He\\nleft\\n3\\n<s> He left\\nthe\\n4\\n<s> He left the\\nhouse\\n5\\n<s> He left the house\\n.\\n첫 번째 문장 끝\\n6\\n<s> He left the house.\\nHe\\n두 번째 문장 시작\\n7\\n<s> He left the house. He\\nmet\\n8\\n<s> He left the house. He met\\nhis\\n9\\n<s> He left the house. He met his\\nfriend\\n10\\n<s> He left the house. He met his friend\\n.\\n두 번째 문장 끝\\n11\\n<s> He left the house. He met his friend.\\nThey\\n세 번째 문장 시작\\n12', '.\\n두 번째 문장 끝\\n11\\n<s> He left the house. He met his friend.\\nThey\\n세 번째 문장 시작\\n12\\n<s> He left the house. He met his friend. They\\nwent\\n13\\n<s> He left the house. He met his friend. They went\\nto\\n14\\n<s> He left the house. He met his friend. They went to\\nthe\\n15\\n<s> He left the house. He met his friend. They went to the\\ncinema\\n16\\n<s> He left the house. He met his friend. They went to the cinema\\n.\\n세 번째 문장 끝\\n17\\n<s> He left the house. He met his friend. They went to the cinema.\\n</s>\\n전체 문장 종료', '.\\n세 번째 문장 끝\\n17\\n<s> He left the house. He met his friend. They went to the cinema.\\n</s>\\n전체 문장 종료\\n학습 과정에서 디코더는 섞인 순서의 문장들을 입력받아 원래 순서의 문장들을 재구성해야 합니다. 마지막 단계에서는 디코더는 문장 종료 토큰 </s>를 생성합니다. 이 토큰은 모델에게 전체 텍스트가 완성되었음을 알려주는 역할을 합니다. 이 과정을 통해 BART는 문장들의 올바른 순서를 예측하는 능력뿐만 아니라, 전체 문서의 구조와 흐름을 이해하고 재구성하는 능력을 얻습니다.\\n주요 포인트\\n섞인 순서의 여러 문장을 원래의 올바른 순서로 재배열하는 것이 핵심 과제입니다.\\n모델은 각 문장의 내용뿐만 아니라 문장들 간의 논리적 연결과 전체적인 문맥을 이해해야 합니다.\\n이 과정을 통해 BART는 더 복잡한 문서 수준의 언어 구조와 의미를 이해하고 생성하는 능력을 개발합니다.', '이 과정을 통해 BART는 더 복잡한 문서 수준의 언어 구조와 의미를 이해하고 생성하는 능력을 개발합니다.\\nSentence Permutation은 Token Masking이나 Text Infilling보다 더 큰 범위의 문맥을 다루기 때문에, 모델의 문서 수준 이해 능력을 크게 향상시킵니다.\\n4) Document Rotation\\n문서 회전(Document Rotation)은 전체 문서의 일부를 잘라내어 그 부분을 문서의 시작으로 설정하는 방식입니다. 디코더는 이렇게 재배열된 문서를 원래의 순서대로 복원하도록 학습됩니다. 이 기법은 모델이 문서 전체의 구조를 이해하고, 어떤 부분이 시작, 중간, 끝에 해당하는지 판단하는 능력을 개발하도록 돕습니다. 예를 들어 다음과 같은 문서가 있다고 가정해봅시다.\\n\"It was raining. She opened her umbrella. She stayed dry.\"', '\"It was raining. She opened her umbrella. She stayed dry.\"\\n이 문서의 일부를 잘라 시작 부분으로 옮긴다면 BART는 인코더에 재배열된 문서가 들어가고, 디코더의 레이블은 원래 순서의 문서가 들어가서 재배열된 문서로부터 원래 순서의 문서를 복원하도록 학습됩니다. 인코더의 입력, 디코더의 레이블, 그리고 실제 예측 과정을 정리해봅시다.\\n인코더 입력과 디코더의 레이블\\n인코더 입력(재배열된 문서): \"She stayed dry. It was raining. She opened her umbrella.\"\\n디코더의 레이블(원래 순서의 문서): \"It was raining. She opened her umbrella. She stayed dry.\"\\n디코더 입력 및 레이블 (각 사이클)\\n기본적으로 트랜스포머 디코더는 한 단어씩 생성합니다. <s>는 문서의 시작을, </s>는 문서의 끝을 나타내는 신호입니다.\\n단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1', '기본적으로 트랜스포머 디코더는 한 단어씩 생성합니다. <s>는 문서의 시작을, </s>는 문서의 끝을 나타내는 신호입니다.\\n단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1\\n<s>\\nIt\\n원래 문서의 시작\\n2\\n<s> It\\nwas\\n3\\n<s> It was\\nraining\\n4\\n<s> It was raining\\n.\\n첫 문장 끝\\n5\\n<s> It was raining.\\nShe\\n두 번째 문장 시작\\n6\\n<s> It was raining. She\\nopened\\n7\\n<s> It was raining. She opened\\nher\\n8\\n<s> It was raining. She opened her\\numbrella\\n9\\n<s> It was raining. She opened her umbrella\\n.\\n두 번째 문장 끝\\n10\\n<s> It was raining. She opened her umbrella.\\nShe\\n세 번째 문장 시작\\n11\\n<s> It was raining. She opened her umbrella. She', 'She\\n세 번째 문장 시작\\n11\\n<s> It was raining. She opened her umbrella. She\\nstayed\\n12\\n<s> It was raining. She opened her umbrella. She stayed\\ndry\\n13\\n<s> It was raining. She opened her umbrella. She stayed dry\\n.\\n세 번째 문장 끝\\n14\\n<s> It was raining. She opened her umbrella. She stayed dry.\\n</s>\\n문서 종료\\n학습 과정에서 디코더는 재배열된 문서를 입력받아 원래 순서의 문서를 재구성해야 합니다. 마지막 단계에서는 디코더는 문서 종료 토큰 </s>를 생성합니다. 이 토큰은 모델에게 전체 문서가 완성되었음을 알려주는 역할을 합니다. 이 과정을 통해 BART는 문서의 올바른 순서를 예측하는 능력뿐만 아니라, 전체 문서의 구조와 흐름을 이해하고 재구성하는 능력을 얻습니다.\\n주요 포인트', '주요 포인트\\n재배열된 문서를 원래의 올바른 순서로 복원하는 것이 핵심 과제입니다.\\n모델은 각 문장의 내용뿐만 아니라 문장들 간의 관계와 전체적인 문서 구조를 이해해야 합니다.\\n이 과정을 통해 BART는 더 복잡한 문서 수준의 언어 구조와 의미를 이해하고 생성하는 능력을 개발합니다.\\nDocument Rotation은 BART의 문서 이해 능력을 종합적으로 향상시키는 역할을 합니다.\\n5) Token Deletion\\n토큰 삭제(Token Deletion)는 입력 문장에서 무작위로 토큰을 삭제하는 방식입니다. 디코더는 이렇게 일부 토큰이 삭제된 문장을 원래의 완전한 문장으로 복원하도록 학습됩니다. 이 기법은 모델이 문맥을 이해하고 누락된 정보를 추론하는 능력을 개발하도록 돕습니다. 예를 들어 다음과 같은 문장이 있다고 가정해봅시다.\\n\"The cat sat on the mat.\"', '\"The cat sat on the mat.\"\\n이 문장에서 \"sat\" 토큰을 삭제한다면 BART는 인코더에 \"sat\"이 삭제된 문장이 들어가고, 디코더의 레이블은 원래 문장이 들어가서 삭제된 토큰이 있는 문장으로부터 원래 문장을 복원하도록 학습됩니다. 인코더의 입력, 디코더의 레이블, 그리고 실제 예측 과정을 정리해봅시다. 여기서 중요한 점은 삭제된 토큰 위치에 <mask> 토큰을 넣지 않으며 아무 표시 없이 완전히 삭제합니다.\\n인코더 입력과 디코더의 레이블\\n인코더 입력(토큰이 삭제된 문장): \"The cat on the mat.\"\\n디코더의 레이블(원래 문장): \"The cat sat on the mat.\"\\n디코더 입력 및 레이블 (각 사이클)\\n기본적으로 트랜스포머 디코더는 한 단어씩 생성합니다. <s>는 문장의 시작을, </s>는 문장의 끝을 나타내는 신호입니다.\\n단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1\\n<s>\\nThe\\n문장 시작\\n2\\n<s> The\\ncat\\n3\\n<s> The cat', '단계\\n디코더 입력\\n디코더 레이블\\n비고\\n1\\n<s>\\nThe\\n문장 시작\\n2\\n<s> The\\ncat\\n3\\n<s> The cat\\nsat\\n삭제된 \"sat\" 토큰 복원\\n4\\n<s> The cat sat\\non\\n5\\n<s> The cat sat on\\nthe\\n6\\n<s> The cat sat on the\\nmat\\n7\\n<s> The cat sat on the mat\\n.\\n8\\n<s> The cat sat on the mat.\\n</s>\\n문장 종료\\n학습 과정에서 디코더는 \"sat\"이 삭제된 문장을 입력받아 원래 문장을 재구성해야 합니다. 마지막 단계에서는 디코더는 문장 종료 토큰 </s>를 생성합니다. 이 토큰은 모델에게 전체 문장이 완성되었음을 알려주는 역할을 합니다. 이 과정을 통해 BART는 삭제된 \"sat\" 토큰을 정확히 예측하는 능력뿐만 아니라, 전체 문장의 구조와 의미를 이해하고 재구성하는 능력을 얻습니다.\\n주요 포인트 및 다른 기법과의 차이점', '주요 포인트 및 다른 기법과의 차이점\\n삭제된 \"sat\" 토큰을 문맥을 통해 정확히 복원하는 것이 핵심 과제입니다.\\n모델은 남아있는 토큰들의 관계와 전체적인 문장 구조를 이해해야 합니다.\\nToken Masking과 Text Infilling과의 차이: Token Deletion은 삭제된 토큰 위치에 <mask> 토큰을 넣지 않으며 아무 표시 없이 완전히 삭제합니다. 다시 말해 삭제된 토큰의 위치 정보가 주어지지 않은 채로 원본 문장을 복원해야 합니다.\\nToken Deletion은 삭제된 토큰의 위치 정보를 제공하지 않아 다른 기법들보다 더 어려운 과제를 푸는 것입니다.\\n이 과정을 통해 BART는 더 강력한 문맥 추론 능력과 누락된 정보 복원 능력을 개발합니다.\\n==================================================\\n--- 20-03. BART 파인 튜닝 실습: 뉴스 요약 ---\\n```', '==================================================\\n--- 20-03. BART 파인 튜닝 실습: 뉴스 요약 ---\\n```\\n배우 배수지가 매니지먼트 숲과 전속계약을 체결해 배우 배수지의 장점과 매력을 극대화할 수 있는 작품 선택부터 국내외 활동, 가수로서의 솔로 활동까지 활발하게 이루어질 수 있도록 지원할 예정이다.\\n```죄송합니다. 해당 페이지는 E-book 구매 시에 볼 수 있는 페이지입니다.\\n아래는 실제 책에 있는 내용 후반부 내용으로 책 구매 후 실습을 따라하면 아래와 같은 결과를 얻을 수 있습니다.\\n... 중략 ...']\n",
      "[\"파인 튜닝이 끝났다면 현재 경로에 검증 데이터에 대한 손실이 가장 적었을 때의 모델인 model이 저장됩니다. 이제 해당 모델을 로드하여 임의의 데이터에 대해서 평가해봅시다.\\n# 모데 로드\\nloaded_model = TFBartForConditionalGeneration.from_pretrained('model')\\n임의로 테스트 데이터의 25번 샘플을 text라는 변수에 저장해봅시다.\\ntext = test_data.loc[25]['news']\\nprint(text)\", \"배우 배수지가 매니지먼트 숲과 전속계약을 체결했다. 수지는 8일 자신의 인스타그램에 '데뷔 때 부터 함께해온 소속사 JYP와 계약기간을 마치고 오늘부터 새로운 소속사 매니지먼트 숲과 함께 하게 되었다'고 밝혔다. 이어 수지는 '연습생으로 시작해서, 데뷔하고 9년의 시간이 흐른 지금까지, JYP와 함께했던 여러 영광의 순간들이 스쳐지나간다'면서 '9년 동안 항상 옆에서 서포트 해주셨던 JYP 모든 직원분들께 진심으로 감사드린다'고 인사를 잊지 않았다. 2010년 걸그룹 '미쓰에이'로 데뷔한 배수지는 2011년 KBS2 드라마 '드림하이'로 첫 연기 활동을 시작했다. 2012년 영화 '건축학개론'을 통해 스크린 데뷔를 한 뒤 가수 활동과 연기 활동을 꾸준히 병행해 오고 있다. 매니지먼트 숲 관계자는 '배우 배수지의 장점과 매력을 극대화할 수 있는 작품 선택부터 국내외 활동, 가수로서의 솔로 활동까지 활발하게 이루어질 수 있도록 지원할 예정이다'고 전했다\", \". 특히 올해는 작품을 통해 연기자 배수지로 대중들과 만날 예정이다. 현재 촬영 중인 SBS 드라마 '배가본드'는 민항 여객기 추락 사고에 연루된 한 남자가 은폐된 진실 속에서 찾아낸 거대한 국가 비리를 파헤치게 되는 과정을 담은 이야기다. 배수지는 국정원 블랙요원 고해리 역으로 출연하며, 뒤이어 영화 '백두산'에도 합류한다. 매니지먼트 숲은 공유, 공효진, 김재욱, 서현진, 이천희, 전도연, 정유미, 남지현, 최우식, 유민규, 이재준, 정가람, 전소니 등 소속되어 있다.\", '앞서 정의한 summarize() 함수를 사용하여 학습한 BART 모델과 함께 요약하고자 하는 위의 뉴스 원문을 입력으로 사용하고 최종적으로 얻은 요약문을 출력해봅시다.\\nsummary = summarize(text, model, tokenizer)\\nprint(summary)\\n배우 배수지가 매니지먼트 숲과 전속계약을 체결해 배우 배수지의 장점과 매력을 극대화할 수 있는 작품 선택부터 국내외 활동, 가수로서의 솔로 활동까지 활발하게 이루어질 수 있도록 지원할 예정이다.\\n정상적으로 요약문이 생성되는 것을 볼 수 있습니다. 저자의 경우, 학습 데이터를 일부로 줄이고 학습하였지만, 제가 제공해드린 데이터를 모두 사용한다면 훨씬 매끄러운 요약문을 얻을 수 있을 것입니다.\\n==================================================\\n--- 20-04. T5 파인 튜닝 실습: 요약 생성기 ---\\n```', '==================================================\\n--- 20-04. T5 파인 튜닝 실습: 요약 생성기 ---\\n```\\n문재인 대통령은 8일 서울 올림픽공원에서 열린 5G플러스 전략발표에 참석해 \"5G가 각 분야에 융합되면, 정보통신산업을 넘어 자동차, 드론, 로봇, 지능형 폐쇄회로TV(CCTV)를 비롯한 제조업과 벤처에 이르기까지 우리 산업 전체의 혁신을 통한 동반성장이 가능하다\"며 \"5G는 대한민국 혁신성장의 인프라\"라고 강조했다.\\n```죄송합니다. 해당 페이지는 E-book 구매 시에 볼 수 있는 페이지입니다.\\n아래는 실제 책에 있는 내용 후반부 내용으로 책 구매 후 실습을 따라하면 아래와 같은 결과를 얻을 수 있습니다.\\n... 중략 ...']\n",
      "[\"임의로 테스트 데이터의 0번 샘플을 text라는 변수에 저장해봅시다.\\ntext = test_data.loc[0]['news']\\nprint(text)\", '[ 박재원 기자 ] \\'대한민국 5G 홍보대사\\'를 자처한 문재인 대통령은 \"넓고, 체증 없는 \\'통신 고속도로\\'가 5G\"라며 \"대한민국의 대전환이 이제 막 시작됐다\"고 기대감을 높였다. 문 대통령은 8일 서울 올림픽공원에서 열린 5G플러스 전략발표에 참석해 \"5G 시대는 우리가 생각하고, 만들면 그것이 세계 표준이 되는 시대\"라며 \"5G는 대한민국 혁신성장의 인프라\"라고 강조했다. 산업화 시대에 고속도로가 우리 경제의 \\'대동맥\\' 역할을 했듯, 5G가 4차 산업혁명 시대의 고속도로가 돼 새로운 기회를 열어 줄 것이란 설명이다. 문 대통령은 \"5G가 각 분야에 융합되면, 정보통신산업을 넘어 자동차, 드론(무인항공기), 로봇, 지능형 폐쇄회로TV(CCTV)를 비롯한 제조업과 벤처에 이르기까지 우리 산업 전체의 혁신을 통한 동반성장이 가능하다\"고 밝혔다. 세계 최초 상용화에 성공한 5G가 반도체를 이을 우리 경제의 새 먹거리가 될 것이란 관측이다', '. 세계 최초 상용화에 성공한 5G가 반도체를 이을 우리 경제의 새 먹거리가 될 것이란 관측이다. 정부는 2026년 세계 5G 시장 규모가 1161조원에 달할 것으로 보고 있다. 작년 반도체 시장 규모가 529조원인 점을 고려하면 2배 이상 큰 미래 시장이 창출되는 셈이다. 문 대통령은 아직은 국민에게 다소 낯선 5G 시대의 미래상을 친절히 설명해 눈길을 끌기도 했다. 문 대통령은 \"\\'지금 스마트폰으로 충분한데, 5G가 왜 필요하지?\\'라고 생각할 수 있다\"며 \"4세대 이동통신은 \\'아직은\\' 빠르지만 가까운 미래에는 결코 빠르지 않다\"고 했다. 그러면서 \"자동차가 많아질수록 더 넓은 길이 필요한 것처럼 사물과 사물을 연결하고, 데이터를 주고받는 이동통신망도 더 넓고 빠른 길이 필요하다\"고 덧붙였다. 문 대통령은 세계 최초 상용화에 성공한 우리 5G 기술을 널리 알리는 홍보대사를 자처하기도 했다', '. 문 대통령은 세계 최초 상용화에 성공한 우리 5G 기술을 널리 알리는 홍보대사를 자처하기도 했다. 5G 시장을 선점하기 위한 각국의 경쟁이 뜨겁게 달아오른 만큼 정부 차원에서 적극 지원하겠다는 방침이다. 문 대통령은 \"평창동계올림픽 360도 중계, 작년 4·27 남북한 정상회담 때 프레스센터에서 사용된 스마트월처럼 기회가 생기면 대통령부터 나서서 우리의 앞선 기술을 홍보하겠다\"고 말했다.', '학습한 T5 모델의 입력으로 사용하고 최종적으로 얻은 요약문을 출력해봅시다.\\nsummary = summarize(text, model, tokenizer)\\nprint(summary)\\n문재인 대통령은 8일 서울 올림픽공원에서 열린 5G플러스 전략발표에 참석해 \"5G가 각 분야에 융합되면, 정보통신산업을 넘어 자동차, 드론, 로봇, 지능형 폐쇄회로TV(CCTV)를 비롯한 제조업과 벤처에 이르기까지 우리 산업 전체의 혁신을 통한 동반성장이 가능하다\"며 \"5G는 대한민국 혁신성장의 인프라\"라고 강조했다.\\n매우 적은 양의 데이터로 학습했음에도 매끄러운 요약문을 생성해내는 것을 볼 수 있습니다. 주어진 데이터를 저자처럼 일부만 사용하지 않고 더 많은 데이터로 학습한다면 더 좋은 요약문을 생성할 수 있습니다.', \"이번 챕터에서 학습한 T5 학습 코드를 그대로 사용하고, 여러분들이 가진 입력과 출력 데이터로 교체한다면 번역기, text-to-sql, 핵심 키워드 추출 등 다양한 자연어 처리 태스크에서 사용할 수 있습니다.\\n==================================================\\n--- 21. 토픽 모델링(Topic Modeling) ---\\n마지막 편집일시 : 2024년 8월 26일 12:21 오전\\n==================================================\\n--- 21-01 잠재 의미 분석(Latent Semantic Analysis, LSA) ---\\n```\\nTopic 1: [('like', 0.2138), ('know', 0.20031), ('people', 0.19334), ('think', 0.17802), ('good', 0.15105)]\", \"Topic 2: [('thanks', 0.32918), ('windows', 0.29093), ('card', 0.18016), ('drive', 0.1739), ('mail', 0.15131)]\\nTopic 3: [('game', 0.37159), ('team', 0.32533), ('year', 0.28205), ('games', 0.25416), ('season', 0.18464)]\\nTopic 4: [('drive', 0.52823), ('scsi', 0.20043), ('disk', 0.15518), ('hard', 0.15511), ('card', 0.14049)]\\nTopic 5: [('windows', 0.40544), ('file', 0.25619), ('window', 0.1806), ('files', 0.16196), ('program', 0.14009)]\", \"Topic 6: [('government', 0.16085), ('chip', 0.16071), ('mail', 0.15626), ('space', 0.15047), ('information', 0.13582)]\\nTopic 7: [('like', 0.67121), ('bike', 0.14274), ('know', 0.11189), ('chip', 0.11043), ('sounds', 0.10389)]\\nTopic 8: [('card', 0.44948), ('sale', 0.21639), ('video', 0.21318), ('offer', 0.14896), ('monitor', 0.1487)]\\nTopic 9: [('know', 0.44869), ('card', 0.35699), ('chip', 0.17169), ('video', 0.15289), ('government', 0.15069)]\", \"Topic 10: [('good', 0.41575), ('know', 0.23137), ('time', 0.18933), ('bike', 0.11317), ('jesus', 0.09421)]\\nTopic 11: [('think', 0.7832), ('chip', 0.10776), ('good', 0.10613), ('thanks', 0.08985), ('clipper', 0.07882)]\\nTopic 12: [('thanks', 0.37279), ('right', 0.21787), ('problem', 0.2172), ('good', 0.21405), ('bike', 0.2116)]\\nTopic 13: [('good', 0.36691), ('people', 0.33814), ('windows', 0.28286), ('know', 0.25238), ('file', 0.18193)]\", \"Topic 14: [('space', 0.39894), ('think', 0.23279), ('know', 0.17956), ('nasa', 0.15218), ('problem', 0.12924)]\\nTopic 15: [('space', 0.3092), ('good', 0.30207), ('card', 0.21615), ('people', 0.20208), ('time', 0.15716)]\\nTopic 16: [('people', 0.46951), ('problem', 0.20879), ('window', 0.16), ('time', 0.13873), ('game', 0.13616)]\\nTopic 17: [('time', 0.3419), ('bike', 0.26896), ('right', 0.26208), ('windows', 0.19632), ('file', 0.19145)]\", \"Topic 18: [('time', 0.60079), ('problem', 0.15209), ('file', 0.13856), ('think', 0.13025), ('israel', 0.10728)]\\nTopic 19: [('file', 0.4489), ('need', 0.25951), ('card', 0.1876), ('files', 0.17632), ('problem', 0.1491)]\\nTopic 20: [('problem', 0.32797), ('file', 0.26268), ('thanks', 0.23414), ('used', 0.19339), ('space', 0.13861)]\", '```LSA는 정확히는 토픽 모델링을 위해 최적화 된 알고리즘은 아니지만, 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘이라고 볼 수 있습니다. 이에 토픽 모델링 알고리즘인 LDA에 앞서 배워보도록 하겠습니다. 뒤에서 배우게 되는 LDA는 LSA의 단점을 개선하여 탄생한 알고리즘으로 토픽 모델링에 보다 적합한 알고리즘입니다.\\nBoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다는 단점이 있었습니다. (이를 토픽 모델링 관점에서는 단어의 토픽을 고려하지 못한다고도 합니다.) 이를 위한 대안으로 DTM의 잠재된(Latent) 의미를 이끌어내는 방법으로 잠재 의미 분석(Latent Semantic Analysis, LSA)이라는 방법이 있습니다. 잠재 의미 분석(Latent Semantic Indexing, LSI)이라고 부르기도 합니다. 이하 LSA라고 명명하겠습니다.', '이 방법을 이해하기 위해서는 선형대수학의 특이값 분해(Singular Value Decomposition, SVD)를 이해할 필요가 있습니다. 이하 이를 SVD라고 명명하겠습니다. 이 실습에서는 SVD를 수행하는 구체적인 선형대수학에 대해서는 설명하지 않고, SVD가 갖고있는 의미를 이해하는 것에 초점을 맞춥니다.']\n",
      "['시작하기 앞서, 여기서의 특이값 분해(Singular Value Decomposition, SVD)는 실수 벡터 공간에 한정하여 내용을 설명함을 명시합니다. SVD란 A가 m × n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것을 말합니다.\\n$$A=UΣV^\\\\text{T}$$\\n여기서 각 3개의 행렬은 다음과 같은 조건을 만족합니다.\\n$U: m × m\\\\ \\\\text{직교행렬}\\\\ (AA^\\\\text{T}=U(ΣΣ^\\\\text{T})U^\\\\text{T})$\\n$V: n × n\\\\ \\\\text{직교행렬}\\\\ (A^\\\\text{T}A=V(Σ^\\\\text{T}Σ)V^\\\\text{T})$\\n$Σ: m × n\\\\ \\\\text{직사각 대각행렬}$', '$V: n × n\\\\ \\\\text{직교행렬}\\\\ (A^\\\\text{T}A=V(Σ^\\\\text{T}Σ)V^\\\\text{T})$\\n$Σ: m × n\\\\ \\\\text{직사각 대각행렬}$\\n여기서 직교행렬(orthogonal matrix)이란 자신과 자신의 전치 행렬(transposed matrix)의 곱 또는 이를 반대로 곱한 결과가 단위행렬(identity matrix)이 되는 행렬을 말합니다. 또한 대각행렬(diagonal matrix)이란 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 의미합니다.\\n이때 SVD로 나온 대각 행렬의 대각 원소의 값을 행렬 A의 특이값(singular value)라고 합니다. 많은 용어가 한꺼번에 나와서 복잡해보이는데 차근, 차근 용어를 정리해보도록 하겠습니다.\\n1) 전치 행렬(Transposed Matrix)', '1) 전치 행렬(Transposed Matrix)\\n전치 행렬(transposed matrix)은 원래의 행렬에서 행과 열을 바꾼 행렬입니다. 즉, 주대각선을 축으로 반사 대칭을 하여 얻는 행렬입니다. 기호는 기존 행렬 표현의 우측 위에 $\\\\text{T}$를 붙입니다. 예를 들어서 기존의 행렬을 $M$이라고 한다면, 전치 행렬은 $M^\\\\text{T}$와 같이 표현합니다.\\n$$\\nM =\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 2\\\\\\\\\\n3\\\\ 4\\\\\\\\\\n5\\\\ 6\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\ \\\\ \\\\\\n$$\\n$$\\nM^\\\\text{T} =\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 3\\\\ 5\\\\\\\\\\n2\\\\ 4\\\\ 6\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\ \\\\ \\\\\\n$$\\n2) 단위 행렬(Identity Matrix)', '\\\\begin{array}{c}\\n1\\\\ 3\\\\ 5\\\\\\\\\\n2\\\\ 4\\\\ 6\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\ \\\\ \\\\\\n$$\\n2) 단위 행렬(Identity Matrix)\\n단위 행렬(identity matrix)은 주대각선의 원소가 모두 1이며 나머지 원소는 모두 0인 정사각 행렬을 말합니다. 보통 줄여서 대문자 $I$로 표현하기도 하는데, 2 × 2 단위 행렬과 3 × 3 단위 행렬을 표현해보면 다음과 같습니다.\\n$$\\nI =\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 0\\\\\\\\\\n0\\\\ 1\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\ \\\\ \\\\\\n$$\\n$$\\nI =\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 0\\\\ 0\\\\\\\\\\n0\\\\ 1\\\\ 0\\\\\\\\\\n0\\\\ 0\\\\ 1\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\ \\\\ \\\\\\n$$\\n3) 역행렬(Inverse Matrix)', '1\\\\ 0\\\\ 0\\\\\\\\\\n0\\\\ 1\\\\ 0\\\\\\\\\\n0\\\\ 0\\\\ 1\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n\\\\ \\\\ \\\\ \\\\\\n$$\\n3) 역행렬(Inverse Matrix)\\n단위 행렬(identity matrix)를 이해했다면 역행렬(inverse matrix)을 정의할 수 있습니다. 만약 행렬 $A$와 어떤 행렬을 곱했을 때, 결과로서 단위 행렬이 나온다면 이때의 어떤 행렬을 $A$의 역행렬이라고 하며, $A^{-1}$라고 표현합니다.\\n$$\\nA\\\\ ×\\\\ A^{-1} = I\\n$$\\n$$\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 2\\\\ 3\\\\\\\\\\n4\\\\ 5\\\\ 6\\\\\\\\\\n7\\\\ 8\\\\ 9\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n×\\n\\\\left[\\n\\\\begin{array}{c}\\n\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\\\\\\\n\\\\ \\\\ \\\\ \\\\ ?\\\\ \\\\ \\\\ \\\\\\\\\\n\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n=\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 0\\\\ 0\\\\\\\\\\n0\\\\ 1\\\\ 0\\\\\\\\\\n0\\\\ 0\\\\ 1\\\\\\\\', '\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n=\\n\\\\left[\\n\\\\begin{array}{c}\\n1\\\\ 0\\\\ 0\\\\\\\\\\n0\\\\ 1\\\\ 0\\\\\\\\\\n0\\\\ 0\\\\ 1\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n4) 직교 행렬(Orthogonal matrix)\\n다시 직교 행렬(orthogonal matrix)의 정의로 돌아가서, 실수 $n × n$행렬 $A$에 대해서 $A\\\\ ×\\\\ A^{T} = I$를 만족하면서 $A^{T}\\\\ ×\\\\ A = I$을 만족하는 행렬 $A$를 직교 행렬이라고 합니다. 그런데 역행렬의 정의를 다시 생각해보면, 결국 직교 행렬은 $A^{-1}=A^{T}$를 만족합니다.\\n5) 대각 행렬(Diagonal matrix)\\n대각행렬(diagonal matrix)은 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 말합니다. 아래의 그림에서는 주대각선의 원소를 $a$라고 표현하고 있습니다. 만약 대각 행렬 Σ가 3 × 3 행렬이라면, 다음과 같은 모양을 가집니다.\\n$$', '$$\\nΣ=\\n\\\\left[\\n\\\\begin{array}{c}\\na\\\\ \\\\ 0\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ a\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ 0\\\\ \\\\ a\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n여기까진 정사각 행렬이기 때문에 직관적으로 이해가 쉽습니다. 그런데 정사각 행렬이 아니라 직사각 행렬이 될 경우를 잘 보아야 헷갈리지 않습니다. 만약 행의 크기가 열의 크기보다 크다면 다음과 같은 모양을 가집니다. 즉, m × n 행렬일 때, m > n인 경우입니다.\\n$$\\nΣ=\\n\\\\left[\\n\\\\begin{array}{c}\\na\\\\ \\\\ 0\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ a\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ 0\\\\ \\\\ a\\\\\\\\\\n0\\\\ \\\\ 0\\\\ \\\\ 0\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n반면 n > m인 경우에는 다음과 같은 모양을 가집니다.\\n$$\\nΣ=\\n\\\\left[\\n\\\\begin{array}{c}\\na\\\\ \\\\ 0\\\\ \\\\ 0\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ a\\\\ \\\\ 0\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ 0\\\\ \\\\ a\\\\ \\\\ 0\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$', '\\\\begin{array}{c}\\na\\\\ \\\\ 0\\\\ \\\\ 0\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ a\\\\ \\\\ 0\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ 0\\\\ \\\\ a\\\\ \\\\ 0\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$\\n[이미지: ]라고 표현한다고 하였을 때 특이값 은 내림차순으로 정렬되어 있다는 특징을 가집니다.\\n아래의 그림은 특이값 12.4, 9.5, 1.3이 내림차순으로 정렬되어져 있는 모습을 보여줍니다.\\n$$\\nΣ=\\n\\\\left[\\n\\\\begin{array}{c}\\n12.4\\\\ \\\\ 0\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ 9.5\\\\ \\\\ 0\\\\\\\\\\n0\\\\ \\\\ 0\\\\ \\\\ 1.3\\\\\\\\\\n\\\\end{array}\\n\\\\right]\\n$$']\n",
      "['위에서 설명한 SVD를 풀 SVD(full SVD)라고 합니다. 하지만 LSA의 경우 풀 SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD(truncated SVD)를 사용하게 됩니다. 그림을 통해 이해해보도록 하겠습니다.\\n[이미지: ]\\n절단된 SVD는 대각 행렬 Σ의 대각 원소의 값 중에서 상위값 t개만 남게 됩니다. 절단된 SVD를 수행하면 값의 손실이 일어나므로 기존의 행렬 A를 복구할 수 없습니다. 또한, U행렬과 V행렬의 t열까지만 남깁니다. 여기서 t는 우리가 찾고자하는 토픽의 수를 반영한 하이퍼파라미터값입니다. 하이퍼파라미터란 사용자가 직접 값을 선택하며 성능에 영향을 주는 매개변수를 말합니다. t를 선택하는 것은 쉽지 않은 일입니다. t를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 가져갈 수 있지만, t를 작게 잡아야만 노이즈를 제거할 수 있기 때문입니다.', '이렇게 일부 벡터들을 삭제하는 것을 데이터의 차원을 줄인다고도 말하는데, 데이터의 차원을 줄이게되면 당연히 풀 SVD를 하였을 때보다 직관적으로 계산 비용이 낮아지는 효과를 얻을 수 있습니다.\\n하지만 계산 비용이 낮아지는 것 외에도 상대적으로 중요하지 않은 정보를 삭제하는 효과를 갖고 있는데, 이는 영상 처리 분야에서는 노이즈를 제거한다는 의미를 갖고 자연어 처리 분야에서는 설명력이 낮은 정보를 삭제하고 설명력이 높은 정보를 남긴다는 의미를 갖고 있습니다. 즉, 다시 말하면 기존의 행렬에서는 드러나지 않았던 심층적인 의미를 확인할 수 있게 해줍니다.']\n",
      "['기존의 DTM이나 DTM에 단어의 중요도에 따른 가중치를 주었던 TF-IDF 행렬은 단어의 의미를 전혀 고려하지 못한다는 단점을 갖고 있었습니다. LSA는 기본적으로 DTM이나 TF-IDF 행렬에 절단된 SVD(truncated SVD)를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 갖고 있습니다. 실습을 통해서 이해해보겠습니다.\\n과일이\\n길고\\n노란\\n먹고\\n바나나\\n사과\\n싶은\\n저는\\n좋아요\\n문서1\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n문서2\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n문서3\\n0\\n1\\n1\\n0\\n2\\n0\\n0\\n0\\n문서4\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1) Full SVD\\nimport numpy as np\\n위와 같은 DTM을 실제로 파이썬을 통해서 만들면 다음과 같습니다.\\nA = np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])', \"A = np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\\nprint('DTM의 크기(shape) :', np.shape(A))\\nDTM의 크기(shape) : (4, 9)\\n4 × 9의 크기를 가지는 DTM이 생성되었습니다. 이에 대해서 풀 SVD(full SVD)를 수행해보겠습니다. 단, 여기서는 대각 행렬의 변수명을 Σ가 아니라 S를 사용합니다. 또한 V의 전치 행렬을 VT라고 하겠습니다. 소수점의 길이가 너무 길게 출력하면 보기 힘들어서 두번째 자리까지만 출력하기위해서 .round(2)를 사용합니다.\\nU, s, VT = np.linalg.svd(A, full_matrices = True)\\nprint('행렬 U :')\\nprint(U.round(2))\\nprint('행렬 U의 크기(shape) :',np.shape(U))\\n행렬 U :\", \"print('행렬 U :')\\nprint(U.round(2))\\nprint('행렬 U의 크기(shape) :',np.shape(U))\\n행렬 U :\\n[[-0.24  0.75  0.   -0.62]\\n[-0.51  0.44 -0.    0.74]\\n[-0.83 -0.49 -0.   -0.27]\\n[-0.   -0.    1.    0.  ]]\\n행렬 U의 크기(shape) : (4, 4)\\n4 × 4의 크기를 가지는 직교 행렬 U가 생성되었습니다. 이제 대각 행렬 S를 확인해봅시다.\\nprint('특이값 벡터 :')\\nprint(s.round(2))\\nprint('특이값 벡터의 크기(shape) :',np.shape(s))\\n특이값 벡터 :\\n[2.69 2.05 1.73 0.77]\\n특이값 벡터의 크기(shape) : (4,)\", \"print('특이값 벡터의 크기(shape) :',np.shape(s))\\n특이값 벡터 :\\n[2.69 2.05 1.73 0.77]\\n특이값 벡터의 크기(shape) : (4,)\\nNumpy의 linalg.svd()는 특이값 분해의 결과로 대각 행렬이 아니라 특이값의 리스트를 반환합니다. 그러므로 앞서 본 수식의 형식으로 보려면 이를 다시 대각 행렬로 바꾸어 주어야 합니다. 우선 특이값을 s에 저장하고 대각 행렬 크기의 행렬을 생성한 후에 그 행렬에 특이값을 삽입해도록 하겠습니다.\\n# 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성\\nS = np.zeros((4, 9))\\n# 특이값을 대각행렬에 삽입\\nS[:4, :4] = np.diag(s)\\nprint('대각 행렬 S :')\\nprint(S.round(2))\\nprint('대각 행렬의 크기(shape) :')\\nprint(np.shape(S))\\n대각 행렬 S :\\n[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\", \"print(np.shape(S))\\n대각 행렬 S :\\n[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\\n[0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\\n[0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\\n[0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\\n대각 행렬의 크기(shape) :\\n(4, 9)\\n4 × 9의 크기를 가지는 대각 행렬 S가 생성되었습니다. 2.69 > 2.05 > 1.73 > 0.77 순으로 값이 내림차순을 보이는 것을 확인할 수 있습니다.\\nprint('직교행렬 VT :')\\nprint(VT.round(2))\\nprint('직교 행렬 VT의 크기(shape) :')\\nprint(np.shape(VT))\\n직교행렬 VT :\\n[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\", 'print(np.shape(VT))\\n직교행렬 VT :\\n[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\\n[ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\\n[ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\\n[ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\\n[-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\\n[-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\\n[-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\\n[-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]', '[-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\\n[-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\\n직교 행렬 VT의 크기(shape) :\\n(9, 9)\\n9 × 9의 크기를 가지는 직교 행렬 VT(V의 전치 행렬)가 생성되었습니다. 즉, U × S × VT를 하면 기존의 행렬 A가 나와야 합니다. Numpy의 allclose()는 2개의 행렬이 동일하면 True를 리턴합니다. 이를 사용하여 정말로 기존의 행렬 A와 동일한지 확인해보겠습니다.\\nnp.allclose(A, np.dot(np.dot(U,S), VT).round(2))\\nTrue\\n2) 절단된 SVD(Truncated SVD)', \"np.allclose(A, np.dot(np.dot(U,S), VT).round(2))\\nTrue\\n2) 절단된 SVD(Truncated SVD)\\n지금까지 수행한 것은 풀 SVD(Full SVD)입니다. 이제 t를 정하고, 절단된 SVD(Truncated SVD)를 수행해보도록 합시다. 여기서는 t=2로 하겠습니다. 우선 대각 행렬 S 내의 특이값 중에서 상위 2개만 남기고 제거해보도록 하겠습니다.\\n# 특이값 상위 2개만 보존\\nS = S[:2,:2]\\nprint('대각 행렬 S :')\\nprint(S.round(2))\\n[[2.69 0.  ]\\n[0.   2.05]]\\n상위 2개의 값만 남기고 나머지는 모두 제거된 것을 볼 수 있습니다. 이제 직교 행렬 U에 대해서도 2개의 열만 남기고 제거합니다.\\nU = U[:,:2]\\nprint('행렬 U :')\\nprint(U.round(2))\\n행렬 U :\\n[[-0.24  0.75]\\n[-0.51  0.44]\\n[-0.83 -0.49]\\n[-0.   -0.  ]]\", \"print('행렬 U :')\\nprint(U.round(2))\\n행렬 U :\\n[[-0.24  0.75]\\n[-0.51  0.44]\\n[-0.83 -0.49]\\n[-0.   -0.  ]]\\n2개의 열만 남기고 모두 제거가 된 것을 볼 수 있습니다. 이제 행렬 V의 전치 행렬인 VT에 대해서 2개의 행만 남기고 제거합니다. 이는 V관점에서는 2개의 열만 남기고 제거한 것이 됩니다.\\nVT = VT[:2,:]\\nprint('직교행렬 VT :')\\nprint(VT.round(2))\\n직교행렬 VT :\\n[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\\n[ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\", '[ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\\n이제 축소된 행렬 U, S, VT에 대해서 다시 U × S × VT연산을 하면 기존의 A와는 다른 결과가 나오게 됩니다. 값이 손실되었기 때문에 이 세 개의 행렬로는 이제 기존의 A행렬을 복구할 수 없습니다. U × S × VT연산을 해서 나오는 값을 A_prime이라 하고 기존의 행렬 A와 값을 비교해보도록 하겠습니다.\\nA_prime = np.dot(np.dot(U,S), VT)\\nprint(A)\\nprint(A_prime.round(2))\\n[[0 0 0 1 0 1 1 0 0]\\n[0 0 0 1 1 0 1 0 0]\\n[0 1 1 0 2 0 0 0 0]\\n[1 0 0 0 0 0 0 1 1]]\\n[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\\n[ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]', '[ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\\n[ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\\n[ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\\n대체적으로 기존에 0인 값들은 0에 가가운 값이 나오고, 1인 값들은 1에 가까운 값이 나오는 것을 볼 수 있습니다. 또한 값이 제대로 복구되지 않은 구간도 존재해보입니다. 이제 이렇게 차원이 축소된 U, S, VT의 크기가 어떤 의미를 가지고 있는지 알아봅시다.', '축소된 U는 4 × 2의 크기를 가지는데, 이는 잘 생각해보면 문서의 개수 × 토픽의 수 t의 크기입니다. 단어의 개수인 9는 유지되지 않는데 문서의 개수인 4의 크기가 유지되었으니 4개의 문서 각각을 2개의 값으로 표현하고 있습니다. 즉, U의 각 행은 잠재 의미를 표현하기 위한 수치화 된 각각의 문서 벡터라고 볼 수 있습니다. 축소된 VT는 2 × 9의 크기를 가지는데, 이는 잘 생각해보면 토픽의 수 t × 단어의 개수의 크기입니다. VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 단어 벡터라고 볼 수 있습니다.\\n이 문서 벡터들과 단어 벡터들을 통해 다른 문서의 유사도, 다른 단어의 유사도, 단어(쿼리)로부터 문서의 유사도를 구하는 것들이 가능해집니다.']\n",
      "['사이킷런에서는 Twenty Newsgroups이라고 불리는 20개의 다른 주제를 가진 뉴스그룹 데이터를 제공합니다. 앞서 언급했듯이 LSA가 토픽 모델링에 최적화 된 알고리즘은 아니지만, 토픽 모델링이라는 분야의 시초가 되는 알고리즘입니다. 여기서는 LSA를 사용해서 문서의 수를 원하는 토픽의 수로 압축한 뒤에 각 토픽당 가장 중요한 단어 5개를 출력하는 실습으로 토픽 모델링을 수행합니다.\\n뉴스그룹 데이터는 뉴스 데이터가 아닙니다.\\n1) 뉴스그룹 데이터에 대한 이해\\nimport pandas as pd\\nfrom sklearn.datasets import fetch_20newsgroups\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.decomposition import TruncatedSVD', \"from sklearn.decomposition import TruncatedSVD\\ndataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\\ndocuments = dataset.data\\nprint('샘플의 수 :',len(documents))\\n샘플의 수 : 11314\\n훈련에 사용할 뉴스그룹 데이터는 총 11,314개입니다. 이 중 첫번째 훈련용 샘플을 출력해보겠습니다.\\ndocuments[1]\", '\"\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\\\nof steam!\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nJim,\\\\n\\\\nSorry I can\\'t pity you, Jim.  And I\\'m sorry that you have these feelings of\\\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\\\nall end happily ever after anyway', '.  Oh well, just pretend that it will\\\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\\\nalt.atheist.hard, you won\\'t be bummin\\' so much?\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBye-Bye, Big Jim.  Don\\'t forget your Flintstone\\'s Chewables!  :) \\\\n--\\\\nBake Timmons, III\"', '뉴스그룹 데이터에는 특수문자가 포함된 다수의 영어문장으로 구성되어져 있습니다. 이런 형식의 샘플이 총 11,314개 존재합니다. 사이킷런이 제공하는 뉴스그룹 데이터에서 target_name에는 본래 이 뉴스그룹 데이터가 어떤 20개의 카테고리를 갖고있었는지가 저장되어져 있습니다. 이를 출력해보겠습니다.\\nprint(dataset.target_names)', \"print(dataset.target_names)\\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\\n2) 텍스트 전처리\", '2) 텍스트 전처리\\n시작하기 앞서, 텍스트 데이터에 대해서 가능한한 정제 과정을 거쳐야만 합니다. 기본적인 아이디어는 알파벳을 제외한 구두점, 숫자, 특수 문자를 제거하는 것입니다. 이는 텍스트 전처리 챕터에서 정제 기법으로 배웠던 정규 표현식을 통해서 해결할 수 있습니다. 또한 짧은 단어는 유용한 정보를 담고있지 않다고 가정하고, 길이가 짧은 단어도 제거합니다. 그리고 마지막으로 모든 알파벳을 소문자로 바꿔서 단어의 개수를 줄이는 작업을 합니다.\\nnews_df = pd.DataFrame({\\'document\\':documents})\\n# 특수 문자 제거\\nnews_df[\\'clean_doc\\'] = news_df[\\'document\\'].str.replace(\"[^a-zA-Z]\", \" \", regex=True)\\n# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)', \"# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\\nnews_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\\n# 전체 단어에 대한 소문자 변환\\nnews_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\\n데이터의 정제가 끝났습니다. 다시 첫번째 훈련용 뉴스그룹 샘플만 출력하여 정제 전, 후에 어떤 차이가 있는지 확인해보겠습니다.\\nnews_df['clean_doc'][1]\", \"데이터의 정제가 끝났습니다. 다시 첫번째 훈련용 뉴스그룹 샘플만 출력하여 정제 전, 후에 어떤 차이가 있는지 확인해보겠습니다.\\nnews_df['clean_doc'][1]\\n'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'\", \"우선 특수문자가 제거되었으며, if나 you와 같은 길이가 3이하인 단어가 제거된 것을 확인할 수 있습니다. 뿐만 아니라 대문자가 전부 소문자로 바뀌었습니다. 이제 뉴스그룹 데이터에서 불용어를 제거합니다. 불용어를 제거하기 위해서 토큰화를 우선 수행합니다. 토큰화와 불용어 제거를 순차적으로 진행합니다.\\n# NLTK로부터 불용어를 받아온다.\\nstop_words = stopwords.words('english')\\ntokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\\ntokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\\n# 불용어를 제거합니다.\\n다시 첫번째 훈련용 뉴스그룹 샘플을 출력합니다.\\nprint(tokenized_doc[1])\", \"# 불용어를 제거합니다.\\n다시 첫번째 훈련용 뉴스그룹 샘플을 출력합니다.\\nprint(tokenized_doc[1])\\n['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\", \"기존에 있었던 불용어에 속하던 your, about, just, that, will, after 단어들이 사라졌을 뿐만 아니라, 토큰화가 수행된 것을 확인할 수 있습니다.\\n3) TF-IDF 행렬 만들기\\n불용어 제거를 위해 토큰화 작업을 수행하였지만, TfidfVectorizer(TF-IDF 실습 참고)는 기본적으로 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용합니다. 그렇기 때문에 TfidfVectorizer를 사용해서 TF-IDF 행렬을 만들기 위해서 다시 토큰화 작업을 역으로 취소하는 작업을 수행해보도록 하겠습니다. 이를 역토큰화(Detokenization)라고 합니다.\\n# 역토큰화 (토큰화 작업을 역으로 되돌림)\\ndetokenized_doc = []\\nfor i in range(len(news_df)):\\nt = ' '.join(tokenized_doc[i])\\ndetokenized_doc.append(t)\", \"for i in range(len(news_df)):\\nt = ' '.join(tokenized_doc[i])\\ndetokenized_doc.append(t)\\nnews_df['clean_doc'] = detokenized_doc\\n역토큰화가 제대로 되었는지 다시 첫번째 훈련용 뉴스그룹 샘플을 출력하여 확인해보겠습니다.\\nnews_df['clean_doc'][1]\\n'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'\", \"정상적으로 불용어가 제거된 상태에서 역토큰화가 수행되었음을 확인할 수 있습니다.\\n이제 사이킷런의 TfidfVectorizer를 통해 단어 1,000개에 대한 TF-IDF 행렬을 만들 것입니다. 물론 텍스트 데이터에 있는 모든 단어를 가지고 행렬을 만들 수는 있겠지만, 여기서는 1,000개의 단어로 제한하도록 하겠습니다.\\nvectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, # 상위 1,000개의 단어를 보존\\nmax_df = 0.5, smooth_idf=True)\\nX = vectorizer.fit_transform(news_df['clean_doc'])\\n# TF-IDF 행렬의 크기 확인\\nprint('TF-IDF 행렬의 크기 :',X.shape)\\nTF-IDF 행렬의 크기 : (11314, 1000)\\n11,314 × 1,000의 크기를 가진 TF-IDF 행렬이 생성되었음을 확인할 수 있습니다.\", \"TF-IDF 행렬의 크기 : (11314, 1000)\\n11,314 × 1,000의 크기를 가진 TF-IDF 행렬이 생성되었음을 확인할 수 있습니다.\\n4) 토픽 모델링(Topic Modeling)\\n이제 TF-IDF 행렬을 다수의 행렬로 분해해보도록 하겠습니다. 여기서는 사이킷런의 절단된 SVD(Truncated SVD)를 사용합니다. 절단된 SVD를 사용하면 차원을 축소할 수 있습니다. 원래 기존 뉴스그룹 데이터가 20개의 카테고리를 갖고있었기 때문에, 20개의 토픽을 가졌다고 가정하고 토픽 모델링을 시도해보겠습니다. 토픽의 숫자는 n_components의 파라미터로 지정이 가능합니다.\\nsvd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\\nsvd_model.fit(X)\\nlen(svd_model.components_)\\n20\", 'svd_model.fit(X)\\nlen(svd_model.components_)\\n20\\n여기서 svd_model.componets_는 앞서 배운 LSA에서 VT에 해당됩니다.\\nnp.shape(svd_model.components_)\\n(20, 1000)\\n정확하게 토픽의 수 t × 단어의 수의 크기를 가지는 것을 볼 수 있습니다.\\nterms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\\ndef get_topics(components, feature_names, n=5):\\nfor idx, topic in enumerate(components):\\nprint(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\\nget_topics(svd_model.components_,terms)', \"get_topics(svd_model.components_,terms)\\n각 20개의 행의 각 1,000개의 열 중 가장 값이 큰 5개의 값을 찾아서 단어로 출력합니다.\\nTopic 1: [('like', 0.2138), ('know', 0.20031), ('people', 0.19334), ('think', 0.17802), ('good', 0.15105)]\\nTopic 2: [('thanks', 0.32918), ('windows', 0.29093), ('card', 0.18016), ('drive', 0.1739), ('mail', 0.15131)]\\nTopic 3: [('game', 0.37159), ('team', 0.32533), ('year', 0.28205), ('games', 0.25416), ('season', 0.18464)]\", \"Topic 4: [('drive', 0.52823), ('scsi', 0.20043), ('disk', 0.15518), ('hard', 0.15511), ('card', 0.14049)]\\nTopic 5: [('windows', 0.40544), ('file', 0.25619), ('window', 0.1806), ('files', 0.16196), ('program', 0.14009)]\\nTopic 6: [('government', 0.16085), ('chip', 0.16071), ('mail', 0.15626), ('space', 0.15047), ('information', 0.13582)]\\nTopic 7: [('like', 0.67121), ('bike', 0.14274), ('know', 0.11189), ('chip', 0.11043), ('sounds', 0.10389)]\", \"Topic 8: [('card', 0.44948), ('sale', 0.21639), ('video', 0.21318), ('offer', 0.14896), ('monitor', 0.1487)]\\nTopic 9: [('know', 0.44869), ('card', 0.35699), ('chip', 0.17169), ('video', 0.15289), ('government', 0.15069)]\\nTopic 10: [('good', 0.41575), ('know', 0.23137), ('time', 0.18933), ('bike', 0.11317), ('jesus', 0.09421)]\\nTopic 11: [('think', 0.7832), ('chip', 0.10776), ('good', 0.10613), ('thanks', 0.08985), ('clipper', 0.07882)]\", \"Topic 12: [('thanks', 0.37279), ('right', 0.21787), ('problem', 0.2172), ('good', 0.21405), ('bike', 0.2116)]\\nTopic 13: [('good', 0.36691), ('people', 0.33814), ('windows', 0.28286), ('know', 0.25238), ('file', 0.18193)]\\nTopic 14: [('space', 0.39894), ('think', 0.23279), ('know', 0.17956), ('nasa', 0.15218), ('problem', 0.12924)]\\nTopic 15: [('space', 0.3092), ('good', 0.30207), ('card', 0.21615), ('people', 0.20208), ('time', 0.15716)]\", \"Topic 16: [('people', 0.46951), ('problem', 0.20879), ('window', 0.16), ('time', 0.13873), ('game', 0.13616)]\\nTopic 17: [('time', 0.3419), ('bike', 0.26896), ('right', 0.26208), ('windows', 0.19632), ('file', 0.19145)]\\nTopic 18: [('time', 0.60079), ('problem', 0.15209), ('file', 0.13856), ('think', 0.13025), ('israel', 0.10728)]\\nTopic 19: [('file', 0.4489), ('need', 0.25951), ('card', 0.1876), ('files', 0.17632), ('problem', 0.1491)]\", \"Topic 20: [('problem', 0.32797), ('file', 0.26268), ('thanks', 0.23414), ('used', 0.19339), ('space', 0.13861)]\"]\n",
      "['정리해보면 LSA는 쉽고 빠르게 구현이 가능할 뿐만 아니라 단어의 잠재적인 의미를 이끌어낼 수 있어 문서의 유사도 계산 등에서 좋은 성능을 보여준다는 장점을 갖고 있습니다. 하지만 SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하여 계산하려고하면 보통 처음부터 다시 계산해야 합니다. 즉, 새로운 정보에 대해 업데이트가 어렵습니다. 이는 최근 LSA 대신 Word2Vec 등 단어의 의미를 벡터화할 수 있는 또 다른 방법론인 인공 신경망 기반의 방법론이 각광받는 이유이기도 합니다.\\n==================================================\\n--- 21-02 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA) ---\\n```\\ntopictable = make_topictable_per_doc(ldamodel, corpus)', \"```\\ntopictable = make_topictable_per_doc(ldamodel, corpus)\\ntopictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\\ntopictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\\ntopictable[:10]\\n```토픽 모델링은 문서의 집합에서 토픽을 찾아내는 프로세스를 말합니다. 이는 검색 엔진, 고객 민원 시스템 등과 같이 문서의 주제를 알아내는 일이 중요한 곳에서 사용됩니다. 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)은 토픽 모델링의 대표적인 알고리즘입니다. 줄여서 LDA라고 합니다.\", 'LDA는 문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다고 가정합니다. 데이터가 주어지면, LDA는 문서가 생성되던 과정을 역추적합니다.\\n참고 링크 : https://lettier.com/projects/lda-topic-modeling/\\n위의 사이트는 코드 작성 없이 입력한 문서들로부터 DTM을 만들고 LDA를 수행한 결과를 보여주는 웹 사이트입니다.']\n",
      "['우선 LDA의 내부 메커니즘에 대해서 이해하기 전에, LDA를 일종의 블랙 박스로 보고 LDA에 문서 집합을 입력하면, 어떤 결과를 보여주는지 간소화 된 예를 들어 보겠습니다. 아래와 같은 3개의 문서가 있다고 합시다. 지금의 예제는 간단해서 눈으로도 토픽 모델링을 할 수 있을 것 같지만, 실제 수십만개 이상의 문서가 있는 경우는 직접 토픽을 찾아내는 것이 어렵기 때문에 LDA의 도움이 필요합니다.\\n문서1 : 저는 사과랑 바나나를 먹어요\\n문서2 : 우리는 귀여운 강아지가 좋아요\\n문서3 : 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요', '문서1 : 저는 사과랑 바나나를 먹어요\\n문서2 : 우리는 귀여운 강아지가 좋아요\\n문서3 : 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요\\nLDA를 수행할 때 문서 집합에서 토픽이 몇 개가 존재할지 가정하는 것은 사용자가 해야 할 일입니다. 여기서는 LDA에 2개의 토픽을 찾으라고 요청하겠습니다. 토픽의 개수를 의미하는 변수를 k라고 하였을 때, k를 2로 한다는 의미입니다. k의 값을 잘못 선택하면 원치않는 이상한 결과가 나올 수 있습니다. 이렇게 모델의 성능에 영향을 주는 사용자가 직접 선택하는 매개변수를 머신 러닝 용어로 하이퍼파라미터라고 합니다. 이러한 하이퍼파라미터의 선택은 여러 실험을 통해 얻은 값일 수도 있고, 우선 시도해보는 값일 수도 있습니다.', 'LDA가 위의 세 문서로부터 2개의 토픽을 찾은 결과는 아래와 같습니다. 여기서는 LDA 입력 전에 주어와 불필요한 조사 등을 제거하는 전처리 과정은 거쳤다고 가정합니다. 즉, 전처리 과정을 거친 DTM이 LDA의 입력이 되었다고 가정합니다.\\nLDA는 각 문서의 토픽 분포와 각 토픽 내의 단어 분포를 추정합니다.\\n<각 문서의 토픽 분포>\\n문서1 : 토픽 A 100%\\n문서2 : 토픽 B 100%\\n문서3 : 토픽 B 60%, 토픽 A 40%\\n<각 토픽의 단어 분포>\\n토픽A : 사과 20%, 바나나 40%, 먹어요 40%, 귀여운 0%, 강아지 0%, 깜찍하고 0%, 좋아요 0%\\n토픽B : 사과 0%, 바나나 0%, 먹어요 0%, 귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%', '토픽B : 사과 0%, 바나나 0%, 먹어요 0%, 귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%\\nLDA는 토픽의 제목을 정해주지 않지만, 이 시점에서 알고리즘의 사용자는 위 결과로부터 두 토픽이 각각 과일에 대한 토픽과 강아지에 대한 토픽이라고 판단해볼 수 있습니다. 이제 LDA에 대해서 알아봅시다. 실제로 LDA는 아래의 설명보다 훨씬 더 복잡하지만, 여기서는 수학적인 수식은 배제하고 개념적 이해에 초점을 둡니다.']\n",
      "[\"LDA는 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘입니다. LDA는 앞서 배운 빈도수 기반의 표현 방법인 BoW의 행렬 DTM 또는 TF-IDF 행렬을 입력으로 하는데, 이로부터 알 수 있는 사실은 LDA는 단어의 순서는 신경쓰지 않겠다는 겁니다.\\nLDA는 문서들로부터 토픽을 뽑아내기 위해서 이러한 가정을 염두해두고 있습니다. 모든 문서 하나, 하나가 작성될 때 그 문서의 작성자는 이러한 생각을 했습니다. '나는 이 문서를 작성하기 위해서 이런 주제들을 넣을거고, 이런 주제들을 위해서는 이런 단어들을 넣을 거야.' 조금 더 구체적으로 알아보겠습니다. 각각의 문서는 다음과 같은 과정을 거쳐서 작성되었다고 가정합니다.\\n1) 문서에 사용할 단어의 개수 N을 정합니다.\\n- Ex) 5개의 단어를 정하였습니다.\\n2) 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정합니다.\", '1) 문서에 사용할 단어의 개수 N을 정합니다.\\n- Ex) 5개의 단어를 정하였습니다.\\n2) 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정합니다.\\n- Ex) 위 예제와 같이 토픽이 2개라고 하였을 때 강아지 토픽을 60%, 과일 토픽을 40%와 같이 선택할 수 있습니다.\\n3) 문서에 사용할 각 단어를 (아래와 같이) 정합니다.\\n3-1) 토픽 분포에서 토픽 T를 확률적으로 고릅니다.\\n- Ex) 60% 확률로 강아지 토픽을 선택하고, 40% 확률로 과일 토픽을 선택할 수 있습니다.\\n3-2) 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고릅니다.\\n- Ex) 강아지 토픽을 선택하였다면, 33% 확률로 강아지란 단어를 선택할 수 있습니다. 이제 3)을 반복하면서 문서를 완성합니다.\\n이러한 과정을 통해 문서가 작성되었다는 가정 하에 LDA는 토픽을 뽑아내기 위하여 위 과정을 역으로 추적하는 역공학(reverse engneering)을 수행합니다.']\n",
      "['이제 LDA의 수행 과정을 정리해보겠습니다.\\n1) 사용자는 알고리즘에게 토픽의 개수 k를 알려줍니다.\\n앞서 말하였듯이 LDA에게 토픽의 개수를 알려주는 역할은 사용자의 역할입니다. LDA는 토픽의 개수 k를 입력받으면, k개의 토픽이 M개의 전체 문서에 걸쳐 분포되어 있다고 가정합니다.\\n2) 모든 단어를 k개 중 하나의 토픽에 할당합니다.\\n이제 LDA는 모든 문서의 모든 단어에 대해서 k개 중 하나의 토픽을 랜덤으로 할당합니다. 이 작업이 끝나면 각 문서는 토픽을 가지며, 토픽은 단어 분포를 가지는 상태입니다. 물론 랜덤으로 할당하였기 때문에 사실 이 결과는 전부 틀린 상태입니다. 만약 한 단어가 한 문서에서 2회 이상 등장하였다면, 각 단어는 서로 다른 토픽에 할당되었을 수도 있습니다.\\n3) 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행합니다. (iterative)', '3) 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행합니다. (iterative)\\n3-1) 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정합니다. 이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당됩니다.\\n- p(topic t | document d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율\\n- p(word w | topic t) : 각 토픽들 t에서 해당 단어 w의 분포\\n이를 반복하면, 모든 할당이 완료된 수렴 상태가 됩니다. 두 가지 기준이 어떤 의미인지 예를 들어보겠습니다. 설명의 편의를 위해서 두 개의 문서라는 새로운 예를 사용합니다.\\n[이미지: ]\\n위의 그림은 두 개의 문서 doc1과 doc2를 보여줍니다. 여기서는 doc1의 세번째 단어 apple의 토픽을 결정하고자 합니다.\\n[이미지: ]', '[이미지: ]\\n위의 그림은 두 개의 문서 doc1과 doc2를 보여줍니다. 여기서는 doc1의 세번째 단어 apple의 토픽을 결정하고자 합니다.\\n[이미지: ]\\n우선 첫번째로 사용하는 기준은 문서 doc1의 단어들이 어떤 토픽에 해당하는지를 봅니다. doc1의 모든 단어들은 토픽 A와 토픽 B에 50 대 50의 비율로 할당되어져 있으므로, 이 기준에 따르면 단어 apple은 토픽 A 또는 토픽 B 둘 중 어디에도 속할 가능성이 있습니다.\\n[이미지: ]\\n두번째 기준은 단어 apple이 전체 문서에서 어떤 토픽에 할당되어져 있는지를 봅니다. 이 기준에 따르면 단어 apple은 토픽 B에 할당될 가능성이 높습니다. 이러한 두 가지 기준을 참고하여 LDA는 doc1의 apple을 어떤 토픽에 할당할지 결정합니다.']\n",
      "['LSA : DTM을 차원 축소 하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.\\nLDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다.']\n",
      "['이제 LDA를 실습을 통해 직접 진행해보도록 하겠습니다. LSA 실습에서는 사이킷런(sklearn)을 사용하였지만, 이번에는 gensim을 사용하므로 앞 실습과 실습 과정이 확연히 다릅니다. 사이킷런을 통해 LDA를 진행하는 실습은 아래의 링크에 작성하였습니다. 아래의 링크에서는 LSA 실습의 실습과 진행 과정이 거의 유사하니 참고하시기 바랍니다.\\n사이킷런으로 LDA 실습 : https://wikidocs.net/40710\\n1) 정수 인코딩과 단어 집합 만들기\\n바로 이전 실습인 LSA 실습에서 사용하였던 Twenty Newsgroups이라고 불리는 20개의 다른 주제를 가진 뉴스 데이터를 다시 사용합니다. 전처리 과정은 이전 실습과 중복되므로 생략합니다. 동일한 전처리 과정을 거친 후에 tokenized_doc으로 저장한 상태라고 합시다. 훈련용 뉴스를 5개만 출력해보겠습니다.\\ntokenized_doc[:5]', 'tokenized_doc[:5]\\n0    [well, sure, about, story, seem, biased, what,...\\n1    [yeah, expect, people, read, actually, accept,...\\n2    [although, realize, that, principle, your, str...\\n3    [notwithstanding, legitimate, fuss, about, thi...\\n4    [well, will, have, change, scoring, playoff, p...\\nName: clean_doc, dtype: object', '4    [well, will, have, change, scoring, playoff, p...\\nName: clean_doc, dtype: object\\n이제 각 단어에 정수 인코딩을 하는 동시에, 각 뉴스에서의 단어의 빈도수를 기록해보겠습니다. 여기서는 각 단어를 (word_id, word_frequency)의 형태로 바꾸고자 합니다. word_id는 단어가 정수 인코딩된 값이고, word_frequency는 해당 뉴스에서의 해당 단어의 빈도수를 의미합니다. 이는 gensim의 corpora.Dictionary()를 사용하여 손쉽게 구할 수 있습니다. 전체 뉴스에 대해서 정수 인코딩을 수행하고, 두번째 뉴스를 출력해봅시다.\\nfrom gensim import corpora\\ndictionary = corpora.Dictionary(tokenized_doc)\\ncorpus = [dictionary.doc2bow(text) for text in tokenized_doc]', 'corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\\nprint(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0\\n[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]', \"두번째 뉴스의 출력 결과를 봅시다. 위의 출력 결과 중에서 (66, 2)는 정수 인코딩이 66으로 할당된 단어가 두번째 뉴스에서는 두 번 등장하였음을 의미합니다. 66이라는 값을 가지는 단어가 정수 인코딩이 되기 전에는 어떤 단어였는지 확인하여봅시다. 이는 dictionary[]에 기존 단어가 무엇인지 알고자하는 정수값을 입력하여 확인할 수 있습니다.\\nprint(dictionary[66])\\nfaith\\n기존에는 단어 'faith'이었음을 알 수 있습니다. 총 학습된 단어의 개수를 확인해보겠습니다. 이는 dictionary의 길이를 확인하면 됩니다.\\nlen(dictionary)\\n65284\\n총 65,284개의 단어가 학습되었습니다. 이제 LDA 모델을 훈련시켜보겠습니다.\\n2) LDA 모델 훈련시키기\\n기존의 뉴스 데이터가 총 20개의 카테고리를 가지고 있었으므로 토픽의 개수를 20으로 하여 LDA 모델을 학습시켜보도록 하겠습니다.\\nimport gensim\", '기존의 뉴스 데이터가 총 20개의 카테고리를 가지고 있었으므로 토픽의 개수를 20으로 하여 LDA 모델을 학습시켜보도록 하겠습니다.\\nimport gensim\\nNUM_TOPICS = 20 # 20개의 토픽, k=20\\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\\ntopics = ldamodel.print_topics(num_words=4)\\nfor topic in topics:\\nprint(topic)\\n(0, \\'0.015*\"drive\" + 0.014*\"thanks\" + 0.012*\"card\" + 0.012*\"system\"\\')\\n(1, \\'0.009*\"back\" + 0.009*\"like\" + 0.009*\"time\" + 0.008*\"went\"\\')', '(1, \\'0.009*\"back\" + 0.009*\"like\" + 0.009*\"time\" + 0.008*\"went\"\\')\\n(2, \\'0.012*\"colorado\" + 0.010*\"david\" + 0.006*\"decenso\" + 0.005*\"tyre\"\\')\\n(3, \\'0.020*\"number\" + 0.018*\"wire\" + 0.013*\"bits\" + 0.013*\"filename\"\\')\\n(4, \\'0.038*\"space\" + 0.013*\"nasa\" + 0.011*\"research\" + 0.010*\"medical\"\\')\\n(5, \\'0.014*\"price\" + 0.010*\"sale\" + 0.009*\"good\" + 0.008*\"shipping\"\\')\\n(6, \\'0.012*\"available\" + 0.009*\"file\" + 0.009*\"information\" + 0.008*\"version\"\\')', '(6, \\'0.012*\"available\" + 0.009*\"file\" + 0.009*\"information\" + 0.008*\"version\"\\')\\n(7, \\'0.021*\"would\" + 0.013*\"think\" + 0.012*\"people\" + 0.011*\"like\"\\')\\n(8, \\'0.035*\"window\" + 0.021*\"display\" + 0.017*\"widget\" + 0.013*\"application\"\\')\\n(9, \\'0.012*\"people\" + 0.010*\"jesus\" + 0.007*\"armenian\" + 0.007*\"israel\"\\')\\n(10, \\'0.008*\"government\" + 0.007*\"system\" + 0.006*\"public\" + 0.006*\"encryption\"\\')\\n(11, \\'0.013*\"germany\" + 0.008*\"sweden\" + 0.008*\"switzerland\" + 0.007*\"gaza\"\\')', '(11, \\'0.013*\"germany\" + 0.008*\"sweden\" + 0.008*\"switzerland\" + 0.007*\"gaza\"\\')\\n(12, \\'0.020*\"game\" + 0.018*\"team\" + 0.015*\"games\" + 0.013*\"play\"\\')\\n(13, \\'0.024*\"apple\" + 0.014*\"water\" + 0.013*\"ground\" + 0.011*\"cable\"\\')\\n(14, \\'0.011*\"evidence\" + 0.010*\"believe\" + 0.010*\"truth\" + 0.010*\"church\"\\')\\n(15, \\'0.016*\"president\" + 0.010*\"states\" + 0.007*\"united\" + 0.007*\"year\"\\')\\n(16, \\'0.047*\"file\" + 0.035*\"output\" + 0.033*\"entry\" + 0.021*\"program\"\\')', '(16, \\'0.047*\"file\" + 0.035*\"output\" + 0.033*\"entry\" + 0.021*\"program\"\\')\\n(17, \\'0.008*\"dept\" + 0.008*\"devils\" + 0.007*\"caps\" + 0.007*\"john\"\\')\\n(18, \\'0.011*\"year\" + 0.009*\"last\" + 0.007*\"first\" + 0.006*\"runs\"\\')\\n(19, \\'0.013*\"outlets\" + 0.013*\"norton\" + 0.012*\"quantum\" + 0.008*\"neck\"\\')', '(19, \\'0.013*\"outlets\" + 0.013*\"norton\" + 0.012*\"quantum\" + 0.008*\"neck\"\\')\\n각 단어 앞에 붙은 수치는 단어의 해당 토픽에 대한 기여도를 보여줍니다. 또한 맨 앞에 있는 토픽 번호는 0부터 시작하므로 총 20개의 토픽은 0부터 19까지의 번호가 할당되어져 있습니다. passes는 알고리즘의 동작 횟수를 말하는데, 알고리즘이 결정하는 토픽의 값이 적절히 수렴할 수 있도록 충분히 적당한 횟수를 정해주면 됩니다. 여기서는 총 15회를 수행하였습니다. 여기서는 num_words=4로 총 4개의 단어만 출력하도록 하였습니다.  만약 10개의 단어를 출력하고 싶다면 아래의 코드를 수행하면 됩니다.\\nprint(ldamodel.print_topics())\\n3) LDA 시각화 하기', 'print(ldamodel.print_topics())\\n3) LDA 시각화 하기\\nLDA 시각화를 위해서는 pyLDAvis의 설치가 필요합니다. 윈도우의 명령 프롬프트나 MAC/UNIX의 터미널에서 아래의 명령을 수행하여 pyLDAvis를 설치하시기 바랍니다.\\npip install pyLDAvis\\n설치가 완료되었다면 LDA 시각화 실습을 진행합니다.\\nimport pyLDAvis.gensim_models\\npyLDAvis.enable_notebook()\\nvis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)\\npyLDAvis.display(vis)\\n[이미지: ]', 'vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)\\npyLDAvis.display(vis)\\n[이미지: ]\\n좌측의 원들은 각각의 20개의 토픽을 나타냅니다. 각 원과의 거리는 각 토픽들이 서로 얼마나 다른지를 보여줍니다. 만약 두 개의 원이 겹친다면, 이 두 개의 토픽은 유사한 토픽이라는 의미입니다. 위의 그림에서는 10번 토픽을 클릭하였고, 이에 따라 우측에는 10번 토픽에 대한 정보가 나타납니다. 한 가지 주의할 점은 LDA 모델의 출력 결과에서는 토픽 번호가 0부터 할당되어 0~19의 숫자가 사용된 것과는 달리 위의 LDA 시각화에서는 토픽의 번호가 1부터 시작하므로 각 토픽 번호는 이제 +1이 된 값인 1~20까지의 값을 가집니다.\\n4) 문서 별 토픽 분포 보기', \"4) 문서 별 토픽 분포 보기\\n위에서 토픽 별 단어 분포는 확인하였으나, 아직 문서 별 토픽 분포에 대해서는 확인하지 못 하였습니다. 우선 문서 별 토픽 분포를 확인하는 방법을 보겠습니다. 각 문서의 토픽 분포는 이미 훈련된 LDA 모델인 ldamodel[]에 전체 데이터가 정수 인코딩 된 결과를 넣은 후에 확인이 가능합니다. 여기서는 책의 지면의 한계로 상위 5개의 문서에 대해서만 토픽 분포를 확인해보겠습니다.\\nfor i, topic_list in enumerate(ldamodel[corpus]):\\nif i==5:\\nbreak\\nprint(i,'번째 문서의 topic 비율은',topic_list)\\n0 번째 문서의 topic 비율은 [(7, 0.3050222), (9, 0.5070568), (11, 0.1319604), (18, 0.042834017)]\", '0 번째 문서의 topic 비율은 [(7, 0.3050222), (9, 0.5070568), (11, 0.1319604), (18, 0.042834017)]\\n1 번째 문서의 topic 비율은 [(0, 0.031606797), (7, 0.7529218), (13, 0.02924682), (14, 0.12861845), (17, 0.037851967)]\\n2 번째 문서의 topic 비율은 [(7, 0.52241164), (9, 0.36602455), (16, 0.09760969)]\\n3 번째 문서의 topic 비율은 [(1, 0.16926806), (5, 0.04912094), (6, 0.04034211), (7, 0.11710636), (10, 0.5854137), (15, 0.02776434)]\\n4 번째 문서의 topic 비율은 [(7, 0.42152268), (12, 0.21917087), (17, 0.32781804)]', '4 번째 문서의 topic 비율은 [(7, 0.42152268), (12, 0.21917087), (17, 0.32781804)]\\n위의 출력 결과에서 (숫자, 확률)은 각각 토픽 번호와 해당 토픽이 해당 문서에서 차지하는 분포도를 의미합니다. 예를 들어 0번째 문서의 토픽 비율에서 (7, 0.3050222)은 7번 토픽이 30%의 분포도를 가지는 것을 의미합니다. 위의 코드를 응용하여 좀 더 깔끔한 형태인 데이터프레임 형식으로 출력해보겠습니다.\\ndef make_topictable_per_doc(ldamodel, corpus):\\ntopic_table = pd.DataFrame()\\n# 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\\nfor i, topic_list in enumerate(ldamodel[corpus]):\\ndoc = topic_list[0] if ldamodel.per_word_topics else topic_list', 'doc = topic_list[0] if ldamodel.per_word_topics else topic_list\\ndoc = sorted(doc, key=lambda x: (x[1]), reverse=True)\\n# 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\\n# EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%),\\n# Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\\n# 48 > 25 > 21 > 5 순으로 정렬이 된 것.\\n# 모든 문서에 대해서 각각 아래를 수행\\nfor j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\\nif j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽', \"if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\\ntopic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\\n# 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\\nelse:\\nbreak\\nreturn(topic_table)\\ntopictable = make_topictable_per_doc(ldamodel, corpus)\\ntopictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\\ntopictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\\ntopictable[:10]\\n[이미지: ]\", \"topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\\ntopictable[:10]\\n[이미지: ]\\n==================================================\\n--- 21-03 사이킷런의 잠재 디리클레 할당(LDA) 실습 ---\\n```\\nTopic 1: [('government', 8725.19), ('sydney', 8393.29), ('queensland', 7720.12), ('change', 5874.27), ('home', 5674.38)]\\nTopic 2: [('australia', 13691.08), ('australian', 11088.95), ('melbourne', 7528.43), ('world', 6707.7), ('south', 6677.03)]\", \"Topic 3: [('death', 5935.06), ('interview', 5924.98), ('kill', 5851.6), ('jail', 4632.85), ('life', 4275.27)]\\nTopic 4: [('house', 6113.49), ('2016', 5488.19), ('state', 4923.41), ('brisbane', 4857.21), ('tasmania', 4610.97)]\\nTopic 5: [('court', 7542.74), ('attack', 6959.64), ('open', 5663.0), ('face', 5193.63), ('warn', 5115.01)]\\nTopic 6: [('market', 5545.86), ('rural', 5502.89), ('plan', 4828.71), ('indigenous', 4223.4), ('power', 3968.26)]\", \"Topic 7: [('charge', 8428.8), ('election', 7561.63), ('adelaide', 6758.36), ('make', 5658.99), ('test', 5062.69)]\\nTopic 8: [('police', 12092.44), ('crash', 5281.14), ('drug', 4290.87), ('beat', 3257.58), ('rise', 2934.92)]\\nTopic 9: [('fund', 4693.03), ('labor', 4047.69), ('national', 4038.68), ('council', 4006.62), ('claim', 3604.75)]\\nTopic 10: [('trump', 11966.41), ('perth', 6456.53), ('report', 5611.33), ('school', 5465.06), ('woman', 5456.76)]\", '```앞서 gensim을 통해서 LDA를 수행하고, 시각화를 진행해보았습니다. 이번에는 LSA 실습에서처럼 사이킷런을 사용하여 LDA를 수행하여 보겠습니다. 사이킷런을 사용하므로 전반적인 과정은 LSA 실습과 유사합니다.']\n",
      "['1) 뉴스 기사 제목 데이터에 대한 이해\\n약 15년 동안 발행되었던 뉴스 기사 제목을 모아놓은 영어 데이터를 아래 링크에서 다운받을 수 있습니다.\\n링크 : https://www.kaggle.com/therohk/million-headlines\\nimport pandas as pd\\nimport urllib.request\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import WordNetLemmatizer\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.decomposition import LatentDirichletAllocation', 'from sklearn.decomposition import LatentDirichletAllocation\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/19.%20Topic%20Modeling%20(LDA%2C%20BERT-Based)/dataset/abcnews-date-text.csv\")\\ndata = pd.read_csv(\\'abcnews-date-text.csv\\', error_bad_lines=False)\\nprint(\\'뉴스 제목 개수 :\\',len(data))\\n뉴스 제목 개수 : 1082168\\n해당 데이터는 약 100만개의 샘플을 갖고 있습니다. 상위 5개의 샘플만 출력해봅시다.\\nprint(data.head(5))\\npublish_date                                      headline_text', 'print(data.head(5))\\npublish_date                                      headline_text\\n0      20030219  aba decides against community broadcasting lic...\\n1      20030219     act fire witnesses must be aware of defamation\\n2      20030219     a g calls for infrastructure protection summit\\n3      20030219           air nz staff in aust strike for pay rise\\n4      20030219      air nz strike to affect australian travellers', \"4      20030219      air nz strike to affect australian travellers\\n이 데이터는 publish_data와 headline_text라는 두 개의 열을 갖고 있습니다. 각각 뉴스가 나온 날짜와 뉴스 기사 제목을 의미합니다. 필요한 데이터는 이 중에서 headline_text 열. 즉, 뉴스 기사 제목이므로 이 부분만 별도로 저장합니다.\\ntext = data[['headline_text']]\\ntext.head(5)\\nheadline_text\\n0  aba decides against community broadcasting lic...\\n1     act fire witnesses must be aware of defamation\\n2     a g calls for infrastructure protection summit\\n3           air nz staff in aust strike for pay rise\", \"3           air nz staff in aust strike for pay rise\\n4      air nz strike to affect australian travellers\\n정상적으로 headline_text 열만 추출된 것을 확인할 수 있습니다.\\n2) 텍스트 전처리\\n이번 실습에서는 불용어 제거, 표제어 추출, 길이가 짧은 단어 제거라는 세 가지 전처리 기법을 사용합니다.\\ntext['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\\nNLTK의 word_tokenize를 통해 단어 토큰화를 수행합니다.\\nprint(text.head(5))\\nheadline_text\\n0  [aba, decides, against, community, broadcastin...\\n1  [act, fire, witnesses, must, be, aware, of, de...\", \"1  [act, fire, witnesses, must, be, aware, of, de...\\n2  [a, g, calls, for, infrastructure, protection,...\\n3  [air, nz, staff, in, aust, strike, for, pay, r...\\n4  [air, nz, strike, to, affect, australian, trav...\\n상위 5개의 샘플만 출력하여 단어 토큰화 결과를 확인합니다. 이제 불용어를 제거합니다.\\nstop_words = stopwords.words('english')\\ntext['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])\\n여기서는 NLTK가 제공하는 영어 불용어를 통해서 text 데이터로부터 불용어를 제거해보도록 하겠습니다.\\nprint(text.head(5))\", '여기서는 NLTK가 제공하는 영어 불용어를 통해서 text 데이터로부터 불용어를 제거해보도록 하겠습니다.\\nprint(text.head(5))\\nheadline_text\\n0   [aba, decides, community, broadcasting, licence]\\n1    [act, fire, witnesses, must, aware, defamation]\\n2     [g, calls, infrastructure, protection, summit]\\n3          [air, nz, staff, aust, strike, pay, rise]\\n4  [air, nz, strike, affect, australian, travellers]', \"4  [air, nz, strike, affect, australian, travellers]\\n상위 5개의 샘플에 대해서 불용어를 제거하기 전과 후의 데이터만 비교해도 확실히 몇 가지 단어들이 사라진 것이 보입니다. against, be, of, a, in, to 등의 단어가 제거되었습니다. 이제 표제어 추출을 수행합니다. 표제어 추출로 3인칭 단수 표현을 1인칭으로 바꾸고, 과거 현재형 동사를 현재형으로 바꿉니다.\\ntext['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\\nprint(text.head(5))\\nheadline_text\\n0       [aba, decide, community, broadcast, licence]\\n1      [act, fire, witness, must, aware, defamation]\", \"1      [act, fire, witness, must, aware, defamation]\\n2      [g, call, infrastructure, protection, summit]\\n3          [air, nz, staff, aust, strike, pay, rise]\\n4  [air, nz, strike, affect, australian, travellers]\\n표제어 추출이 된 것을 확인할 수 있습니다. 이제 길이가 3이하인 단어에 대해서 제거하는 작업을 수행합니다.\\ntokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3])\\nprint(tokenized_doc[:5])\\n0       [decide, community, broadcast, licence]\\n1      [fire, witness, must, aware, defamation]\", '0       [decide, community, broadcast, licence]\\n1      [fire, witness, must, aware, defamation]\\n2    [call, infrastructure, protection, summit]\\n3                   [staff, aust, strike, rise]\\n4      [strike, affect, australian, travellers]\\n길이가 3이하인 단어들에 대해서 제거가 된 것을 볼 수 있습니다. 이제 TF-IDF 행렬을 만들어보겠습니다.\\n3) TF-IDF 행렬 만들기\\nTF-IDF 실습에서 배운 TfidfVectorizer는 기본적으로 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용합니다. 이를 사용하기 위해 다시 토큰화 작업을 역으로 취소하는 역토큰화(Detokenization)작업을 수행해보겠습니다.\\n# 역토큰화 (토큰화 작업을 되돌림)\\ndetokenized_doc = []', \"# 역토큰화 (토큰화 작업을 되돌림)\\ndetokenized_doc = []\\nfor i in range(len(text)):\\nt = ' '.join(tokenized_doc[i])\\ndetokenized_doc.append(t)\\n# 다시 text['headline_text']에 재저장\\ntext['headline_text'] = detokenized_doc\\n역토큰화가 되었는지 text['headline_text']의 5개의 샘플을 출력해보겠습니다.\\ntext['headline_text'][:5]\\n0       decide community broadcast licence\\n1       fire witness must aware defamation\\n2    call infrastructure protection summit\\n3                   staff aust strike rise\\n4      strike affect australian travellers\", \"3                   staff aust strike rise\\n4      strike affect australian travellers\\nName: headline_text, dtype: object\\n정상적으로 역토큰화가 수행되었음을 확인할 수 있습니다. 이제 사이킷런의 TfidfVectorizer를 TF-IDF 행렬을 만들 것입니다. 텍스트 데이터에 있는 모든 단어를 가지고 행렬을 만들 수도 있겠지만, 여기서는 간단히 1,000개의 단어로 제한하겠습니다.\\n# 상위 1,000개의 단어를 보존\\nvectorizer = TfidfVectorizer(stop_words='english', max_features= 1000)\\nX = vectorizer.fit_transform(text['headline_text'])\\n# TF-IDF 행렬의 크기 확인\\nprint('TF-IDF 행렬의 크기 :',X.shape)\\nTF-IDF 행렬의 크기 : (1082168, 1000)\", \"# TF-IDF 행렬의 크기 확인\\nprint('TF-IDF 행렬의 크기 :',X.shape)\\nTF-IDF 행렬의 크기 : (1082168, 1000)\\n1,082,168 × 1,000의 크기를 가진 가진 TF-IDF 행렬이 생겼습니다. 이제 이에 LDA를 수행합니다.\\n4) 토픽 모델링\\nlda_model = LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1)\\nlda_top = lda_model.fit_transform(X)\\nprint(lda_model.components_)\\nprint(lda_model.components_.shape)\\n[[1.00001533e-01 1.00001269e-01 1.00004179e-01 ... 1.00006124e-01\\n1.00003111e-01 1.00003064e-01]\", '[[1.00001533e-01 1.00001269e-01 1.00004179e-01 ... 1.00006124e-01\\n1.00003111e-01 1.00003064e-01]\\n[1.00001199e-01 1.13513398e+03 3.50170830e+03 ... 1.00009349e-01\\n1.00001896e-01 1.00002937e-01]\\n[1.00001811e-01 1.00001151e-01 1.00003566e-01 ... 1.00002693e-01\\n1.00002061e-01 7.53381835e+02]\\n...\\n[1.00001065e-01 1.00001689e-01 1.00003278e-01 ... 1.00006721e-01\\n1.00004902e-01 1.00004759e-01]\\n[1.00002401e-01 1.00000732e-01 1.00002989e-01 ... 1.00003517e-01\\n1.00001428e-01 1.00005266e-01]', '[1.00002401e-01 1.00000732e-01 1.00002989e-01 ... 1.00003517e-01\\n1.00001428e-01 1.00005266e-01]\\n[1.00003427e-01 1.00002313e-01 1.00007340e-01 ... 1.00003732e-01\\n1.00001207e-01 1.00005153e-01]]\\n(10, 1000)\\n# 단어 집합. 1,000개의 단어가 저장됨.\\nterms = vectorizer.get_feature_names()\\ndef get_topics(components, feature_names, n=5):\\nfor idx, topic in enumerate(components):\\nprint(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])', \"get_topics(lda_model.components_,terms)\\nTopic 1: [('government', 8725.19), ('sydney', 8393.29), ('queensland', 7720.12), ('change', 5874.27), ('home', 5674.38)]\\nTopic 2: [('australia', 13691.08), ('australian', 11088.95), ('melbourne', 7528.43), ('world', 6707.7), ('south', 6677.03)]\\nTopic 3: [('death', 5935.06), ('interview', 5924.98), ('kill', 5851.6), ('jail', 4632.85), ('life', 4275.27)]\", \"Topic 4: [('house', 6113.49), ('2016', 5488.19), ('state', 4923.41), ('brisbane', 4857.21), ('tasmania', 4610.97)]\\nTopic 5: [('court', 7542.74), ('attack', 6959.64), ('open', 5663.0), ('face', 5193.63), ('warn', 5115.01)]\\nTopic 6: [('market', 5545.86), ('rural', 5502.89), ('plan', 4828.71), ('indigenous', 4223.4), ('power', 3968.26)]\\nTopic 7: [('charge', 8428.8), ('election', 7561.63), ('adelaide', 6758.36), ('make', 5658.99), ('test', 5062.69)]\", \"Topic 8: [('police', 12092.44), ('crash', 5281.14), ('drug', 4290.87), ('beat', 3257.58), ('rise', 2934.92)]\\nTopic 9: [('fund', 4693.03), ('labor', 4047.69), ('national', 4038.68), ('council', 4006.62), ('claim', 3604.75)]\\nTopic 10: [('trump', 11966.41), ('perth', 6456.53), ('report', 5611.33), ('school', 5465.06), ('woman', 5456.76)]\\n==================================================\\n--- 21-04 BERT를 이용한 키워드 추출 : 키버트(KeyBERT) ---\\n```\\n['algorithm generalize training',\\n'labels unseen instances',\", \"```\\n['algorithm generalize training',\\n'labels unseen instances',\\n'new examples optimal',\\n'determine class labels',\\n'supervised learning algorithm']\\n```키버트 실습을 위해서는 우선 SBERT를 위한 패키지인 sentence_transformers를 설치해야 합니다.\\n!pip install sentence_transformers\"]\n",
      "['import numpy as np\\nimport itertools\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom sentence_transformers import SentenceTransformer\\n이 튜토리얼에서는 지도 학습에 대한 영어 문서를 사용합니다. 여러분들이 이미 친숙한 주제에 대한 문서이므로 키워드 추출이 잘 되고 있는지 여러분들이 직관적으로 판단하기에 좋은 예시일 것입니다.\\ndoc = \"\"\"\\nSupervised learning is the machine learning task of\\nlearning a function that maps an input to an output based\\non example input-output pairs.[1] It infers a function', 'on example input-output pairs.[1] It infers a function\\nfrom labeled training data consisting of a set of\\ntraining examples.[2] In supervised learning, each\\nexample is a pair consisting of an input object\\n(typically a vector) and a desired output value (also\\ncalled the supervisory signal). A supervised learning\\nalgorithm analyzes the training data and produces an\\ninferred function, which can be used for mapping new\\nexamples. An optimal scenario will allow for the algorithm', 'examples. An optimal scenario will allow for the algorithm\\nto correctly determine the class labels for unseen\\ninstances. This requires the learning algorithm to\\ngeneralize from the training data to unseen situations\\nin a \\'reasonable\\' way (see inductive bias).\\n\"\"\"\\n여기서는 사이킷런의 CountVectorizer를 사용하여 단어를 추출합니다. CountVectorizer를 사용하는 이유는 n_gram_range의 인자를 사용하면 쉽게 n-gram을 추출할 수 있기 때문입니다. 예를 들어, (3, 3)로 설정하면 결과 후보는 3개의 단어를 한 묶음으로 간주하는 trigram을 추출합니다.\\n# 3개의 단어 묶음인 단어구 추출\\nn_gram_range = (3, 3)', '# 3개의 단어 묶음인 단어구 추출\\nn_gram_range = (3, 3)\\nstop_words = \"english\"\\ncount = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\\ncandidates = count.get_feature_names_out()\\nprint(\\'trigram 개수 :\\',len(candidates))\\nprint(\\'trigram 다섯개만 출력 :\\',candidates[:5])\\ntrigram 개수 : 72\\ntrigram 다섯개만 출력 : [\\'algorithm analyzes training\\' \\'algorithm correctly determine\\'\\n\\'algorithm generalize training\\' \\'allow algorithm correctly\\'\\n\\'analyzes training data\\']', \"'algorithm generalize training' 'allow algorithm correctly'\\n'analyzes training data']\\n다음으로 이제 문서와 문서로부터 추출한 키워드들을 SBERT를 통해서 수치화 할 차례입니다.\\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\\ndoc_embedding = model.encode([doc])\\ncandidate_embeddings = model.encode(candidates)\\n이제 문서와 가장 유사한 키워드들을 추출합니다. 여기서는 문서와 가장 유사한 키워드들은 문서를 대표하기 위한 좋은 키워드라고 가정합니다. 상위 5개의 키워드를 출력합니다.\\ntop_n = 5\\ndistances = cosine_similarity(doc_embedding, candidate_embeddings)\", \"top_n = 5\\ndistances = cosine_similarity(doc_embedding, candidate_embeddings)\\nkeywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\\nprint(keywords)\\n['algorithm analyzes training',\\n'learning algorithm generalize',\\n'learning machine learning',\\n'learning algorithm analyzes',\\n'algorithm generalize training']\", \"'learning machine learning',\\n'learning algorithm analyzes',\\n'algorithm generalize training']\\n5개의 키워드가 출력되는데, 이들의 의미가 좀 비슷해보입니다. 비슷한 의미의 키워드들이 리턴되는 데는 이유가 있습니다. 당연히 이 키워드들이 문서를 가장 잘 나타내고 있기 때문입니다. 만약, 지금 출력한 것보다는 좀 더 다양한 의미의 키워드들이 출력된다면 이들을 그룹으로 본다는 관점에서는 해당 키워드들이 문서를 잘 나타낼 가능성이 적을 수도 있습니다. 따라서 키워드들을 다양하게 출력하고 싶다면 키워드 선정의 정확성과 키워드들의 다양성 사이의 미묘한 균형이 필요합니다.\\n여기서는 다양한 키워드들을 얻기 위해서 두 가지 알고리즘을 사용합니다.\\nMax Sum Similarity\\nMaximal Marginal Relevance\"]\n",
      "['데이터 쌍 사이의 최대 합 거리는 데이터 쌍 간의 거리가 최대화되는 데이터 쌍으로 정의됩니다. 여기서의 의도는 후보 간의 유사성을 최소화하면서 문서와의 후보 유사성을 극대화하고자 하는 것입니다.\\ndef max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\\n# 문서와 각 키워드들 간의 유사도\\ndistances = cosine_similarity(doc_embedding, candidate_embeddings)\\n# 각 키워드들 간의 유사도\\ndistances_candidates = cosine_similarity(candidate_embeddings,\\ncandidate_embeddings)\\n# 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\\nwords_idx = list(distances.argsort()[0][-nr_candidates:])', 'words_idx = list(distances.argsort()[0][-nr_candidates:])\\nwords_vals = [candidates[index] for index in words_idx]\\ndistances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\\n# 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\\nmin_sim = np.inf\\ncandidate = None\\nfor combination in itertools.combinations(range(len(words_idx)), top_n):\\nsim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\\nif sim < min_sim:\\ncandidate = combination\\nmin_sim = sim', \"if sim < min_sim:\\ncandidate = combination\\nmin_sim = sim\\nreturn [words_vals[idx] for idx in candidate]\\n이를 위해 상위 10개의 키워드를 선택하고 이 10개 중에서 서로 가장 유사성이 낮은 5개를 선택합니다.\\n낮은 nr_candidates를 설정하면 결과는 출력된 키워드 5개는 기존의 코사인 유사도만 사용한 것과 매우 유사한 것으로 보입니다.\\nmax_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)\\n['requires learning algorithm',\\n'signal supervised learning',\\n'learning function maps',\\n'algorithm analyzes training',\\n'learning machine learning']\", \"'learning function maps',\\n'algorithm analyzes training',\\n'learning machine learning']\\n그러나 상대적으로 높은 nr_candidates는 더 다양한 키워드 5개를 만듭니다.\\nmax_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)\\n['set training examples',\\n'generalize training data',\\n'requires learning algorithm',\\n'supervised learning algorithm',\\n'learning machine learning']\"]\n",
      "['결과를 다양화하는 마지막 방법은 MMR(Maximal Marginal Relevance)입니다. MMR은 텍스트 요약 작업에서 중복을 최소화하고 결과의 다양성을 극대화하기 위해 노력합니다. 참고 할 수 있는 자료로 EmbedRank(https://arxiv.org/pdf/1801.04470.pdf) 라는 키워드 추출 알고리즘은 키워드/키프레이즈를 다양화하는 데 사용할 수 있는 MMR을 구현했습니다. 먼저 문서와 가장 유사한 키워드/키프레이즈를 선택합니다. 그런 다음 문서와 유사하고 이미 선택된 키워드/키프레이즈와 유사하지 않은 새로운 후보를 반복적으로 선택합니다.\\ndef mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\\n# 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\\nword_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)', 'word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\\n# 각 키워드들 간의 유사도\\nword_similarity = cosine_similarity(candidate_embeddings)\\n# 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\\n# 만약, 2번 문서가 가장 유사도가 높았다면\\n# keywords_idx = [2]\\nkeywords_idx = [np.argmax(word_doc_similarity)]\\n# 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\\n# 만약, 2번 문서가 가장 유사도가 높았다면\\n# ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\\ncandidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]', 'candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\\n# 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\\n# ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\\nfor _ in range(top_n - 1):\\ncandidate_similarities = word_doc_similarity[candidates_idx, :]\\ntarget_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\\n# MMR을 계산\\nmmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\\nmmr_idx = candidates_idx[np.argmax(mmr)]', \"mmr_idx = candidates_idx[np.argmax(mmr)]\\n# keywords & candidates를 업데이트\\nkeywords_idx.append(mmr_idx)\\ncandidates_idx.remove(mmr_idx)\\nreturn [words[idx] for idx in keywords_idx]\\n만약 상대적으로 낮은 diversity 값을 설정한다면, 결과는 기존의 코사인 유사도만 사용한 것과 매우 유사한 것으로 보입니다.\\nmmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\\n['algorithm generalize training',\\n'supervised learning algorithm',\\n'learning machine learning',\\n'learning algorithm analyzes',\\n'learning algorithm generalize']\", \"'learning machine learning',\\n'learning algorithm analyzes',\\n'learning algorithm generalize']\\n그러나 상대적으로 높은 diversity값은 다양한 키워드 5개를 만들어냅니다.\\nmmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)\\n['algorithm generalize training',\\n'labels unseen instances',\\n'new examples optimal',\\n'determine class labels',\\n'supervised learning algorithm']\\n==================================================\\n--- 21-05 한국어 키버트(Korean KeyBERT)를 이용한 키워드 추출 ---\\n마지막 편집일시 : 2024년 8월 26일 12:22 오전\", \"--- 21-05 한국어 키버트(Korean KeyBERT)를 이용한 키워드 추출 ---\\n마지막 편집일시 : 2024년 8월 26일 12:22 오전\\n==================================================\\n--- 21-06 BERT 기반 복합 토픽 모델(Combined Topic Models, CTM) ---\\n```\\n[['politician', 'member', 'born', 'party', 'served'],\\n['mi', 'county', 'km', 'south', 'east'],\\n['church', 'century', 'roman', 'greek', 'latin'],\\n['team', 'season', 'home', 'football', 'games'],\\n['album', 'released', 'band', 'first', 'music'],\\n['school', 'university', 'college', 'education', 'public'],\", \"['school', 'university', 'college', 'education', 'public'],\\n['university', 'professor', 'author', 'research', 'work'],\\n['party', 'member', 'election', 'council', 'elections'],\\n['located', 'river', 'north', 'state', 'km'],\\n['mi', 'village', 'approximately', 'km', 'south'],\\n['mi', 'west', 'approximately', 'east', 'kilometres'],\\n['school', 'high', 'county', 'district', 'located'],\\n['built', 'house', 'story', 'style', 'building'],\\n['station', 'radio', 'licensed', 'fm', 'owned'],\", \"['built', 'house', 'story', 'style', 'building'],\\n['station', 'radio', 'licensed', 'fm', 'owned'],\\n['war', 'air', 'royal', 'navy', 'army'],\\n['american', 'born', 'former', 'football', 'college'],\\n['state', 'city', 'area', 'county', 'located'],\\n['station', 'railway', 'city', 'line', 'airport'],\\n['published', 'game', 'book', 'first', 'released'],\\n['family', 'genus', 'brown', 'species', 'found'],\\n['season', 'league', 'club', 'football', 'team'],\\n['american', 'team', 'season', 'football', 'played'],\", \"['american', 'team', 'season', 'football', 'played'],\\n['war', 'world', 'ii', 'cross', 'summer'],\\n['son', 'de', 'king', 'french', 'daughter'],\\n['series', 'produced', 'television', 'film', 'directed'],\\n['united', 'states', 'county', 'national', 'park'],\\n['railway', 'bridge', 'river', 'island', 'station'],\\n['written', 'film', 'novel', 'directed', 'published'],\\n['born', 'played', 'made', 'english', 'first'],\\n['film', 'directed', 'produced', 'best', 'stars'],\", \"['born', 'played', 'made', 'english', 'first'],\\n['film', 'directed', 'produced', 'best', 'stars'],\\n['born', 'world', 'competed', 'silver', 'summer'],\\n['island', 'mountain', 'islands', 'range', 'land'],\\n['held', 'world', 'championship', 'tournament', 'cup'],\\n['system', 'data', 'systems', 'computer', 'software'],\\n['organization', 'established', 'education', 'research', 'international'],\\n['either', 'used', 'term', 'space', 'using'],\\n['album', 'released', 'band', 'rock', 'music'],\", \"['either', 'used', 'term', 'space', 'using'],\\n['album', 'released', 'band', 'rock', 'music'],\\n['family', 'species', 'found', 'mm', 'native'],\\n['government', 'established', 'political', 'responsible', 'act'],\\n['company', 'based', 'founded', 'group', 'business'],\\n['american', 'known', 'born', 'best', 'york'],\\n['village', 'town', 'england', 'parish', 'civil'],\\n['states', 'united', 'state', 'served', 'member'],\\n['also', 'district', 'population', 'persian', 'iran'],\", \"['also', 'district', 'population', 'persian', 'iran'],\\n['painter', 'studied', 'artist', 'work', 'composer'],\\n['television', 'radio', 'show', 'series', 'music'],\\n['played', 'professional', 'born', 'league', 'player'],\\n['often', 'chemical', 'different', 'means', 'associated'],\\n['game', 'series', 'car', 'held', 'racing'],\\n['region', 'municipality', 'area', 'province', 'located']]\", \"['region', 'municipality', 'area', 'province', 'located']]\\n```이번 챕터에서는 전통적인 빈도수 기반 문서 벡터화 방식인 Bag of Words와 사전 훈련된 언어 모델의 문서 임베딩 방식인 SBERT를 결합하여 사용하는 복합 토픽 모델(Combined Topic Models, CTM)에 대해서 실습해봅시다. 실습을 위해 문맥을 반영한 토픽 모델 라이브러리인 contextualized-topic-models와 LDA 시각화 라이브러리인 pyldavis를 설치합니다.\\npip install contextualized-topic-models==2.2.0\\npip install pyldavis\"]\n",
      "['시작에 앞서 문맥을 반영한 토픽 모델(Contextualized Topic Models)의 개념을 소개해봅시다. 문맥을 반영한 토픽 모델(Contextualized Topic Models)은 문맥을 반영한 BERT의 문서 임베딩의 표현력과 기존 토픽 모델의 비지도 학습 능력을 결합하여 문서에서 주제를 가져오는 토픽 모델을 말합니다. 그리고 여기서 소개할 복합 토픽 모델(Combined Topic Models, CTM)은 문맥을 반영한 토픽 모델의 일종입니다.']\n",
      "['학습을 위한 데이터로 하나의 라인(line)에 하나의 문서로 구성된 파일이 필요한데요. 우선, 여러분들의 데이터가 없다면 여기서 준비한 영문 위키피디아 파일로 실습을 해봅시다.\\n# 데이터 다운로드\\n!wget https://raw.githubusercontent.com/vinid/data/master/dbpedia_sample_abstract_20k_unprep.txt\\n상위 3개의 라인만 출력해봅시다.\\n!head -n 3 dbpedia_sample_abstract_20k_unprep.txt', '상위 3개의 라인만 출력해봅시다.\\n!head -n 3 dbpedia_sample_abstract_20k_unprep.txt\\nThe Mid-Peninsula Highway is a proposed freeway across the Niagara Peninsula in the Canadian province of Ontario. Although plans for a highway connecting Hamilton to Fort Erie south of the Niagara Escarpment have surfaced for decades,it was not until The Niagara Frontier International Gateway Study was published by the Ministry', \"Monte Zucker (died March 15, 2007) was an American photographer. He specialized in wedding photography, entering it as a profession in 1947. In the 1970s he operated a studio in Silver Spring, Maryland. Later he lived in Florida. He was Brides Magazine's Wedding Photographer of the Year for 1990 and\", 'Henry Howard, 13th Earl of Suffolk, 6th Earl of Berkshire (8 August 1779 – 10 August 1779) was a British peer, the son of Henry Howard, 12th Earl of Suffolk. His father died on 7 March 1779, leaving behind his pregnant widow. The Earldom of Suffolk became dormant until she\\n실제 파일에는 줄바꿈이 없으나 위 출력 결과에서는 임의로 보기 편하도록 라인 별로 줄바꿈을 하였습니다. 파일명을 변수에 저장해둡시다.\\ntext_file = \"dbpedia_sample_abstract_20k_unprep.txt\"']\n",
      "['from contextualized_topic_models.models.ctm import CombinedTM\\nfrom contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\\nfrom contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing\\nimport nltk\\nBag of Words 기반의 접근에서는 전처리 챕터에서 학습했던 정규화(Normalization) 과정이 굉장히 중요합니다. 다시 말해 특수 문자가 붙어서 동일한 단어가 다른 단어로 인식되지 않도록 특수 문자를 제거해주고, 불용어를 제거하여 불필요한 단어들을 제거한 후에 빈도수가 높은 단어들을 사용하는 것이 좋습니다. 여기서는 NLTK에서 제공하는 영어 불용어들을 제거하고, 단어와 붙어있는 특수 문자를 제거하며, 영단어의 소문자화를 진행합니다.', 'nltk.download(\\'stopwords\\')\\ndocuments = [line.strip() for line in open(text_file, encoding=\"utf-8\").readlines()]\\nsp = WhiteSpacePreprocessing(documents, stopwords_language=\\'english\\')\\npreprocessed_documents, unpreprocessed_corpus, vocab = sp.preprocess()\\n전처리 전, 후의 문서를 상위 2개를 출력하여 비교해봅시다.\\n# 전처리 전 문서\\nunpreprocessed_corpus[:2]', \"전처리 전, 후의 문서를 상위 2개를 출력하여 비교해봅시다.\\n# 전처리 전 문서\\nunpreprocessed_corpus[:2]\\n['The Mid-Peninsula Highway is a proposed freeway across the Niagara Peninsula in the Canadian province of Ontario. Although plans for a highway connecting Hamilton to Fort Erie south of the Niagara Escarpment have surfaced for decades,it was not until The Niagara Frontier International Gateway Study was published by the Ministry',\", '\"Monte Zucker (died March 15, 2007) was an American photographer. He specialized in wedding photography, entering it as a profession in 1947. In the 1970s he operated a studio in Silver Spring, Maryland. Later he lived in Florida. He was Brides Magazine\\'s Wedding Photographer of the Year for 1990 and\"]\\n# normalization 전처리 후 문서\\npreprocessed_documents[:2]', \"# normalization 전처리 후 문서\\npreprocessed_documents[:2]\\n['mid peninsula highway proposed across peninsula canadian province ontario although highway connecting hamilton fort south international study published ministry',\\n'died march american photographer specialized photography operated studio silver spring maryland later lived florida magazine photographer year']\\n전체 단어 집합의 크기를 출력해봅시다.\\n# 전체 단어 집합의 크기\\nprint('bag of words에 사용 될 단어 집합의 크기 :',len(vocab))\\nbag of words에 사용 될 단어 집합의 크기 : 2000\", 'print(\\'bag of words에 사용 될 단어 집합의 크기 :\\',len(vocab))\\nbag of words에 사용 될 단어 집합의 크기 : 2000\\n전체 단어 집합의 크기는 2,000입니다. WhiteSpacePreprocessing()의 vocabulary_size 인자의 기본값이 2000이기 때문입니다.\\n전처리 되지 않은 문서는 문맥을 반영한 SBERT의 문서 임베딩을 얻기 위한 입력으로 사용할 것이기 때문에 제거해서는 안 됩니다. 전처리 전 문서와 전처리 후 문서를 TopicModelDataPreparation 객체에 넘겨줍니다. 이 객체는 bag of words와 문맥을 반영한 문서의 BERT 임베딩을 얻습니다. 여기서 사용할 pretrained BERT는 paraphrase-distilroberta-base-v1입니다.\\ntp = TopicModelDataPreparation(\"paraphrase-distilroberta-base-v1\")', 'tp = TopicModelDataPreparation(\"paraphrase-distilroberta-base-v1\")\\ntraining_dataset = tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)\\n이제 tp.vocab을 하면 단어 집합을 얻을 수 있는데, 이는 앞에서 vocab에 저장된 단어 집합과 동일합니다.\\nlen(tp.vocab)\\n2000']\n",
      "['이제 토픽 모델을 학습합니다. 여기서는 하이퍼파라미터에 해당하는 토픽의 개수(n_components)로는 50개를 선정합니다.\\nctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, n_components=50, num_epochs=20)\\nctm.fit(training_dataset)\\nEpoch: [20/20]   Seen Samples: [400000/400000]  Train Loss: 135.494220703125    Time: 0:00:05.428048: : 20it [01:49,  5.47s/it]']\n",
      "[\"학습 후에는 토픽 모델이 선정한 토픽들을 보려면 get_topic_lists라는 메소드를 사용합니다. 해당 메소드에는 각 토픽마다 몇 개의 단어를 보고 싶은지에 해당하는 파라미터를 넣어즐 수 있습니다. 여기서는 5개를 선택했습니다. 아래의 토픽들은 위키피디아(일반적인 주제)으로부터 얻은 토픽을 보여줍니다. 우리는 영어 문서로 학습하였으므로 각 토픽에 해당하는 단어들도 영어 단어들입니다.\\nctm.get_topic_lists(5)\\n[['mi', 'kilometres', 'village', 'north', 'municipality'],\\n['national', 'house', 'built', 'historic', 'county'],\\n['used', 'defined', 'mathematics', 'number', 'typically'],\\n['film', 'directed', 'best', 'films', 'produced'],\", \"['film', 'directed', 'best', 'films', 'produced'],\\n['united', 'states', 'company', 'air', 'international'],\\n['species', 'family', 'found', 'native', 'genus'],\\n['composer', 'january', 'painter', 'studied', 'son'],\\n['station', 'line', 'located', 'railway', 'street'],\\n['church', 'roman', 'century', 'ancient', 'catholic'],\\n['county', 'school', 'high', 'state', 'located'],\\n['published', 'book', 'work', 'books', 'writer'],\\n['held', 'season', 'tournament', 'cup', 'championship'],\", \"['held', 'season', 'tournament', 'cup', 'championship'],\\n['made', 'born', 'english', 'first', 'played'],\\n['built', 'house', 'story', 'building', 'style'],\\n['war', 'world', 'series', 'television', 'first'],\\n['government', 'political', 'party', 'council', 'act'],\\n['american', 'new', 'york', 'known', 'born'],\\n['school', 'high', 'college', 'located', 'secondary'],\\n['published', 'written', 'book', 'fictional', 'novel'],\\n['area', 'river', 'located', 'park', 'lake'],\", \"['area', 'river', 'located', 'park', 'lake'],\\n['born', 'summer', 'world', 'olympics', 'competed'],\\n['de', 'french', 'son', 'king', 'daughter'],\\n['university', 'served', 'american', 'law', 'born'],\\n['film', 'directed', 'written', 'drama', 'produced'],\\n['university', 'research', 'professor', 'india', 'institute'],\\n['war', 'class', 'royal', 'navy', 'army'],\\n['music', 'band', 'rock', 'singer', 'best'],\\n['born', 'played', 'former', 'professional', 'american'],\", \"['born', 'played', 'former', 'professional', 'american'],\\n['album', 'band', 'released', 'rock', 'music'],\\n['used', 'use', 'system', 'software', 'model'],\\n['radio', 'company', 'station', 'broadcasting', 'owned'],\\n['played', 'born', 'football', 'former', 'league'],\\n['world', 'held', 'women', 'championships', 'place'],\\n['family', 'found', 'species', 'plant', 'mm'],\\n['member', 'party', 'politician', 'election', 'parliament'],\\n['island', 'point', 'antarctic', 'land', 'expedition'],\", \"['island', 'point', 'antarctic', 'land', 'expedition'],\\n['states', 'united', 'county', 'list', 'national'],\\n['district', 'also', 'population', 'iran', 'persian'],\\n['enzyme', 'gene', 'belongs', 'chemical', 'humans'],\\n['region', 'area', 'province', 'municipality', 'part'],\\n['team', 'season', 'football', 'division', 'university'],\\n['released', 'album', 'band', 'studio', 'recorded'],\\n['city', 'town', 'england', 'located', 'miles'],\\n['mi', 'west', 'south', 'km', 'village'],\", \"['city', 'town', 'england', 'located', 'miles'],\\n['mi', 'west', 'south', 'km', 'village'],\\n['mi', 'west', 'south', 'east', 'km'],\\n['league', 'club', 'football', 'team', 'season'],\\n['series', 'game', 'television', 'show', 'video'],\\n['line', 'station', 'railway', 'chinese', 'near'],\\n['company', 'based', 'founded', 'business', 'research'],\\n['politician', 'member', 'john', 'served', 'became']]\"]\n",
      "['토픽들을 시각화하기 위해서는 PyLDAvis를 사용합니다.\\nlda_vis_data = ctm.get_ldavis_data_format(tp.vocab, training_dataset, n_samples=10)\\nSampling: [10/10]: : 10it [00:52,  5.25s/it]\\nimport pyLDAvis as vis\\nlda_vis_data = ctm.get_ldavis_data_format(tp.vocab, training_dataset, n_samples=10)\\nctm_pd = vis.prepare(**lda_vis_data)\\nvis.display(ctm_pd)\\n[이미지: ]']\n",
      "['import numpy as np\\n임의의 문서를 가져와서 어떤 토픽이 할당되었는지 확인할 수 있습니다. 예를 들어, 반도(peninsula)에 대한 주제를 담고 있는 첫번째 전처리 된 문서의 토픽을 예측해 봅시다.\\ntopics_predictions = ctm.get_thetas(training_dataset, n_samples=5) # get all the topic predictions\\nSampling: [5/5]: : 5it [00:23,  4.74s/it]\\n# 전처리 문서의 첫번째 문서\\nprint(preprocessed_documents[0])\\nmid peninsula highway proposed across peninsula canadian province ontario although highway connecting hamilton fort south international study published ministry\\n첫번째 문서에 대한 토픽을 추출해봅시다.', \"첫번째 문서에 대한 토픽을 추출해봅시다.\\nctm.get_topic_lists(5)[topic_number]\\n['located', 'river', 'north', 'state', 'km']\"]\n",
      "['현재 경로에 모델을 저장합니다.\\nctm.save(models_dir=\"./\")\\n현재 ctm이라는 변수에 할당된 모델을 삭제합니다.\\ndel ctm\\n저장한 모델을 로드합니다.\\nctm = CombinedTM(bow_size=len(tp.vocab), contextual_size=768, num_epochs=100, n_components=50)\\nctm.load(\"/content/contextualized_topic_model_nc_50_tpm_0.0_tpv_0.98_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99\",\\nepoch=19)\\nctm.get_topic_lists(5)\\n[[\\'politician\\', \\'member\\', \\'born\\', \\'party\\', \\'served\\'],\\n[\\'mi\\', \\'county\\', \\'km\\', \\'south\\', \\'east\\'],\\n[\\'church\\', \\'century\\', \\'roman\\', \\'greek\\', \\'latin\\'],', \"['mi', 'county', 'km', 'south', 'east'],\\n['church', 'century', 'roman', 'greek', 'latin'],\\n['team', 'season', 'home', 'football', 'games'],\\n['album', 'released', 'band', 'first', 'music'],\\n['school', 'university', 'college', 'education', 'public'],\\n['university', 'professor', 'author', 'research', 'work'],\\n['party', 'member', 'election', 'council', 'elections'],\\n['located', 'river', 'north', 'state', 'km'],\\n['mi', 'village', 'approximately', 'km', 'south'],\", \"['located', 'river', 'north', 'state', 'km'],\\n['mi', 'village', 'approximately', 'km', 'south'],\\n['mi', 'west', 'approximately', 'east', 'kilometres'],\\n['school', 'high', 'county', 'district', 'located'],\\n['built', 'house', 'story', 'style', 'building'],\\n['station', 'radio', 'licensed', 'fm', 'owned'],\\n['war', 'air', 'royal', 'navy', 'army'],\\n['american', 'born', 'former', 'football', 'college'],\\n['state', 'city', 'area', 'county', 'located'],\\n['station', 'railway', 'city', 'line', 'airport'],\", \"['state', 'city', 'area', 'county', 'located'],\\n['station', 'railway', 'city', 'line', 'airport'],\\n['published', 'game', 'book', 'first', 'released'],\\n['family', 'genus', 'brown', 'species', 'found'],\\n['season', 'league', 'club', 'football', 'team'],\\n['american', 'team', 'season', 'football', 'played'],\\n['war', 'world', 'ii', 'cross', 'summer'],\\n['son', 'de', 'king', 'french', 'daughter'],\\n['series', 'produced', 'television', 'film', 'directed'],\", \"['series', 'produced', 'television', 'film', 'directed'],\\n['united', 'states', 'county', 'national', 'park'],\\n['railway', 'bridge', 'river', 'island', 'station'],\\n['written', 'film', 'novel', 'directed', 'published'],\\n['born', 'played', 'made', 'english', 'first'],\\n['film', 'directed', 'produced', 'best', 'stars'],\\n['born', 'world', 'competed', 'silver', 'summer'],\\n['island', 'mountain', 'islands', 'range', 'land'],\\n['held', 'world', 'championship', 'tournament', 'cup'],\", \"['held', 'world', 'championship', 'tournament', 'cup'],\\n['system', 'data', 'systems', 'computer', 'software'],\\n['organization', 'established', 'education', 'research', 'international'],\\n['either', 'used', 'term', 'space', 'using'],\\n['album', 'released', 'band', 'rock', 'music'],\\n['family', 'species', 'found', 'mm', 'native'],\\n['government', 'established', 'political', 'responsible', 'act'],\\n['company', 'based', 'founded', 'group', 'business'],\\n['american', 'known', 'born', 'best', 'york'],\", \"['american', 'known', 'born', 'best', 'york'],\\n['village', 'town', 'england', 'parish', 'civil'],\\n['states', 'united', 'state', 'served', 'member'],\\n['also', 'district', 'population', 'persian', 'iran'],\\n['painter', 'studied', 'artist', 'work', 'composer'],\\n['television', 'radio', 'show', 'series', 'music'],\\n['played', 'professional', 'born', 'league', 'player'],\\n['often', 'chemical', 'different', 'means', 'associated'],\\n['game', 'series', 'car', 'held', 'racing'],\", \"['game', 'series', 'car', 'held', 'racing'],\\n['region', 'municipality', 'area', 'province', 'located']]\\n==================================================\\n--- 21-07 BERT 기반 한국어 복합 토픽 모델(Korean CTM) ---\\n```\\ndefaultdict(list,\\n{0: ['된다는', '인원', '충분', '고통', '분명'],\\n1: ['제작', '발표회', '스퀘어', '압구정', '족보'],\\n2: ['대통령', '유용', '불법행위', '갤러리', '재단'],\\n3: ['공시', '투데이', '머니', '리얼타임', '취득'],\\n4: ['아이템', '컬러', '재킷', '소재', '다운'],\\n5: ['수수료', '편의점', '인출', '현금', '서비스'],\\n6: ['도어', '기관사', '출입문', '스크린', '전동차'],\", \"5: ['수수료', '편의점', '인출', '현금', '서비스'],\\n6: ['도어', '기관사', '출입문', '스크린', '전동차'],\\n7: ['제작', '남자', '사랑', '압구정', '김영광'],\\n8: ['마포구', '케이스', '취하', '엔터', '강남구'],\\n9: ['환자', '수술', '권역', '응급', '외상'],\\n10: ['패션', '디자이너', '디자인', '2017', '위크'],\\n11: ['에서', '어요', '지만', '소설', '사람'],\\n12: ['캔디', '그램', '전현무', '장근석', '고성희'],\\n13: ['방송', '가이드라인', '지상파', '사업자', '유료'],\\n14: ['자유', '된다는', '크기', '고양', '평창'],\\n15: ['기권', '장관', '회고록', '남북', '안보'],\\n16: ['뉴스', '여의도', '코리아', '회견', '선언'],\\n17: ['네이버', '이사회', '의장', '한성숙', '선임'],\", \"16: ['뉴스', '여의도', '코리아', '회견', '선언'],\\n17: ['네이버', '이사회', '의장', '한성숙', '선임'],\\n18: ['디자이너', '패션', '디자인', '신진', '2017'],\\n19: ['는다는', '따로', '2008', '된다는', '통해서'],\\n20: ['습니다', '범인', '경찰', '경찰관', '사제'],\\n21: ['서울', '법원', '판결', '가정', '지법'],\\n22: ['가계', '부채', '경제', '건전', '으로'],\\n23: ['으로', '에서', '투자', '일자', '센서스'],\\n24: ['대본', '연기', '도깨비', '드라마', '모습'],\\n25: ['습니다', '으로', '때문', '공제', '는데요'],\\n26: ['국회', '병우', '국감', '운영', '수석'],\\n27: ['기온', '아침', '동해', '구름', '기상청'],\\n28: ['중구', '포토', '동아닷컴', '을지로', '열린'],\", \"27: ['기온', '아침', '동해', '구름', '기상청'],\\n28: ['중구', '포토', '동아닷컴', '을지로', '열린'],\\n29: ['뉴시스', '영상', '공감', '언론', '제보'],\\n30: ['테스트', '성능', '20', '테슬라', '소프트웨어'],\\n31: ['84', '지상', '가구', '면적', '블록'],\\n32: ['공개', '박해진', '화보', '메이크업', '촬영'],\\n33: ['으로', '에서', '기업', '한다', '경제'],\\n34: ['지검', '소환', '문체', '기소', '검찰'],\\n35: ['그룹', '이스', '데뷔', '음원', '차트'],\\n36: ['배우', '영화', '차태현', '사랑', '제작'],\\n37: ['에게', '커플', '사람', '나리', '정원'],\\n38: ['대표', '개헌', '탈당', '공화국', '지대'],\\n39: ['부터', '행사', '이벤트', '여행', '체험'],\", \"38: ['대표', '개헌', '탈당', '공화국', '지대'],\\n39: ['부터', '행사', '이벤트', '여행', '체험'],\\n40: ['모여', '세월', '2008', '바뀌', '상인'],\\n41: ['원대', '매물', '주체', '거래량', '으로'],\\n42: ['교육', '청소년', '마당', '진로', '대회'],\\n43: ['뉴시스', '영상', '공감', '언론', '사진'],\\n44: ['편입', '경력', '기업', '채용', '한전'],\\n45: ['트럼프', '토론', '대선', '공화', '후보'],\\n46: ['영상', '뉴시스', '언론', '공감', '제보'],\\n47: ['화성', '미국', '착륙', '위협', '방어'],\\n48: ['원유', '유가', '뉴욕', '오른', '연방'],\\n49: ['지역', '사업', '지원', '복지', '시흥']})\", \"48: ['원유', '유가', '뉴욕', '오른', '연방'],\\n49: ['지역', '사업', '지원', '복지', '시흥']})\\n```이번 챕터에서는 전통적인 빈도수 기반 문서 벡터화 방식인 Bag of Words와 사전 훈련된 언어 모델의 문서 임베딩 방식인 SBERT를 결합하여 사용하는 복합 토픽 모델(Combined Topic Models, CTM)에 대해서 실습해봅시다. 실습을 위해 문맥을 반영한 토픽 모델 라이브러리인 contextualized-topic-models와 LDA 시각화 라이브러리인 pyldavis, 그리고 형태소 분석기 Mecab을 설치합니다.\\npip install contextualized-topic-models==2.2.0\\npip install pyldavis\\n# Colab에 Mecab 설치\\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\", '# Colab에 Mecab 설치\\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\\n%cd Mecab-ko-for-Google-Colab\\n!bash install_mecab-ko_on_colab190912.sh']\n",
      "['시작에 앞서 문맥을 반영한 토픽 모델(Contextualized Topic Models)의 개념을 소개해봅시다. 문맥을 반영한 토픽 모델(Contextualized Topic Models)은 문맥을 반영한 BERT의 문서 임베딩의 표현력과 기존 토픽 모델의 비지도 학습 능력을 결합하여 문서에서 주제를 가져오는 토픽 모델을 말합니다. 그리고 여기서 소개할 복합 토픽 모델(Combined Topic Models, CTM)은 문맥을 반영한 토픽 모델의 일종입니다.']\n",
      "[\"복합 토픽 모델을 한국어에 적용하기 위해서는 추가적인 코드 수정이 필요합니다. 우선 CTM은 내부적으로 사이킷런의 CountVectorizer를 사용하고 있으나, CountVectorizer는 단순히 띄어쓰기 토큰화를 수행하므로 한국어에는 적절하지 않습니다. 이에 형태소 분석기 Mecab을 사용하였으며, SBERT도 한국어에 대한 문서 임베딩 값을 얻을 수 있어야 하므로 SBERT의 임베딩 또한 한국어를 포함한 다국어 BERT로 변경하였습니다.\\n상세 코드는 github에 공개하였으며 여기서는 결과만 공유하겠습니다. 별도의 불용어 처리는 해주지 않았으므로 좀 더 섬세한 결과를 얻고 싶다면 불용어도 추가해보시기 바랍니다.\\n깃허브 링크 : https://github.com/ukairia777/bert-topic-models\\nctm.get_topics(5)\\ndefaultdict(list,\\n{0: ['된다는', '인원', '충분', '고통', '분명'],\", \"ctm.get_topics(5)\\ndefaultdict(list,\\n{0: ['된다는', '인원', '충분', '고통', '분명'],\\n1: ['제작', '발표회', '스퀘어', '압구정', '족보'],\\n2: ['대통령', '유용', '불법행위', '갤러리', '재단'],\\n3: ['공시', '투데이', '머니', '리얼타임', '취득'],\\n4: ['아이템', '컬러', '재킷', '소재', '다운'],\\n5: ['수수료', '편의점', '인출', '현금', '서비스'],\\n6: ['도어', '기관사', '출입문', '스크린', '전동차'],\\n7: ['제작', '남자', '사랑', '압구정', '김영광'],\\n8: ['마포구', '케이스', '취하', '엔터', '강남구'],\\n9: ['환자', '수술', '권역', '응급', '외상'],\\n10: ['패션', '디자이너', '디자인', '2017', '위크'],\\n11: ['에서', '어요', '지만', '소설', '사람'],\", \"10: ['패션', '디자이너', '디자인', '2017', '위크'],\\n11: ['에서', '어요', '지만', '소설', '사람'],\\n12: ['캔디', '그램', '전현무', '장근석', '고성희'],\\n13: ['방송', '가이드라인', '지상파', '사업자', '유료'],\\n14: ['자유', '된다는', '크기', '고양', '평창'],\\n15: ['기권', '장관', '회고록', '남북', '안보'],\\n16: ['뉴스', '여의도', '코리아', '회견', '선언'],\\n17: ['네이버', '이사회', '의장', '한성숙', '선임'],\\n18: ['디자이너', '패션', '디자인', '신진', '2017'],\\n19: ['는다는', '따로', '2008', '된다는', '통해서'],\\n20: ['습니다', '범인', '경찰', '경찰관', '사제'],\\n21: ['서울', '법원', '판결', '가정', '지법'],\\n22: ['가계', '부채', '경제', '건전', '으로'],\", \"21: ['서울', '법원', '판결', '가정', '지법'],\\n22: ['가계', '부채', '경제', '건전', '으로'],\\n23: ['으로', '에서', '투자', '일자', '센서스'],\\n24: ['대본', '연기', '도깨비', '드라마', '모습'],\\n25: ['습니다', '으로', '때문', '공제', '는데요'],\\n26: ['국회', '병우', '국감', '운영', '수석'],\\n27: ['기온', '아침', '동해', '구름', '기상청'],\\n28: ['중구', '포토', '동아닷컴', '을지로', '열린'],\\n29: ['뉴시스', '영상', '공감', '언론', '제보'],\\n30: ['테스트', '성능', '20', '테슬라', '소프트웨어'],\\n31: ['84', '지상', '가구', '면적', '블록'],\\n32: ['공개', '박해진', '화보', '메이크업', '촬영'],\\n33: ['으로', '에서', '기업', '한다', '경제'],\", \"32: ['공개', '박해진', '화보', '메이크업', '촬영'],\\n33: ['으로', '에서', '기업', '한다', '경제'],\\n34: ['지검', '소환', '문체', '기소', '검찰'],\\n35: ['그룹', '이스', '데뷔', '음원', '차트'],\\n36: ['배우', '영화', '차태현', '사랑', '제작'],\\n37: ['에게', '커플', '사람', '나리', '정원'],\\n38: ['대표', '개헌', '탈당', '공화국', '지대'],\\n39: ['부터', '행사', '이벤트', '여행', '체험'],\\n40: ['모여', '세월', '2008', '바뀌', '상인'],\\n41: ['원대', '매물', '주체', '거래량', '으로'],\\n42: ['교육', '청소년', '마당', '진로', '대회'],\\n43: ['뉴시스', '영상', '공감', '언론', '사진'],\\n44: ['편입', '경력', '기업', '채용', '한전'],\", \"43: ['뉴시스', '영상', '공감', '언론', '사진'],\\n44: ['편입', '경력', '기업', '채용', '한전'],\\n45: ['트럼프', '토론', '대선', '공화', '후보'],\\n46: ['영상', '뉴시스', '언론', '공감', '제보'],\\n47: ['화성', '미국', '착륙', '위협', '방어'],\\n48: ['원유', '유가', '뉴욕', '오른', '연방'],\\n49: ['지역', '사업', '지원', '복지', '시흥']})\\n위에서 출력한 토픽 번호는 pyLDAvis에서 할당한 토픽 번호와 일치하지 않으므로 주의합시다.\\n가령, 48번 토픽이었던 ['원유', '유가', '뉴욕', '오른', '연방']가 아래의 PyLDAvis에서는 24번 토픽이 되었습니다.\\n[이미지: ]\\n==================================================\\n--- 21-08 버토픽(BERTopic) ---\\n```\", '[이미지: ]\\n==================================================\\n--- 21-08 버토픽(BERTopic) ---\\n```\\nmodel.save(\"my_topics_model\")\\nBerTopic_model = BERTopic.load(\"my_topics_model\")\\n```SBERT를 이용한 토픽 모델인 BERTopic은 별도 논문은 나오지 않은 모델이지만, github에서 2k 이상의 스타를 받았을만큼 굉장히 주목받고 있는 토픽 모델입니다. 개발자는 BERTopic이 LDA를 대체할 수 있을만큼의 기술이라고 확신을 얻었다고 주장합니다. 여기서는 BERT 기반의 토픽 모델링 구현체인 BERTopic의 간단한 사용 방법에 대해서 다룹니다.\\n!pip install bertopic[visualization]']\n",
      "['BERTopic은 BERT embeddings과 클래스 기반(class-based) TF-IDF를 활용하여 주제 설명에서 중요한 단어를 유지하면서도 쉽게 해석할 수 있는 조밀한 클러스터를 만드는 토픽 모델링 기술입니다. BERTopic 알고리즘은 크게 세 가지 과정을 거칩니다.\\n1) 텍스트 데이터를 SBERT로 임베딩합니다.\\nSBERT를 사용하여 문서를 임베딩합니다. 이때, BERTopic은 기본적으로 아래의 BERT들을 사용합니다.\\n\"paraphrase-MiniLM-L6-v2\" : 영어 데이터로 학습된 SBERT\\n\"paraphrase-multilingual-MiniLM-L12-v2\" : 50개 이상의 언어로 학습된 다국어 SBERT\\n2) 문서를 군집화합니다.\\nUMAP을 사용하여 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용하여 차원 축소된 임베딩을 클러스터링하고 의미적으로 유사한 문서 클러스터를 생성합니다.\\n3) 토픽 표현을 생성', 'UMAP을 사용하여 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용하여 차원 축소된 임베딩을 클러스터링하고 의미적으로 유사한 문서 클러스터를 생성합니다.\\n3) 토픽 표현을 생성\\n마지막 단계는 클래스 기반 TF-IDF로 토픽을 추출합니다.']\n",
      "[\"from bertopic import BERTopic\\nfrom sklearn.datasets import fetch_20newsgroups\\n사이킷런에서 제공하는 유명 데이터셋인 20뉴스그룹 데이터를 로드하고 상위 5개의 샘플을 출력해봅시다.\\ndocs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\\ndocs[:5]\\n지면의 한계로 여기서는 별도로 보여드리진 않았습니다. 현재 docs는 문자열의 리스트입니다. 전체 샘플의 개수를 출력해봅시다.\\nprint('총 문서의 수 :', len(docs))\\n총 문서의 수 : 18846\\n총 문서의 수는 18,846개입니다.\"]\n",
      "[\"BERTopic의 모델 객체를 만들고, fit_transform 메소드에 문자열들의 리스트를 입력으로 넣으면 토픽 모델링을 수행합니다.\\nmodel = BERTopic()\\ntopics, probabilities = model.fit_transform(docs)\\nprint('각 문서의 토픽 번호 리스트 :',len(topics))\\nprint('첫번째 문서의 토픽 번호 :', topics[0])\\n각 문서의 토픽 번호 리스트 : 18846\\n첫번째 문서의 토픽 번호 : 0\\nget_topic_info() 메소드를 사용하여 토픽의 개수, 토픽의 크기, 각 토픽에 할당된 단어들을 일부 볼 수 있습니다.\\nmodel.get_topic_info()\\n[이미지: ]\\nCount 열의 값을 모두 합하면 총 문서의 수입니다.\\nmodel.get_topic_info()['Count'].sum()\\n18846\", \"[이미지: ]\\nCount 열의 값을 모두 합하면 총 문서의 수입니다.\\nmodel.get_topic_info()['Count'].sum()\\n18846\\n위의 출력에서 Topic -1이 가장 큰 것으로 보입니다. -1은 토픽이 할당되지 않은 모든 이상치 문서(outliers)들을 나타냅니다. 현재 0번 토픽부터 210번 토픽까지 있는데, 임의로 5번 토픽에 대해서 단어들을 출력해봅시다. get_topic() 메소드의 입력으로 보고자하는 토픽의 번호를 넣어줍니다.\\nmodel.get_topic(5)\\n[('drive', 0.036501379524217024),\\n('scsi', 0.027358077330910547),\\n('drives', 0.0229861502896249),\\n('ide', 0.019274207233754368),\\n('disk', 0.01808211458113983),\\n('controller', 0.016803056719952875),\", \"('ide', 0.019274207233754368),\\n('disk', 0.01808211458113983),\\n('controller', 0.016803056719952875),\\n('hard', 0.013004806725656367),\\n('scsi2', 0.012107882273732159),\\n('bios', 0.009949766797753059),\\n('scsi1', 0.009350150086818809)]\"]\n",
      "['BERTopic을 사용하면 LDAvis와 매우 유사한 방식으로 생성된 토픽을 시각화할 수 있습니다. 시각화를 통해 생성된 토픽에 대해 더 많은 통찰력을 얻을 수 있습니다. 우선 visualize_topics() 메소드로 시각화를 진행해봅시다.\\nmodel.visualize_topics()\\n[이미지: ]']\n",
      "['Visualization_barchart() 메소드는 c-TF-IDF 점수에서 막대 차트를 만들어 각 토픽에 대해 선택된 단어들을 표시합니다. 각 토픽에 대해서 선택된 단어들을 비교할 수 있습니다.\\n[이미지: ]']\n",
      "['각 토픽들이 서로 얼마나 유사한지 시각화할 수도 있습니다. visualize_heatmap() 메소드를 사용하여 히트맵을 시각화합니다.\\n해당 히트맵의 원하는 위치에 마우스를 갖다대면 실질적인 유사도 값을 확인할 수 있습니다.\\nmodel.visualize_heatmap()\\n[이미지: ]']\n",
      "['때때로 너무 많은 토픽이 생성되거나 너무 적은 토픽이 생성될 수 있습니다. 토픽의 수를 직접 정하고 싶다면 몇 가지 방법이 존재합니다. 첫번째 방법은 모델 객체 생성 시에 nr_topics 값으로 원하는 토픽 수를 입력하여 원하는 토픽의 수를 설정할 수 있습니다. BERTopic은 유사한 토픽들을 찾아 하나의 토픽으로 병합합니다. 다음은 20개의 토픽으로 토픽의 수를 축소한 후 시각화하는 모습을 보여줍니다.\\nmodel = BERTopic(nr_topics=20)\\ntopics, probabilities = model.fit_transform(docs)\\nmodel.visualize_topics()\\n[이미지: ]\\n또 다른 방법은 모델이 자동으로 토픽의 수를 줄이도록 설정하는 것입니다. 이 옵션을 사용하려면 모델 객체 생성 시에 \"nr_topics\"의 값을 \"auto\"로 설정하면 됩니다.\\nmodel = BERTopic(nr_topics=\"auto\")', 'model = BERTopic(nr_topics=\"auto\")\\ntopics, probabilities = model.fit_transform(docs)\\nmodel.get_topic_info()\\n[이미지: ]\\n토픽의 개수를 지정하지 않았을 때는 0번 토픽부터 210번 토픽까지 총 211개의 토픽이 존재하였으나, 자동으로 토픽의 수가 줄어들도록 설정하자 토픽의 수가 0번부터 143번까지 총 144개로 줄어든 것을 확인할 수 있습니다.']\n",
      "[\"학습된 토픽 모델에 어떤 임의의 문서를 입력하여 해당 문서의 주요 토픽이 무엇인지를 예측하고 싶다면 transform()이라는 메소드를 사용합니다. 학습에 사용했던 첫번째 문서를 입력으로 하여 해당 문서의 주요 토픽 번호를 출력해봅시다.\\nnew_doc = docs[0]\\nprint(new_doc)\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\", \"to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\", \"regular season game.          PENS RULE!!!\\ntopics, probs = model.transform([new_doc])\\nprint('예측한 토픽 번호 :', topics)\\n예측한 토픽 번호 : [0]\"]\n",
      "['save()와 load() 메소드를 사용하여 모델을 저장하고 로드할 수 있습니다.\\nmodel.save(\"my_topics_model\")\\nBerTopic_model = BERTopic.load(\"my_topics_model\")\\n==================================================\\n--- 21-09 한국어 버토픽(Korean BERTopic) ---\\n```\\n!pip install bertopic[visualization]\\n# Colab에 Mecab 설치\\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\\n%cd Mecab-ko-for-Google-Colab\\n!bash install_mecab-ko_on_colab190912.sh', '%cd Mecab-ko-for-Google-Colab\\n!bash install_mecab-ko_on_colab190912.sh\\n```SBERT를 이용한 토픽 모델인 BERTopic은 github에서 2k 이상의 스타를 받았을만큼 굉장히 주목받고 있는 토픽 모델입니다. 개발자는 BERTopic이 LDA를 대체할 수 있을만큼의 기술이라고 확신을 얻었다고 주장합니다. 여기서는 BERT 기반의 토픽 모델링 구현체인 BERTopic의 간단한 사용 방법에 대해서 다룹니다.\\n!pip install bertopic[visualization]\\n# Colab에 Mecab 설치\\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\\n%cd Mecab-ko-for-Google-Colab\\n!bash install_mecab-ko_on_colab190912.sh']\n",
      "['BERTopic은 BERT embeddings과 클래스 기반(class-based) TF-IDF를 활용하여 주제 설명에서 중요한 단어를 유지하면서도 쉽게 해석할 수 있는 조밀한 클러스터를 만드는 토픽 모델링 기술입니다. BERTopic 알고리즘은 크게 세 가지 과정을 거칩니다.\\n1) 텍스트 데이터를 SBERT로 임베딩합니다.\\n이때, 이번 실습에서는 한국어 데이터를 사용하므로 SBERT로 한국어 BERT나 한국어가 포함된 다국어 BERT를 사용해야 합니다.\\n2) 문서를 군집화합니다.\\nUMAP을 사용하여 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용하여 차원 축소된 임베딩을 클러스터링하고 의미적으로 유사한 문서 클러스터를 생성합니다.\\n3) 토픽 표현을 생성', 'UMAP을 사용하여 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용하여 차원 축소된 임베딩을 클러스터링하고 의미적으로 유사한 문서 클러스터를 생성합니다.\\n3) 토픽 표현을 생성\\n마지막 단계는 클래스 기반 TF-IDF로 토픽을 추출합니다. 이때 사이킷런의 CountVectorizer를 사용하는데, CountVectorizer는 별도 토크나이저를 지정해주지 않으면 띄어쓰기 단위 토큰화를 수행합니다. 하지만, 한국어에서 띄어쓰기 단위 토큰화는 지양되므로 이번 실습에서는 형태소 분석기 Mecab을 사용합니다.']\n",
      "['복합 토픽 모델을 한국어에 적용하기 위해서는 추가적인 코드 수정이 필요합니다. 우선 BERTopic은 내부적으로 사이킷런의 CountVectorizer를 사용하고 있으나, CountVectorizer는 단순히 띄어쓰기 토큰화를 수행하므로 한국어에는 적절하지 않습니다. 이에 형태소 분석기 Mecab을 사용하였으며, SBERT도 한국어에 대한 문서 임베딩 값을 얻을 수 있어야 하므로 SBERT의 임베딩 또한 한국어를 포함한 다국어 BERT로 변경하였습니다.\\n상세 코드는 github에 공개하였으며 여기서는 결과만 공유하겠습니다. 별도의 불용어 처리는 해주지 않았으므로 좀 더 섬세한 결과를 얻고 싶다면 불용어도 추가해보시기 바랍니다.\\n깃허브 링크 : https://github.com/ukairia777/KoBERTopic\\n[이미지: ]\\n==================================================', '[이미지: ]\\n==================================================\\n--- 22. 텍스트 요약(Text Summarization) ---\\n마지막 편집일시 : 2024년 8월 26일 12:34 오전\\n==================================================\\n--- 22-01 어텐션을 이용한 텍스트 요약(Text Summarization with Attention mechanism) ---\\n```\\n원문 : great product husband eat kind price could little lower even like jerky eater\\n실제 요약문 : best jerky there is\\n예측 요약문: great jerky\\n원문 : perfect stress free afternoon aroma tea makes house smell great drink grade honey bliss', '원문 : perfect stress free afternoon aroma tea makes house smell great drink grade honey bliss\\n실제 요약문 : relax cup of tea\\n예측 요약문: great tea\\n원문 : dog loves stuff ground sprinkled dry food gobbles additives fillers carbs also use treat best price amazon quick delivery\\n실제 요약문 : great\\n예측 요약문: great dog food\\n원문 : got bbq popchips amazon promotion price came taste good wish less salty would certainly purchase came less salty version\\n실제 요약문 : tasty but wish it was less salty\\n예측 요약문 : not the best', '실제 요약문 : tasty but wish it was less salty\\n예측 요약문 : not the best\\n원문 : product arrived broken pieces flavor good actually threw garbage disappointing\\n실제 요약문 : very disappointed\\n예측 요약문: not as described\\n원문 : buying quaker oats granola bars nature valley chewy bars better tasting make great snack go chocolate peanuts raisins get better\\n실제 요약문 : my new granola bar\\n예측 요약문: great snack\\n원문 : yuck worst chocolate ever save money brand find another even taste taste like chocolate threw rest away', '실제 요약문 : horrible chocolate\\n예측 요약문 : awful\\n원문 : kit great rd kit made easy follow instructions new making wines really good kit learn product quite tasty good tropical fruit wine purchased store get idea would making like better store bought one\\n실제 요약문 : the kit is great\\n예측 요약문 : great for cooking\\n원문 : son particularly picky eater occasionally gets fussy matter mood great meal option fall back always eat also pouch sturdy makes good travel meal option little expensive worth every penny', '실제 요약문 : my son loves these\\n예측 요약문 : great for baby food\\n원문 : labeled green tea touch pomegranate raspberry essence might rated higher however name pomegranate raspberry therefore expected vary flavors overall taste mild except greater sweetness tea second ingredient chamomile distinctive presence find flavor unpleasant nothing would make reach one teas\\n실제 요약문 : the flavor was not what expected\\n예측 요약문 : not for me', '실제 요약문 : the flavor was not what expected\\n예측 요약문 : not for me\\n원문 : delighted food looks nice smells really great smaller size kibble old dog teeth comes ziplock pouch importantly though old dog getting little suddenly gotten since starting chow also noticed terrible daily gone fair time switched wet food harmony foods may also factor extremely happy food continue buy\\n실제 요약문 : highly recommend this\\n예측 요약문 : my dog loves this', '실제 요약문 : highly recommend this\\n예측 요약문 : my dog loves this\\n```텍스트 요약은 상대적으로 큰 원문을 핵심 내용만 간추려서 상대적으로 작은 요약문으로 변환하는 것을 말합니다. 읽는 사람이 시간을 단축해서 내용을 빠르게 이해할 수 있다는 점에서 글을 많이 쓰는 사람들에게는 꼭 필요한 능력 중 하나인 것 같습니다. 그런데 만약 기계가 이를 자동으로 해줄 수만 있다면 얼마나 좋을까요? 이번 실습에서는 그 중 한 가지 방법인 seq2seq를 구현해보겠습니다. 그리고 어텐션 메커니즘(attention mechanism)을 적용해봅시다.\\n이번 챕터는 시퀀스-투-시퀀스(Sequences-to-Sequence, seq2seq) 챕터를 선행하시는 것이 좋습니다. 모델을 설계하는 코드가 거의 동일합니다.']\n",
      "['텍스트 요약은 크게 추출적 요약(extractive summarization)과 추상적 요약(abstractive summarization)으로 나뉩니다.\\n1) 추출적 요약(extractive summarization)\\n추출적 요약은 원문에서 중요한 핵심 문장 또는 단어구를 몇 개 뽑아서 이들로 구성된 요약문을 만드는 방법입니다. 그렇기 때문에 추출적 요약의 결과로 나온 요약문의 문장이나 단어구들은 전부 원문에 있는 문장들입니다. 추출적 요약의 대표적인 알고리즘으로 머신 러닝 알고리즘인 텍스트랭크(TextRank)가 있는데, 아래의 링크에서 텍스트랭크로 구현된 세 줄 요약기를 시험해볼 수 있습니다.\\n링크 : https://summariz3.herokuapp.com/', '링크 : https://summariz3.herokuapp.com/\\n위 링크로 이동하여 인터넷 뉴스나 가지고 있는 글을을 복사 + 붙여넣기하여 결과를 살펴보세요. 세 개의 문장은 전부 원문에 존재하던 문장들입니다. 이 방법의 단점이라면, 이미 존재하는 문장이나 단어구로만 구성하므로 모델의 언어 표현 능력이 제한된다는 점입니다.\\n그렇다면 마치 사람처럼 원문에 없던 단어나 문장을 사용하면서 핵심만 간추려서 표현하는 요약 방법은 없을까요?\\n2) 추상적 요약(abstractive summarization)', \"그렇다면 마치 사람처럼 원문에 없던 단어나 문장을 사용하면서 핵심만 간추려서 표현하는 요약 방법은 없을까요?\\n2) 추상적 요약(abstractive summarization)\\n추상적 요약은 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법입니다. 마치 사람이 요약하는 것 같은 방식인데, 당연히 추출적 요약보다는 난이도가 높습니다. 이 방법은 주로 인공 신경망을 사용하며 대표적인 모델로 seq2seq가 있습니다. 이 방법의 단점이라면 seq2seq와 같은 인공 신경망들은 기본적으로 지도 학습이라는 점입니다. 다시 말해 추상적 요약을 인공 신경망으로 훈련하기 위해서는 '원문' 뿐만 아니라 '실제 요약문'이라는 레이블 데이터가 있어야 합니다. 그렇기 때문에 데이터를 구성하는 것 자체가 하나의 부담입니다. 이번 챕터에서는 이미 공개된 데이터를 사용해서 추상적 요약을 실습해보겠습니다.\"]\n",
      "['이번 챕터에서 사용할 데이터는 아마존 리뷰 데이터입니다. 아래의 링크에서 데이터를 다운로드합니다.\\n링크 : https://www.kaggle.com/snap/amazon-fine-food-reviews\\n우선 실습에 필요한 도구들을 임포트합니다.\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nimport matplotlib.pyplot as plt\\nfrom nltk.corpus import stopwords\\nfrom bs4 import BeautifulSoup\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nimport urllib.request\\nnp.random.seed(seed=0)\\n1) 데이터 로드하기', 'import urllib.request\\nnp.random.seed(seed=0)\\n1) 데이터 로드하기\\nReviews.csv 파일을 불러와 데이터프레임에 저장하겠습니다. 이 데이터는 실제로는 약 56만개의 샘플을 가지고 있습니다. 하지만 여기서는 간단히 10만개의 샘플만 사용하겠습니다. 이는 pd.read_csv의 nrows의 인자로 10만이라는 숫자를 적어주면 됩니다.\\n# Reviews.csv 파일을 data라는 이름의 데이터프레임에 저장. 단, 10만개의 행(rows)으로 제한.\\ndata = pd.read_csv(\"Reviews.csv 파일의 경로\", nrows = 100000)\\nprint(\\'전체 리뷰 개수 :\\',(len(data)))\\n전체 리뷰 개수 : 100000\\n전체 리뷰 개수가 10만개인 것을 확인했습니다. 5개의 샘플만 출력해봅시다.\\ndata.head()\\n지면의 한계로 생략', \"전체 리뷰 개수 : 100000\\n전체 리뷰 개수가 10만개인 것을 확인했습니다. 5개의 샘플만 출력해봅시다.\\ndata.head()\\n지면의 한계로 생략\\n5개의 샘플을 출력해보면 'Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'이라는 10개의 열이 존재함을 알 수 있습니다. 그런데 사실 이 중 필요한 열은 'Text'열과 'Summary'열 뿐입니다.\\nText열과 Summary열만을 분리하고, 다른 열들은 데이터에서 제외시켜서 재저장합니다. 그리고 5개의 샘플을 출력합니다.\\ndata = data[['Text','Summary']]\\ndata.head()\\n[이미지: ]\", \"data = data[['Text','Summary']]\\ndata.head()\\n[이미지: ]\\nText열과 Summary열만 저장된 것을 확인할 수 있습니다. Text열이 원문이고, Summary열이 Text열에 대한 요약입니다. 다시 말해 모델은 Text(원문)으로부터 Summary(요약)을 예측하도록 훈련됩니다. 랜덤으로 샘플 몇 가지를 더 출력해봅시다.\\n# 랜덤으로 10개의 샘플 출력\\ndata.sample(10)\\n[이미지: ]\\n여기서는 data.sample(10)를 한 번만 실행했지만 지속적으로 몇 차례 더 실행하면서 샘플의 구조를 확인해보세요. 원문은 꽤 긴 반면에, Summary에는 3~4개의 단어만으로 구성된 경우도 많아보입니다.\\n2) 데이터 정제하기\\n데이터에 중복 샘플이 있는지 확인해보겠습니다.\\nprint('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\", \"2) 데이터 정제하기\\n데이터에 중복 샘플이 있는지 확인해보겠습니다.\\nprint('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\\nprint('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())\\nText 열에서 중복을 배제한 유일한 샘플의 수 : 88426\\nSummary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\\n전체 데이터는 10만개의 샘플이 존재하지만, 실제로는 꽤 많은 원문이 중복되어 중복을 배제한 유일한 원문의 개수는 88,426개입니다. 중복 샘플이 무려 약 1,200개나 있다는 의미지요. Summary는 중복이 더 많지만, 원문은 다르더라도 짧은 문장인 요약은 내용이 겹칠 수 있음을 가정하고 일단 두겠습니다. Summary의 길이 분포는 뒤에서 확인하겠습니다.\\n# text 열에서 중복인 내용이 있다면 중복 제거\", '# text 열에서 중복인 내용이 있다면 중복 제거\\ndata.drop_duplicates(subset=[\\'Text\\'], inplace=True)\\nprint(\"전체 샘플수 :\", len(data))\\n전체 샘플수 : 88426\\n중복을 제거하여 88,426개의 샘플만 존재합니다. 이제 Null 샘플이 존재하는지 확인해봅시다.\\nprint(data.isnull().sum())\\nText       0\\nSummary    1\\ndtype: int64\\nSummary에서 1개의 Null 샘플이 남아있습니다. 이를 제거해줍니다.\\n# Null 값을 가진 샘플 제거\\ndata.dropna(axis=0, inplace=True)\\nprint(\\'전체 샘플수 :\\',(len(data)))\\n전체 샘플수 : 88425', '# Null 값을 가진 샘플 제거\\ndata.dropna(axis=0, inplace=True)\\nprint(\\'전체 샘플수 :\\',(len(data)))\\n전체 샘플수 : 88425\\n이제 남은 샘플 수는 88,425개입니다. 지금까지는 불필요한 샘플의 수를 줄이기 위한 정제 과정이었습니다. 이제 샘플 내부를 전처리해야 합니다. 단어 정규화와 불용어 제거를 위해 각각의 참고 자료가 필요합니다. 동일한 의미를 가졌지만 스펠링이 다른 단어들을 정규화하기 위한 사전을 만듭니다. 이 사전은 아래의 링크를 참고하여 만들어진 사전입니다.\\n링크 : https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\\n# 전처리 함수 내 사용\\ncontractions = {\"\\'cause\": \\'because\\',\\n\"I\\'d\": \\'I would\\',\\n\"I\\'d\\'ve\": \\'I would have\\',', '# 전처리 함수 내 사용\\ncontractions = {\"\\'cause\": \\'because\\',\\n\"I\\'d\": \\'I would\\',\\n\"I\\'d\\'ve\": \\'I would have\\',\\n\"I\\'ll\": \\'I will\\',\\n\"I\\'ll\\'ve\": \\'I will have\\',\\n\"I\\'m\": \\'I am\\',\\n\"I\\'ve\": \\'I have\\',\\n\"ain\\'t\": \\'is not\\',\\n\"aren\\'t\": \\'are not\\',\\n\"can\\'t\": \\'cannot\\',\\n\"could\\'ve\": \\'could have\\',\\n\"couldn\\'t\": \\'could not\\',\\n\"didn\\'t\": \\'did not\\',\\n\"doesn\\'t\": \\'does not\\',\\n\"don\\'t\": \\'do not\\',\\n\"hadn\\'t\": \\'had not\\',\\n\"hasn\\'t\": \\'has not\\',\\n\"haven\\'t\": \\'have not\\',\\n\"he\\'d\": \\'he would\\',\\n\"he\\'ll\": \\'he will\\',\\n\"he\\'s\": \\'he is\\',\\n\"here\\'s\": \\'here is\\',', '\"he\\'d\": \\'he would\\',\\n\"he\\'ll\": \\'he will\\',\\n\"he\\'s\": \\'he is\\',\\n\"here\\'s\": \\'here is\\',\\n\"how\\'d\": \\'how did\\',\\n\"how\\'d\\'y\": \\'how do you\\',\\n\"how\\'ll\": \\'how will\\',\\n\"how\\'s\": \\'how is\\',\\n\"i\\'d\": \\'i would\\',\\n\"i\\'d\\'ve\": \\'i would have\\',\\n\"i\\'ll\": \\'i will\\',\\n\"i\\'ll\\'ve\": \\'i will have\\',\\n\"i\\'m\": \\'i am\\',\\n\"i\\'ve\": \\'i have\\',\\n\"isn\\'t\": \\'is not\\',\\n\"it\\'d\": \\'it would\\',\\n\"it\\'d\\'ve\": \\'it would have\\',\\n\"it\\'ll\": \\'it will\\',\\n\"it\\'ll\\'ve\": \\'it will have\\',\\n\"it\\'s\": \\'it is\\',\\n\"let\\'s\": \\'let us\\',\\n\"ma\\'am\": \\'madam\\',\\n\"mayn\\'t\": \\'may not\\',', '\"it\\'s\": \\'it is\\',\\n\"let\\'s\": \\'let us\\',\\n\"ma\\'am\": \\'madam\\',\\n\"mayn\\'t\": \\'may not\\',\\n\"might\\'ve\": \\'might have\\',\\n\"mightn\\'t\": \\'might not\\',\\n\"mightn\\'t\\'ve\": \\'might not have\\',\\n\"must\\'ve\": \\'must have\\',\\n\"mustn\\'t\": \\'must not\\',\\n\"mustn\\'t\\'ve\": \\'must not have\\',\\n\"needn\\'t\": \\'need not\\',\\n\"needn\\'t\\'ve\": \\'need not have\\',\\n\"o\\'clock\": \\'of the clock\\',\\n\"oughtn\\'t\": \\'ought not\\',\\n\"oughtn\\'t\\'ve\": \\'ought not have\\',\\n\"sha\\'n\\'t\": \\'shall not\\',\\n\"shan\\'t\": \\'shall not\\',\\n\"shan\\'t\\'ve\": \\'shall not have\\',\\n\"she\\'d\": \\'she would\\',', '\"sha\\'n\\'t\": \\'shall not\\',\\n\"shan\\'t\": \\'shall not\\',\\n\"shan\\'t\\'ve\": \\'shall not have\\',\\n\"she\\'d\": \\'she would\\',\\n\"she\\'d\\'ve\": \\'she would have\\',\\n\"she\\'ll\": \\'she will\\',\\n\"she\\'ll\\'ve\": \\'she will have\\',\\n\"she\\'s\": \\'she is\\',\\n\"should\\'ve\": \\'should have\\',\\n\"shouldn\\'t\": \\'should not\\',\\n\"shouldn\\'t\\'ve\": \\'should not have\\',\\n\"so\\'s\": \\'so as\\',\\n\"so\\'ve\": \\'so have\\',\\n\"that\\'d\": \\'that would\\',\\n\"that\\'d\\'ve\": \\'that would have\\',\\n\"that\\'s\": \\'that is\\',\\n\"there\\'d\": \\'there would\\',\\n\"there\\'d\\'ve\": \\'there would have\\',\\n\"there\\'s\": \\'there is\\',', '\"there\\'d\": \\'there would\\',\\n\"there\\'d\\'ve\": \\'there would have\\',\\n\"there\\'s\": \\'there is\\',\\n\"they\\'d\": \\'they would\\',\\n\"they\\'d\\'ve\": \\'they would have\\',\\n\"they\\'ll\": \\'they will\\',\\n\"they\\'ll\\'ve\": \\'they will have\\',\\n\"they\\'re\": \\'they are\\',\\n\"they\\'ve\": \\'they have\\',\\n\"this\\'s\": \\'this is\\',\\n\"to\\'ve\": \\'to have\\',\\n\"wasn\\'t\": \\'was not\\',\\n\"we\\'d\": \\'we would\\',\\n\"we\\'d\\'ve\": \\'we would have\\',\\n\"we\\'ll\": \\'we will\\',\\n\"we\\'ll\\'ve\": \\'we will have\\',\\n\"we\\'re\": \\'we are\\',\\n\"we\\'ve\": \\'we have\\',\\n\"weren\\'t\": \\'were not\\',\\n\"what\\'ll\": \\'what will\\',', '\"we\\'re\": \\'we are\\',\\n\"we\\'ve\": \\'we have\\',\\n\"weren\\'t\": \\'were not\\',\\n\"what\\'ll\": \\'what will\\',\\n\"what\\'ll\\'ve\": \\'what will have\\',\\n\"what\\'re\": \\'what are\\',\\n\"what\\'s\": \\'what is\\',\\n\"what\\'ve\": \\'what have\\',\\n\"when\\'s\": \\'when is\\',\\n\"when\\'ve\": \\'when have\\',\\n\"where\\'d\": \\'where did\\',\\n\"where\\'s\": \\'where is\\',\\n\"where\\'ve\": \\'where have\\',\\n\"who\\'ll\": \\'who will\\',\\n\"who\\'ll\\'ve\": \\'who will have\\',\\n\"who\\'s\": \\'who is\\',\\n\"who\\'ve\": \\'who have\\',\\n\"why\\'s\": \\'why is\\',\\n\"why\\'ve\": \\'why have\\',\\n\"will\\'ve\": \\'will have\\',\\n\"won\\'t\": \\'will not\\',', '\"why\\'s\": \\'why is\\',\\n\"why\\'ve\": \\'why have\\',\\n\"will\\'ve\": \\'will have\\',\\n\"won\\'t\": \\'will not\\',\\n\"won\\'t\\'ve\": \\'will not have\\',\\n\"would\\'ve\": \\'would have\\',\\n\"wouldn\\'t\": \\'would not\\',\\n\"wouldn\\'t\\'ve\": \\'would not have\\',\\n\"y\\'all\": \\'you all\\',\\n\"y\\'all\\'d\": \\'you all would\\',\\n\"y\\'all\\'d\\'ve\": \\'you all would have\\',\\n\"y\\'all\\'re\": \\'you all are\\',\\n\"y\\'all\\'ve\": \\'you all have\\',\\n\"you\\'d\": \\'you would\\',\\n\"you\\'d\\'ve\": \\'you would have\\',\\n\"you\\'ll\": \\'you will\\',\\n\"you\\'ll\\'ve\": \\'you will have\\',\\n\"you\\'re\": \\'you are\\',\\n\"you\\'ve\": \\'you have\\'}', '\"you\\'ll\": \\'you will\\',\\n\"you\\'ll\\'ve\": \\'you will have\\',\\n\"you\\'re\": \\'you are\\',\\n\"you\\'ve\": \\'you have\\'}\\nNLTK의 불용어를 저장하고 개수를 확인해봅시다.\\n# NLTK의 불용어\\nstop_words = set(stopwords.words(\\'english\\'))\\nprint(\\'불용어 개수 :\\', len(stop_words))\\nprint(stop_words)\\n불용어 개수 : 179\\n{\\'this\\', \"doesn\\'t\", \\'until\\', \\'as\\', ... 중략 ... ,\\'whom\\', \\'here\\', \\'ma\\', \"it\\'s\", \\'am\\', \\'your\\'}\\n전처리 함수를 설계합니다.\\n# 전처리 함수\\ndef preprocess_sentence(sentence, remove_stopwords = True):\\nsentence = sentence.lower() # 텍스트 소문자화', 'def preprocess_sentence(sentence, remove_stopwords = True):\\nsentence = sentence.lower() # 텍스트 소문자화\\nsentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\\nsentence = re.sub(r\\'\\\\([^)]*\\\\)\\', \\'\\', sentence) # 괄호로 닫힌 문자열  제거 Ex) my husband (and myself) for => my husband for\\nsentence = re.sub(\\'\"\\',\\'\\', sentence) # 쌍따옴표 \" 제거\\nsentence = \\' \\'.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화', 'sentence = re.sub(r\"\\'s\\\\b\",\"\",sentence) # 소유격 제거. Ex) roland\\'s -> roland\\nsentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\\nsentence = re.sub(\\'[m]{2,}\\', \\'mm\\', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\\n# 불용어 제거 (Text)\\nif remove_stopwords:\\ntokens = \\' \\'.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\\n# 불용어 미제거 (Summary)\\nelse:\\ntokens = \\' \\'.join(word for word in sentence.split() if len(word) > 1)\\nreturn tokens', \"else:\\ntokens = ' '.join(word for word in sentence.split() if len(word) > 1)\\nreturn tokens\\n함수 내부의 각 줄에 주석을 달았으므로 자세한 설명은 생략하겠습니다. 여기서는 Text 열에서는 불용어를 제거하고, Summary 열에서는 불용어를 제거하지 않기로 결정했습니다. Summary를 입력으로 할 때는 두번째 인자를 0으로 줘서 불용어를 제거하지 않는 버전을 실행하겠습니다. 임의의 Text 문장과 Summary 문장을 만들어 전처리 함수를 통한 전처리 후의 결과를 확인해보겠습니다.\\ntemp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\\ntemp_summary = 'Great way to start (or finish) the day!!!'\", \"temp_summary = 'Great way to start (or finish) the day!!!'\\nprint(preprocess_sentence(temp_text))\\nprint(preprocess_sentence(temp_summary, 0))\\neverything bought great infact ordered twice third ordered wasfor mother father\\ngreat way to start the day\\n우선 Text 열에 대해서 전처리를 수행하겠습니다. 전처리 후에는 5개의 전처리 된 샘플을 출력합니다.\\n# Text 열 전처리\\nclean_text = []\\nfor s in data['Text']:\\nclean_text.append(preprocess_sentence(s))\\nclean_text[:5]\", \"clean_text = []\\nfor s in data['Text']:\\nclean_text.append(preprocess_sentence(s))\\nclean_text[:5]\\n['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\\n'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\", \"'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\\n'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\\n'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']\", \"'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']\\n이제 Summary 열에 대해서 전처리를 수행하겠습니다. 전처리 후에는 5개의 전처리 된 샘플을 출력합니다.\\n# Summary 열 전처리\\nclean_summary = []\\nfor s in data['Summary']:\\nclean_summary.append(preprocess_sentence(s, 0))\\nclean_summary[:5]\\n['good quality dog food',\\n'not as advertised',\\n'delight says it all',\\n'cough medicine',\\n'great taffy']\\n전처리 후의 결과를 다시 데이터프레임에 저장합니다.\\ndata['Text'] = clean_text\\ndata['Summary'] = clean_summary\", \"전처리 후의 결과를 다시 데이터프레임에 저장합니다.\\ndata['Text'] = clean_text\\ndata['Summary'] = clean_summary\\n혹시 전처리 과정에서 빈 값이 생겼다면 Null 값으로 변경한 후에 Null 값을 가진 샘플이 생겼는지 확인합니다.\\n# 길이가 공백인 샘플은 NULL 값으로 변환\\ndata.replace('', np.nan, inplace=True)\\nprint(data.isnull().sum())\\nText        0\\nSummary    70\\ndtype: int64\\nSummary 열에서 70개의 샘플이 Null 값을 가집니다. 이 샘플들을 제거해주고, 전체 샘플수를 확인합니다.\\ndata.dropna(axis = 0, inplace = True)\\nprint('전체 샘플수 :',(len(data)))\\n전체 샘플수 : 88355\\n이제 Text 열과 Summary 열에 대해서 길이 분포를 확인해보겠습니다.\\n# 길이 분포 출력\", \"print('전체 샘플수 :',(len(data)))\\n전체 샘플수 : 88355\\n이제 Text 열과 Summary 열에 대해서 길이 분포를 확인해보겠습니다.\\n# 길이 분포 출력\\ntext_len = [len(s.split()) for s in data['Text']]\\nsummary_len = [len(s.split()) for s in data['Summary']]\\nprint('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\\nprint('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\\nprint('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\\nprint('요약의 최소 길이 : {}'.format(np.min(summary_len)))\\nprint('요약의 최대 길이 : {}'.format(np.max(summary_len)))\", \"print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\\nprint('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\\nplt.subplot(1,2,1)\\nplt.boxplot(summary_len)\\nplt.title('Summary')\\nplt.subplot(1,2,2)\\nplt.boxplot(text_len)\\nplt.title('Text')\\nplt.tight_layout()\\nplt.show()\\nplt.title('Summary')\\nplt.hist(summary_len, bins=40)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\nplt.title('Text')\\nplt.hist(text_len, bins=40)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\", \"plt.hist(text_len, bins=40)\\nplt.xlabel('length of samples')\\nplt.ylabel('number of samples')\\nplt.show()\\n텍스트의 최소 길이 : 2\\n텍스트의 최대 길이 : 1235\\n텍스트의 평균 길이 : 38.792428272310566\\n요약의 최소 길이 : 1\\n요약의 최대 길이 : 28\\n요약의 평균 길이 : 4.010729443721352\\n[이미지: ]\\n원문 텍스트는 대체적으로 100이하의 길이를 가집니다. 또한, 평균 길이는 38입니다. 요약의 경우에는 대체적으로 15이하의 길이를 가지며 평균 길이는 4입니다. 여기서 패딩의 길이를 정하겠습니다. 평균 길이보다는 크게 잡아 각각 50과 8로 결정합니다.\\ntext_max_len = 50\\nsummary_max_len = 8\\n50과 8이라는 이 두 길이가 얼마나 많은 샘플들의 길이보다 큰지 확인해보겠습니다.\", \"text_max_len = 50\\nsummary_max_len = 8\\n50과 8이라는 이 두 길이가 얼마나 많은 샘플들의 길이보다 큰지 확인해보겠습니다.\\ndef below_threshold_len(max_len, nested_list):\\ncnt = 0\\nfor s in nested_list:\\nif(len(s.split()) <= max_len):\\ncnt = cnt + 1\\nprint('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\\n우선 Text열에 대해서 확인해봅시다.\\nbelow_threshold_len(text_max_len, data['Text'])\\n전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\\nText 열은 길이가 50 이하인 비율이 77%입니다. 약 23%의 샘플이 길이 50보다 큽니다. Summary 열에 대해서 확인해봅시다.\", \"Text 열은 길이가 50 이하인 비율이 77%입니다. 약 23%의 샘플이 길이 50보다 큽니다. Summary 열에 대해서 확인해봅시다.\\nbelow_threshold_len(summary_max_len, data['Summary'])\\n전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\\nSummary 열은 길이가 8 이하인 경우가 94%입니다. 여기서는 정해준 최대 길이보다 큰 샘플들은 제거하겠습니다.\\ndata = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\\ndata = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\\nprint('전체 샘플수 :',(len(data)))\\n전체 샘플수 : 65818\\n이제 샘플수가 65,818개로 줄었습니다. 정제 작업이 완료된 상위 샘플 5개를 출력해봅시다.\", \"print('전체 샘플수 :',(len(data)))\\n전체 샘플수 : 65818\\n이제 샘플수가 65,818개로 줄었습니다. 정제 작업이 완료된 상위 샘플 5개를 출력해봅시다.\\ndata.head()\\n[이미지: ]\\nseq2seq 훈련을 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가할 필요가 있습니다. 시작 토큰은 'sostoken', 종료 토큰은 'eostoken'이라 명명하고 앞, 뒤로 추가하겠습니다.\\n# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\\ndata['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\\ndata['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\\ndata.head()\\n[이미지: ]\\n인코더의 입력, 디코더의 입력과 레이블을 각각 저장해줍니다.\", \"data.head()\\n[이미지: ]\\n인코더의 입력, 디코더의 입력과 레이블을 각각 저장해줍니다.\\nencoder_input = np.array(data['Text'])\\ndecoder_input = np.array(data['decoder_input'])\\ndecoder_target = np.array(data['decoder_target'])\\n3) 데이터의 분리\\n훈련 데이터와 테스트 데이터를 분리해봅시다. 우선, 순서가 섞인 정수 시퀀스를 만들어줍니다.\\nindices = np.arange(encoder_input.shape[0])\\nnp.random.shuffle(indices)\\nprint(indices)\\n[29546 43316 24839 ... 45891 42613 43567]\\n이 정수 시퀀스 순서를 데이터의 샘플 순서로 정의해주면 샘플의 순서는 섞이게 됩니다.\\nencoder_input = encoder_input[indices]\", \"이 정수 시퀀스 순서를 데이터의 샘플 순서로 정의해주면 샘플의 순서는 섞이게 됩니다.\\nencoder_input = encoder_input[indices]\\ndecoder_input = decoder_input[indices]\\ndecoder_target = decoder_target[indices]\\n이제 섞인 데이터를 8:2의 비율로 훈련 데이터와 테스트 데이터로 분리해주겠습니다.\\nn_of_val = int(len(encoder_input)*0.2)\\nprint('테스트 데이터의 수 :',n_of_val)\\n테스트 데이터의 수 : 13163\\n테스트 데이터는 전체 데이터에서 20%에 해당하는 13,163개를 사용하겠습니다.\\nencoder_input_train = encoder_input[:-n_of_val]\\ndecoder_input_train = decoder_input[:-n_of_val]\\ndecoder_target_train = decoder_target[:-n_of_val]\", \"decoder_input_train = decoder_input[:-n_of_val]\\ndecoder_target_train = decoder_target[:-n_of_val]\\nencoder_input_test = encoder_input[-n_of_val:]\\ndecoder_input_test = decoder_input[-n_of_val:]\\ndecoder_target_test = decoder_target[-n_of_val:]\\nprint('훈련 데이터의 개수 :', len(encoder_input_train))\\nprint('훈련 레이블의 개수 :',len(decoder_input_train))\\nprint('테스트 데이터의 개수 :',len(encoder_input_test))\\nprint('테스트 레이블의 개수 :',len(decoder_input_test))\\n훈련 데이터의 개수 : 52655\\n훈련 레이블의 개수 : 52655\\n테스트 데이터의 개수 : 13163\", '훈련 데이터의 개수 : 52655\\n훈련 레이블의 개수 : 52655\\n테스트 데이터의 개수 : 13163\\n테스트 레이블의 개수 : 13163\\n4) 정수 인코딩\\n이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다. 우선, 원문에 해당되는 encoder_input_train에 대해서 수행합니다.\\nsrc_tokenizer = Tokenizer()\\nsrc_tokenizer.fit_on_texts(encoder_input_train)\\n이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. 이는 src_tokenizer.word_index에 저장되어져 있습니다. 여기서는 빈도수가 낮은 단어들은 자연어 처리에서 배제하고자 합니다. 등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다.\\nthreshold = 7', \"threshold = 7\\ntotal_cnt = len(src_tokenizer.word_index) # 단어의 수\\nrare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\\ntotal_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\\nrare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\\n# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\\nfor key, value in src_tokenizer.word_counts.items():\\ntotal_freq = total_freq + value\\n# 단어의 등장 빈도수가 threshold보다 작으면\\nif(value < threshold):\\nrare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint('단어 집합(vocabulary)의 크기 :',total_cnt)\", 'rare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint(\\'단어 집합(vocabulary)의 크기 :\\',total_cnt)\\nprint(\\'등장 빈도가 %s번 이하인 희귀 단어의 수: %s\\'%(threshold - 1, rare_cnt))\\nprint(\\'단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s\\'%(total_cnt - rare_cnt))\\nprint(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\\nprint(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\\n단어 집합(vocabulary)의 크기 : 32031\\n등장 빈도가 6번 이하인 희귀 단어의 수: 23779\\n단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8252\\n단어 집합에서 희귀 단어의 비율: 74.23745746308263', '단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8252\\n단어 집합에서 희귀 단어의 비율: 74.23745746308263\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.393443023084609\\n등장 빈도가 threshold 값인 7회 미만. 즉, 6회 이하인 단어들은 단어 집합에서 무려 70% 이상을 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%밖에 되지 않습니다. 여기서는 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 배제시키고자 합니다. 위에서 이를 제외한 단어 집합의 크기를 8,233으로 계산했는데, 저자는 깔끔한 값을 선호하여 이와 비슷한 값으로 단어 집합의 크기를 8000으로 제한하겠습니다.\\nsrc_vocab = 8000\\nsrc_tokenizer = Tokenizer(num_words = src_vocab)', 'src_vocab = 8000\\nsrc_tokenizer = Tokenizer(num_words = src_vocab)\\nsrc_tokenizer.fit_on_texts(encoder_input_train)\\n# 텍스트 시퀀스를 정수 시퀀스로 변환\\nencoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\\nencoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\\n정수 인코딩이 정상 진행되었는지 훈련 데이터에 대해서 3개의 샘플을 출력해봅시다.\\nprint(encoder_input_train[:3])', '정수 인코딩이 정상 진행되었는지 훈련 데이터에 대해서 3개의 샘플을 출력해봅시다.\\nprint(encoder_input_train[:3])\\n[[1882, 805, 844, 1855, 1120, 72, 131, 203, 1120, 83, 3896, 1, 1013, 844, 757, 167, 601, 350, 519, 435, 2482, 626, 72, 302, 1120, 132, 4281, 1007, 102, 449, 3450, 1, 75, 90, 343, 2307, 1188, 114, 1639, 166, 431, 1333, 1847, 70], [53, 4, 15, 901, 355, 37, 784, 97, 9, 8, 217, 441, 129, 101], [40, 1261, 473, 3, 909, 39, 3249, 2978, 221, 24, 37, 287, 2719, 6125, 56, 1371, 83, 390, 378]]\\n이제 레이블에 해당하는 요약 데이터에 대해서도 수행하겠습니다.', '이제 레이블에 해당하는 요약 데이터에 대해서도 수행하겠습니다.\\ntar_tokenizer = Tokenizer()\\ntar_tokenizer.fit_on_texts(decoder_input_train)\\n이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. 이는 tar_tokenizer.word_index에 저장되어져 있습니다. 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다.\\nthreshold = 6\\ntotal_cnt = len(tar_tokenizer.word_index) # 단어의 수\\nrare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\\ntotal_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\\nrare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\\n# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.', \"rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\\n# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\\nfor key, value in tar_tokenizer.word_counts.items():\\ntotal_freq = total_freq + value\\n# 단어의 등장 빈도수가 threshold보다 작으면\\nif(value < threshold):\\nrare_cnt = rare_cnt + 1\\nrare_freq = rare_freq + value\\nprint('단어 집합(vocabulary)의 크기 :',total_cnt)\\nprint('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\\nprint('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\", 'print(\\'단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s\\'%(total_cnt - rare_cnt))\\nprint(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\\nprint(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\\n단어 집합(vocabulary)의 크기 : 10510\\n등장 빈도가 5번 이하인 희귀 단어의 수: 8128\\n단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2382\\n단어 집합에서 희귀 단어의 비율: 77.33587059942911\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.896286343062141', '단어 집합에서 희귀 단어의 비율: 77.33587059942911\\n전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.896286343062141\\n등장 빈도가 5회 이하인 단어들은 단어 집합에서 약 77%를 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 5.89%밖에 되지 않습니다. 이 단어들은 정수 인코딩 과정에서 배제시키겠습니다.\\ntar_vocab = 2000\\ntar_tokenizer = Tokenizer(num_words = tar_vocab)\\ntar_tokenizer.fit_on_texts(decoder_input_train)\\ntar_tokenizer.fit_on_texts(decoder_target_train)\\n# 텍스트 시퀀스를 정수 시퀀스로 변환\\ndecoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)', '# 텍스트 시퀀스를 정수 시퀀스로 변환\\ndecoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\\ndecoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\\ndecoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\\ndecoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\\n정수 인코딩이 정상 진행되었는지 훈련 데이터에 대해서 5개의 샘플을 출력해봅시다.\\nprint(decoder_input_train[:5])', '정수 인코딩이 정상 진행되었는지 훈련 데이터에 대해서 5개의 샘플을 출력해봅시다.\\nprint(decoder_input_train[:5])\\n[[1, 687], [1, 53, 21, 182, 1162, 240], [1, 6, 480, 113, 278, 181], [1, 15, 108, 215], [1, 54, 178, 21]]\\nprint(decoder_target_train[:5])\\n[[687, 2], [53, 21, 182, 1162, 240, 2], [6, 480, 113, 278, 181, 2], [15, 108, 215, 2], [54, 178, 21, 2]]\\n5) 빈 샘플(empty samples) 제거', '전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었다는 것을 의미합니다. 이 현상은 길이가 상대적으로 길었던 원문(Text)의 경우에는 문제가 별로 없겠지만, 애초에 평균 길이가 4밖에 되지 않았던 요약문(Summary)의 경우에는 이 현상이 굉장히 두드러졌을 가능성이 높습니다. 요약문에서 길이가 0이 된 샘플들의 인덱스를 받아옵시다. 주의할 점은 요약문인 decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플수와 동일하여 단어 집합 제한에도 삭제 되지 않습니다. 그래서 이제 길이가 0이 된 요약문의 실질적 길이는 1입니다. decoder_input에는 sostoken, decoder_target에는 eostoken만 남았을 것이기 때문입니다.', \"drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\\ndrop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\\n훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 각각 drop_train과 drop_test에 저장하였습니다. 이 샘플들을 모두 삭제하고자 합니다. 삭제할 개수는 각각 몇개일까요?\\nprint('삭제할 훈련 데이터의 개수 :',len(drop_train))\\nprint('삭제할 테스트 데이터의 개수 :',len(drop_test))\\n삭제할 훈련 데이터의 개수 : 1235\\n삭제할 테스트 데이터의 개수 : 337\\n삭제 후의 개수는 다음과 같습니다.\", '삭제할 훈련 데이터의 개수 : 1235\\n삭제할 테스트 데이터의 개수 : 337\\n삭제 후의 개수는 다음과 같습니다.\\nencoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\\ndecoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\\ndecoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\\nencoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\\ndecoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\\ndecoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)', \"decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\\nprint('훈련 데이터의 개수 :', len(encoder_input_train))\\nprint('훈련 레이블의 개수 :',len(decoder_input_train))\\nprint('테스트 데이터의 개수 :',len(encoder_input_test))\\nprint('테스트 레이블의 개수 :',len(decoder_input_test))\\n훈련 데이터의 개수 : 51420\\n훈련 레이블의 개수 : 51420\\n테스트 데이터의 개수 : 12826\\n테스트 레이블의 개수 : 12826\\n6) 패딩하기\\n앞서 계산해둔 최대 길이로 맞추어 훈련 데이터와 테스트 데이터에 대해서 패딩 작업을 수행합니다.\\nencoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\", \"encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\\nencoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\\ndecoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\\ndecoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\\ndecoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\", \"decoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\\ndecoder_target_test = pad_sequences(decoder_target_test, maxlen = summary_max_len, padding='post')\"]\n",
      "['우선 필요한 도구들을 임포트합니다.\\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\n인코더를 설계해보겠습니다. 인코더는 LSTM 층을 3개 쌓습니다.\\nembedding_dim = 128\\nhidden_size = 256\\n# 인코더\\nencoder_inputs = Input(shape=(text_max_len,))\\n# 인코더의 임베딩 층\\nenc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\\n# 인코더의 LSTM 1', '# 인코더의 임베딩 층\\nenc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\\n# 인코더의 LSTM 1\\nencoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\\n# 인코더의 LSTM 2\\nencoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\\n# 인코더의 LSTM 3', 'encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\\n# 인코더의 LSTM 3\\nencoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\\n디코더를 설계해보겠습니다. 단, 출력층은 제외하고 설계하겠습니다. 디코더의 설계는 인코더와 사실상 동일하지만 초기 상태(initial_state)를 인코더의 상태로 주어야 하는 것에 주의합시다.\\n# 디코더\\ndecoder_inputs = Input(shape=(None,))\\n# 디코더의 임베딩 층\\ndec_emb_layer = Embedding(tar_vocab, embedding_dim)', \"# 디코더의 임베딩 층\\ndec_emb_layer = Embedding(tar_vocab, embedding_dim)\\ndec_emb = dec_emb_layer(decoder_inputs)\\n# 디코더의 LSTM\\ndecoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\\ndecoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\\n이제 디코더의 출력층을 설계합니다.\\n# 디코더의 출력층\\ndecoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\\ndecoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)\\n# 모델 정의\", 'decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)\\n# 모델 정의\\nmodel = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\\nmodel.summary()\\nmodel.summary() 결과는 지면의 한계로 여기에 올리지 않겠습니다. 총 3,633,104개의 매개변수를 가진 seq2seq 모델이 설계됩니다. 지금까지의 모델 설계는 앞서 seq2seq 챕터에서 배웠던 내용과 동일합니다.', '그런데 이번 챕터에서는 어텐션 메커니즘을 사용할 예정이므로 위에서 설계한 출력층을 사용하지 않고, 어텐션 메커니즘이 결합된 새로운 출력층을 설계해보겠습니다. 어텐션 함수를 직접 작성하지 않고 이미 저자의 깃허브에 작성된 어텐션을 사용할 것이므로 아래의 코드를 통해 attention.py 파일을 다운로드하고, AttentionLayer를 임포트합니다. (바다나우 어텐션입니다.)\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/20.%20Text%20Summarization%20with%20Attention/attention.py\", filename=\"attention.py\")\\nfrom attention import AttentionLayer\\n어텐션 메커니즘을 이용해 디코더의 출력층을 새롭게 설계합니다.\\n# 어텐션 층(어텐션 함수)', \"from attention import AttentionLayer\\n어텐션 메커니즘을 이용해 디코더의 출력층을 새롭게 설계합니다.\\n# 어텐션 층(어텐션 함수)\\nattn_layer = AttentionLayer(name='attention_layer')\\nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\\n# 어텐션의 결과와 디코더의 hidden state들을 연결\\ndecoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\\n# 디코더의 출력층\\ndecoder_softmax_layer = Dense(tar_vocab, activation='softmax')\\ndecoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\\n# 모델 정의\", \"decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\\n# 모델 정의\\nmodel = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\\nmodel.summary()\\n총 4,276,432개의 파라미터를 가진 모델이 설계됩니다. 이제 모델을 컴파일합니다.\\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\\n조기 종료 조건을 설정하고 모델을 학습시킵니다.\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\\nhistory = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\\\", 'history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\\\\nvalidation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\\nbatch_size = 256, callbacks=[es], epochs = 50)\\nTrain on 51420 samples, validate on 12826 samples\\nEpoch 1/50\\n51404/51404 [==============================] - 79s 2ms/sample - loss: 3.0293 - val_loss: 2.7390\\n... 중략 ...\\nEpoch 26/50', \"... 중략 ...\\nEpoch 26/50\\n51404/51404 [==============================] - 67s 1ms/sample - loss: 1.7364 - val_loss: 2.0805\\nEpoch 00026: early stopping\\n학습 과정하면서 기록된 훈련 데이터의 손실과 테스트 데이터의 손실 히스토리를 시각화하여 출력합니다\\nplt.plot(history.history['loss'], label='train')\\nplt.plot(history.history['val_loss'], label='test')\\nplt.legend()\\nplt.show()\\n[이미지: ]\\n테스트 데이터의 손실이 지속적으로 줄어들다가 어느 순간부터 정체하게 됩니다.\"]\n",
      "['테스트를 위해 필요한 3개의 사전을 만듭니다.\\nsrc_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\\ntar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\\ntar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\\nseq2seq는 훈련 단계와 테스트 단계의 동작이 다르므로 테스트 단계의 모델을 별도로 다시 설계해줄 필요가 있습니다. 다시 새로운 seq2seq 모델을 만들겠습니다. 우선 인코더를 정의합니다.\\n# 인코더 설계\\nencoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\\n이제 테스트 단계의 디코더를 설계합니다.\\n# 이전 시점의 상태들을 저장하는 텐서', '이제 테스트 단계의 디코더를 설계합니다.\\n# 이전 시점의 상태들을 저장하는 텐서\\ndecoder_state_input_h = Input(shape=(hidden_size,))\\ndecoder_state_input_c = Input(shape=(hidden_size,))\\ndec_emb2 = dec_emb_layer(decoder_inputs)\\n# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\\n# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\\n# 어텐션 함수', \"# 어텐션 함수\\ndecoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\\n# 디코더의 출력층\\ndecoder_outputs2 = decoder_softmax_layer(decoder_inf_concat)\\n# 최종 디코더 모델\\ndecoder_model = Model(\\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\", \"[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\\n[decoder_outputs2] + [state_h2, state_c2])\\n테스트 단계를 위한 모델이 완성되었습니다. 테스트를 위해 사용되는 함수 decode_sequence를 설계합니다.\\ndef decode_sequence(input_seq):\\n# 입력으로부터 인코더의 상태를 얻음\\ne_out, e_h, e_c = encoder_model.predict(input_seq)\\n# <SOS>에 해당하는 토큰 생성\\ntarget_seq = np.zeros((1,1))\\ntarget_seq[0, 0] = tar_word_to_index['sostoken']\\nstop_condition = False\\ndecoded_sentence = ''\", \"target_seq[0, 0] = tar_word_to_index['sostoken']\\nstop_condition = False\\ndecoded_sentence = ''\\nwhile not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\\noutput_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\\nsampled_token_index = np.argmax(output_tokens[0, -1, :])\\nsampled_token = tar_index_to_word[sampled_token_index]\\nif(sampled_token!='eostoken'):\\ndecoded_sentence += ' '+sampled_token\\n#  <eos>에 도달하거나 최대 길이를 넘으면 중단.\", \"if(sampled_token!='eostoken'):\\ndecoded_sentence += ' '+sampled_token\\n#  <eos>에 도달하거나 최대 길이를 넘으면 중단.\\nif (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\\nstop_condition = True\\n# 길이가 1인 타겟 시퀀스를 업데이트\\ntarget_seq = np.zeros((1,1))\\ntarget_seq[0, 0] = sampled_token_index\\n# 상태를 업데이트 합니다.\\ne_h, e_c = h, c\\nreturn decoded_sentence\\n테스트 단계에서 원문과 실제 요약문, 예측 요약문을 비교하기 위해 정수 시퀀스를 텍스트 시퀀스로 만드는 함수를 설계합니다.\\n# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\\ndef seq2text(input_seq):\\nsentence=''\", \"# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\\ndef seq2text(input_seq):\\nsentence=''\\nfor i in input_seq:\\nif(i!=0):\\nsentence = sentence + src_index_to_word[i]+' '\\nreturn sentence\\n# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\\ndef seq2summary(input_seq):\\nsentence=''\\nfor i in input_seq:\\nif((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\\nsentence = sentence + tar_index_to_word[i] + ' '\\nreturn sentence\\n테스트 샘플 중 500번부터 1000번까지 테스트해봅시다.\\nfor i in range(500, 1000):\", 'return sentence\\n테스트 샘플 중 500번부터 1000번까지 테스트해봅시다.\\nfor i in range(500, 1000):\\nprint(\"원문 : \",seq2text(encoder_input_test[i]))\\nprint(\"실제 요약문 :\",seq2summary(decoder_input_test[i]))\\nprint(\"예측 요약문 :\",decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\\nprint(\"\\\\n\")\\n출력되는 결과가 너무 많으므로 그 중 몇가지만 보겠습니다.\\n원문 : great product husband eat kind price could little lower even like jerky eater\\n실제 요약문 : best jerky there is\\n예측 요약문: great jerky', '실제 요약문 : best jerky there is\\n예측 요약문: great jerky\\n원문 : perfect stress free afternoon aroma tea makes house smell great drink grade honey bliss\\n실제 요약문 : relax cup of tea\\n예측 요약문: great tea\\n원문 : dog loves stuff ground sprinkled dry food gobbles additives fillers carbs also use treat best price amazon quick delivery\\n실제 요약문 : great\\n예측 요약문: great dog food\\n원문 : got bbq popchips amazon promotion price came taste good wish less salty would certainly purchase came less salty version', '실제 요약문 : tasty but wish it was less salty\\n예측 요약문 : not the best\\n원문 : product arrived broken pieces flavor good actually threw garbage disappointing\\n실제 요약문 : very disappointed\\n예측 요약문: not as described\\n원문 : buying quaker oats granola bars nature valley chewy bars better tasting make great snack go chocolate peanuts raisins get better\\n실제 요약문 : my new granola bar\\n예측 요약문: great snack\\n원문 : yuck worst chocolate ever save money brand find another even taste taste like chocolate threw rest away', '실제 요약문 : horrible chocolate\\n예측 요약문 : awful\\n원문 : kit great rd kit made easy follow instructions new making wines really good kit learn product quite tasty good tropical fruit wine purchased store get idea would making like better store bought one\\n실제 요약문 : the kit is great\\n예측 요약문 : great for cooking\\n원문 : son particularly picky eater occasionally gets fussy matter mood great meal option fall back always eat also pouch sturdy makes good travel meal option little expensive worth every penny', '실제 요약문 : my son loves these\\n예측 요약문 : great for baby food\\n원문 : labeled green tea touch pomegranate raspberry essence might rated higher however name pomegranate raspberry therefore expected vary flavors overall taste mild except greater sweetness tea second ingredient chamomile distinctive presence find flavor unpleasant nothing would make reach one teas\\n실제 요약문 : the flavor was not what expected\\n예측 요약문 : not for me', '실제 요약문 : the flavor was not what expected\\n예측 요약문 : not for me\\n원문 : delighted food looks nice smells really great smaller size kibble old dog teeth comes ziplock pouch importantly though old dog getting little suddenly gotten since starting chow also noticed terrible daily gone fair time switched wet food harmony foods may also factor extremely happy food continue buy\\n실제 요약문 : highly recommend this\\n예측 요약문 : my dog loves this\\n실제 요약문과 완전히 똑같지 않으면서 원문의 맥락을 잘 잡아서 예측된 요약문들이 존재하는 것을 확인할 수 있습니다.', '예측 요약문 : my dog loves this\\n실제 요약문과 완전히 똑같지 않으면서 원문의 맥락을 잘 잡아서 예측된 요약문들이 존재하는 것을 확인할 수 있습니다.\\n사실 여러분이 기대하는 수준의 생성 요약을 하는 것은 쉽지 않습니다. 그럼에도 꽤 뛰어난 성능의 생성 요약을 하고 싶다면 이렇게 직접 seq2seq를 구현하여 처음부터 학습하는 것보다는 사전 학습된 트랜스포머의 디코더를 장착한 GPT-2나 BART, T5와 같은 모델을 사용하는 것이 좋습니다. 이 책에서는 BART와 T-5까지는 다루지 않지만, 향후 작성될 22챕터에서 GPT-2를 이용한 문장 생성에 대해서 다룰 예정입니다. GPT-2에 대한 설명은 아직 집필되지 않지만, GPT-2에 대한 실습 코드는 이미 저자의 깃허브에 업로드가 되어 있으니 참고하시기 바랍니다.\\n==================================================', '==================================================\\n--- 22-02 문장 임베딩 기반 텍스트 랭크(TextRank Based on Sentence Embedding) ---\\n```\\n1 번 문서', \"원문 : Maria Sharapova has basically no friends as tennis players on the WTA Tour. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. I think everyone knows this is my job here\", \". I think everyone knows this is my job here. When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I'm a pretty competitive girl. I say my hellos, but I'm not sending any players flowers as well\", \". I'm a pretty competitive girl. I say my hellos, but I'm not sending any players flowers as well. Uhm, I'm not really friendly or close to many players. I have not a lot of friends away from the courts.' When she said she is not really close to a lot of players, is that something strategic that she is doing? Is it different on the men's tour than the women's tour? 'No, not at all\", \". I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players. I think every person has different interests. I have friends that have completely different jobs and interests, and I've met them in very different parts of my life. I think everyone just thinks because we're tennis players we should be the greatest of friends\", \". I think everyone just thinks because we're tennis players we should be the greatest of friends. But ultimately tennis is just a very small part of what we do. There are so many other things that we're interested in, that we do.'\", \"요약 : I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players\", \". When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I think everyone just thinks because we're tennis players we should be the greatest of friends.\", '2 번 문서', '원문 : BASEL, Switzerland (AP), Roger Federer advanced to the 14th Swiss Indoors final of his career by beating seventh-seeded Daniil Medvedev 6-1, 6-4 on Saturday. Seeking a ninth title at his hometown event, and a 99th overall, Federer will play 93th-ranked Marius Copil on Sunday. Federer dominated the 20th-ranked Medvedev and had his first match-point chance to break serve again at 5-1', \". He then dropped his serve to love, and let another match point slip in Medvedev's next service game by netting a backhand. He clinched on his fourth chance when Medvedev netted from the baseline. Copil upset expectations of a Federer final against Alexander Zverev in a 6-3, 6-7 (6), 6-4 win over the fifth-ranked German in the earlier semifinal. The Romanian aims for a first title after arriving at Basel without a career win over a top-10 opponent. Copil has two after also beating No\", \". Copil has two after also beating No. 6 Marin Cilic in the second round. Copil fired 26 aces past Zverev and never dropped serve, clinching after 2 1/2 hours with a forehand volley winner to break Zverev for the second time in the semifinal. He came through two rounds of qualifying last weekend to reach the Basel main draw, including beating Zverev's older brother, Mischa. Federer had an easier time than in his only previous match against Medvedev, a three-setter at Shanghai two weeks ago.\", '요약 : Federer had an easier time than in his only previous match against Medvedev, a three-setter at Shanghai two weeks ago. Federer dominated the 20th-ranked Medvedev and had his first match-point chance to break serve again at 5-1. Copil fired 26 aces past Zverev and never dropped serve, clinching after 2 1/2 hours with a forehand volley winner to break Zverev for the second time in the semifinal.\\n3 번 문서', '원문 : Roger Federer has revealed that organisers of the re-launched and condensed Davis Cup gave him three days to decide if he would commit to the controversial competition. Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment. \"They only left me three days to decide\", Federer said', '. \"They only left me three days to decide\", Federer said. \"I didn\\'t to have time to consult with all the people I had to consult. \"I could not make a decision in that time, so I told them to do what they wanted.\" The 20-time Grand Slam champion has voiced doubts about the wisdom of the one-week format to be introduced by organisers Kosmos, who have promised the International Tennis Federation up to $3 billion in prize money over the next quarter-century', \". The competition is set to feature 18 countries in the November 18-24 finals in Madrid next year, and will replace the classic home-and-away ties played four times per year for decades. Kosmos is headed by Barcelona footballer Gerard Pique, who is hoping fellow Spaniard Rafael Nadal will play in the upcoming event. Novak Djokovic has said he will give precedence to the ATP's intended re-launch of the defunct World Team Cup in January 2020, at various Australian venues\", '. Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest. Federer said earlier this month in Shanghai in that his chances of playing the Davis Cup were all but non-existent. \"I highly doubt it, of course. We will see what happens,\" he said. \"I do not think this was designed for me, anyhow', '. We will see what happens,\" he said. \"I do not think this was designed for me, anyhow. This was designed for the future generation of players.\" Argentina and Britain received wild cards to the new-look event, and will compete along with the four 2018 semi-finalists and the 12 teams who win qualifying rounds next February. \"I don\\'t like being under that kind of pressure,\" Federer said of the deadline Kosmos handed him.', '요약 : Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest. Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment. \"They only left me three days to decide\", Federer said.\\n4 번 문서', \"원문 : Kei Nishikori will try to end his long losing streak in ATP finals and Kevin Anderson will go for his second title of the year at the Erste Bank Open on Sunday. The fifth-seeded Nishikori reached his third final of 2018 after beating Mikhail Kukushkin of Kazakhstan 6-4, 6-3 in the semifinals. A winner of 11 ATP events, Nishikori hasn't triumphed since winning in Memphis in February 2016. He has lost eight straight finals since\", '. He has lost eight straight finals since. The second-seeded Anderson defeated Fernando Verdasco 6-3, 3-6, 6-4. Anderson has a shot at a fifth career title and second of the year after winning in New York in February. Nishikori leads Anderson 4-2 on career matchups, but the South African won their only previous meeting this year. With a victory on Sunday, Anderson will qualify for the ATP Finals', '. With a victory on Sunday, Anderson will qualify for the ATP Finals. Currently in ninth place, Nishikori with a win could move to within 125 points of the cut for the eight-man event in London next month. Nishikori held serve throughout against Kukushkin, who came through qualifying. He used his first break point to close out the first set before going up 3-0 in the second and wrapping up the win on his first match point. Against Verdasco, Anderson hit nine of his 19 aces in the opening set', \". Against Verdasco, Anderson hit nine of his 19 aces in the opening set. The Spaniard broke Anderson twice in the second but didn't get another chance on the South African's serve in the final set.\", \"요약 : Kei Nishikori will try to end his long losing streak in ATP finals and Kevin Anderson will go for his second title of the year at the Erste Bank Open on Sunday. The Spaniard broke Anderson twice in the second but didn't get another chance on the South African's serve in the final set. He has lost eight straight finals since.\\n5 번 문서\", '원문 : Federer, 37, first broke through on tour over two decades ago and he has since gone on to enjoy a glittering career. The 20-time Grand Slam winner is chasing his 99th ATP title at the Swiss Indoors this week and he faces Jan-Lennard Struff in the second round on Thursday (6pm BST). Davenport enjoyed most of her success in the late 1990s and her third and final major tournament win came at the 2000 Australian Open', '. But she claims the mentality of professional tennis players slowly began to change after the new millennium. \"It seems pretty friendly right now,\" said Davenport. \"I think there is a really nice environment and a great atmosphere, especially between some of the veteran players helping some of the younger players out. \"It\\'s a very pleasant atmosphere, I\\'d have to say, around the locker rooms', '. \"It\\'s a very pleasant atmosphere, I\\'d have to say, around the locker rooms. \"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments. \"And even though maybe we had smaller teams, I still think we kept to ourselves quite a bit', '. \"And even though maybe we had smaller teams, I still think we kept to ourselves quite a bit. \"Not always, but I really feel like in the mid-2000 years there was a huge shift of the attitudes of the top players and being more friendly and being more giving, and a lot of that had to do with players like Roger coming up', '. \"I just felt like it really kind of changed where people were a little bit, definitely in the 90s, a lot more quiet, into themselves, and then it started to become better.\" Meanwhile, Federer is hoping he can improve his service game as he hunts his ninth Swiss Indoors title this week. \"I didn\\'t serve very well [against first-round opponent Filip Kranjovic,\" Federer said. \"I think I was misfiring the corners, I was not hitting the lines enough', '. \"I think I was misfiring the corners, I was not hitting the lines enough. \"Clearly you make your life more difficult, but still I was up 6-2, 3-1, break points, so things could have ended very quickly today, even though I didn\\'t have the best serve percentage stats. \"But maybe that\\'s exactly what caught up to me eventually. It\\'s just getting used to it. This is where the first rounds can be tricky.\"', '요약 : \"Not always, but I really feel like in the mid-2000 years there was a huge shift of the attitudes of the top players and being more friendly and being more giving, and a lot of that had to do with players like Roger coming up. \"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments', '. \"Clearly you make your life more difficult, but still I was up 6-2, 3-1, break points, so things could have ended very quickly today, even though I didn\\'t have the best serve percentage stats.', '6 번 문서', \"원문 : Nadal has not played tennis since he was forced to retire from the US Open semi-finals against Juan Martin Del Porto with a knee injury. The world No 1 has been forced to miss Spain's Davis Cup clash with France and the Asian hard court season. But with the ATP World Tour Finals due to begin next month, Nadal is ready to prove his fitness before the season-ending event at the 02 Arena\", '. Nadal flew to Paris on Friday and footage from the Paris Masters official Twitter account shows the Spaniard smiling as he strides onto court for practice. The Paris Masters draw has been made and Nadal will start his campaign on Tuesday or Wednesday against either Fernando Verdasco or Jeremy Chardy. Nadal could then play defending champion Jack Sock in the third round before a potential quarter-final with either Borna Coric or Dominic Thiem', \". Nadal's appearance in Paris is a big boost to the tournament organisers who could see Roger Federer withdraw. Federer is in action at the Swiss Indoors in Basel and if he reaches the final, he could pull out of Paris in a bid to stay fresh for London. But as it stands, Federer is in the draw and is scheduled to face either former world No 3 Milos Raonic or Jo-Wilfried Tsonga in the second round\", \". Federer's projected route to the Paris final could also lead to matches against Kevin Anderson and Novak Djokovic. Djokovic could play Marco Cecchinato in the second round. British No 1 Kyle Edmund is the 12th seed in Paris and will get underway in round two against either Karen Khachanov or Filip Krajinovic.\", \"요약 : Nadal's appearance in Paris is a big boost to the tournament organisers who could see Roger Federer withdraw. Federer's projected route to the Paris final could also lead to matches against Kevin Anderson and Novak Djokovic. But as it stands, Federer is in the draw and is scheduled to face either former world No 3 Milos Raonic or Jo-Wilfried Tsonga in the second round.\\n7 번 문서\", \"원문 : Tennis giveth, and tennis taketh away. The end of the season is finally in sight, and with so many players defending,or losing,huge chunks of points in Singapore, Zhuhai and London, podcast co-hosts Nina Pantic and Irina Falconi discuss the art of defending points (02:14). It's no secret that Jack Sock has struggled on the singles court this year (his record is 7-19)\", \". It's no secret that Jack Sock has struggled on the singles court this year (his record is 7-19). He could lose 1,400 points in the next few weeks, but instead of focusing on the negative, it can all be about perspective (06:28). Let's also not forget his two Grand Slam doubles triumphs this season. Two players, Stefanos Tsitsipas and Kyle Edmund, won their first career ATP titles last week (13:26). It's a big deal because you never forget your first\", \". It's a big deal because you never forget your first. Irina looks back at her WTA title win in Bogota in 2016, and tells an unforgettable story about her semifinal drama (14:04). In Singapore, one of the biggest storylines (aside from the matches, of course) has been the on-court coaching debate. Nina and Irina give their opinions on what coaching should look like in the future, on both tours (18:55).\", \"요약 : Let's also not forget his two Grand Slam doubles triumphs this season. The end of the season is finally in sight, and with so many players defending,or losing,huge chunks of points in Singapore, Zhuhai and London, podcast co-hosts Nina Pantic and Irina Falconi discuss the art of defending points (02:14). In Singapore, one of the biggest storylines (aside from the matches, of course) has been the on-court coaching debate.\\n8 번 문서\", '원문 : Federer won the Swiss Indoors last week by beating Romanian qualifier Marius Copil in the final. The 37-year-old claimed his 99th ATP title and is hunting the century in the French capital this week. Federer has been handed a difficult draw where could could come across Kevin Anderson, Novak Djokovic and Rafael Nadal in the latter rounds', '. But first the 20-time Grand Slam winner wants to train on the Paris Masters court this afternoon before deciding whether to appear for his opening match against either Milos Raonic or Jo-Wilfried Tsonga. \"On Monday, I am free and will look how I feel,\" Federer said after winning the Swiss Indoors. \"On Tuesday I will fly to Paris and train in the afternoon to be ready for my first match on Wednesday night. \"I felt good all week and better every day', '. \"I felt good all week and better every day. \"We also had the impression that at this stage it might be better to play matches than to train. \"And as long as I fear no injury, I play.\" Federer\\'s success in Basel last week was the ninth time he has won his hometown tournament. And he was delighted to be watched on by all of his family and friends as he purchased 60 tickets for the final for those dearest to him. \"My children, my parents, my sister and my team are all there,\" Federer added', '. \"My children, my parents, my sister and my team are all there,\" Federer added. \"It is always very emotional for me to thank my team. And sometimes it tilts with the emotions, sometimes I just stumble. \"It means the world to me. It makes me incredibly happy to win my home tournament and make people happy here. \"I do not know if it\\'s maybe my last title, so today I try a lot more to absorb that and enjoy the moments much more consciously. \"Maybe I should celebrate as if it were my last title', '. \"Maybe I should celebrate as if it were my last title. \"There are very touching moments: seeing the ball children, the standing ovations, all the familiar faces in the audience. Because it was not always easy in the last weeks.\"', '요약 : \"We also had the impression that at this stage it might be better to play matches than to train. \"Maybe I should celebrate as if it were my last title. \"On Monday, I am free and will look how I feel,\" Federer said after winning the Swiss Indoors.\\n```앞서 추상적 요약(abstractive summarization)을 통한 텍스트 요약을 수행해보았습니다. 이번 실습에서는 텍스트랭크(TextRank) 알고리즘으로 사용하여 또 다른 텍스트 요약 방법인 추출적 요약을 진행해보겠습니다.']\n",
      "['텍스트랭크 알고리즘에 대해서 이해하기 위해서, 텍스트랭크 알고리즘의 기반이 된 페이지랭크 알고리즘에 대해서 간단히 이해해보겠습니다. 페이지랭크 알고리즘은 검색 엔진에서 웹 페이지의 순위를 정하기 위해 사용되던 알고리즘입니다. 텍스트랭크 알고리즘은 페이지랭크를 기반으로 한 텍스트 요약 알고리즘입니다. 텍스트랭크에서 그래프의 노드들은 문장들이며, 각 간선의 가중치는 문장들 간의 유사도를 의미합니다. 텍스트 랭크에 대한 상세 설명은 하지 않겠습니다.']\n",
      "['이번 실습에서는 사전 훈련된 임베딩을 사용합니다. 워드 임베딩 방법으로는 여러가지가 있겠지만, 대표적으로 사용할 수 있는 임베딩 방법인 GloVe, FastText, Word2Vec의 사전 훈련된 임베딩 사용 방법은 다음과 같습니다. 각 임베딩을 다운로드하기 위해서는 어느정도 시간이 소요되므로 실습을 위해서는 우선 GloVe만을 다운로드 하는 것을 권합니다.\\nimport numpy as np\\nimport gensim\\nfrom urllib.request import urlretrieve, urlopen\\nimport gzip\\nimport zipfile']\n",
      "['urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\\nzf = zipfile.ZipFile(\\'glove.6B.zip\\')\\nzf.extractall()\\nzf.close()\\nglove_dict = dict()\\nf = open(\\'glove.6B.100d.txt\\', encoding=\"utf8\") # 100차원의 GloVe 벡터를 사용\\nfor line in f:\\nword_vector = line.split()\\nword = word_vector[0]\\nword_vector_arr = np.asarray(word_vector[1:], dtype=\\'float32\\') # 100개의 값을 가지는 array로 변환\\nglove_dict[word] = word_vector_arr\\nf.close()\\n만약 단어 \\'cat\\'에 대한 임베딩 벡터를 얻고싶다면 다음과 같이 얻을 수 있습니다.\\nglove_dict[\\'cat\\']']\n",
      "[\"!pip install fasttext\\n# 300차원의 FastText 벡터 사용\\nimport fasttext.util\\nfasttext.util.download_model('en', if_exists='ignore')\\nft = fasttext.load_model('cc.en.300.bin')\\n만약 단어 'cat'에 대한 임베딩 벡터를 얻고싶다면 다음과 같이 얻을 수 있습니다.\\nft.get_word_vector('cat')\"]\n",
      "['# 300차원의 Word2Vec 벡터 사용\\nurlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\\\\nfilename=\"GoogleNews-vectors-negative300.bin.gz\")\\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\\'GoogleNews-vectors-negative300.bin.gz\\', binary=True)\\n만약 단어 \\'cat\\'에 대한 임베딩 벡터를 얻고싶다면 다음과 같이 얻을 수 있습니다.\\nword2vec_model[\\'cat\\']']\n",
      "['여러분이 어떤 다수의 문장을 가지고 있다고 해봅시다. 그리고 이 문장들을 서로 비교하고 싶습니다. 만약, 문장들을 각 문장을 표현하는 고정된 길이의 벡터로 변환한다면 벡터 간 비교로 문장을 비교할 수 있을 것입니다. 각 문장을 문장 벡터로 변환하는 방법은 여러가지 방법이 존재하지만, 여기서는 가장 간단한 방법 한 가지를 소개하고자 합니다.\\n문장 벡터를 얻는 가장 간단한 방법은 문장에 존재하는 단어 벡터들의 평균을 구하는 것입니다. 이번 챕터에서는 앞에서 소개한 방법으로 다운로드한 사전 훈련된 임베딩을 사용합니다. 예를 들어 사전 훈련된 GloVe로부터 문장 벡터는 다음과 같이 얻을 수 있습니다.\\n현재 glove_dict에는 100차원의 GloVe 벡터들이 저장되어져 있습니다. OOV 문제. 즉, glove_dict에 존재하지 않는 단어가 문장에 존재할 경우 해당 단어의 임베딩값으로 사용할 100차원의 영벡터도 만들어둡니다.\\nembedding_dim = 100', 'embedding_dim = 100\\nzero_vector = np.zeros(embedding_dim)\\n아래 함수는 문장의 각 단어를 사전 훈련된 GloVe 벡터로 변환하면서, OOV 문제가 발생할 경우에는 해당 단어를 영벡터로 변환합니다. 그리고 이렇게 모인 단어 벡터들의 평균을 구하여 반환합니다.\\n# 단어 벡터의 평균으로부터 문장 벡터를 얻는다.\\ndef calculate_sentence_vector(sentence):\\nreturn sum([glove_dict.get(word, zero_vector)\\nfor word in sentence])/len(sentence)\\n만약 I am a student라는 문장 벡터의 값을 얻고싶다면 해당 문장을 calculate_sentence_vector 함수의 입력으로 사용하면 됩니다. 이렇게 반환된 벡터값의 크기는 100차원이 될 것입니다. 여기서는 책의 지면의 한계로 값을 확인하진 않겠습니다.', \"eng_sent = ['I', 'am', 'a', 'student']\\nsentence_vector = calculate_sentence_vector(eng_sent)\\nprint(len(sentence_vector))\\n100\\n현재 사용하고 있는 사전 훈련된 GloVe는 영어에 대해서 학습된 임베딩입니다. 그래서 한국어를 넣으면 당연히 모든 단어에 대해서 OOV 문제가 발생합니다. 즉, 모든 단어가 영벡터이므로 평균을 구해도 영벡터가 반환됩니다. 실제로 값을 확인해봅시다.\\nkor_sent = ['전', '좋은', '학생', '입니다']\\nsentence_vector = calculate_sentence_vector(kor_sent)\\nprint(sentence_vector)\\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\"]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\"여기서는 앞서 사전 훈련된 GloVe를 다운로드 하였다는 가정 하에 진행합니다.\\nimport numpy as np\\nimport re\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom urllib.request import urlretrieve\\nimport zipfile\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nimport networkx as nx\\nNLTK에서 제공하는 불용어를 받아옵니다.\\nstop_words = stopwords.words('english')\\n텍스트 요약에 사용할 테니스 관련 기사를 다운로드하고, 데이터프레임에 저장합니다.\", 'stop_words = stopwords.words(\\'english\\')\\n텍스트 요약에 사용할 테니스 관련 기사를 다운로드하고, 데이터프레임에 저장합니다.\\nurlretrieve(\"https://raw.githubusercontent.com/prateekjoshi565/textrank_text_summarization/master/tennis_articles_v4.csv\", filename=\"tennis_articles_v4.csv\")\\ndata = pd.read_csv(\"tennis_articles_v4.csv\")\\ndata.head()\\n[이미지: ]\\n총 3개의 열이 존재하지만 여기서 사용할 열은 기사 본문에 해당하는 article_text입니다. 해당 article_text열만 사용하기로 하고, 해당 기사를 문장 토큰화한 결과를 저장한 sentences라는 열을 새로 만듭니다.\\ndata = data[[\\'article_text\\']]', \"data = data[['article_text']]\\ndata['sentences'] = data['article_text'].apply(sent_tokenize)\\ndata\\n[이미지: ]\\n토큰화와 전처리를 위한 함수들을 정의합니다.\\n# 토큰화 함수\\ndef tokenization(sentences):\\nreturn [word_tokenize(sentence) for sentence in sentences]\\n# 전처리 함수\\ndef preprocess_sentence(sentence):\\n# 영어를 제외한 숫자, 특수 문자 등은 전부 제거. 모든 알파벳은 소문자화\\nsentence = [re.sub(r'[^a-zA-z\\\\s]', '', word).lower() for word in sentence]\\n# 불용어가 아니면서 단어가 실제로 존재해야 한다.\\nreturn [word for word in sentence if word not in stop_words and word]\", \"# 불용어가 아니면서 단어가 실제로 존재해야 한다.\\nreturn [word for word in sentence if word not in stop_words and word]\\n# 위 전처리 함수를 모든 문장에 대해서 수행. 이 함수를 호출하면 모든 행에 대해서 수행.\\ndef preprocess_sentences(sentences):\\nreturn [preprocess_sentence(sentence) for sentence in sentences]\\n문장 토큰화를 진행한 'sentences'열에 대해서 단어 토큰화와 전처리를 적용한 'tokenized_sentences' 열을 새로 만듭니다.\\ndata['tokenized_sentences'] = data['sentences'].apply(tokenization)\\ndata['tokenized_sentences'] = data['tokenized_sentences'].apply(preprocess_sentences)\\ndata\", \"data['tokenized_sentences'] = data['tokenized_sentences'].apply(preprocess_sentences)\\ndata\\n[이미지: ]\\n현재 사용할 GloVe 벡터의 차원은 100입니다. 100차원의 영벡터를 만듭니다.\\nembedding_dim = 100\\nzero_vector = np.zeros(embedding_dim)\\n단어 벡터의 평균을 구하는 함수를 정의합니다. 단, 문장 길이가 0일 경우에는 100차원의 영벡터를 리턴합니다. 현재 불용어를 제거하였기 때문에 문장의 모든 단어가 불용어인 경우에는 길이가 0인 문장이 생길 수 있기 때문입니다.\\n# 단어 벡터의 평균으로부터 문장 벡터를 얻는다.\\ndef calculate_sentence_vector(sentence):\\nif len(sentence) != 0:\\nreturn sum([glove_dict.get(word, zero_vector)\", \"if len(sentence) != 0:\\nreturn sum([glove_dict.get(word, zero_vector)\\nfor word in sentence])/len(sentence)\\nelse:\\nreturn zero_vector\\n이를 모든 행에 대해서 수행하기 위해서 모든 문장에 대해서 수행하는 함수를 정의합니다.\\n# 각 문장에 대해서 문장 벡터를 반환\\ndef sentences_to_vectors(sentences):\\nreturn [calculate_sentence_vector(sentence)\\nfor sentence in sentences]\\n모든 문장에 대해서 문장 벡터를 만듭니다.\\ndata['SentenceEmbedding'] = data['tokenized_sentences'].apply(sentences_to_vectors)\\ndata[['SentenceEmbedding']]\\n[이미지: ]\", \"data[['SentenceEmbedding']]\\n[이미지: ]\\n문장 벡터들 간의 코사인 유사도를 구한 유사도 행렬을 만듭니다. 이 유사도 행렬의 크기는 (문장 개수 × 문장 개수)입니다.\\ndef similarity_matrix(sentence_embedding):\\nsim_mat = np.zeros([len(sentence_embedding), len(sentence_embedding)])\\nfor i in range(len(sentence_embedding)):\\nfor j in range(len(sentence_embedding)):\\nsim_mat[i][j] = cosine_similarity(sentence_embedding[i].reshape(1, embedding_dim),\\nsentence_embedding[j].reshape(1, embedding_dim))[0,0]\\nreturn sim_mat\\n이 결과를 저장한 'SimMatrix'열을 만듭니다.\", \"sentence_embedding[j].reshape(1, embedding_dim))[0,0]\\nreturn sim_mat\\n이 결과를 저장한 'SimMatrix'열을 만듭니다.\\ndata['SimMatrix'] = data['SentenceEmbedding'].apply(similarity_matrix)\\ndata['SimMatrix']\\n[이미지: ]\\n두번째 샘플을 기준으로 지금까지 만든 열들의 크기를 확인해봅시다.\\nprint('두번째 샘플의 문장 개수 :',len(data['tokenized_sentences'][1]))\\nprint('두번째 샘플의 문장 벡터가 모인 문장 행렬의 크기(shape) :',np.shape(data['SentenceEmbedding'][1]))\\nprint('두번째 샘플의 유사도 행렬의 크기(shape) :',data['SimMatrix'][1].shape)\\n두번째 샘플의 문장 개수 : 12\", \"print('두번째 샘플의 유사도 행렬의 크기(shape) :',data['SimMatrix'][1].shape)\\n두번째 샘플의 문장 개수 : 12\\n두번째 샘플의 문장 벡터가 모인 문장 행렬의 크기(shape) : (12, 100)\\n두번째 샘플의 유사도 행렬의 크기(shape) : (12, 12)\\n두번째 샘플의 경우에는 총 문장이 12개가 존재합니다. 그래서 문장 벡터 또한 12개가 존재하며, 각 문장 벡터는 100의 크기를 가집니다. 유사도 행렬은 각 문장 벡터들의 유사도가 기록된 유사도 행렬이므로 (문장 개수 × 문장 개수)의 크기를 가집니다.\\n주어진 유사도 행렬로부터 그래프를 그려봅시다.\\ndef draw_graphs(sim_matrix):\\nnx_graph = nx.from_numpy_array(sim_matrix)\\nplt.figure(figsize=(10, 10))\\npos = nx.spring_layout(nx_graph)\", \"plt.figure(figsize=(10, 10))\\npos = nx.spring_layout(nx_graph)\\nnx.draw(nx_graph, with_labels=True, font_weight='bold')\\nnx.draw_networkx_edge_labels(nx_graph,pos,font_color='red')\\nplt.show()\\n두번째 샘플의 유사도 행렬로부터 그린 그래프는 다음과 같습니다.\\ndraw_graphs(data['SimMatrix'][1])\\n[이미지: ]\\n문장 개수가 12개였으므로 총 12개의 노드가 존재합니다. 이러한 그래프를 페이지랭크 알고리즘의 입력으로 사용하여 각 문장의 점수를 구합니다. 이렇게 'score'라는 열을 만듭니다.\\ndef calculate_score(sim_matrix):\\nnx_graph = nx.from_numpy_array(sim_matrix)\\nscores = nx.pagerank(nx_graph)\\nreturn scores\", \"nx_graph = nx.from_numpy_array(sim_matrix)\\nscores = nx.pagerank(nx_graph)\\nreturn scores\\ndata['score'] = data['SimMatrix'].apply(calculate_score)\\ndata[['SimMatrix', 'score']]\\n[이미지: ]\\n두번째 샘플의 각 문장의 점수를 출력해봅시다. 총 12개 문장에 대해서 점수가 기록되어져 있습니다.\\ndata['score'][1]\\n{0: 0.08315094474060455,\\n1: 0.08498611405296501,\\n2: 0.08555019786198463,\\n3: 0.08383717299575927,\\n4: 0.0813794030791188,\\n5: 0.08439285067975581,\\n6: 0.08507725735628792,\\n7: 0.08092839280412682,\\n8: 0.07454046000848007,\\n9: 0.08535836572027003,\", '6: 0.08507725735628792,\\n7: 0.08092839280412682,\\n8: 0.07454046000848007,\\n9: 0.08535836572027003,\\n10: 0.0849824249168908,\\n11: 0.08581641578375629}\\n이제 이 점수가 가장 높은 문장들을 상위 n개 선택하여 이 문서의 요약문으로 삼을 것입니다. 점수가 가장 높은 상위 3개의 문장을 선택하는 함수를 만듭니다. 점수에 따라서 정렬 후에 상위 3개 문장만을 반환합니다.\\ndef ranked_sentences(sentences, scores, n=3):\\ntop_scores = sorted(((scores[i],s)\\nfor i,s in enumerate(sentences)),\\nreverse=True)\\ntop_n_sentences = [sentence\\nfor score,sentence in top_scores[:n]]\\nreturn \" \".join(top_n_sentences)', 'top_n_sentences = [sentence\\nfor score,sentence in top_scores[:n]]\\nreturn \" \".join(top_n_sentences)\\n이 함수를 사용하여 \\'summary\\'라는 열을 만듭니다.\\ndata[\\'summary\\'] = data.apply(lambda x:\\nranked_sentences(x.sentences,\\nx.score), axis=1)\\n모든 문서에 대해서 원문과 요약문을 출력해볼까요?\\nfor i in range(0, len(data)):\\nprint(i+1,\\'번 문서\\')\\nprint(\\'원문 :\\',data.loc[i].article_text)\\nprint(\\'\\')\\nprint(\\'요약 :\\',data.loc[i].summary)\\nprint(\\'\\')\\n1 번 문서', \"원문 : Maria Sharapova has basically no friends as tennis players on the WTA Tour. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. I think everyone knows this is my job here\", \". I think everyone knows this is my job here. When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I'm a pretty competitive girl. I say my hellos, but I'm not sending any players flowers as well\", \". I'm a pretty competitive girl. I say my hellos, but I'm not sending any players flowers as well. Uhm, I'm not really friendly or close to many players. I have not a lot of friends away from the courts.' When she said she is not really close to a lot of players, is that something strategic that she is doing? Is it different on the men's tour than the women's tour? 'No, not at all\", \". I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players. I think every person has different interests. I have friends that have completely different jobs and interests, and I've met them in very different parts of my life. I think everyone just thinks because we're tennis players we should be the greatest of friends\", \". I think everyone just thinks because we're tennis players we should be the greatest of friends. But ultimately tennis is just a very small part of what we do. There are so many other things that we're interested in, that we do.'\", \"요약 : I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players\", \". When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I think everyone just thinks because we're tennis players we should be the greatest of friends.\", '2 번 문서', '원문 : BASEL, Switzerland (AP), Roger Federer advanced to the 14th Swiss Indoors final of his career by beating seventh-seeded Daniil Medvedev 6-1, 6-4 on Saturday. Seeking a ninth title at his hometown event, and a 99th overall, Federer will play 93th-ranked Marius Copil on Sunday. Federer dominated the 20th-ranked Medvedev and had his first match-point chance to break serve again at 5-1', \". He then dropped his serve to love, and let another match point slip in Medvedev's next service game by netting a backhand. He clinched on his fourth chance when Medvedev netted from the baseline. Copil upset expectations of a Federer final against Alexander Zverev in a 6-3, 6-7 (6), 6-4 win over the fifth-ranked German in the earlier semifinal. The Romanian aims for a first title after arriving at Basel without a career win over a top-10 opponent. Copil has two after also beating No\", \". Copil has two after also beating No. 6 Marin Cilic in the second round. Copil fired 26 aces past Zverev and never dropped serve, clinching after 2 1/2 hours with a forehand volley winner to break Zverev for the second time in the semifinal. He came through two rounds of qualifying last weekend to reach the Basel main draw, including beating Zverev's older brother, Mischa. Federer had an easier time than in his only previous match against Medvedev, a three-setter at Shanghai two weeks ago.\", '요약 : Federer had an easier time than in his only previous match against Medvedev, a three-setter at Shanghai two weeks ago. Federer dominated the 20th-ranked Medvedev and had his first match-point chance to break serve again at 5-1. Copil fired 26 aces past Zverev and never dropped serve, clinching after 2 1/2 hours with a forehand volley winner to break Zverev for the second time in the semifinal.\\n3 번 문서', '원문 : Roger Federer has revealed that organisers of the re-launched and condensed Davis Cup gave him three days to decide if he would commit to the controversial competition. Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment. \"They only left me three days to decide\", Federer said', '. \"They only left me three days to decide\", Federer said. \"I didn\\'t to have time to consult with all the people I had to consult. \"I could not make a decision in that time, so I told them to do what they wanted.\" The 20-time Grand Slam champion has voiced doubts about the wisdom of the one-week format to be introduced by organisers Kosmos, who have promised the International Tennis Federation up to $3 billion in prize money over the next quarter-century', \". The competition is set to feature 18 countries in the November 18-24 finals in Madrid next year, and will replace the classic home-and-away ties played four times per year for decades. Kosmos is headed by Barcelona footballer Gerard Pique, who is hoping fellow Spaniard Rafael Nadal will play in the upcoming event. Novak Djokovic has said he will give precedence to the ATP's intended re-launch of the defunct World Team Cup in January 2020, at various Australian venues\", '. Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest. Federer said earlier this month in Shanghai in that his chances of playing the Davis Cup were all but non-existent. \"I highly doubt it, of course. We will see what happens,\" he said. \"I do not think this was designed for me, anyhow', '. We will see what happens,\" he said. \"I do not think this was designed for me, anyhow. This was designed for the future generation of players.\" Argentina and Britain received wild cards to the new-look event, and will compete along with the four 2018 semi-finalists and the 12 teams who win qualifying rounds next February. \"I don\\'t like being under that kind of pressure,\" Federer said of the deadline Kosmos handed him.', '요약 : Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest. Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment. \"They only left me three days to decide\", Federer said.\\n4 번 문서', \"원문 : Kei Nishikori will try to end his long losing streak in ATP finals and Kevin Anderson will go for his second title of the year at the Erste Bank Open on Sunday. The fifth-seeded Nishikori reached his third final of 2018 after beating Mikhail Kukushkin of Kazakhstan 6-4, 6-3 in the semifinals. A winner of 11 ATP events, Nishikori hasn't triumphed since winning in Memphis in February 2016. He has lost eight straight finals since\", '. He has lost eight straight finals since. The second-seeded Anderson defeated Fernando Verdasco 6-3, 3-6, 6-4. Anderson has a shot at a fifth career title and second of the year after winning in New York in February. Nishikori leads Anderson 4-2 on career matchups, but the South African won their only previous meeting this year. With a victory on Sunday, Anderson will qualify for the ATP Finals', '. With a victory on Sunday, Anderson will qualify for the ATP Finals. Currently in ninth place, Nishikori with a win could move to within 125 points of the cut for the eight-man event in London next month. Nishikori held serve throughout against Kukushkin, who came through qualifying. He used his first break point to close out the first set before going up 3-0 in the second and wrapping up the win on his first match point. Against Verdasco, Anderson hit nine of his 19 aces in the opening set', \". Against Verdasco, Anderson hit nine of his 19 aces in the opening set. The Spaniard broke Anderson twice in the second but didn't get another chance on the South African's serve in the final set.\", \"요약 : Kei Nishikori will try to end his long losing streak in ATP finals and Kevin Anderson will go for his second title of the year at the Erste Bank Open on Sunday. The Spaniard broke Anderson twice in the second but didn't get another chance on the South African's serve in the final set. He has lost eight straight finals since.\\n5 번 문서\", '원문 : Federer, 37, first broke through on tour over two decades ago and he has since gone on to enjoy a glittering career. The 20-time Grand Slam winner is chasing his 99th ATP title at the Swiss Indoors this week and he faces Jan-Lennard Struff in the second round on Thursday (6pm BST). Davenport enjoyed most of her success in the late 1990s and her third and final major tournament win came at the 2000 Australian Open', '. But she claims the mentality of professional tennis players slowly began to change after the new millennium. \"It seems pretty friendly right now,\" said Davenport. \"I think there is a really nice environment and a great atmosphere, especially between some of the veteran players helping some of the younger players out. \"It\\'s a very pleasant atmosphere, I\\'d have to say, around the locker rooms', '. \"It\\'s a very pleasant atmosphere, I\\'d have to say, around the locker rooms. \"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments. \"And even though maybe we had smaller teams, I still think we kept to ourselves quite a bit', '. \"And even though maybe we had smaller teams, I still think we kept to ourselves quite a bit. \"Not always, but I really feel like in the mid-2000 years there was a huge shift of the attitudes of the top players and being more friendly and being more giving, and a lot of that had to do with players like Roger coming up', '. \"I just felt like it really kind of changed where people were a little bit, definitely in the 90s, a lot more quiet, into themselves, and then it started to become better.\" Meanwhile, Federer is hoping he can improve his service game as he hunts his ninth Swiss Indoors title this week. \"I didn\\'t serve very well [against first-round opponent Filip Kranjovic,\" Federer said. \"I think I was misfiring the corners, I was not hitting the lines enough', '. \"I think I was misfiring the corners, I was not hitting the lines enough. \"Clearly you make your life more difficult, but still I was up 6-2, 3-1, break points, so things could have ended very quickly today, even though I didn\\'t have the best serve percentage stats. \"But maybe that\\'s exactly what caught up to me eventually. It\\'s just getting used to it. This is where the first rounds can be tricky.\"', '요약 : \"Not always, but I really feel like in the mid-2000 years there was a huge shift of the attitudes of the top players and being more friendly and being more giving, and a lot of that had to do with players like Roger coming up. \"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments', '. \"Clearly you make your life more difficult, but still I was up 6-2, 3-1, break points, so things could have ended very quickly today, even though I didn\\'t have the best serve percentage stats.', '6 번 문서', \"원문 : Nadal has not played tennis since he was forced to retire from the US Open semi-finals against Juan Martin Del Porto with a knee injury. The world No 1 has been forced to miss Spain's Davis Cup clash with France and the Asian hard court season. But with the ATP World Tour Finals due to begin next month, Nadal is ready to prove his fitness before the season-ending event at the 02 Arena\", '. Nadal flew to Paris on Friday and footage from the Paris Masters official Twitter account shows the Spaniard smiling as he strides onto court for practice. The Paris Masters draw has been made and Nadal will start his campaign on Tuesday or Wednesday against either Fernando Verdasco or Jeremy Chardy. Nadal could then play defending champion Jack Sock in the third round before a potential quarter-final with either Borna Coric or Dominic Thiem', \". Nadal's appearance in Paris is a big boost to the tournament organisers who could see Roger Federer withdraw. Federer is in action at the Swiss Indoors in Basel and if he reaches the final, he could pull out of Paris in a bid to stay fresh for London. But as it stands, Federer is in the draw and is scheduled to face either former world No 3 Milos Raonic or Jo-Wilfried Tsonga in the second round\", \". Federer's projected route to the Paris final could also lead to matches against Kevin Anderson and Novak Djokovic. Djokovic could play Marco Cecchinato in the second round. British No 1 Kyle Edmund is the 12th seed in Paris and will get underway in round two against either Karen Khachanov or Filip Krajinovic.\", \"요약 : Nadal's appearance in Paris is a big boost to the tournament organisers who could see Roger Federer withdraw. Federer's projected route to the Paris final could also lead to matches against Kevin Anderson and Novak Djokovic. But as it stands, Federer is in the draw and is scheduled to face either former world No 3 Milos Raonic or Jo-Wilfried Tsonga in the second round.\\n7 번 문서\", \"원문 : Tennis giveth, and tennis taketh away. The end of the season is finally in sight, and with so many players defending,or losing,huge chunks of points in Singapore, Zhuhai and London, podcast co-hosts Nina Pantic and Irina Falconi discuss the art of defending points (02:14). It's no secret that Jack Sock has struggled on the singles court this year (his record is 7-19)\", \". It's no secret that Jack Sock has struggled on the singles court this year (his record is 7-19). He could lose 1,400 points in the next few weeks, but instead of focusing on the negative, it can all be about perspective (06:28). Let's also not forget his two Grand Slam doubles triumphs this season. Two players, Stefanos Tsitsipas and Kyle Edmund, won their first career ATP titles last week (13:26). It's a big deal because you never forget your first\", \". It's a big deal because you never forget your first. Irina looks back at her WTA title win in Bogota in 2016, and tells an unforgettable story about her semifinal drama (14:04). In Singapore, one of the biggest storylines (aside from the matches, of course) has been the on-court coaching debate. Nina and Irina give their opinions on what coaching should look like in the future, on both tours (18:55).\", \"요약 : Let's also not forget his two Grand Slam doubles triumphs this season. The end of the season is finally in sight, and with so many players defending,or losing,huge chunks of points in Singapore, Zhuhai and London, podcast co-hosts Nina Pantic and Irina Falconi discuss the art of defending points (02:14). In Singapore, one of the biggest storylines (aside from the matches, of course) has been the on-court coaching debate.\\n8 번 문서\", '원문 : Federer won the Swiss Indoors last week by beating Romanian qualifier Marius Copil in the final. The 37-year-old claimed his 99th ATP title and is hunting the century in the French capital this week. Federer has been handed a difficult draw where could could come across Kevin Anderson, Novak Djokovic and Rafael Nadal in the latter rounds', '. But first the 20-time Grand Slam winner wants to train on the Paris Masters court this afternoon before deciding whether to appear for his opening match against either Milos Raonic or Jo-Wilfried Tsonga. \"On Monday, I am free and will look how I feel,\" Federer said after winning the Swiss Indoors. \"On Tuesday I will fly to Paris and train in the afternoon to be ready for my first match on Wednesday night. \"I felt good all week and better every day', '. \"I felt good all week and better every day. \"We also had the impression that at this stage it might be better to play matches than to train. \"And as long as I fear no injury, I play.\" Federer\\'s success in Basel last week was the ninth time he has won his hometown tournament. And he was delighted to be watched on by all of his family and friends as he purchased 60 tickets for the final for those dearest to him. \"My children, my parents, my sister and my team are all there,\" Federer added', '. \"My children, my parents, my sister and my team are all there,\" Federer added. \"It is always very emotional for me to thank my team. And sometimes it tilts with the emotions, sometimes I just stumble. \"It means the world to me. It makes me incredibly happy to win my home tournament and make people happy here. \"I do not know if it\\'s maybe my last title, so today I try a lot more to absorb that and enjoy the moments much more consciously. \"Maybe I should celebrate as if it were my last title', '. \"Maybe I should celebrate as if it were my last title. \"There are very touching moments: seeing the ball children, the standing ovations, all the familiar faces in the audience. Because it was not always easy in the last weeks.\"', '요약 : \"We also had the impression that at this stage it might be better to play matches than to train. \"Maybe I should celebrate as if it were my last title. \"On Monday, I am free and will look how I feel,\" Federer said after winning the Swiss Indoors.\\n==================================================\\n--- 23. 질의 응답(Question Answering, QA) ---\\n마지막 편집일시 : 2024년 8월 26일 12:22 오전\\n==================================================\\n--- 23-01 메모리 네트워크(Memory Network, MemN)를 이용한 QA ---\\n```', '--- 23-01 메모리 네트워크(Memory Network, MemN)를 이용한 QA ---\\n```\\nplt.subplot(211)\\nplt.title(\"Accuracy\")\\nplt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\\nplt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\\nplt.legend(loc=\"best\")\\nplt.subplot(212)\\nplt.title(\"Loss\")\\nplt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\\nplt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\\nplt.legend(loc=\"best\")\\nplt.tight_layout()\\nplt.show()\\nytest = np.argmax(Ytest, axis=1)', 'plt.legend(loc=\"best\")\\nplt.tight_layout()\\nplt.show()\\nytest = np.argmax(Ytest, axis=1)\\nYtest_ = model.predict([Xstest, Xqtest])\\nytest_ = np.argmax(Ytest_, axis=1)\\n```이번 챕터에서는 메모리 네트워크에 대해서 학습하고, 이를 통해 페이스북이 공개한 QA 데이터셋을 풀어봅시다.\\n질의 응답(Question Answering)의 다른 풀이법이 궁금하시면 18챕터의 KoBERT를 이용한 기계 독해 챕터를 참고하세요.']\n",
      "['babi Project의 데이터셋은 기본적으로 다음과 같은 형식을 갖고 있습니다.\\nID text\\nID text\\nID text\\nID question[tab]answer[tab]supporting_fact ID.\\n...\\nID는 각 문장의 번호구요. 스토리가 시작될 때는 1번으로 시작합니다. 텍스트로 예를 들어볼게요. 우선 4개의 문장으로 구성된 스토리가 있다고 해봅시다.\\n1 Sandra travelled to the kitchen.\\n2 Sandra travelled to the hallway.\\n3 Mary went to the bathroom.\\n4 Sandra moved to the garden.', '2 Sandra travelled to the hallway.\\n3 Mary went to the bathroom.\\n4 Sandra moved to the garden.\\n이제 위에 있는 4개의 문장으로 구성된 스토리를 기반으로 질문과 정답이 나옵니다. Sandra는 어디있나요? Where is Sandra? 스토리에 있는 문장들을 보면 네번째 문장에서 우리는 Sandra가 garden에 갔다는 사실을 알 수 있습니다. 레이블에 해당되는 Garden은 질문 다음 tab 뒤에 등장합니다.\\n5 Where is Sandra?      Garden         4', '5 Where is Sandra?      Garden         4\\nGarden 옆에 숫자 4는 Supporting fact라고 해서 실제 정답이 주어진 스토리에서 몇 번 id 문장에 있었는지를 알려줍니다. 근데 이걸 알면 학습이 너무 쉽겠죠? 그래서 실제로는 인공지능의 훈련이 단계에서는 Supporting fact는 왠만하면 학습하지 않는 것이 원칙입니다. 저희도 Supporting fact는 학습하지 않을 예정입니다.\\n지금은 스토리, 질문, 답이라는 세 가지를 하나로 묶은 걸 본 것입니다. 실제로는 스토리와 질문이 굉장히 많겠죠? 그래서 실제 데이터셋은 다음과 같이 구성됩니다.\\n1 Mary moved to the bathroom.\\n2 John went to the hallway.\\n3 Where is Mary?        bathroom        1\\n4 Daniel went back to the hallway.\\n5 Sandra moved to the garden.', '4 Daniel went back to the hallway.\\n5 Sandra moved to the garden.\\n6 Where is Daniel?      hallway         4\\n7 John moved to the office.\\n8 Sandra journeyed to the bathroom.\\n9 Where is Daniel?      hallway         4\\n10 Mary moved to the hallway.\\n11 Daniel travelled to the office.\\n12 Where is Daniel?     office          11\\n13 John went back to the garden.\\n14 John moved to the bedroom.\\n15 Where is Sandra?     bathroom        8\\n1 Sandra travelled to the office.\\n2 Sandra went to the bathroom.', '1 Sandra travelled to the office.\\n2 Sandra went to the bathroom.\\n3 Where is Sandra?      bathroom        2\\n여기서 주의할 점은 첫번째 스토리는 무려 ID가 15번까지 이어졌는데, 중간에 질문은 3번, 6번, 9번, 11번, 15번으로 다섯 번이나 나왔다는 점입니다. 질문이 한 번 나왔다고해서 스토리가 끝나는 게 아니에요. 스토리는 계속 이어지고 질문도 계속 이어집니다. 두번째 스토리가 시작되자 15번 다음에는 다시 ID가 1번부터 시작되고 있네요.']\n",
      "['[이미지: ]\\n위 그림은 메모리 네트워크 구조를 간단히 표현한 그림입니다. 그림의 가장 상단을 보면 두 개의 입력이 들어옵니다. 각각 스토리 문장과 질문 문장입니다. 이 두 문장은 각각의 임베딩 과정을 거쳐야 합니다.\\n이제 그림의 우측 상단에 있는 초록색 박스와 하늘색 박스를 주목해주세요. 스토리 문장은 Embedding C를 통해서 임베딩이 되고, 질문 문장은 Embedding B를 통해서 임베딩이 됩니다. 여기서 임베딩이란 문장 내 각 단어가 임베딩이 되어 각 단어가 임베딩 벡터로 변환된 문장을 얻는다는 의미입니다.', \"임베딩이 된 두 개의 문장은 내적(dot product)을 통해 각 단어 간 유사도를 구하고, 이렇게 구해진 유사도는 소프트맥스 함수를 지나서 Embedding A로 임베딩이 된 스토리 문장에 더해집니다. 현재 위 그림에서 '덧셈'이라고 적혀있는 부분까지 연산이 진행된 상태입니다. (Embedding A, B, C는 각각 별 개의 임베딩 층(Embedding layer)입니다.)\\n그런데 지금까지 설명한 연산을 표현을 조금 바꿔서 다시 설명해보겠습니다. 스토리 문장을 Value와 Key라고 하고, 질문 문장을 Query라고 해보겠습니다. Query는 Key와 유사도를 구하고, 소프트맥스 함수를 통해 값을 정규화하여 Value에 더해서 이 유사도값을 반영해줍니다. 어디서 많이 들어본 설명아닌가요? 결국 지금까지의 연산 과정은 어텐션 메커니즘의 의도를 갖고 있습니다.\", \"지금까지 설명한 위 그림에서 '덧셈'이라고 적혀있는 부분까지의 연산은 어텐션 메커니즘을 통해서 질문 문장과의 유사도를 반영한 스토리 문장 표현을 얻기 위한 여정이었습니다. 이 스토리 문장 표현을 질문 문장을 임베딩한 질문 표현과 연결(concatenate)해줍니다. 그리고 이 표현을 LSTM과 밀집층(dense layer)의 입력으로 사용하여 정답을 예측합니다.\"]\n",
      "[\"Babi 데이터셋 전처리를 위해 필요한 도구들을 임포트합니다.\\nfrom tensorflow.keras.utils import get_file\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\nimport numpy as np\\nimport tarfile\\nfrom nltk import FreqDist\\nfrom functools import reduce\\nimport os\\nimport re\\nimport matplotlib.pyplot as plt\\n케라스의 get_file을 통해 데이터셋을 다운로드합니다.\\npath = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/'\\n'babi_tasks_1-20_v1-2.tar.gz')\", '\\'babi_tasks_1-20_v1-2.tar.gz\\')\\n압축을 풀고 훈련 데이터와 테스트 데이터를 각각 얻습니다.\\nwith tarfile.open(path) as tar:\\ntar.extractall()\\ntar.close()\\nDATA_DIR = \\'tasks_1-20_v1-2/en-10k\\'\\nTRAIN_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_train.txt\")\\nTEST_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_test.txt\")\\n훈련 데이터로부터 상위 20개의 라인(line)만 읽고 출력해보겠습니다.\\ni = 0\\nlines = open(TRAIN_FILE , \"rb\")\\nfor line in lines:\\nline = line.decode(\"utf-8\").strip()\\n# lno, text = line.split(\" \", 1) # ID와 TEXT 분리', 'line = line.decode(\"utf-8\").strip()\\n# lno, text = line.split(\" \", 1) # ID와 TEXT 분리\\ni = i + 1\\nprint(line)\\nif i == 20:\\nbreak\\n1 Mary moved to the bathroom.\\n2 John went to the hallway.\\n3 Where is Mary?    bathroom    1\\n4 Daniel went back to the hallway.\\n5 Sandra moved to the garden.\\n6 Where is Daniel?  hallway 4\\n7 John moved to the office.\\n8 Sandra journeyed to the bathroom.\\n9 Where is Daniel?  hallway 4\\n10 Mary moved to the hallway.\\n11 Daniel travelled to the office.', '9 Where is Daniel?  hallway 4\\n10 Mary moved to the hallway.\\n11 Daniel travelled to the office.\\n12 Where is Daniel?     office  11\\n13 John went back to the garden.\\n14 John moved to the bedroom.\\n15 Where is Sandra?     bathroom    8\\n1 Sandra travelled to the office.\\n2 Sandra went to the bathroom.\\n3 Where is Sandra?  bathroom    2\\n4 Mary went to the bedroom.\\n5 Daniel moved to the hallway.', '3 Where is Sandra?  bathroom    2\\n4 Mary went to the bedroom.\\n5 Daniel moved to the hallway.\\n숫자 1부터 15까지 한 개의 스토리입니다. 그 중간 중간에 질문이 나오고 있습니다. 3번, 6번, 9번, 12번, 15번 라인이 각 스토리 중간에 이어지는 질문에 해당되지요. 그리고 그 각각의 질문 옆에는 질문에 해당되는 정답이 적혀져있고, 그 정답 옆에 나오는 번호는 해당 정답이 몇 번 번호의 라인에 있었는지를 알려줍니다.\\n그리고 숫자 1이 다시 나오면 이제부터는 다시 별개의 스토리가 시작됨을 의미합니다. 이렇게 복잡한 형태의 텍스트를 기계에게 바로 학습시키는 것은 조금 까다롭습니다. 그렇기 때문에 전처리를 거쳐서 스토리, 질문, 답변을 전부 별도로 저장해두겠습니다.\\ndef read_data(dir):', 'def read_data(dir):\\nstories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정\\nstory_temp = [] # 현재 시점의 스토리 임시 저장\\nlines = open(dir, \"rb\")\\nfor line in lines:\\nline = line.decode(\"utf-8\") # b\\' 제거\\nline = line.strip() # \\'\\\\n\\' 제거\\nidx, text = line.split(\" \", 1) # 맨 앞에 있는 id number 분리\\n# 여기까지는 모든 줄에 적용되는 전처리\\nif int(idx) == 1:\\nstory_temp = []\\nif \"\\\\t\" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\\nquestion, answer, _ = text.split(\"\\\\t\") # 질문과 답변을 각각 저장', 'question, answer, _ = text.split(\"\\\\t\") # 질문과 답변을 각각 저장\\nstories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장\\nquestions.append(question)\\nanswers.append(answer)\\nelse: # 현재 읽는 줄이 스토리인 경우\\nstory_temp.append(text) # 임시 저장\\nlines.close()\\nreturn stories, questions, answers\\ntrain_data = read_data(TRAIN_FILE)\\ntest_data = read_data(TEST_FILE)\\ntrain_stories, train_questions, train_answers = read_data(TRAIN_FILE)\\ntest_stories, test_questions, test_answers = read_data(TEST_FILE)', \"test_stories, test_questions, test_answers = read_data(TEST_FILE)\\nprint('훈련용 스토리의 개수 :', len(train_stories))\\nprint('훈련용 질문의 개수 :',len(train_questions))\\nprint('훈련용 답변의 개수 :',len(train_answers))\\nprint('테스트용 스토리의 개수 :',len(test_stories))\\nprint('테스트용 질문의 개수 :',len(test_questions))\\nprint('테스트용 답변의 개수 :',len(test_answers))\\n훈련용 스토리의 개수 : 10000\\n훈련용 질문의 개수 : 10000\\n훈련용 답변의 개수 : 10000\\n테스트용 스토리의 개수 : 1000\\n테스트용 질문의 개수 : 1000\\n테스트용 답변의 개수 : 1000\\n임의로 3,576번째 스토리를 출력해봅시다.\\ntrain_stories[3576]\", \"테스트용 질문의 개수 : 1000\\n테스트용 답변의 개수 : 1000\\n임의로 3,576번째 스토리를 출력해봅시다.\\ntrain_stories[3576]\\n['John went back to the garden.',\\n'Mary went to the kitchen.',\\n'Sandra went back to the bedroom.',\\n'John travelled to the bedroom.']\\n임의로 3,576번째 질문을 출력해봅시다.\\ntrain_questions[3576]\\n'Where is John? '\\n'존은 어디야'라는 질문이네요. 위의 스토리에 따르면 존은 bedroom에 있습니다. 3,576번째 답변을 출력해볼까요?\\ntrain_answers[3576]\\n'bedroom'\\nbedroom이 출력됩니다.\\ndef tokenize(sent):\\nreturn [ x.strip() for x in re.split('(\\\\W+)', sent) if x and x.strip()]\", \"def tokenize(sent):\\nreturn [ x.strip() for x in re.split('(\\\\W+)', sent) if x and x.strip()]\\ndef preprocess_data(train_data, test_data):\\ncounter = FreqDist()\\n# 두 문장의 story를 하나의 문장으로 통합하는 함수\\nflatten = lambda data: reduce(lambda x, y: x + y, data)\\n# 각 샘플의 길이를 저장하는 리스트\\nstory_len = []\\nquestion_len = []\\nfor stories, questions, answers in [train_data, test_data]:\\nfor story in stories:\\nstories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\\nstory_len.append(len(stories)) # 각 story의 길이 저장\", 'story_len.append(len(stories)) # 각 story의 길이 저장\\nfor word in stories: # 단어 집합에 단어 추가\\ncounter[word] += 1\\nfor question in questions:\\nquestion = tokenize(question)\\nquestion_len.append(len(question))\\nfor word in question:\\ncounter[word] += 1\\nfor answer in answers:\\nanswer = tokenize(answer)\\nfor word in answer:\\ncounter[word] += 1\\n# 단어 집합 생성\\nword2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\\nidx2word = {idx : word for word, idx in word2idx.items()}\\n# 가장 긴 샘플의 길이', 'idx2word = {idx : word for word, idx in word2idx.items()}\\n# 가장 긴 샘플의 길이\\nstory_max_len = np.max(story_len)\\nquestion_max_len = np.max(question_len)\\nreturn word2idx, idx2word, story_max_len, question_max_len\\nword2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)\\nprint(word2idx)', \"print(word2idx)\\n{'to': 1, 'the': 2, '.': 3, 'went': 4, 'Sandra': 5, 'John': 6, 'Daniel': 7, 'Mary': 8, 'travelled': 9, 'journeyed': 10, 'back': 11, 'bathroom': 12, 'garden': 13, 'hallway': 14, 'moved': 15, 'office': 16, 'kitchen': 17, 'bedroom': 18, 'Where': 19, 'is': 20, '?': 21}\\nvocab_size = len(word2idx) + 1\\nprint('스토리의 최대 길이 :',story_max_len)\\nprint('질문의 최대 길이 :',question_max_len)\\n스토리의 최대 길이 : 68\\n질문의 최대 길이 : 4\\ndef vectorize(data, word2idx, story_maxlen, question_maxlen):\", '스토리의 최대 길이 : 68\\n질문의 최대 길이 : 4\\ndef vectorize(data, word2idx, story_maxlen, question_maxlen):\\nXs, Xq, Y = [], [], []\\nflatten = lambda data: reduce(lambda x, y: x + y, data)\\nstories, questions, answers = data\\nfor story, question, answer in zip(stories, questions, answers):\\nxs = [word2idx[w] for w in tokenize(flatten(story))]\\nxq = [word2idx[w] for w in tokenize(question)]\\nXs.append(xs)\\nXq.append(xq)\\nY.append(word2idx[answer])\\n# 스토리와 질문은 각각의 최대 길이로 패딩\\n# 정답은 원-핫 인코딩', 'Xs.append(xs)\\nXq.append(xq)\\nY.append(word2idx[answer])\\n# 스토리와 질문은 각각의 최대 길이로 패딩\\n# 정답은 원-핫 인코딩\\nreturn pad_sequences(Xs, maxlen=story_maxlen),\\\\\\npad_sequences(Xq, maxlen=question_maxlen),\\\\\\nto_categorical(Y, num_classes=len(word2idx) + 1)\\nXstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\\nXstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)', 'Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)\\nprint(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)\\n(10000, 68) (10000, 4) (10000, 22) (1000, 68) (1000, 4) (1000, 22)']\n",
      "['from tensorflow.keras.models import Sequential, Model\\nfrom tensorflow.keras.layers import Embedding\\nfrom tensorflow.keras.layers import Permute, dot, add, concatenate\\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation\\n# 에포크 횟수\\ntrain_epochs = 120\\n# 배치 크기\\nbatch_size = 32\\n# 임베딩 크기\\nembed_size = 50\\n# LSTM의 크기\\nlstm_size = 64\\n# 과적합 방지 기법인 드롭아웃 적용 비율\\ndropout_rate = 0.30\\n# 플레이스 홀더. 입력을 담는 변수\\ninput_sequence = Input((story_max_len,))\\nquestion = Input((question_max_len,))', '# 플레이스 홀더. 입력을 담는 변수\\ninput_sequence = Input((story_max_len,))\\nquestion = Input((question_max_len,))\\nprint(\\'Stories :\\', input_sequence)\\nprint(\\'Question:\\', question)\\nStories : Tensor(\"input_1:0\", shape=(None, 68), dtype=float32)\\nQuestion: Tensor(\"input_2:0\", shape=(None, 4), dtype=float32)\\n# 스토리를 위한 첫번째 임베딩. 그림에서의 Embedding A\\ninput_encoder_m = Sequential()\\ninput_encoder_m.add(Embedding(input_dim=vocab_size,\\noutput_dim=embed_size))\\ninput_encoder_m.add(Dropout(dropout_rate))', 'output_dim=embed_size))\\ninput_encoder_m.add(Dropout(dropout_rate))\\n# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\\n# 스토리를 위한 두번째 임베딩. 그림에서의 Embedding C\\n# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\\ninput_encoder_c = Sequential()\\ninput_encoder_c.add(Embedding(input_dim=vocab_size,\\noutput_dim=question_max_len))\\ninput_encoder_c.add(Dropout(dropout_rate))\\n# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)', '# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)\\n# 질문을 위한 임베딩. 그림에서의 Embedding B\\nquestion_encoder = Sequential()\\nquestion_encoder.add(Embedding(input_dim=vocab_size,\\noutput_dim=embed_size,\\ninput_length=question_max_len))\\nquestion_encoder.add(Dropout(dropout_rate))\\n# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원\\n# 실질적인 임베딩 과정\\ninput_encoded_m = input_encoder_m(input_sequence)', \"# 실질적인 임베딩 과정\\ninput_encoded_m = input_encoder_m(input_sequence)\\ninput_encoded_c = input_encoder_c(input_sequence)\\nquestion_encoded = question_encoder(question)\\nprint('Input encoded m', input_encoded_m)\\nprint('Input encoded c', input_encoded_c)\\nprint('Question encoded', question_encoded)\\n# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\\n# 유사도는 내적을 사용한다.\\nmatch = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\\nmatch = Activation('softmax')(match)\\nprint('Match shape', match)\", \"match = Activation('softmax')(match)\\nprint('Match shape', match)\\n# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이\\nresponse = add([match, input_encoded_c])  # (samples, story_max_len, question_max_len)\\nresponse = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\\nprint('Response shape', response)\\n# 질문 벡터와 답변 벡터를 연결\\nanswer = concatenate([response, question_encoded])\\nprint('Answer shape', answer)\\nanswer = LSTM(lstm_size)(answer)\", 'print(\\'Answer shape\\', answer)\\nanswer = LSTM(lstm_size)(answer)\\nanswer = Dropout(dropout_rate)(answer)\\nanswer = Dense(vocab_size)(answer)\\nanswer = Activation(\\'softmax\\')(answer)\\nMatch shape Tensor(\"activation/Identity:0\", shape=(None, 68, 4), dtype=float32)\\nResponse shape Tensor(\"permute/Identity:0\", shape=(None, 4, 68), dtype=float32)\\nAnswer shape Tensor(\"concatenate/Identity:0\", shape=(None, 4, 118), dtype=float32)\\nMatch shape Tensor(\"activation/Identity:0\", shape=(None, 68, 4), dtype=float32)', 'Match shape Tensor(\"activation/Identity:0\", shape=(None, 68, 4), dtype=float32)\\nResponse shape Tensor(\"permute/Identity:0\", shape=(None, 4, 68), dtype=float32)\\nAnswer shape Tensor(\"concatenate/Identity:0\", shape=(None, 4, 118), dtype=float32)\\nmodel = Model([input_sequence, question], answer)\\nmodel.compile(optimizer=\\'rmsprop\\', loss=\\'categorical_crossentropy\\',\\nmetrics=[\\'acc\\'])\\nprint(model.summary())\\nhistory = model.fit([Xstrain, Xqtrain],\\nYtrain, batch_size, train_epochs,', 'print(model.summary())\\nhistory = model.fit([Xstrain, Xqtrain],\\nYtrain, batch_size, train_epochs,\\nvalidation_data=([Xstest, Xqtest], Ytest))\\nmodel.save(\\'model.h5\\')\\nprint(\"\\\\n 테스트 정확도: %.4f\" % (model.evaluate([Xstest, Xqtest], Ytest)[1]))\\n테스트 정확도: 0.9620\\nplt.subplot(211)\\nplt.title(\"Accuracy\")\\nplt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\\nplt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\\nplt.legend(loc=\"best\")\\nplt.subplot(212)\\nplt.title(\"Loss\")', 'plt.legend(loc=\"best\")\\nplt.subplot(212)\\nplt.title(\"Loss\")\\nplt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\\nplt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\\nplt.legend(loc=\"best\")\\nplt.tight_layout()\\nplt.show()\\nytest = np.argmax(Ytest, axis=1)\\nYtest_ = model.predict([Xstest, Xqtest])\\nytest_ = np.argmax(Ytest_, axis=1)\\n==================================================\\n--- 23-02 MemN으로 한국어 QA 해보기 ---\\n```\\n질문                |실제값  |예측값', '--- 23-02 MemN으로 한국어 QA 해보기 ---\\n```\\n질문                |실제값  |예측값\\n---------------------------------------\\n은경이 는 어디 야 ?        : 복도      복도\\n필웅이 는 어디 야 ?        : 화장실     화장실\\n경임이 는 어디 야 ?        : 부엌      부엌\\n경임이 는 어디 야 ?        : 복도      복도\\n경임이 는 어디 야 ?        : 부엌      부엌\\n경임이 는 어디 야 ?        : 복도      복도\\n경임이 는 어디 야 ?        : 정원      정원\\n수종이 는 어디 야 ?        : 복도      복도\\n경임이 는 어디 야 ?        : 사무실     복도\\n수종이 는 어디 야 ?        : 사무실     복도\\n필웅이 는 어디 야 ?        : 부엌      부엌', '경임이 는 어디 야 ?        : 사무실     복도\\n수종이 는 어디 야 ?        : 사무실     복도\\n필웅이 는 어디 야 ?        : 부엌      부엌\\n필웅이 는 어디 야 ?        : 정원      정원\\n수종이 는 어디 야 ?        : 사무실     사무실\\n필웅이 는 어디 야 ?        : 침실      침실\\n필웅이 는 어디 야 ?        : 침실      침실\\n은경이 는 어디 야 ?        : 부엌      부엌\\n은경이 는 어디 야 ?        : 정원      정원\\n은경이 는 어디 야 ?        : 부엌      부엌\\n수종이 는 어디 야 ?        : 사무실     부엌\\n은경이 는 어디 야 ?        : 부엌      정원\\n필웅이 는 어디 야 ?        : 복도      복도\\n은경이 는 어디 야 ?        : 사무실     사무실\\n은경이 는 어디 야 ?        : 사무실     사무실', '은경이 는 어디 야 ?        : 사무실     사무실\\n은경이 는 어디 야 ?        : 사무실     사무실\\n경임이 는 어디 야 ?        : 복도      사무실\\n수종이 는 어디 야 ?        : 침실      침실\\n경임이 는 어디 야 ?        : 침실      침실\\n필웅이 는 어디 야 ?        : 침실      침실\\n수종이 는 어디 야 ?        : 부엌      부엌\\n수종이 는 어디 야 ?        : 부엌      부엌\\n수종이 는 어디 야 ?        : 부엌      복도\\n```이번 챕터에서는 한국어 Babi 데이터셋에 대해서 이전 챕터에서 구현한 메모리 네트워크를 테스트해보겠습니다.\\n질의 응답(Question Answering)의 다른 풀이법이 궁금하시면 18챕터의 KoBERT를 이용한 기계 독해 챕터를 참고하세요.']\n",
      "[\"영어권 언어는 띄어쓰기만해도 단어들이 잘 분리되지만, 한국어는 그렇지 않다고 앞에서 몇 차례 언급했었습니다. 한국어 데이터를 사용하여 모델을 구현하는 것만큼 이번에는 형태소 분석기를 사용해서 단어 토큰화를 해보겠습니다. 그런데 형태소 분석기를 사용할 때, 이런 상황에 봉착한다면 어떻게 해야할까요?\\n형태소 분석 입력 : '은경이는 사무실로 갔습니다.'\\n형태소 분석 결과 : ['은', '경이', '는', '사무실', '로', '갔습니다', '.']\\n사실 위 문장에서 '은경이'는 사람 이름이므로 제대로 된 결과를 얻기 위해서는 '은', '경이'와 같이 글자가 분리되는 것이 아니라 '은경이' 또는 최소한 '은경'이라는 단어 토큰을 얻어야만 합니다. 이런 경우에는 형태소 분석기에 사용자 사전을 추가해줄 수 있습니다. '은경이'는 하나의 단어이기 때문에 분리하지말라고 형태소 분석기에 알려주는 것입니다.\", \"사용자 사전을 추가하는 방법은 형태소 분석기마다 다른데, 생각보다 복잡한 경우들이 많습니다. 이번 실습에서는 Customized Konlpy라는 사용자 사전 추가가 매우 쉬운 패키지를 사용합니다. 프롬프트에서 아래의 커맨드를 통해 형태소 분석기를 설치합니다.\\npip install customized_konlpy\\n이제 customized_konlpy에서 제공하는 형태소 분석기 Twitter를 사용하여 앞서 소개했던 예문을 단어 토큰화해봅시다.\\nfrom ckonlpy.tag import Twitter\\ntwitter = Twitter()\\ntwitter.morphs('은경이는 사무실로 갔습니다.')\\n['은', '경이', '는', '사무실', '로', '갔습니다', '.']\", \"twitter = Twitter()\\ntwitter.morphs('은경이는 사무실로 갔습니다.')\\n['은', '경이', '는', '사무실', '로', '갔습니다', '.']\\n앞서 소개한 예시와 마찬가지로 '은경이'라는 단어가 '은', '경이'와 같이 분리됩니다. 이때, 형태소 분석기 Twitter에 add_dictionary('단어', '품사')와 같은 형식으로 사전 추가를 해줄 수 있습니다.\\ntwitter.add_dictionary('은경이', 'Noun')\\n제대로 반영되었는지 동일한 예문을 다시 형태소 분석해봅시다.\\ntwitter.morphs('은경이는 사무실로 갔습니다.')\\n['은경이', '는', '사무실', '로', '갔습니다', '.']\\n이제는 '은경이'라는 단어가 제대로 하나의 토큰으로 인식되는 것을 확인할 수 있습니다.\"]\n",
      "['한국어 Babi 데이터셋은 저자가 영어 데이터셋을 참고하여 만들었으며, 아래의 링크에서 다운로드 할 수 있습니다.\\n훈련 데이터 : https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/refs/heads/main/21.%20Memory%20Network/dataset/qa1_single-supporting-fact_train_kor.txt\\n테스트 데이터 : https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/refs/heads/main/21.%20Memory%20Network/dataset/qa1_single-supporting-fact_test_kor.txt\\n우선 필요한 도구들을 임포트합니다.\\nfrom ckonlpy.tag import Twitter\\nfrom tensorflow.keras.utils import get_file', '우선 필요한 도구들을 임포트합니다.\\nfrom ckonlpy.tag import Twitter\\nfrom tensorflow.keras.utils import get_file\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.utils import to_categorical\\nimport numpy as np\\nfrom nltk import FreqDist\\nfrom functools import reduce\\nimport os\\nimport re\\nimport matplotlib.pyplot as plt\\n다운로드한 데이터를 로드합니다.\\nTRAIN_FILE = os.path.join(\"qa1_single-supporting-fact_train_kor.txt\")\\nTEST_FILE = os.path.join(\"qa1_single-supporting-fact_test_kor.txt\")', 'TEST_FILE = os.path.join(\"qa1_single-supporting-fact_test_kor.txt\")\\n훈련 데이터로부터 상위 20개의 문장을 출력합니다.\\ni = 0\\nlines = open(TRAIN_FILE , \"rb\")\\nfor line in lines:\\nline = line.decode(\"utf-8\").strip()\\ni = i + 1\\nprint(line)\\nif i == 20:\\nbreak\\n1 필웅이는 화장실로 갔습니다.\\n2 은경이는 복도로 이동했습니다.\\n3 필웅이는 어디야?     화장실 1\\n4 수종이는 복도로 복귀했습니다.\\n5 경임이는 정원으로 갔습니다.\\n6 수종이는 어디야?     복도  4\\n7 은경이는 사무실로 갔습니다.\\n8 경임이는 화장실로 뛰어갔습니다.\\n9 수종이는 어디야?     복도  4\\n10 필웅이는 복도로 갔습니다.\\n11 수종이는 사무실로 가버렸습니다.\\n12 수종이는 어디야?    사무실 11\\n13 은경이는 정원으로 복귀했습니다.', '10 필웅이는 복도로 갔습니다.\\n11 수종이는 사무실로 가버렸습니다.\\n12 수종이는 어디야?    사무실 11\\n13 은경이는 정원으로 복귀했습니다.\\n14 은경이는 침실로 갔습니다.\\n15 경임이는 어디야?    화장실 8\\n1 경임이는 사무실로 가버렸습니다.\\n2 경임이는 화장실로 이동했습니다.\\n3 경임이는 어디야?     화장실 2\\n4 필웅이는 침실로 이동했습니다.\\n5 수종이는 복도로 갔습니다.\\n영어 Babi 데이터셋과 형식이 같은 것을 확인할 수 있습니다. read_data() 함수를 사용하여 훈련 데이터와 테스트 데이터를 로드합니다.\\ndef read_data(dir):\\n// 영어 데이터셋에 사용했던 이전 실습의 read_data() 함수와 동일한 함수입니다.\\nread_data() 함수는 영어 데이터셋을 사용한 이전 실습에서 사용했던 함수와 동일합니다.\\ntrain_data = read_data(TRAIN_FILE)\\ntest_data = read_data(TEST_FILE)', \"train_data = read_data(TRAIN_FILE)\\ntest_data = read_data(TEST_FILE)\\n훈련 데이터와 테스트 데이터가 로드되었습니다. 데이터의 구성을 확인하기 위해서 스토리, 질문, 답변을 각각 분리해서 로드해봅시다.\\ntrain_stories, train_questions, train_answers = read_data(TRAIN_FILE)\\ntest_stories, test_questions, test_answers = read_data(TEST_FILE)\\nprint('훈련용 스토리의 개수 :', len(train_stories))\\nprint('훈련용 질문의 개수 :',len(train_questions))\\nprint('훈련용 답변의 개수 :',len(train_answers))\\nprint('테스트용 스토리의 개수 :',len(test_stories))\\nprint('테스트용 질문의 개수 :',len(test_questions))\", \"print('테스트용 스토리의 개수 :',len(test_stories))\\nprint('테스트용 질문의 개수 :',len(test_questions))\\nprint('테스트용 답변의 개수 :',len(test_answers))\\n훈련용 스토리의 개수 : 10000\\n훈련용 질문의 개수 : 10000\\n훈련용 답변의 개수 : 10000\\n테스트용 스토리의 개수 : 1000\\n테스트용 질문의 개수 : 1000\\n테스트용 답변의 개수 : 1000\\n임의로 3573번째 스토리, 질문, 답변을 출력해보겠습니다.\\ntrain_stories[3572]\\n['은경이는 부엌으로 가버렸습니다.',\\n'필웅이는 사무실로 가버렸습니다.',\\n'수종이는 복도로 뛰어갔습니다.',\\n'은경이는 사무실로 복귀했습니다.',\\n'경임이는 사무실로 이동했습니다.',\\n'경임이는 침실로 갔습니다.']\\ntrain_questions[3572]\\n은경이는 어디야?\\ntrain_answers[3572]\\n사무실\", \"'경임이는 사무실로 이동했습니다.',\\n'경임이는 침실로 갔습니다.']\\ntrain_questions[3572]\\n은경이는 어디야?\\ntrain_answers[3572]\\n사무실\\n이제 토큰화 함수를 정의하고, 이로부터 Vocabulary를 생성하는 함수를 만들어봅시다. 아래의 함수는 이전 챕터에서 영어 데이터셋에 사용했던 토큰화 함수와 동일합니다. 현재는 한국어이므로 아래의 토큰화 함수를 그대로 사용하는 것은 바람직하지는 않지만, 임시로 사용해보겠습니다. 어절 단위로 했을 때 어떤 단어들이 있는지 출력해보기 위함입니다.\\ndef tokenize(sent):\\nreturn [ x.strip() for x in re.split('(\\\\W+)?', sent) if x.strip()]\\n이제 Vocabulary를 만드는 함수를 정의합니다.\\ndef preprocess_data(train_data, test_data):\", '이제 Vocabulary를 만드는 함수를 정의합니다.\\ndef preprocess_data(train_data, test_data):\\n// 영어 데이터셋에 사용했던 이전 챕터의 preprocess_data 함수와 동일한 함수입니다.\\n해당 함수로부터 word2idx, idx2word, story_max_len, question_max_len를 리턴받겠습니다.\\nword2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)\\nword2idx를 출력해봅시다.\\nprint(word2idx)', \"word2idx를 출력해봅시다.\\nprint(word2idx)\\n{'.': 1, '경임이는': 2, '은경이는': 3, '수종이는': 4, '필웅이는': 5, '이동했습니다': 6, '가버렸습니다': 7, '뛰어갔습니다': 8, '복귀했습니다': 9, '갔습니다': 10, '화장실로': 11, '정원으로': 12, '복도로': 13, '어디야': 14, '?': 15, '부엌으로': 16, '사무실로': 17, '침실로': 18, '화장실': 19, '정원': 20, '사무실': 21, '침실': 22, '복도': 23, '부엌': 24}\", \"띄어쓰기 단위, 다시 말해 어절 단위로 했을 때 나오는 총 토큰의 수는 24개입니다. 19번 토큰부터 24번 토큰까지를 봤을 때 장소에 해당되는 명사들은 '화장실', '정원', '사무실', '침실', '복도', '부엌'이 있는 것 같습니다. 그렇다면, 11번 토큰부터 19번 토큰 사이에 등장하는 '화장실로', '정원으로', '복도로', '부엌으로', '사무실로', '침실로'로 분리된 토큰들은 형태소 분석을 하였을 때 전부  '화장실', '정원', '사무실', '침실', '복도', '부엌'으로 분리되어야 합니다.\\n사람의 이름은 2번 토큰부터 5번 토큰을 참고하였을 때, '경임이는', '은경이는', '수종이는', '필웅이는' 이렇게 4개의 토큰이 존재하는 것 같습니다. 그렇다면 적어도 형태소 분석기를 하였을 때 사람의 이름이 제대로 분리되는 것을 목표로 해야합니다. 만약, '은경이는'을 형태소 분석하였을 때 '은', '경이', '는'과 같이 사람의 이름이 분리되서는 안 됩니다.\", \"이제 여기서 해야할 일은 형태소 분석기를 실제로 사용해서 우리가 원하는 결과가 정상적으로 출력되는지 확인하고, 그렇지 않다면 이에 대한 조치를 취해주는 것입니다. 형태소 분석기 Twitter()를 사용해봅시다.\\ntwitter = Twitter()\\n경우의 수를 조합하여 사람 이름, 장소, 동사가 최소한 한 번씩 등장하도록 다음의 여섯 문장을 만들어보았습니다. 사람 이름은 4개, 동사가 5개, 장소가 6개로 장소의 개수가 제일 많기 때문에 총 여섯 문장이 나왔습니다. 각각 형태소 분석하여 우리가 의도하는대로 결과가 나오는지 확인해보았습니다.\\nprint(twitter.morphs('은경이는 화장실로 이동했습니다.'))\\nprint(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\\nprint(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\\nprint(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\", \"print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\\nprint(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\\nprint(twitter.morphs('수종이는 사무실로 갔습니다.'))\\nprint(twitter.morphs('은경이는 침실로 갔습니다.'))\\n['은', '경이', '는', '화장실', '로', '이동', '했습니다', '.']\\n['경', '임', '이', '는', '정원', '으로', '가버렸습니다', '.']\\n['수종', '이', '는', '복도', '로', '뛰어갔습니다', '.']\\n['필웅이', '는', '부엌', '으로', '복귀', '했습니다', '.']\\n['수종', '이', '는', '사무실', '로', '갔습니다', '.']\\n['은', '경이', '는', '침실', '로', '갔습니다', '.']\", \"['수종', '이', '는', '사무실', '로', '갔습니다', '.']\\n['은', '경이', '는', '침실', '로', '갔습니다', '.']\\n화장실, 정원, 복도, 부엌, 사무실, 침실은 의도한대로 모두 조사 '로'와 분리되어 제대로 된 명사의 형태를 갖춥니다. 문제는 사람의 이름을 유지하지 못하고 분리되는 경우인데, '필웅이'를 제외하고 모두 '--이' 형태로 분리할 수 있도록 사전을 추가해주겠습니다.\\ntwitter.add_dictionary('은경이', 'Noun')\\ntwitter.add_dictionary('경임이', 'Noun')\\ntwitter.add_dictionary('수종이', 'Noun')\\n사전 추가 후에는 이름이 제대로 분리되는 것을 확인할 수 있습니다.\\nprint(twitter.morphs('은경이는 화장실로 이동했습니다.'))\\nprint(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\", \"print(twitter.morphs('은경이는 화장실로 이동했습니다.'))\\nprint(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\\nprint(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\\nprint(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\\nprint(twitter.morphs('수종이는 사무실로 갔습니다.'))\\nprint(twitter.morphs('은경이는 침실로 갔습니다.'))\\n['은경이', '는', '화장실', '로', '이동', '했습니다', '.']\\n['경임이', '는', '정원', '으로', '가버렸습니다', '.']\\n['수종이', '는', '복도', '로', '뛰어갔습니다', '.']\\n['필웅이', '는', '부엌', '으로', '복귀', '했습니다', '.']\\n['수종이', '는', '사무실', '로', '갔습니다', '.']\\n['은경이', '는', '침실', '로', '갔습니다', '.']\", \"['수종이', '는', '사무실', '로', '갔습니다', '.']\\n['은경이', '는', '침실', '로', '갔습니다', '.']\\n이제 이를 새로운 토큰화 함수로 정의합니다.\\ndef tokenize(sent):\\nreturn twitter.morphs(sent)\\nword2idx, idx2word, story_max_len, question_max_len를 리턴받겠습니다. 형태소 분석기를 토큰화 함수를 변경하였기 때문에 이전보다는 시간이 좀 더 소요됩니다.\\nword2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)\\n새로 생성된 word2idx를 출력해봅시다.\\nprint(word2idx)\", \"새로 생성된 word2idx를 출력해봅시다.\\nprint(word2idx)\\n{'는': 1, '.': 2, '로': 3, '했습니다': 4, '으로': 5, '경임이': 6, '은경이': 7, '수종이': 8, '필웅이': 9, '이동': 10, '가버렸습니다': 11, '뛰어갔습니다': 12, '복귀': 13, '화장실': 14, '정원': 15, '복도': 16, '갔습니다': 17, '사무실': 18, '부엌': 19, '침실': 20, '어디': 21, '야': 22, '?': 23}\\n단어 집합의 크기를 정의합니다.\\nvocab_size = len(word2idx) + 1\\nprint(vocab_size)\\n24\\n스토리와 질문의 최대 길이를 출력해봅시다.\\nprint('스토리의 최대 길이 :',story_max_len)\\nprint('질문의 최대 길이 :',question_max_len)\\n스토리의 최대 길이 : 70\\n질문의 최대 길이 : 5\", \"print('질문의 최대 길이 :',question_max_len)\\n스토리의 최대 길이 : 70\\n질문의 최대 길이 : 5\\ndef vectorize(data, word2idx, story_maxlen, question_maxlen):\\n// 영어 데이터셋에 사용했던 이전 챕터의 vectorize함수와 동일한 함수입니다.\\nXstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\\nXstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)\\nprint(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)\", 'print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)\\n(10000, 70) (10000, 5) (10000, 24) (1000, 70) (1000, 5) (1000, 24)']\n",
      "['from tensorflow.keras.models import Sequential, Model\\nfrom tensorflow.keras.layers import Embedding\\nfrom tensorflow.keras.layers import Permute, dot, add, concatenate\\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation\\n# 에포크 횟수\\ntrain_epochs = 120\\n# 배치 크기\\nbatch_size = 32\\n# 임베딩 크기\\nembed_size = 50\\n# LSTM의 크기\\nlstm_size = 64\\n# 과적합 방지 기법인 드롭아웃 적용 비율\\ndropout_rate = 0.30\\n# 플레이스 홀더. 입력을 담는 변수\\ninput_sequence = Input((story_max_len,))\\nquestion = Input((question_max_len,))', '# 플레이스 홀더. 입력을 담는 변수\\ninput_sequence = Input((story_max_len,))\\nquestion = Input((question_max_len,))\\nprint(\\'Stories :\\', input_sequence)\\nprint(\\'Question:\\', question)\\nStories : Tensor(\"input_1:0\", shape=(None, 70), dtype=float32)\\nQuestion: Tensor(\"input_2:0\", shape=(None, 5), dtype=float32)\\n# 스토리를 위한 첫번째 임베딩. 그림에서의 Embedding A\\ninput_encoder_m = Sequential()\\ninput_encoder_m.add(Embedding(input_dim=vocab_size,\\noutput_dim=embed_size))\\ninput_encoder_m.add(Dropout(dropout_rate))', 'output_dim=embed_size))\\ninput_encoder_m.add(Dropout(dropout_rate))\\n# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\\n# 스토리를 위한 두번째 임베딩. 그림에서의 Embedding C\\n# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\\ninput_encoder_c = Sequential()\\ninput_encoder_c.add(Embedding(input_dim=vocab_size,\\noutput_dim=question_max_len))\\ninput_encoder_c.add(Dropout(dropout_rate))\\n# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)', '# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)\\n# 질문을 위한 임베딩. 그림에서의 Embedding B\\nquestion_encoder = Sequential()\\nquestion_encoder.add(Embedding(input_dim=vocab_size,\\noutput_dim=embed_size,\\ninput_length=question_max_len))\\nquestion_encoder.add(Dropout(dropout_rate))\\n# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원\\n# 실질적인 임베딩 과정\\ninput_encoded_m = input_encoder_m(input_sequence)', '# 실질적인 임베딩 과정\\ninput_encoded_m = input_encoder_m(input_sequence)\\ninput_encoded_c = input_encoder_c(input_sequence)\\nquestion_encoded = question_encoder(question)\\nprint(\\'Input encoded m\\', input_encoded_m)\\nprint(\\'Input encoded c\\', input_encoded_c)\\nprint(\\'Question encoded\\', question_encoded)\\nInput encoded m Tensor(\"sequential/Identity:0\", shape=(None, 70, 50), dtype=float32)\\nInput encoded c Tensor(\"sequential_1/Identity:0\", shape=(None, 70, 5), dtype=float32)', 'Input encoded c Tensor(\"sequential_1/Identity:0\", shape=(None, 70, 5), dtype=float32)\\nQuestion encoded Tensor(\"sequential_2/Identity:0\", shape=(None, 5, 50), dtype=float32)\\n# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\\n# 유사도는 내적을 사용한다.\\nmatch = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\\nmatch = Activation(\\'softmax\\')(match)\\nprint(\\'Match shape\\', match)\\n# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이', \"# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이\\nresponse = add([match, input_encoded_c])  # (samples, story_max_len, question_max_len)\\nresponse = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\\nprint('Response shape', response)\\nanswer = concatenate([response, question_encoded])\\nprint('Answer shape', answer)\\nanswer = LSTM(lstm_size)(answer)\\nanswer = Dropout(dropout_rate)(answer)\\nanswer = Dense(vocab_size)(answer)\", 'answer = Dropout(dropout_rate)(answer)\\nanswer = Dense(vocab_size)(answer)\\nanswer = Activation(\\'softmax\\')(answer)\\nMatch shape Tensor(\"activation/Identity:0\", shape=(None, 70, 5), dtype=float32)\\nResponse shape Tensor(\"permute/Identity:0\", shape=(None, 5, 70), dtype=float32)\\nAnswer shape Tensor(\"concatenate/Identity:0\", shape=(None, 5, 120), dtype=float32)\\nmodel = Model([input_sequence, question], answer)\\nmodel.compile(optimizer=\\'rmsprop\\', loss=\\'categorical_crossentropy\\',\\nmetrics=[\\'acc\\'])', \"model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit([Xstrain, Xqtrain],\\nYtrain, batch_size, train_epochs,\\nvalidation_data=([Xstest, Xqtest], Ytest))\\nmodel.save('model.h5')\\nEpoch 1/120\\n313/313 [==============================] - 4s 13ms/step - loss: 1.8982 - acc: 0.1693 - val_loss: 1.7868 - val_acc: 0.2470\\n... 중략 ...\\nEpoch 120/120\", '... 중략 ...\\nEpoch 120/120\\n313/313 [==============================] - 3s 10ms/step - loss: 0.0397 - acc: 0.9883 - val_loss: 0.5235 - val_acc: 0.9060\\nplt.subplot(211)\\nplt.title(\"Accuracy\")\\nplt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\\nplt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\\nplt.legend(loc=\"best\")\\nplt.subplot(212)\\nplt.title(\"Loss\")\\nplt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\\nplt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")', 'plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\\nplt.legend(loc=\"best\")\\nplt.tight_layout()\\nplt.show()\\nytest = np.argmax(Ytest, axis=1)\\nYtest_ = model.predict([Xstest, Xqtest])\\nytest_ = np.argmax(Ytest_, axis=1)\\n[이미지: ]\\nNUM_DISPLAY = 30\\nprint(\"{:18}|{:5}|{}\".format(\"질문\", \"실제값\", \"예측값\"))\\nprint(39 * \"-\")\\nfor i in range(NUM_DISPLAY):\\nquestion = \" \".join([idx2word[x] for x in Xqtest[i].tolist()])\\nlabel = idx2word[ytest[i]]\\nprediction = idx2word[ytest_[i]]', 'label = idx2word[ytest[i]]\\nprediction = idx2word[ytest_[i]]\\nprint(\"{:20}: {:7} {}\".format(question, label, prediction))\\n질문                |실제값  |예측값\\n---------------------------------------\\n은경이 는 어디 야 ?        : 복도      복도\\n필웅이 는 어디 야 ?        : 화장실     화장실\\n경임이 는 어디 야 ?        : 부엌      부엌\\n경임이 는 어디 야 ?        : 복도      복도\\n경임이 는 어디 야 ?        : 부엌      부엌\\n경임이 는 어디 야 ?        : 복도      복도\\n경임이 는 어디 야 ?        : 정원      정원\\n수종이 는 어디 야 ?        : 복도      복도\\n경임이 는 어디 야 ?        : 사무실     복도', '경임이 는 어디 야 ?        : 정원      정원\\n수종이 는 어디 야 ?        : 복도      복도\\n경임이 는 어디 야 ?        : 사무실     복도\\n수종이 는 어디 야 ?        : 사무실     복도\\n필웅이 는 어디 야 ?        : 부엌      부엌\\n필웅이 는 어디 야 ?        : 정원      정원\\n수종이 는 어디 야 ?        : 사무실     사무실\\n필웅이 는 어디 야 ?        : 침실      침실\\n필웅이 는 어디 야 ?        : 침실      침실\\n은경이 는 어디 야 ?        : 부엌      부엌\\n은경이 는 어디 야 ?        : 정원      정원\\n은경이 는 어디 야 ?        : 부엌      부엌\\n수종이 는 어디 야 ?        : 사무실     부엌\\n은경이 는 어디 야 ?        : 부엌      정원\\n필웅이 는 어디 야 ?        : 복도      복도', '수종이 는 어디 야 ?        : 사무실     부엌\\n은경이 는 어디 야 ?        : 부엌      정원\\n필웅이 는 어디 야 ?        : 복도      복도\\n은경이 는 어디 야 ?        : 사무실     사무실\\n은경이 는 어디 야 ?        : 사무실     사무실\\n경임이 는 어디 야 ?        : 복도      사무실\\n수종이 는 어디 야 ?        : 침실      침실\\n경임이 는 어디 야 ?        : 침실      침실\\n필웅이 는 어디 야 ?        : 침실      침실\\n수종이 는 어디 야 ?        : 부엌      부엌\\n수종이 는 어디 야 ?        : 부엌      부엌\\n수종이 는 어디 야 ?        : 부엌      복도\\n==================================================\\n--- 24. 교육 문의 ---', '==================================================\\n--- 24. 교육 문의 ---\\n마지막 편집일시 : 2024년 8월 26일 12:18 오전\\n==================================================']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 초기화\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # 청크 크기 설정\n",
    "    chunk_overlap=100,  # 청크 중복 설정\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "for doc in first_docs :\n",
    "    splits = splitter.split_text(doc['content'])\n",
    "    print(splits)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
