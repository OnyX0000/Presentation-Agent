{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_chapter_meta(chapter_num: str, chapter_title: str) -> dict:\n",
    "    \"\"\"\n",
    "    chapter_numì´ '01' â†’ level 1, parent ì—†ìŒ\n",
    "    chapter_numì´ '01-02' â†’ level 2, parent '01'\n",
    "    \"\"\"\n",
    "    if \"-\" in chapter_num:\n",
    "        parent = chapter_num.split(\"-\")[0]\n",
    "        level = 2\n",
    "    else:\n",
    "        parent = None\n",
    "        level = 1\n",
    "    return {\n",
    "        \"chapter\": chapter_num,\n",
    "        \"title\": chapter_title,\n",
    "        \"level\": level,\n",
    "        \"parent\": parent\n",
    "    }\n",
    "\n",
    "def split_by_chapter(text: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Wikidocs í˜•ì‹ í…ìŠ¤íŠ¸ë¥¼ ì±•í„°ë³„ë¡œ ë‚˜ëˆ„ê³  ê³„ì¸µ ì •ë³´ë¥¼ í¬í•¨í•œ Documentë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    pattern = r\"(?=^---\\s+(\\d{2}(?:-\\d{2})?)\\.\\s+(.*?)\\s+---)\"\n",
    "    matches = list(re.finditer(pattern, text, flags=re.MULTILINE))\n",
    "\n",
    "    documents = []\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "        chapter_num = match.group(1)\n",
    "        chapter_title = match.group(2).strip()\n",
    "        chapter_text = text[start:end].strip()\n",
    "\n",
    "        metadata = extract_chapter_meta(chapter_num, chapter_title)\n",
    "\n",
    "        doc = Document(page_content=chapter_text, metadata=metadata)\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def load_and_split_wikidocs(path: str) -> List[Document]:\n",
    "    text = load_txt(path)\n",
    "    return split_by_chapter(text)\n",
    "\n",
    "def filter_chapters(\n",
    "    documents: List[Document],\n",
    "    level: Optional[int] = None,\n",
    "    parent: Optional[str] = None,\n",
    "    contains_title: Optional[str] = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    ì±•í„° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¡°ê±´ì— ë§ëŠ” ì±•í„°ë§Œ í•„í„°ë§\n",
    "    - level: 1 (ëŒ€ì±•í„°), 2 (ì†Œì±•í„°)\n",
    "    - parent: '01' ë“± ìƒìœ„ ì±•í„° ë²ˆí˜¸\n",
    "    - contains_title: ì œëª© í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for doc in documents:\n",
    "        meta = doc.metadata\n",
    "        if level and meta[\"level\"] != level:\n",
    "            continue\n",
    "        if parent and meta[\"parent\"] != parent:\n",
    "            continue\n",
    "        if contains_title and contains_title not in meta[\"title\"]:\n",
    "            continue\n",
    "        filtered.append(doc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: 165\n",
      "ğŸ”¹ ì†Œì±•í„° ìˆ˜: 0\n",
      "ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: 0\n",
      "ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "first_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt\")\n",
    "\n",
    "# ì†Œì±•í„°ë§Œ ì¶”ì¶œ (level 2)\n",
    "subchapters = filter_chapters(first_docs, level=2)\n",
    "\n",
    "# ëŒ€ì±•í„° 02ì˜ ëª¨ë“  ì†Œì±•í„° ì¶”ì¶œ\n",
    "chap02_children = filter_chapters(first_docs, parent=\"02\")\n",
    "\n",
    "# \"ì„¤ì¹˜\"ë¼ëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì±•í„°ë§Œ ì¶”ì¶œ\n",
    "install_sections = filter_chapters(first_docs, contains_title=\"ì„¤ì¹˜\")\n",
    "\n",
    "print(f\"ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: {len(first_docs)}\")\n",
    "print(f\"ğŸ”¹ ì†Œì±•í„° ìˆ˜: {len(subchapters)}\")\n",
    "print(f\"ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: {len(chap02_children)}\")\n",
    "print(f\"ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: 27\n",
      "ğŸ”¹ ì†Œì±•í„° ìˆ˜: 0\n",
      "ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: 0\n",
      "ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "second_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_02.txt\")\n",
    "\n",
    "# ì†Œì±•í„°ë§Œ ì¶”ì¶œ (level 2)\n",
    "subchapters = filter_chapters(second_docs, level=2)\n",
    "\n",
    "# ëŒ€ì±•í„° 02ì˜ ëª¨ë“  ì†Œì±•í„° ì¶”ì¶œ\n",
    "chap02_children = filter_chapters(second_docs, parent=\"02\")\n",
    "\n",
    "# \"ì„¤ì¹˜\"ë¼ëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì±•í„°ë§Œ ì¶”ì¶œ\n",
    "install_sections = filter_chapters(second_docs, contains_title=\"ì„¤ì¹˜\")\n",
    "\n",
    "print(f\"ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: {len(second_docs)}\")\n",
    "print(f\"ğŸ”¹ ì†Œì±•í„° ìˆ˜: {len(subchapters)}\")\n",
    "print(f\"ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: {len(chap02_children)}\")\n",
    "print(f\"ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: 33\n",
      "ğŸ”¹ ì†Œì±•í„° ìˆ˜: 4\n",
      "ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: 0\n",
      "ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "third_docs = load_and_split_wikidocs(\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_03.txt\")\n",
    "\n",
    "# ì†Œì±•í„°ë§Œ ì¶”ì¶œ (level 2)\n",
    "subchapters = filter_chapters(third_docs, level=2)\n",
    "\n",
    "# ëŒ€ì±•í„° 02ì˜ ëª¨ë“  ì†Œì±•í„° ì¶”ì¶œ\n",
    "chap02_children = filter_chapters(third_docs, parent=\"02\")\n",
    "\n",
    "# \"ì„¤ì¹˜\"ë¼ëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì±•í„°ë§Œ ì¶”ì¶œ\n",
    "install_sections = filter_chapters(third_docs, contains_title=\"ì„¤ì¹˜\")\n",
    "\n",
    "print(f\"ğŸ“Œ ì´ ë¬¸ì„œ ìˆ˜: {len(third_docs)}\")\n",
    "print(f\"ğŸ”¹ ì†Œì±•í„° ìˆ˜: {len(subchapters)}\")\n",
    "print(f\"ğŸ”¹ 02ë²ˆ ì±•í„° í•˜ìœ„ ìˆ˜: {len(chap02_children)}\")\n",
    "print(f\"ğŸ”¹ 'ì„¤ì¹˜' í¬í•¨ ì œëª© ìˆ˜: {len(install_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "27\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(first_docs))\n",
    "print(len(second_docs))\n",
    "print(len(third_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: 4746\n",
      "ë‘ ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: 1983\n",
      "ì„¸ ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: 2964\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” (í•˜ë‚˜ì˜ ìŠ¤í”Œë¦¬í„°ë§Œ ì‚¬ìš©)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # ì²­í¬ í¬ê¸°\n",
    "    chunk_overlap=50,  # ì²­í¬ ê°„ ì¤‘ë³µ\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # ë¶„í•  ê¸°ì¤€\n",
    ")\n",
    "\n",
    "first_splits = []\n",
    "second_splits = []\n",
    "third_splits = []\n",
    "\n",
    "for doc in first_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    first_splits.extend(splits)\n",
    "    # print(first_splits)\n",
    "\n",
    "for doc in second_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    second_splits.extend(splits)\n",
    "    # print(second_splits)\n",
    "\n",
    "for doc in third_docs:\n",
    "    splits = splitter.split_text(doc.page_content)\n",
    "    third_splits.extend(splits)\n",
    "    # print(third_splits)\n",
    "\n",
    "print(f\"ì²« ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(first_splits)}\")\n",
    "print(f\"ë‘ ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(second_splits)}\")\n",
    "print(f\"ì„¸ ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(third_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4640\\1310670779.py:30: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  first_db.persist()\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embeddings = OpenAIEmbeddings()  # OpenAI ì„ë² ë”© ëª¨ë¸ ì§€ì •\n",
    "\n",
    "# DB ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ì‚­ì œ\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\")\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\")\n",
    "if os.path.exists(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\"):\n",
    "    shutil.rmtree(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë¬¸ì„œ DB ìƒì„±\n",
    "first_db = Chroma.from_texts(\n",
    "    texts=first_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\first_db\"\n",
    ")\n",
    "first_db.persist()\n",
    "\n",
    "# # ë‘ ë²ˆì§¸ ë¬¸ì„œ DB ìƒì„±\n",
    "# second_db = Chroma.from_texts(\n",
    "#     texts=second_splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\second_db\"\n",
    "# )\n",
    "# second_db.persist()\n",
    "\n",
    "# # ì„¸ ë²ˆì§¸ ë¬¸ì„œ DB ìƒì„±\n",
    "# third_db = Chroma.from_texts(\n",
    "#     texts=third_splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\third_db\"\n",
    "# )\n",
    "# third_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# ì›í•˜ëŠ” Pandas DataFrameì„ ì •ì˜í•©ë‹ˆë‹¤.\\ndf = pd.read_csv(\"./data/titanic.csv\")\\ndf.head()'),\n",
       " Document(metadata={}, page_content='PassengerId\\nSurvived\\nPclass\\nName\\nSex\\nAge\\nSibSp\\nParch\\nTicket\\nFare\\nCabin\\nEmbarked\\n1\\n0\\n3\\nBraund, Mr. Owen Harris\\nmale\\n22\\n1\\n0\\nA/5 21171\\n7.25\\nS\\n2\\n1\\n1\\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\\nfemale\\n38\\n1\\n0\\nPC 17599\\n71.2833\\nC85\\nC\\n3\\n1\\n3\\nHeikkinen, Miss. Laina\\nfemale\\nDataFrameLoader\\nPandasëŠ” Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„° ë¶„ì„ ë° ì¡°ì‘ ë„êµ¬ì…ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë°ì´í„° ê³¼í•™, ë¨¸ì‹ ëŸ¬ë‹, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ë°ì´í„° ì‘ì—…ì— ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\\nimport pandas as pd\\n# CSV íŒŒì¼ ì½ê¸°\\ndf = pd.read_csv(\"./data/titanic.csv\")\\nì²« 5ê°œ í–‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤.'),\n",
       " Document(metadata={}, page_content=\"ì—°ê´€í‚¤ì›Œë“œ: ë”¥ëŸ¬ë‹, ìì—°ì–´ ì²˜ë¦¬, ì‹œí€€ìŠ¤ ëª¨ë¸ë§\\níŒë‹¤ìŠ¤ (Pandas)\\nMetadata: {'source': './data/appendix-keywords.txt', 'id': 10, 'relevance_score': 0.9997084}\"),\n",
       " Document(metadata={}, page_content='# !pip install -qU langchain-teddynote\\nfrom langchain_teddynote import logging\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"CH15-Agent-Toolkits\")\\nLangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\\n[í”„ë¡œì íŠ¸ëª…]\\nCH15-Agent-Toolkits\\nimport pandas as pd\\ndf = pd.read_csv(\"./data/titanic.csv\")  # CSV íŒŒì¼ì„ ì½ìŠµë‹ˆë‹¤.\\n# df = pd.read_excel(\"./data/titanic.xlsx\") # ì—‘ì…€ íŒŒì¼ë„ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\ndf.head()\\n[ì´ë¯¸ì§€: ]\\nfrom langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\\nfrom langchain.agents.agent_types import AgentType'),\n",
       " Document(metadata={}, page_content=\"[ì´ë¯¸ì§€: ]\\n[ë„êµ¬ í˜¸ì¶œ]\\nTool: python_repl_ast\\nquery: import pandas as pd\\nimport matplotlib.pyplot as plt\\n# ë‚¨ìì™€ ì—¬ì ìŠ¹ê°ì˜ ìƒì¡´ìœ¨ ê³„ì‚°\\nsurvival_rate = df.groupby('Sex')['Survived'].mean()\\n# barplot ì‹œê°í™”\\nsurvival_rate.plot(kind='bar', color=['blue', 'pink'])\\nplt.title('Survival Rate by Gender')\\nplt.xlabel('Gender')\\nplt.ylabel('Survival Rate')\\nplt.xticks(rotation=0)\\nplt.show()\\nLog:\"),\n",
       " Document(metadata={}, page_content='# !pip install -qU langchain-teddynote\\nfrom langchain_teddynote import logging\\n# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\\nlogging.langsmith(\"CH16-Evaluations\")\\nLangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\\n[í”„ë¡œì íŠ¸ëª…]\\nCH16-Evaluations\\nì €ì¥í•œ CSV íŒŒì¼ë¡œë¶€í„° ë¡œë“œ\\ndata/ragas_synthetic_dataset.csv íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\\nimport pandas as pd\\ndf = pd.read_csv(\"data/ragas_synthetic_dataset.csv\")\\ndf.head()\\n[ì´ë¯¸ì§€: ]\\nfrom datasets import Dataset\\ntest_dataset = Dataset.from_pandas(df)\\ntest_dataset'),\n",
       " Document(metadata={}, page_content='ì•„ë˜ ì½”ë“œëŠ” ì—…ë¡œë“œí•œ HuggingFace Dataset ì„ í™œìš©í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\\n(ì°¸ê³ ) ì•„ë˜ì˜ ì£¼ì„ì„ í’€ê³  ì‹¤í–‰í•˜ì—¬ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì—…ë°ì´íŠ¸ í›„ ì§„í–‰í•´ ì£¼ì„¸ìš”.\\n# !pip install -qU datasets\\nimport pandas as pd\\nfrom datasets import load_dataset, Dataset\\nimport os\\n# huggingface Datasetì—ì„œ repo_idë¡œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\\ndataset = load_dataset(\\n\"teddylee777/rag-synthetic-dataset\",  # ë°ì´í„°ì…‹ ì´ë¦„\\ntoken=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # private ë°ì´í„°ì¸ ê²½ìš° í•„ìš”í•©ë‹ˆë‹¤.\\n)\\n# ë°ì´í„°ì…‹ì—ì„œ split ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ\\nhuggingface_df = dataset[\"korean_v1\"].to_pandas()\\nhuggingface_df.head()\\n[ì´ë¯¸ì§€: ]'),\n",
       " Document(metadata={}, page_content='----------------------------------------------------------------------------------------------------\\nDocument 6:\\níŒë‹¤ìŠ¤ (Pandas)\\nì •ì˜: íŒë‹¤ìŠ¤ëŠ” íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ìœ„í•œ ë°ì´í„° ë¶„ì„ ë° ì¡°ì‘ ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ë¶„ì„ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\\nì˜ˆì‹œ: íŒë‹¤ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ CSV íŒŒì¼ì„ ì½ê³ , ë°ì´í„°ë¥¼ ì •ì œí•˜ë©°, ë‹¤ì–‘í•œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì—°ê´€í‚¤ì›Œë“œ: ë°ì´í„° ë¶„ì„, íŒŒì´ì¬, ë°ì´í„° ì²˜ë¦¬\\nGPT (Generative Pretrained Transformer)\\nì •ì˜: GPTëŠ” ëŒ€ê·œëª¨ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ìƒì„±ì  ì–¸ì–´ ëª¨ë¸ë¡œ, ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ì—…ì— í™œìš©ë©ë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì˜ˆì‹œ: ì‚¬ìš©ìê°€ ì œê³µí•œ ì§ˆë¬¸ì— ëŒ€í•´ ìì„¸í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì±—ë´‡ì€ GPT ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'),\n",
       " Document(metadata={}, page_content='```\\n22.19937\\n```PandasDataFrameOutputParser\\nPandas DataFrameì€ Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° êµ¬ì¡°ë¡œ, ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•´ í”íˆ ì‚¬ìš©ë©ë‹ˆë‹¤. êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ í¬ê´„ì ì¸ ë„êµ¬ ì„¸íŠ¸ë¥¼ ì œê³µí•˜ì—¬, ë°ì´í„° ì •ì œ, ë³€í™˜ ë° ë¶„ì„ê³¼ ê°™ì€ ì‘ì—…ì— ë‹¤ì–‘í•˜ê²Œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì´ ì¶œë ¥ íŒŒì„œëŠ” ì‚¬ìš©ìê°€ ì„ì˜ì˜ Pandas DataFrameì„ ì§€ì •í•˜ê³  í•´ë‹¹ DataFrameì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ í˜•ì‹í™”ëœ ì‚¬ì „ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” LLMì„ ìš”ì²­í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nTrue\\n# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com\\n# !pip install langchain-teddynote\\nfrom langchain_teddynote import logging'),\n",
       " Document(metadata={}, page_content='Document 7:\\níŒë‹¤ìŠ¤ (Pandas)\\nì •ì˜: íŒë‹¤ìŠ¤ëŠ” íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ìœ„í•œ ë°ì´í„° ë¶„ì„ ë° ì¡°ì‘ ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ë¶„ì„ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\\nì˜ˆì‹œ: íŒë‹¤ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ CSV íŒŒì¼ì„ ì½ê³ , ë°ì´í„°ë¥¼ ì •ì œí•˜ë©°, ë‹¤ì–‘í•œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì—°ê´€í‚¤ì›Œë“œ: ë°ì´í„° ë¶„ì„, íŒŒì´ì¬, ë°ì´í„° ì²˜ë¦¬\\nGPT (Generative Pretrained Transformer)\\nì •ì˜: GPTëŠ” ëŒ€ê·œëª¨ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ìƒì„±ì  ì–¸ì–´ ëª¨ë¸ë¡œ, ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ì—…ì— í™œìš©ë©ë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì–¸ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì˜ˆì‹œ: ì‚¬ìš©ìê°€ ì œê³µí•œ ì§ˆë¬¸ì— ëŒ€í•´ ìì„¸í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì±—ë´‡ì€ GPT ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nì—°ê´€í‚¤ì›Œë“œ: ìì—°ì–´ ì²˜ë¦¬, í…ìŠ¤íŠ¸ ìƒì„±, ë”¥ëŸ¬ë‹\\nInstructGPT')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10}).invoke('import pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from typing import List\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def load_txt(path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     í…ìŠ¤íŠ¸ íŒŒì¼ì„ UTF-8ë¡œ ë¡œë“œ\n",
    "#     \"\"\"\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return f.read()\n",
    "\n",
    "# def extract_chapter_meta(chapter_num: str, chapter_title: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     ê°œì„ ëœ ë²„ì „: ì œëª© ë‚´ [ì¹´í…Œê³ ë¦¬] â†’ parent\n",
    "#     \"\"\"\n",
    "#     # [ì¹´í…Œê³ ë¦¬]ê°€ ì¡´ì¬í•˜ë©´ parentë¡œ ì¶”ì •\n",
    "#     category_match = re.match(r\"\\[(.*?)\\]\", chapter_title)\n",
    "#     if category_match:\n",
    "#         parent = category_match.group(1)\n",
    "#     else:\n",
    "#         parent = None\n",
    "\n",
    "#     return {\n",
    "#         \"chapter\": chapter_num,\n",
    "#         \"title\": chapter_title,\n",
    "#         \"level\": 1,  # ì‹¤ì œ íŒŒì¼ì—ëŠ” ëŒ€ì±•í„°ë§Œ ì¡´ì¬\n",
    "#         \"parent\": parent\n",
    "#     }\n",
    "\n",
    "# def protect_code_blocks(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     ```ë¡œ ê°ì‹¸ì§„ ì½”ë“œ ë¸”ë¡ì„ <CODE_BLOCK>ìœ¼ë¡œ ê°ì‹¸ ë³´ì¡´\n",
    "#     \"\"\"\n",
    "#     code_pattern = re.compile(r\"```.*?\\n.*?```\", re.DOTALL)\n",
    "#     protected = []\n",
    "#     last_end = 0\n",
    "\n",
    "#     for match in code_pattern.finditer(text):\n",
    "#         start, end = match.span()\n",
    "#         protected.append(text[last_end:start])\n",
    "#         code = match.group()\n",
    "#         protected.append(f\"\\n<CODE_BLOCK>\\n{code}\\n</CODE_BLOCK>\\n\")\n",
    "#         last_end = end\n",
    "\n",
    "#     protected.append(text[last_end:])\n",
    "#     return \"\".join(protected)\n",
    "\n",
    "# def split_by_chapter_with_code(text: str) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     ì±•í„° í—¤ë”(--- 01. ì œëª© ---) ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ë˜, ê° ì±•í„° ë‚´ ì½”ë“œë¸”ë¡ ë³´ì¡´\n",
    "#     \"\"\"\n",
    "#     pattern = r\"(?=^---\\s+(\\d{2}(?:-\\d{2})?)\\.\\s+(.*?)\\s+---)\"\n",
    "#     matches = list(re.finditer(pattern, text, flags=re.MULTILINE))\n",
    "\n",
    "#     documents = []\n",
    "#     for i, match in enumerate(matches):\n",
    "#         start = match.start()\n",
    "#         end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "#         chapter_num = match.group(1)\n",
    "#         chapter_title = match.group(2).strip()\n",
    "#         chapter_text = text[start:end].strip()\n",
    "\n",
    "#         protected_text = protect_code_blocks(chapter_text)\n",
    "#         metadata = extract_chapter_meta(chapter_num, chapter_title)\n",
    "\n",
    "#         documents.append(Document(page_content=protected_text, metadata=metadata))\n",
    "\n",
    "#     return documents\n",
    "\n",
    "# def process_wikidocs_files(paths: List[str]) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     ì—¬ëŸ¬ Wikidocs í˜•ì‹ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "#     \"\"\"\n",
    "#     all_docs = []\n",
    "#     for path in paths:\n",
    "#         text = load_txt(path)\n",
    "#         docs = split_by_chapter_with_code(text)\n",
    "#         all_docs.extend(docs)\n",
    "#     return all_docs\n",
    "\n",
    "# def extract_and_replace_code_blocks(text: str):\n",
    "#     \"\"\"\n",
    "#     <CODE_BLOCK>...</CODE_BLOCK> êµ¬ê°„ì„ [[CODE:0]], [[CODE:1]], ...ë¡œ ì¹˜í™˜\n",
    "#     \"\"\"\n",
    "#     code_blocks = []\n",
    "#     pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "\n",
    "#     def replacer(match):\n",
    "#         code_blocks.append(match.group())\n",
    "#         return f\"[[CODE:{len(code_blocks) - 1}]]\"\n",
    "\n",
    "#     modified_text = pattern.sub(replacer, text)\n",
    "#     return modified_text, code_blocks\n",
    "\n",
    "# def split_protected_chunks(docs: List[Document], chunk_size=1000, chunk_overlap=200) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     ì½”ë“œ ë¸”ë¡ì´ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë³´í˜¸í•œ ìƒíƒœë¡œ chunk ë¶„í• \n",
    "#     \"\"\"\n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap\n",
    "#     )\n",
    "\n",
    "#     chunked_docs = []\n",
    "#     for doc in docs:\n",
    "#         # ì½”ë“œ ë¸”ë¡ì„ ì„ì‹œ í† í°ìœ¼ë¡œ ì¹˜í™˜\n",
    "#         mod_text, code_blocks = extract_and_replace_code_blocks(doc.page_content)\n",
    "#         temp_doc = Document(page_content=mod_text, metadata=doc.metadata)\n",
    "\n",
    "#         # ë¶„í• \n",
    "#         split_docs = splitter.split_documents([temp_doc])\n",
    "\n",
    "#         # ì½”ë“œ ë¸”ë¡ ë³µì›\n",
    "#         for d in split_docs:\n",
    "#             restored_text = d.page_content\n",
    "#             for i, block in enumerate(code_blocks):\n",
    "#                 restored_text = restored_text.replace(f\"[[CODE:{i}]]\", block)\n",
    "#             d.page_content = restored_text\n",
    "#             chunked_docs.append(d)\n",
    "\n",
    "#     return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_txt(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def protect_code_blocks(text: str) -> str:\n",
    "    code_pattern = re.compile(r\"```.*?\\n.*?```\", re.DOTALL)\n",
    "    protected = []\n",
    "    last_end = 0\n",
    "    for match in code_pattern.finditer(text):\n",
    "        start, end = match.span()\n",
    "        protected.append(text[last_end:start])\n",
    "        code = match.group()\n",
    "        protected.append(f\"\\n<CODE_BLOCK>\\n{code}\\n</CODE_BLOCK>\\n\")\n",
    "        last_end = end\n",
    "    protected.append(text[last_end:])\n",
    "    return \"\".join(protected)\n",
    "\n",
    "def split_by_structure_with_code(text: str) -> List[Document]:\n",
    "    lines = text.splitlines()\n",
    "    documents = []\n",
    "    buffer = []\n",
    "\n",
    "    current_chapter_num = None\n",
    "    current_chapter_title = None\n",
    "    current_section_num = None\n",
    "    current_section_title = None\n",
    "\n",
    "    chapter_pattern = re.compile(r\"^---\\s+CH(\\d+)\\s+(.*?)\\s+---$\")\n",
    "    section_pattern = re.compile(r\"^---\\s+(\\d{2})\\.\\s+(.*?)\\s+---$\")\n",
    "\n",
    "    for line in lines:\n",
    "        chapter_match = chapter_pattern.match(line)\n",
    "        section_match = section_pattern.match(line)\n",
    "\n",
    "        if chapter_match:\n",
    "            # flush ì´ì „ buffer\n",
    "            if buffer:\n",
    "                chunk = \"\\n\".join(buffer).strip()\n",
    "                protected = protect_code_blocks(chunk)\n",
    "                documents.append(Document(\n",
    "                    page_content=protected,\n",
    "                    metadata={\n",
    "                        \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                        \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "                    }\n",
    "                ))\n",
    "                buffer = []\n",
    "\n",
    "            current_chapter_num = chapter_match.group(1)\n",
    "            current_chapter_title = chapter_match.group(2).strip()\n",
    "            current_section_num, current_section_title = None, None\n",
    "\n",
    "        elif section_match:\n",
    "            if buffer:\n",
    "                chunk = \"\\n\".join(buffer).strip()\n",
    "                protected = protect_code_blocks(chunk)\n",
    "                documents.append(Document(\n",
    "                    page_content=protected,\n",
    "                    metadata={\n",
    "                        \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                        \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "                    }\n",
    "                ))\n",
    "                buffer = []\n",
    "\n",
    "            current_section_num = section_match.group(1)\n",
    "            current_section_title = section_match.group(2).strip()\n",
    "\n",
    "        buffer.append(line)\n",
    "\n",
    "    if buffer:\n",
    "        chunk = \"\\n\".join(buffer).strip()\n",
    "        protected = protect_code_blocks(chunk)\n",
    "        documents.append(Document(\n",
    "            page_content=protected,\n",
    "            metadata={\n",
    "                \"chapter_info\": f\"CH{current_chapter_num} {current_chapter_title}\" if current_chapter_num else None,\n",
    "                \"section_info\": f\"{current_section_num}. {current_section_title}\" if current_section_num else None,\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def process_wikidocs_files(paths: List[str]) -> List[Document]:\n",
    "    all_docs = []\n",
    "    for path in paths:\n",
    "        text = load_txt(path)\n",
    "        docs = split_by_structure_with_code(text)\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    "def extract_and_replace_code_blocks(text: str):\n",
    "    code_blocks = []\n",
    "    pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "    def replacer(match):\n",
    "        code_blocks.append(match.group())\n",
    "        return f\"[[CODE:{len(code_blocks) - 1}]]\"\n",
    "    modified_text = pattern.sub(replacer, text)\n",
    "    return modified_text, code_blocks\n",
    "\n",
    "def split_protected_chunks(docs: List[Document], chunk_size=2000, chunk_overlap=50) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunked_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        mod_text, code_blocks = extract_and_replace_code_blocks(doc.page_content)\n",
    "        temp_doc = Document(page_content=mod_text, metadata=doc.metadata)\n",
    "        split_docs = splitter.split_documents([temp_doc])\n",
    "\n",
    "        for d in split_docs:\n",
    "            # ì‹¤ì œë¡œ ì‚¬ìš©ëœ [[CODE:X]]ë§Œ ì°¾ì•„ì„œ ë³µì›\n",
    "            used_codes = re.findall(r\"\\[\\[CODE:(\\d+)\\]\\]\", d.page_content)\n",
    "            for code_index in set(used_codes):\n",
    "                code_index = int(code_index)\n",
    "                if code_index < len(code_blocks):\n",
    "                    d.page_content = d.page_content.replace(f\"[[CODE:{code_index}]]\", code_blocks[code_index])\n",
    "            chunked_docs.append(d)\n",
    "\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ì›ë³¸ ì±•í„° ìˆ˜: 184\n",
      "ì´ ë¶„í• ëœ ë¬¸ì„œ ìˆ˜ (ì„ë² ë”©ìš©): 1241\n",
      "--- ìƒ˜í”Œ ---\n",
      "--- CH01 LangChain ì‹œì‘í•˜ê¸° ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Wikidocs í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì±•í„° ë‹¨ìœ„ë¡œ ë¡œë”© (ì„¤ëª… + ì½”ë“œ ë³´ì¡´)\n",
    "docs = process_wikidocs_files([\n",
    "    \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt\",\n",
    "    # \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_02.txt\",\n",
    "    # \"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_03.txt\"\n",
    "])\n",
    "\n",
    "# 2. ì„ë² ë”© ì „ìš© chunk ë‹¨ìœ„ë¡œ ë¶„í•  (ì½”ë“œë¸”ë¡ ë³´í˜¸ë¨)\n",
    "chunked_docs = split_protected_chunks(docs)\n",
    "\n",
    "# 3. í™•ì¸\n",
    "print(f\"ì´ ì›ë³¸ ì±•í„° ìˆ˜: {len(docs)}\")\n",
    "print(f\"ì´ ë¶„í• ëœ ë¬¸ì„œ ìˆ˜ (ì„ë² ë”©ìš©): {len(chunked_docs)}\")\n",
    "print(\"--- ìƒ˜í”Œ ---\")\n",
    "print(chunked_docs[1].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶”ì¶œ\n",
    "text_contents = [doc.page_content for doc in chunked_docs]\n",
    "\n",
    "test_db = Chroma.from_texts(\n",
    "    texts=text_contents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\db\\chromadb\\test_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='--- 03. LangChain Hub ---'),\n",
       " Document(metadata={}, page_content='=== <ë­ì²´ì¸LangChain ë…¸íŠ¸> - LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡· ===\\n\\n=================================================='),\n",
       " Document(metadata={}, page_content='--- CH01 LangChain ì‹œì‘í•˜ê¸° ---'),\n",
       " Document(metadata={}, page_content='--- 05. ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” RAG ì²´ì¸ ---'),\n",
       " Document(metadata={}, page_content='--- 08. LCEL Chain ì— ë©”ëª¨ë¦¬ ì¶”ê°€ ---')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}).invoke('ë­ì²´ì¸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: None\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 01. ì„¤ì¹˜ ì˜ìƒë³´ê³  ë”°ë¼í•˜ê¸°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 02. OpenAI API í‚¤ ë°œê¸‰ ë° í…ŒìŠ¤íŠ¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 03. LangSmith ì¶”ì  ì„¤ì •\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 04. OpenAI API ì‚¬ìš©(GPT-4o ë©€í‹°ëª¨ë‹¬)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 05. LangChain Expression Language(LCEL)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 06. LCEL ì¸í„°í˜ì´ìŠ¤\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH01 LangChain ì‹œì‘í•˜ê¸°\n",
      "  section_info: 07. Runnable\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: 01. í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: 02. í“¨ìƒ· í”„ë¡¬í”„íŠ¸(FewShotPromptTemplate)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: 03. LangChain Hub\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH02 í”„ë¡¬í”„íŠ¸(Prompt)\n",
      "  section_info: 04. ê°œì¸í™”ëœ í”„ë¡¬í”„íŠ¸(Hubì— ì—…ë¡œë“œ)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 01. Pydantic ì¶œë ¥ íŒŒì„œ(PydanticOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 02. ì½¤ë§ˆ êµ¬ë¶„ì ì¶œë ¥ íŒŒì„œ(CommaSeparatedListOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 03. êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì„œ(StructuredOuputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 04. JSON ì¶œë ¥ íŒŒì„œ(JsonOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 05. ë°ì´í„°í”„ë ˆì„ ì¶œë ¥ íŒŒì„œ(PandasDataFrameOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 06. ë‚ ì§œ í˜•ì‹ ì¶œë ¥ íŒŒì„œ(DatetimeOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 07. ì—´ê±°í˜• ì¶œë ¥ íŒŒì„œ(EnumOutputParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH03 ì¶œë ¥ íŒŒì„œ(Output Parsers)\n",
      "  section_info: 08. ì¶œë ¥ ìˆ˜ì • íŒŒì„œ(OutputFixingParser)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 01. ë‹¤ì–‘í•œ LLM ëª¨ë¸ í™œìš©\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 02. ìºì‹±(Cache)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 03. ëª¨ë¸ ì§ë ¬í™”(Serialization) - ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 04. í† í° ì‚¬ìš©ëŸ‰ í™•ì¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 05. êµ¬ê¸€ ìƒì„± AI(Google Generative AI)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 06. í—ˆê¹…í˜ì´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸(HuggingFace Endpoints)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 07. í—ˆê¹…í˜ì´ìŠ¤ ë¡œì»¬(HuggingFace Local)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 08. í—ˆê¹…í˜ì´ìŠ¤ íŒŒì´í”„ë¼ì¸(HuggingFace Pipeline)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 09. ì˜¬ë¼ë§ˆ(Ollama)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 10. GPT4ALL\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH04 ëª¨ë¸(Model)\n",
      "  section_info: 11. ë¹„ë””ì˜¤(Video) ì§ˆì˜ ì‘ë‹µ LLM (Gemini)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 01. ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬(ConversationBufferMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 02. ëŒ€í™” ë²„í¼ ìœˆë„ìš° ë©”ëª¨ë¦¬(ConversationBufferWindowMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 03. ëŒ€í™” í† í° ë²„í¼ ë©”ëª¨ë¦¬(ConversationTokenBufferMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 04. ëŒ€í™” ì—”í‹°í‹° ë©”ëª¨ë¦¬(ConversationEntityMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 05. ëŒ€í™” ì§€ì‹ê·¸ë˜í”„ ë©”ëª¨ë¦¬(ConversationKGMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 06. ëŒ€í™” ìš”ì•½ ë©”ëª¨ë¦¬(ConversationSummaryMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 07. ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ ë©”ëª¨ë¦¬(VectorStoreRetrieverMemory)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 08. LCEL Chain ì— ë©”ëª¨ë¦¬ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 09. SQLite ì— ëŒ€í™”ë‚´ìš© ì €ì¥\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH05 ë©”ëª¨ë¦¬(Memory)\n",
      "  section_info: 10. RunnableWithMessageHistoryì— ChatMessageHistoryì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 01. ë„íë¨¼íŠ¸(Document) ì˜ êµ¬ì¡°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 02. PDF\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 03. í•œê¸€(HWP)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 04. CSV\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 05. Excel\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 06. Word\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 07. PowerPoint\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 08. ì›¹ ë¬¸ì„œ(WebBaseLoader)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 09. í…ìŠ¤íŠ¸(TextLoader)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 10. JSON\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 11. Arxiv\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 12. UpstageLayoutAnalysisLoader\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH06 ë¬¸ì„œ ë¡œë”(Document Loader)\n",
      "  section_info: 13. LlamaParser\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 01. ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• (CharacterTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 02. ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• (RecursiveCharacterTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 03. í† í° í…ìŠ¤íŠ¸ ë¶„í• (TokenTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 04. ì‹œë©˜í‹± ì²­ì»¤(SemanticChunker)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 05. ì½”ë“œ ë¶„í• (Python, Markdown, JAVA, C++, C#, GO, JS, Latex ë“±)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 06. ë§ˆí¬ë‹¤ìš´ í—¤ë” í…ìŠ¤íŠ¸ ë¶„í• (MarkdownHeaderTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 07. HTML í—¤ë” í…ìŠ¤íŠ¸ ë¶„í• (HTMLHeaderTextSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH07 í…ìŠ¤íŠ¸ ë¶„í• (Text Splitter)\n",
      "  section_info: 08. ì¬ê·€ì  JSON ë¶„í• (RecursiveJsonSplitter)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 01. OpenAIEmbeddings\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 02. ìºì‹œ ì„ë² ë”©(CacheBackedEmbeddings)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 03. í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”©(HuggingFace Embeddings)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 04. UpstageEmbeddings\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 05. OllamaEmbeddings\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 06. GPT4ALL ì„ë² ë”©\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH08 ì„ë² ë”©(Embedding)\n",
      "  section_info: 07. Llama CPP ì„ë² ë”©\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)\n",
      "  section_info: 01. Chroma\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)\n",
      "  section_info: 02. FAISS\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH09 ë²¡í„°ì €ì¥ì†Œ(VectorStore)\n",
      "  section_info: 03. Pinecone\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 01. ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ ê²€ìƒ‰ê¸°(VectorStore-backed Retriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 02. ë¬¸ë§¥ ì••ì¶• ê²€ìƒ‰ê¸°(ContextualCompressionRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 03. ì•™ìƒë¸” ê²€ìƒ‰ê¸°(EnsembleRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 04. ê¸´ ë¬¸ë§¥ ì¬ì •ë ¬(LongContextReorder)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 05. ìƒìœ„ ë¬¸ì„œ ê²€ìƒ‰ê¸°(ParentDocumentRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 06. ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°(MultiQueryRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 07. ë‹¤ì¤‘ ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ê¸°(MultiVectorRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 08. ì…€í”„ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°(SelfQueryRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 09. ì‹œê°„ ê°€ì¤‘ ë²¡í„°ì €ì¥ì†Œ ê²€ìƒ‰ê¸°(TimeWeightedVectorStoreRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 10. í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸°(Kiwi, Kkma, Okt) + BM25 ê²€ìƒ‰ê¸°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH10 ê²€ìƒ‰ê¸°(Retriever)\n",
      "  section_info: 11. Convex Combination(CC) ì ìš©ëœ ì•™ìƒë¸” ê²€ìƒ‰ê¸°(EnsembleRetriever)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: 01. Cross Encoder Reranker\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: 02. Cohere Reranker\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: 03. Jina Reranker\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH11 ë¦¬ë­ì»¤(Reranker)\n",
      "  section_info: 04. FlashRank Reranker\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 01. PDF ë¬¸ì„œ ê¸°ë°˜ QA(Question-Answer)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 02. ë„¤ì´ë²„ ë‰´ìŠ¤ê¸°ì‚¬ QA(Question-Answer)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 03. RAG ì˜ ê¸°ëŠ¥ë³„ ë‹¤ì–‘í•œ ëª¨ë“ˆ í™œìš©ê¸°\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 04. RAPTOR: ê¸´ ë¬¸ë§¥ ìš”ì•½(Long Context Summary)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH12 Retrieval Augmented Generation(RAG)\n",
      "  section_info: 05. ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” RAG ì²´ì¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 01. RunnablePassthrough\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 02. Runnable êµ¬ì¡°(ê·¸ë˜í”„) ê²€í† \n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 03. RunnableLambda\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 04. LLM ì²´ì¸ ë¼ìš°íŒ…(RunnableLambda, RunnableBranch)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 05. RunnableParallel\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 06. ë™ì  ì†ì„± ì§€ì •(configurable_fields, configurable_alternatives)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 07. @chain ë°ì½”ë ˆì´í„°ë¡œ Runnable êµ¬ì„±\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 08. RunnableWithMessageHistory\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 09. ì‚¬ìš©ì ì •ì˜ ì œë„¤ë ˆì´í„°(generator)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 10. Runtime Arguments ë°”ì¸ë”©\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH13 LangChain Expression Language(LCEL)\n",
      "  section_info: 11. í´ë°±(fallback) ëª¨ë¸ ì§€ì •\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH14 ì²´ì¸(Chains)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH14 ì²´ì¸(Chains)\n",
      "  section_info: 01. ë¬¸ì„œ ìš”ì•½\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH14 ì²´ì¸(Chains)\n",
      "  section_info: 02. SQL\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH14 ì²´ì¸(Chains)\n",
      "  section_info: 03. êµ¬ì¡°í™”ëœ ì¶œë ¥ ì²´ì¸(with_structered_output)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 01. í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±(RAGAS)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 02. RAGAS ë¥¼ í™œìš©í•œ í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 03. ìƒì„±í•œ í‰ê°€ìš© ë°ì´í„°ì…‹ ì—…ë¡œë“œ(HuggingFace Dataset)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 04. LangSmith ë°ì´í„°ì…‹ ìƒì„±\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 05. LLM-as-Judge\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 06. ì„ë² ë”© ê¸°ë°˜ í‰ê°€(embedding_distance)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 07. ì‚¬ìš©ì ì •ì˜(Custom) LLM í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 08. Rouge, BLEU, METEOR, SemScore ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 09. ì‹¤í—˜(Experiment) í‰ê°€ ë¹„êµ\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 10. ìš”ì•½(Summary) ë°©ì‹ì˜ í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 11. Groundedness(í• ë£¨ì‹œë„¤ì´ì…˜) í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 12. ì‹¤í—˜ ë¹„êµ(Pairwise Evaluation)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 13. ë°˜ë³µ í‰ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH15 í‰ê°€(Evaluations)\n",
      "  section_info: 14. ì˜¨ë¼ì¸ í‰ê°€ë¥¼ í™œìš©í•œ í‰ê°€ ìë™í™”\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 01. ë„êµ¬(Tools)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 02. ë„êµ¬ ë°”ì¸ë”©(Binding Tools)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 03. ì—ì´ì „íŠ¸(Agent)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 04. Claude, Gemini, Ollama, Together.ai ë¥¼ í™œìš©í•œ Agent\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 05. Iteration ê¸°ëŠ¥ê³¼ ì‚¬ëŒ ê°œì…(Human-in-the-loop)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 06. Agentic RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 07. CSVExcel ë°ì´í„° ë¶„ì„ Agent\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 08. Toolkits í™œìš© Agent\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 09. RAG + Image Generator Agent(ë³´ê³ ì„œ ì‘ì„±)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH16 ì—ì´ì „íŠ¸(Agent)\n",
      "  section_info: 10. ë„êµ¬ë¥¼ í™œìš©í•œ í† ë¡  ì—ì´ì „íŠ¸(Two Agent Debates with Tools)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. í•µì‹¬ ê¸°ëŠ¥\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. LangGraph ì— ìì£¼ ë“±ì¥í•˜ëŠ” Python ë¬¸ë²•ì´í•´\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. LangGraphë¥¼ í™œìš©í•œ ì±—ë´‡ êµ¬ì¶•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. LangGraphë¥¼ í™œìš©í•œ Agent êµ¬ì¶•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. Agent ì— ë©”ëª¨ë¦¬(memory) ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. ë…¸ë“œì˜ ë‹¨ê³„ë³„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. Human-in-the-loop(ì‚¬ëŒì˜ ê°œì…)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. ì¤‘ê°„ë‹¨ê³„ ê°œì…  ë˜ëŒë¦¼ì„ í†µí•œ ìƒíƒœ ìˆ˜ì •ê³¼ Replay\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 08. ì‚¬ëŒ(Human)ì—ê²Œ ë¬¼ì–´ë³´ëŠ” ë…¸ë“œ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 09. ë©”ì‹œì§€ ì‚­ì œ(RemoveMessage)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 10. ToolNode ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 11. ë³‘ë ¬ ë…¸ë“œ ì‹¤í–‰ì„ ìœ„í•œ ë¶„ê¸° ìƒì„± ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 12. ëŒ€í™” ê¸°ë¡ ìš”ì•½ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 13. ì„œë¸Œê·¸ë˜í”„ ì¶”ê°€ ë° ì‚¬ìš© ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 14. ì„œë¸Œê·¸ë˜í”„ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë³€í™˜í•˜ëŠ” ë°©ë²•\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 15. LangGraph ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì˜ ëª¨ë“  ê²ƒ\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. êµ¬ì¡° ì„¤ê³„\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. ê¸°ë³¸ ê·¸ë˜í”„ ìƒì„±\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. Naive RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. ê´€ë ¨ì„± ì²´ì»¤(Relevance Checker) ëª¨ë“ˆ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. ì›¹ ê²€ìƒ‰ ëª¨ë“ˆ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. ì¿¼ë¦¬ ì¬ì‘ì„± ëª¨ë“ˆ ì¶”ê°€\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. Agentic RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. Adaptive RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. Use Cases\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 01. ì—ì´ì „íŠ¸ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜ (ê³ ê° ì‘ëŒ€ ì‹œë‚˜ë¦¬ì˜¤)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 02. ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì—ì´ì „íŠ¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 03. CRAG(Corrective RAG)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 04. Self-RAG\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 05. ê³„íš í›„ ì‹¤í–‰(Plan-and-Execute)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 06. ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… ë„¤íŠ¸ì›Œí¬(Multi-Agent Collaboration Network)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 07. ë©€í‹° ì—ì´ì „íŠ¸ ê°ë…ì(Multi-Agent Supervisor)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 08. ê³„ì¸µì  ë©€í‹° ì—ì´ì „íŠ¸ íŒ€(Hierarchical Multi-Agent Teams)\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 09. SQL ë°ì´í„°ë² ì´ìŠ¤ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì—ì´ì „íŠ¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH17 LangGraph\n",
      "  section_info: 10. STORM ê°œë…ì„ ë„ì…í•œ ì—°êµ¬ë¥¼ ìœ„í•œ ë©€í‹° ì—ì´ì „íŠ¸\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH18 ê¸°íƒ€ ì •ë³´\n",
      "  section_info: None\n",
      "--------------------------------------------------\n",
      "ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n",
      "  chapter_info: CH18 ê¸°íƒ€ ì •ë³´\n",
      "  section_info: 01. StreamEvent íƒ€ì…ë³„ ì •ë¦¬\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(f\"ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        if key == 'level':\n",
    "            print(f\"  {'  ' * (int(value) - 1)}â””â”€ {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# def extract_code_blocks_from_documents(docs: List[Document]) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     ê° Document ê°ì²´ ë‚´ì˜ <CODE_BLOCK>...</CODE_BLOCK> êµ¬ê°„ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "#     \"\"\"\n",
    "#     code_blocks = []\n",
    "#     pattern = re.compile(r\"<CODE_BLOCK>\\s*```.*?\\n.*?```\\s*</CODE_BLOCK>\", re.DOTALL)\n",
    "\n",
    "#     for doc in docs:\n",
    "#         matches = pattern.findall(doc.page_content)\n",
    "#         code_blocks.extend(matches)\n",
    "\n",
    "#     return code_blocks\n",
    "\n",
    "# chunked_docs = split_protected_chunks(docs)\n",
    "\n",
    "# code_blocks = extract_code_blocks_from_documents(chunked_docs)\n",
    "\n",
    "# # ì¶œë ¥ í™•ì¸\n",
    "# for i, code in enumerate(code_blocks[100:150]):\n",
    "#     print(f\"ğŸ”¹ Code Block {i + 1}:\\n{code}\\n{'-'*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ë‹µë³€:\n",
      " ë­ì²´ì¸(LangChain)ì€ ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ í†µí•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ë©°, ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ëŠ¥ì„ ë§ì¶¤ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ë„êµ¬ì™€ ì»´í¬ë„ŒíŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë­ì²´ì¸ì€ ëŒ€í™”í˜• AI, ë°ì´í„° ì²˜ë¦¬, ì •ë³´ ê²€ìƒ‰ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“ ì¶œì²˜ ë¬¸ì„œ:\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "=== <ë­ì²´ì¸LangChain ë…¸íŠ¸> - LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡· ===\n",
      "\n",
      "================================================== \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "--- 05. ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” RAG ì²´ì¸ --- \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      ")\n",
      "# ì²´ì¸ ìƒì„±\n",
      "chain = prompt | llm | StrOutputParser()\n",
      "# ì§ˆì˜ ì‹¤í–‰\n",
      "response = chain.invoke({\"question\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\"})\n",
      "ëŒ€í•œë¯¼êµ­(South Korea)ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.\n",
      "ì„œìš¸ì€ ì•½ 1000ë§Œ ëª…ì˜ ì¸êµ¬ë¥¼ ê°€ì§„ ëŒ€ë„ì‹œë¡œ, í•œë°˜ë„ ë¶ë¶€ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. ì„¸ê³„ì—ì„œ ê°€ì¥ í° ë„ì‹œ ì¤‘ í•˜ë‚˜ë¡œ ê°„ì£¼ë˜ë©° ë¬¸í™”, ê²½ì œ ë° ì •ì¹˜ ì¤‘ì‹¬ì§€ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë„ì‹œì˜ ì—­ì‚¬ëŠ” ì‚¼í•œ ì‹œëŒ€ê¹Œì§€ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ì´í›„ ë°±ì œ, ê³ ë ¤ ê·¸ë¦¬ê³  ì¡°ì„  ì‹œëŒ€ì— ì¤‘ìš”í•œ ì§€ì—­ìœ¼ë¡œ \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "ë¬¸ì„œ: data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (í˜ì´ì§€ 10)\n",
      "LangSmith: https://smith.langchain.com/public/4449e744-f0a0-42d2-a3df-855bd7f41652/r\n",
      "# ë‹¨ê³„ 8: ì²´ì¸ ì‹¤í–‰(Run Chain)\n",
      "# ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
      "question = \"ì‚¼ì„± ê°€ìš°ìŠ¤ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”\"\n",
      "response = rag_chain.invoke(question)\n",
      "print(response)\n",
      "ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸ë¡œ \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "<CODE_BLOCK>\n",
      "```\n",
      "- ë¹Œ ê²Œì´ì¸ ëŠ” 5ë…„ ë‚´ì— ì¼ìƒ ì–¸ì–´ë¡œ ëª¨ë“  ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” AI ì—ì´ì „íŠ¸ê°€ ë³´ê¸‰ë  ê²ƒì´ë¼ê³  ì „ë§í•˜ë©°, ì´ëŸ¬í•œ AI ì—ì´ì „íŠ¸ê°€ ì»´í“¨í„° ì‚¬ìš© ë°©ì‹ì„ ì™„ì „íˆ ë³€í™”ì‹œí‚¤ê³  ì†Œí”„íŠ¸ì›¨ì–´ ì‚°ì—…ì—ë„ í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•˜ê³  ìˆë‹¤. AI ì—ì´ì „íŠ¸ì˜ ë„ì…ì€ ì˜ë£Œ, êµìœ¡, ìƒì‚°ì„±, ì—”í„°í…Œì¸ë¨¼íŠ¸ ë° ì‡¼í•‘ ë“± ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ ê³ ê°€ì˜ ì„œë¹„ìŠ¤ê°€ ëŒ€ì¤‘í™”ë˜ëŠ” ê³„ê¸°ê°€ ë  ê²ƒì´ë‹¤.\n",
      "- ìœ íŠœë¸ŒëŠ” 2024ë…„ë¶€í„° AIê°€ ìƒì„±í•œ ì½˜í…ì¸ ì— ëŒ€í•œ í‘œì‹œë¥¼ ì˜ë¬´í™”í•  ê³„íšì´ë‹¤.\n",
      "- ì˜êµ­ ê³¼í•™í˜ì‹ ê¸°ìˆ ë¶€ëŠ” AI ì•ˆì „ ì—°êµ¬ì†Œë¥¼ ì„¤ë¦½í•œë‹¤ê³  \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "<CODE_BLOCK>\n",
      "```\n",
      "ğŸŒŒğŸ”¬âœ¨ ì–‘ìì—­í•™: ë¯¸ì‹œ ì„¸ê³„ì˜ ì‹ ë¹„ë¥¼ íƒêµ¬í•˜ë‹¤!\n",
      "ì–‘ìì—­í•™ì€ ì›ìì™€ ì•„ì›ì ì…ìë“¤ì˜ ë†€ë¼ìš´ í˜„ìƒì„ ì„¤ëª…í•˜ëŠ” ì´ë¡ ì´ì—ìš”. ğŸ§ªğŸ’« ê³ ì „ì—­í•™ê³¼ëŠ” ë‹¤ë¥´ê²Œ, ì…ìì˜ ìœ„ì¹˜ì™€ ìš´ë™ëŸ‰ì„ ë™ì‹œì— ì •í™•íˆ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ì ì´ ë§¤ë ¥ì ì´ì£ ! ğŸ¤”ğŸ”\n",
      "ì—¬ê¸° ëª‡ ê°€ì§€ í•µì‹¬ ê°œë…ì„ ì†Œê°œí• ê²Œìš”:\n",
      "- íŒŒë™-ì…ì ì´ì¤‘ì„± ğŸŒŠâ¡ï¸âš›ï¸\n",
      "- ë¶ˆí™•ì •ì„± ì›ë¦¬ â“ğŸ”„\n",
      "- ì–‘ì ì–½í˜ ğŸ”—ğŸ’–\n",
      "ì´ ëª¨ë“  ê²ƒì´ í˜„ëŒ€ ë¬¼ë¦¬í•™ê³¼ ê¸°ìˆ ì˜ ê¸°ì´ˆê°€ ëœë‹µë‹ˆë‹¤! ğŸŒğŸ’¡ ì–‘ì ì„¸ê³„ì˜ ì‹ ë¹„ë¥¼ í•¨ê»˜ íƒí—˜í•´ë´ìš”! ğŸš€ğŸ”­ #ì–‘ìì—­í•™ #ë¬¼ë¦¬í•™ #ê³¼í•™ì˜ë¯¸ë˜ #ì‹ ë¹„ë¡œìš´ì„¸ê³„\n",
      "```\n",
      "</CODE_BL \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "ê²€ìƒ‰ ğŸ“š\n",
      "'ë°ì´í„° ê°•í™” ìƒì„±'ì— ì´ˆì ì„ ë§ì¶˜ ì´ ëª¨ë“ˆì€ ìƒì„± ë‹¨ê³„ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì‘ì—…ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
      "ì—ì´ì „íŠ¸ ğŸ¤–\n",
      "ì–¸ì–´ ëª¨ë¸ì´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í• ì§€ ê²°ì •í•˜ê³ , í•´ë‹¹ ì¡°ì¹˜ë¥¼ ì‹¤í–‰í•˜ë©°, ê´€ì°°í•˜ê³ , í•„ìš”í•œ ê²½ìš° ë°˜ë³µí•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "LangChainì„ í™œìš©í•˜ë©´, ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê°œë°œì„ ë³´ë‹¤ ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìœ¼ë©°, í•„ìš”ì— ë§ê²Œ ê¸°ëŠ¥ì„ ë§ì¶¤ ì„¤ì •í•˜ê³ , ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ í†µí•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "--- 08. LCEL Chain ì— ë©”ëª¨ë¦¬ ì¶”ê°€ --- \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "--- CH15 í‰ê°€(Evaluations) ---\n",
      "\n",
      "í‰ê°€ (Evaluations)\n",
      "LLM(Large Language Model) í‰ê°€ëŠ” ì¸ê³µì§€ëŠ¥ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥, ì •í™•ì„±, ì¼ê´€ì„± ë° ê¸°íƒ€ ì¤‘ìš”í•œ ì¸¡ë©´ì„ ì¸¡ì •í•˜ê³  ë¶„ì„í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ê°œì„ , ë¹„êµ, ì„ íƒ ë° ì‘ìš© í”„ë¡œê·¸ë¨ì— ì í•©í•œ ëª¨ë¸ ê²°ì •ì— í•„ìˆ˜ì ì¸ ë‹¨ê³„ì…ë‹ˆë‹¤.\n",
      "í‰ê°€ ë°©ë²•\n",
      "LLM í‰ê°€ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ìˆ˜í–‰ë  ìˆ˜ ìˆìœ¼ë©°, ì£¼ìš” ì ‘ê·¼ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "ìë™í™”ëœ ë©”íŠ¸ë¦­: BLEU, ROUGE, METEOR, SemScore ë“±ì˜ ì§€í‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "ì¸ê°„ í‰ê°€: ì „ \n",
      "---\n",
      "\n",
      "ğŸ”¹ ì •ë³´ ì—†ìŒ > ì •ë³´ ì—†ìŒ\n",
      "- ê´‘ì£¼ì‹œ: 17ì‚´ ë  ë•Œê¹Œì§€ 7400ë§Œì› ì§€ê¸‰\n",
      "LangSmith Trace ë³´ê¸° (https://smith.langchain.com/public/1a613ee7-6eaa-482f-a45f-8c22b4e60fbf/r)LangSmith Trace ë³´ê¸°\n",
      "answer = rag_chain.stream(\"ë¶€ì˜ê·¸ë£¹ì˜ ì„ì§ì› ìˆ«ìëŠ” ëª‡ëª…ì¸ê°€ìš”?\")\n",
      "stream_response(answer)\n",
      "ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. ë¡œì»¬ LLM (gemma:3.12b)\n",
    "# llm = Ollama(model=\"gemma3:12b\")\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# 4. ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ì˜ (ì‚¬ìš© í•„ë“œë§Œ)\n",
    "metadata_field_info = [\n",
    "    {\"name\": \"chapter_info\", \"type\": \"string\"},\n",
    "    {\"name\": \"section_info\", \"type\": \"string\"},\n",
    "]\n",
    "\n",
    "# 5. ParentDocumentRetriever êµ¬ì„±\n",
    "retriever = test_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# 6. QA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 7. ì§ˆë¬¸ ì‹¤í–‰\n",
    "query = \"ë­ì²´ì¸ì´ ë­”ì§€ ì„¤ëª…í•´\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# 8. ì¶œë ¥\n",
    "print(\"ğŸ§  ë‹µë³€:\\n\", result[\"result\"])\n",
    "print(\"\\nğŸ“ ì¶œì²˜ ë¬¸ì„œ:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    meta = doc.metadata\n",
    "    print(f\"ğŸ”¹ {meta.get('chapter_info', 'ì •ë³´ ì—†ìŒ')} > {meta.get('section_info', 'ì •ë³´ ì—†ìŒ')}\")\n",
    "    print(doc.page_content[:300], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    # íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\wikidocs_01.txt\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    # ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  docs ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìì‹ ë¶„í• ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"test_01\", embedding_function=HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Retriever ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œë¥¼ ê²€ìƒ‰ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤. docsëŠ” ë¬¸ì„œ ëª©ë¡ì´ê³ , idsëŠ” ë¬¸ì„œì˜ ê³ ìœ  ì‹ë³„ì ëª©ë¡ì…ë‹ˆë‹¤.\n",
    "retriever.add_documents(docs, ids=None, add_to_docstore=True)\n",
    "\n",
    "# ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "sub_docs = vectorstore.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5ca58a77-98d8-4874-a6b7-bf0e4c74bef0', metadata={'doc_id': '7738ffb4-e36b-43e2-8aa0-b00d465827ae', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='1. **Introduction to LangChain and LCEL**: LangChain offers a way to build complex chains from basic components, supporting functionalities like streaming, parallelism, and logging. LCEL (LangChain Expression Language) simplifies the process of chaining together prompts, models, and output parsers to perform tasks like generating jokes based on a given topic or conducting retrieval-augmented generation.'),\n",
       " Document(id='b20744d0-77c4-404e-a72b-d399c3c12604', metadata={'doc_id': '7738ffb4-e36b-43e2-8aa0-b00d465827ae', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='\"langchain-kr/README.md at main Â· teddylee777/langchain-kr - GitHub\", \"url\": \"https://github.com/teddylee777/langchain-kr/blob/main/README.md\", \"content\": \"ğŸ“˜ LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ ğŸŒŸ LangChain ê³µì‹ Document, Cookbook, ê·¸ ë°–ì˜ ì‹¤ìš© ì˜ˆì œ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•œ í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ì…ë‹ˆë‹¤.'),\n",
       " Document(id='c64ee76b-0eff-479f-af01-82abe735b61c', metadata={'doc_id': '7738ffb4-e36b-43e2-8aa0-b00d465827ae', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='```\\n[Query] LangChain ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”.\\n====================================\\n[0] ìœ ì‚¬ë„: 399.644 | LangChainì€ ì´ˆê±°ëŒ€ ì–¸ì–´ëª¨ë¸ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.\\n[1] ìœ ì‚¬ë„: 356.518 | ë­ì²´ì¸ í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ì€ LangChainì˜ ê³µì‹ ë¬¸ì„œ, cookbook ë° ë‹¤ì–‘í•œ ì‹¤ìš© ì˜ˆì œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ ì‚¬ìš©ìê°€ LangChainì„ ë” ì‰½ê³  íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\\n[2] ìœ ì‚¬ë„: 322.359 | LangChain simplifies the process of building applications with large language models\\n[3] ìœ ì‚¬ë„: 321.078 | ì•ˆë…•, ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ.'),\n",
       " Document(id='989b1a40-741e-4001-99da-8f7874b47a5d', metadata={'doc_id': '7738ffb4-e36b-43e2-8aa0-b00d465827ae', 'source': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Presentation-Agent\\\\data\\\\txt\\\\wikidocs_01.txt'}, page_content='```Document & Document Loaders\\nì°¸ê³ \\nLangChain ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ë¡œë” (https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)LangChain ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ë¡œë”\\nLangChain ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¡œë” ëª©ë¡ (https://python.langchain.com/v0.1/docs/integrations/document_loaders/)LangChain ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¡œë” ëª©ë¡\\nì‹¤ìŠµì— í™œìš©í•œ ë¬¸ì„œ\\nì†Œí”„íŠ¸ì›¨ì–´ì •ì±…ì—°êµ¬ì†Œ(SPRi) - 2023ë…„ 12ì›”í˜¸\\nì €ì: ìœ ì¬í¥(AIì •ì±…ì—°êµ¬ì‹¤ ì±…ì„ì—°êµ¬ì›), ì´ì§€ìˆ˜(AIì •ì±…ì—°êµ¬ì‹¤ ìœ„ì´‰ì—°êµ¬ì›)\\nhttps://spri.kr/posts/view/23669 (https://spri.kr/posts/view/23669)https://spri.kr/posts/view/23669')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Introduction to LangChain and LCEL**: LangChain offers a way to build complex chains from basic components, supporting functionalities like streaming, parallelism, and logging. LCEL (LangChain Expression Language) simplifies the process of chaining together prompts, models, and output parsers to perform tasks like generating jokes based on a given topic or conducting retrieval-augmented generation.\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œì˜ ê¸¸ì´: 1800480\n",
      "\n",
      "=====================\n",
      "\n",
      "ëŠ” ëª¨ë“ˆì‹ìœ¼ë¡œ ì„¤ê³„ë˜ì–´, ì‚¬ìš©í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ì´ëŠ” ê°œë°œìê°€ LangChain í”„ë ˆì„ì›Œí¬ë¥¼ ììœ ë¡­ê²Œ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
      "ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´ì¸ ğŸš€\n",
      "ê³ ìˆ˜ì¤€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì»´í¬ë„ŒíŠ¸ì˜ ë‚´ì¥ ì¡°í•©ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "ì´ëŸ¬í•œ ì²´ì¸ì€ ê°œë°œ ê³¼ì •ì„ ê°„ì†Œí™”í•˜ê³  ì†ë„ë¥¼ ë†’ì—¬ì¤ë‹ˆë‹¤.\n",
      "ì£¼ìš” ëª¨ë“ˆ ğŸ“Œ\n",
      "ëª¨ë¸ I/O ğŸ“ƒ\n",
      "í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ìµœì í™” ë° LLMê³¼ì˜ ì¼ë°˜ì ì¸ ì¸í„°í˜ì´ìŠ¤ì™€ ì‘ì—…ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "ê²€ìƒ‰ ğŸ“š\n",
      "'ë°ì´í„° ê°•í™” ìƒì„±'ì— ì´ˆì ì„ ë§ì¶˜ ì´ ëª¨ë“ˆì€ ìƒì„± ë‹¨ê³„ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì‘ì—…ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
      "ì—ì´ì „íŠ¸ ğŸ¤–\n",
      "ì–¸ì–´ ëª¨ë¸ì´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í• ì§€ ê²°ì •í•˜ê³ , í•´ë‹¹ ì¡°ì¹˜ë¥¼ ì‹¤í–‰í•˜ë©°, ê´€ì°°í•˜ê³ , í•„ìš”í•œ ê²½ìš° ë°˜ë³µí•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "LangChainì„ í™œìš©í•˜ë©´, ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê°œë°œì„ ë³´ë‹¤ ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìœ¼ë©°, í•„ìš”ì— ë§ê²Œ ê¸°ëŠ¥ì„ ë§ì¶¤ ì„¤ì •í•˜ê³ , ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ í†µí•©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Langchain\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì˜ ê¸¸ì´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(\n",
    "    f\"ë¬¸ì„œì˜ ê¸¸ì´: {len(retrieved_docs[0].page_content)}\",\n",
    "    end=\"\\n\\n=====================\\n\\n\",\n",
    ")\n",
    "\n",
    "# ë¬¸ì„œì˜ ì¼ë¶€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(retrieved_docs[0].page_content[2000:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶€ëª¨ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì…ë‹ˆë‹¤.\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 250, separators=['=================================================='])\n",
    "# ìì‹ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì…ë‹ˆë‹¤.\n",
    "# ë¶€ëª¨ë³´ë‹¤ ì‘ì€ ë¬¸ì„œë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 100)\n",
    "# ìì‹ ì²­í¬ë¥¼ ì¸ë±ì‹±í•˜ëŠ” ë° ì‚¬ìš©í•  ë²¡í„° ì €ì¥ì†Œì…ë‹ˆë‹¤.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    ")\n",
    "# ë¶€ëª¨ ë¬¸ì„œì˜ ì €ì¥ ê³„ì¸µì…ë‹ˆë‹¤.\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    # ë²¡í„° ì €ì¥ì†Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    vectorstore=vectorstore,\n",
    "    # ë¬¸ì„œ ì €ì¥ì†Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    docstore=store,\n",
    "    # í•˜ìœ„ ë¬¸ì„œ ë¶„í• ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    child_splitter=child_splitter,\n",
    "    # ìƒìœ„ ë¬¸ì„œ ë¶„í• ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== <ë­ì²´ì¸LangChain ë…¸íŠ¸> - LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡· ===\n"
     ]
    }
   ],
   "source": [
    "retriever.add_documents(docs)\n",
    "\n",
    "# ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "sub_docs = vectorstore.similarity_search(\"Langchain\")\n",
    "\n",
    "# sub_docs ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œì˜ page_content ì†ì„±ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== <ë­ì²´ì¸LangChain ë…¸íŠ¸> - LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡· ===\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "retrieved_docs = retriever.invoke(\"Langchain\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== <ë­ì²´ì¸LangChain ë…¸íŠ¸> - LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡· ===\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- 04. RAPTOR: ê¸´ ë¬¸ë§¥ ìš”ì•½(Long Context Summary) ---\n",
      "\n",
      "```\n",
      "Self-querying ë°©ë²•ì€ ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ë°›ì•„ êµ¬ì¡°í™”ëœ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ VectorStoreì— ì ìš©í•˜ì—¬ ë¬¸ì„œì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¹„êµ ë° ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œí•œ í•„í„°ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì‹¤í–‰í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Chroma vector storeë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜í™” ìš”ì•½ ë¬¸ì„œê°€ í¬í•¨ëœ ì‘ì€ ë°ëª¨ ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ í†µí•´ ìì²´ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ìì²´ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ ì½”ë“œì…ë‹ˆë‹¤:\n",
      "```python\n",
      "%pip install --upgrade --quiet  lark chromadb\n",
      "from langchain_community.vectorstores import Chroma\n",
      "from langchain_core.documents import Document\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "docs = [\n",
      "Document(\n",
      "page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
      "metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
      "),\n",
      "# Additional documents...\n",
      "]\n",
      "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n",
      "from langchain.chains.query_constructor.base import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "from langchain_openai import ChatOpenAI\n",
      "metadata_field_info = [\n",
      "AttributeInfo(\n",
      "name=\"genre\",\n",
      "description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
      "type=\"string\",\n",
      "),\n",
      "# Additional metadata fields...\n",
      "]\n",
      "document_content_description = \"Brief summary of a movie\"\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "retriever = SelfQueryRetriever.from_llm(\n",
      "llm,\n",
      "vectorstore,\n",
      "document_content_description,\n",
      "metadata_field_info,\n",
      ")\n",
      "# example usage\n",
      "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")\n",
      "```ì„¤ì¹˜\n",
      "pip install -qU langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic\n",
      "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "RAPTOR (https://arxiv.org/pdf/2401.18059.pdf)RAPTOR ë…¼ë¬¸ì€ ë¬¸ì„œì˜ ìƒ‰ì¸ ìƒì„± ë° ê²€ìƒ‰ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
      "í…Œë””ë…¸íŠ¸ ë…¼ë¬¸ ìš”ì•½ê¸€(ë…¸ì…˜) (https://teddylee777.notion.site/RAPTOR-e835d306fc664dc2ad76191dee1cd859?pvs=4)í…Œë””ë…¸íŠ¸ ë…¼ë¬¸ ìš”ì•½ê¸€(ë…¸ì…˜)\n",
      "leafsëŠ” ì‹œì‘ ë¬¸ì„œ ì§‘í•©ì…ë‹ˆë‹¤.\n",
      "leafsëŠ” ì„ë² ë”©ë˜ì–´ í´ëŸ¬ìŠ¤í„°ë§ë©ë‹ˆë‹¤.\n",
      "ê·¸ëŸ° ë‹¤ìŒ í´ëŸ¬ìŠ¤í„°ëŠ” ìœ ì‚¬í•œ ë¬¸ì„œë“¤ ê°„ì˜ ì •ë³´ë¥¼ ë” ë†’ì€ ìˆ˜ì¤€(ë” ì¶”ìƒì ì¸)ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\n",
      "ì´ ê³¼ì •ì€ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´, ì›ë³¸ ë¬¸ì„œ(leafs)ì—ì„œ ë” ì¶”ìƒì ì¸ ìš”ì•½ìœ¼ë¡œ ì´ì–´ì§€ëŠ” \"íŠ¸ë¦¬\"ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤.\n",
      "ì´ë¥¼ ë‹¤ì–‘í•œ ê·œëª¨ì—ì„œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤; leafsëŠ” ë‹¤ìŒê³¼ ê°™ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "ë‹¨ì¼ ë¬¸ì„œì—ì„œì˜ í…ìŠ¤íŠ¸ ì²­í¬(ë…¼ë¬¸ì—ì„œ ë³´ì—¬ì¤€ ê²ƒì²˜ëŸ¼)\n",
      "ì „ì²´ ë¬¸ì„œ(ì•„ë˜ì—ì„œ ë³´ì—¬ì£¼ëŠ” ê²ƒì²˜ëŸ¼)\n",
      "ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ì˜ LLMsë¥¼ ì‚¬ìš©í•˜ë©´, ì „ì²´ ë¬¸ì„œì— ëŒ€í•´ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ë¬¸ì„œ\n",
      "LangChainì˜ LCEL ë¬¸ì„œì— ì´ë¥¼ ì ìš©í•´ ë´…ì‹œë‹¤.\n",
      "ì´ ê²½ìš°, ê° docì€ LCEL ë¬¸ì„œì˜ ê³ ìœ í•œ ì›¹ í˜ì´ì§€ì…ë‹ˆë‹¤.\n",
      "ì½˜í…ìŠ¤íŠ¸ëŠ” 2,000 í† í° ë¯¸ë§Œì—ì„œ 10,000 í† í° ì´ìƒê¹Œì§€ ë‹¤ì–‘í•©ë‹ˆë‹¤.\n",
      "ì›¹ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì¸ì½”ë”© ì´ë¦„ì— ë”°ë¼ ë¬¸ìì—´ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
      "RecursiveUrlLoader í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ URLì—ì„œ ì›¹ ë¬¸ì„œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ BeautifulSoupë¥¼ í™œìš©í•˜ì—¬ HTML ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
      "ì—¬ëŸ¬ URLì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ ëª¨ë“  í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ì— ëª¨ìë‹ˆë‹¤.\n",
      "ê° ë¬¸ì„œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ num_tokens_from_string í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
      "matplotlibë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ëœ í† í° ìˆ˜ì˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤. íˆìŠ¤í† ê·¸ë¨ì€ í† í° ìˆ˜ë¥¼ xì¶•ì—, í•´ë‹¹ í† í° ìˆ˜ë¥¼ ê°€ì§„ ë¬¸ì„œì˜ ë¹ˆë„ìˆ˜ë¥¼ yì¶•ì— ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
      "íˆìŠ¤í† ê·¸ë¨ì€ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ë©°, íŠ¹íˆ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ê¸¸ì´ ë¶„í¬ë¥¼ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
      "from bs4 import BeautifulSoup as Soup\n",
      "import tiktoken\n",
      "import matplotlib.pyplot as plt\n",
      "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
      "# ì£¼ì–´ì§„ ë¬¸ìì—´ì—ì„œ í† í°ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "encoding = tiktoken.get_encoding(encoding_name)\n",
      "num_tokens = len(encoding.encode(string))\n",
      "return num_tokens\n",
      "# LCEL ë¬¸ì„œ ë¡œë“œ\n",
      "url = \"https://python.langchain.com/docs/expression_language/\"\n",
      "loader = RecursiveUrlLoader(\n",
      "url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
      ")\n",
      "docs = loader.load()\n",
      "# PydanticOutputParserë¥¼ ì‚¬ìš©í•œ LCEL ë¬¸ì„œ ë¡œë“œ (ê¸°ë³¸ LCEL ë¬¸ì„œ ì™¸ë¶€)\n",
      "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
      "loader = RecursiveUrlLoader(\n",
      "url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
      ")\n",
      "docs_pydantic = loader.load()\n",
      "# Self Queryë¥¼ ì‚¬ìš©í•œ LCEL ë¬¸ì„œ ë¡œë“œ (ê¸°ë³¸ LCEL ë¬¸ì„œ ì™¸ë¶€)\n",
      "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
      "loader = RecursiveUrlLoader(\n",
      "url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
      ")\n",
      "docs_sq = loader.load()\n",
      "# ë¬¸ì„œ í…ìŠ¤íŠ¸\n",
      "docs.extend([*docs_pydantic, *docs_sq])\n",
      "docs_texts = [d.page_content for d in docs]\n",
      "# ê° ë¬¸ì„œì— ëŒ€í•œ í† í° ìˆ˜ ê³„ì‚°\n",
      "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
      "# í† í° ìˆ˜ì˜ íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë¦½ë‹ˆë‹¤.\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
      "plt.title(\"Token Counts in LCEL Documents\")\n",
      "plt.xlabel(\"Token Count\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "plt.grid(axis=\"y\", alpha=0.75)\n",
      "# íˆìŠ¤í† ê·¸ë¨ì„ í‘œì‹œí•©ë‹ˆë‹¤.\n",
      "plt.show\n",
      "ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ê³  ì—°ê²°í•˜ì—¬ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "ë¬¸ì„œ(docs)ë¥¼ ë©”íƒ€ë°ì´í„°ì˜ \"source\" í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
      "ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—­ìˆœìœ¼ë¡œ ë’¤ì§‘ìŠµë‹ˆë‹¤.\n",
      "ì—­ìˆœìœ¼ë¡œ ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ íŠ¹ì • êµ¬ë¶„ì(\"\\n\\n\\n --- \\n\\n\\n\")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê²°í•©ë‹ˆë‹¤.\n",
      "ì—°ê²°ëœ ë‚´ìš©ì˜ í† í° ìˆ˜ë¥¼ num_tokens_from_string í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ì´ë•Œ, \"cl100k_base\" ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "# ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
      "# ë¬¸ì„œë¥¼ ì¶œì²˜ ë©”íƒ€ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
      "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
      "d_reversed = list(reversed(d_sorted))  # ì •ë ¬ëœ ë¬¸ì„œë¥¼ ì—­ìˆœìœ¼ë¡œ ë°°ì—´í•©ë‹ˆë‹¤.\n",
      "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
      "[\n",
      "# ì—­ìˆœìœ¼ë¡œ ë°°ì—´ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì—°ê²°í•©ë‹ˆë‹¤.\n",
      "doc.page_content\n",
      "for doc in d_reversed\n",
      "]\n",
      ")\n",
      "print(\n",
      "\"Num tokens in all context: %s\"  # ëª¨ë“  ë¬¸ë§¥ì—ì„œì˜ í† í° ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
      "% num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
      ")\n",
      "[ì´ë¯¸ì§€: ]Num tokens in all context: 69074\n",
      "RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "chunk_size_tok ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬, ê° í…ìŠ¤íŠ¸ ì²­í¬ì˜ í¬ê¸°ë¥¼ 2000 í† í°ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
      "RecursiveCharacterTextSplitterì˜ from_tiktoken_encoder ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì²­í¬ í¬ê¸°(chunk_size)ì™€ ì²­í¬ ê°„ ê²¹ì¹¨(chunk_overlap)ì„ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
      "ì´ˆê¸°í™”ëœ í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì˜ split_text ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ì—¬, concatenated_contentë¼ëŠ” ë³€ìˆ˜ì— ì €ì¥ëœ ì—°ê²°ëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤. ë¶„í•  ê²°ê³¼ëŠ” texts_split ë³€ìˆ˜ì— ì €ì¥ë©ë‹ˆë‹¤.\n",
      "# í…ìŠ¤íŠ¸ ë¶„í• ì„ ìœ„í•œ ì½”ë“œ\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "chunk_size_tok = 2000  # í† í°ì˜ ì²­í¬ í¬ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
      "# ì¬ê·€ì  ë¬¸ì í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. í† í° ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²­í¬ í¬ê¸°ì™€ ì¤‘ë³µì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
      "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
      "chunk_size=chunk_size_tok, chunk_overlap=0\n",
      ")\n",
      "texts_split = text_splitter.split_text(\n",
      "concatenated_content\n",
      ")  # ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\n",
      "ëª¨ë¸\n",
      "Claude3 (https://www.anthropic.com/news/claude-3-family)Claude3 ê³„ì—´ë„ í¬í•¨ë©ë‹ˆë‹¤.\n",
      "ê´€ë ¨ API í‚¤ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”.\n",
      "OPENAI_API_KEY, Anthropic ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ANTHROPIC_API_KEY\n",
      "ChatOpenAI í˜¹ì€ ChatAnthropic + OpenAIEmbeddingsë¥¼ ì‚¬ìš©í•˜ì—¬ ì±—ë´‡ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
      "OpenAIEmbeddingsë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ì—¬ OpenAIì˜ ì„ë² ë”© ê¸°ëŠ¥ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
      "ChatOpenAI í˜¹ì€ ChatAnthropic ì„ ì‚¬ìš©í•˜ì—¬ temperatureë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ê³ , ì±—ë´‡ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv()\n",
      "True\n",
      "Cache Embedding ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "from langchain.embeddings import CacheBackedEmbeddings\n",
      "from langchain.storage import LocalFileStore\n",
      "store = LocalFileStore(\"./cache/\")\n",
      "# embeddings ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "embd = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\n",
      "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
      "embd, store, namespace=embd.model\n",
      ")\n",
      "ëª¨ë¸ì„ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.\n",
      "from langchain_anthropic import ChatAnthropic\n",
      "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
      "from langchain.callbacks.base import BaseCallbackHandler\n",
      "class StreamCallback(BaseCallbackHandler):\n",
      "def on_llm_new_token(self, token: str, **kwargs):\n",
      "print(token, end=\"\", flush=True)\n",
      "# ChatOpenAI ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì€ \"gpt-4-turbo-preview\"ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "model = ChatOpenAI(\n",
      "model=\"gpt-4-turbo-preview\",\n",
      "temperature=0,\n",
      "streaming=True,\n",
      "callbacks=[StreamCallback()],\n",
      ")\n",
      "# ChatAnthropic ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì˜¨ë„ëŠ” 0ìœ¼ë¡œ ì„¤ì •í•˜ê³ , ëª¨ë¸ì€ \"claude-3-opus-20240229\"ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "# model = ChatAnthropic(temperature=0, model=\"claude-3-opus-20240229\")\n",
      "íŠ¸ë¦¬ êµ¬ì¶•\n",
      "íŠ¸ë¦¬ êµ¬ì¶•ì—ì„œì˜ í´ëŸ¬ìŠ¤í„°ë§ ì ‘ê·¼ ë°©ì‹ì—ëŠ” ëª‡ ê°€ì§€ í¥ë¯¸ë¡œìš´ ì•„ì´ë””ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
      "GMM (ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸)\n",
      "ë‹¤ì–‘í•œ í´ëŸ¬ìŠ¤í„°ì— ê±¸ì³ ë°ì´í„° í¬ì¸íŠ¸ì˜ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤.\n",
      "ëª¨ë¸ì˜ ë² ì´ì§€ì•ˆ ì •ë³´ ê¸°ì¤€(BIC)ì„ í‰ê°€í•˜ì—¬ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
      "UMAP (Uniform Manifold Approximation and Projection)\n",
      "í´ëŸ¬ìŠ¤í„°ë§ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "ê³ ì°¨ì› ë°ì´í„°ì˜ ì°¨ì›ì„ ì¶•ì†Œí•©ë‹ˆë‹¤.\n",
      "UMAPì€ ë°ì´í„° í¬ì¸íŠ¸ì˜ ìœ ì‚¬ì„±ì— ê¸°ë°˜í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ê·¸ë£¹í™”ë¥¼ ê°•ì¡°í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n",
      "ì§€ì—­ ë° ì „ì—­ í´ëŸ¬ìŠ¤í„°ë§\n",
      "ë‹¤ì–‘í•œ ê·œëª¨ì—ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "ë°ì´í„° ë‚´ì˜ ì„¸ë°€í•œ íŒ¨í„´ê³¼ ë” ë„“ì€ íŒ¨í„´ ëª¨ë‘ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤.\n",
      "ì„ê³„ê°’ ì„¤ì •\n",
      "GMMì˜ ë§¥ë½ì—ì„œ í´ëŸ¬ìŠ¤í„° ë©¤ë²„ì‹­ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì ìš©ë©ë‹ˆë‹¤.\n",
      "í™•ë¥  ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤(ë°ì´í„° í¬ì¸íŠ¸ë¥¼ â‰¥ 1 í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹).\n",
      "GMM ë° ì„ê³„ê°’ ì„¤ì •ì— ëŒ€í•œ ì½”ë“œëŠ” ì•„ë˜ ë‘ ì¶œì²˜ì—ì„œ ì–¸ê¸‰ëœ Sarthi et alì˜ ê²ƒì…ë‹ˆë‹¤:\n",
      "ì›ë³¸ ì €ì¥ì†Œ (https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py)ì›ë³¸ ì €ì¥ì†Œ\n",
      "ì†Œì†Œí•œ ì¡°ì • (https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/clustering.py)ì†Œì†Œí•œ ì¡°ì •\n",
      "ë‘ ì €ì ëª¨ë‘ì—ê²Œ ì „ì ì¸ ê³µë¡œë¥¼ ì¸ì •í•©ë‹ˆë‹¤.\n",
      "global_cluster_embeddings í•¨ìˆ˜ëŠ” ì„ë² ë”©ì˜ ê¸€ë¡œë²Œ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ UMAPì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "ì…ë ¥ëœ ì„ë² ë”©(embeddings)ì„ UMAPì„ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ì°¨ì›(dim)ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œí•©ë‹ˆë‹¤.\n",
      "n_neighborsëŠ” ê° í¬ì¸íŠ¸ë¥¼ ê³ ë ¤í•  ì´ì›ƒì˜ ìˆ˜ë¥¼ ì§€ì •í•˜ë©°, ì œê³µë˜ì§€ ì•Šì„ ê²½ìš° ì„ë² ë”© ìˆ˜ì˜ ì œê³±ê·¼ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì •ë©ë‹ˆë‹¤.\n",
      "metricì€ UMAPì— ì‚¬ìš©ë  ê±°ë¦¬ ì¸¡ì • ê¸°ì¤€ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
      "ê²°ê³¼ë¡œ, ì§€ì •ëœ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œëœ ì„ë² ë”©ì´ numpy ë°°ì—´ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.\n",
      "from typing import Dict, List, Optional, Tuple\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import umap\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "from sklearn.mixture import GaussianMixture\n",
      "RANDOM_SEED = 42  # ì¬í˜„ì„±ì„ ìœ„í•œ ê³ ì •ëœ ì‹œë“œ ê°’\n",
      "### --- ìœ„ì˜ ì¸ìš©ëœ ì½”ë“œì—ì„œ ì£¼ì„ê³¼ ë¬¸ì„œí™”ë¥¼ ì¶”ê°€í•¨ --- ###\n",
      "def global_cluster_embeddings(\n",
      "embeddings: np.ndarray,\n",
      "dim: int,\n",
      "n_neighbors: Optional[int] = None,\n",
      "metric: str = \"cosine\",\n",
      ") -> np.ndarray:\n",
      "\"\"\"\n",
      "UMAPì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì˜ ì „ì—­ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- embeddings: numpy ë°°ì—´ë¡œ ëœ ì…ë ¥ ì„ë² ë”©.\n",
      "- dim: ì¶•ì†Œëœ ê³µê°„ì˜ ëª©í‘œ ì°¨ì›.\n",
      "- n_neighbors: ì„ íƒ ì‚¬í•­; ê° ì ì„ ê³ ë ¤í•  ì´ì›ƒì˜ ìˆ˜.\n",
      "ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ì„ë² ë”© ìˆ˜ì˜ ì œê³±ê·¼ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì •ë©ë‹ˆë‹¤.\n",
      "- metric: UMAPì— ì‚¬ìš©í•  ê±°ë¦¬ ì¸¡ì • ê¸°ì¤€.\n",
      "ë°˜í™˜ê°’:\n",
      "- ì§€ì •ëœ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œëœ ì„ë² ë”©ì˜ numpy ë°°ì—´.\n",
      "\"\"\"\n",
      "if n_neighbors is None:\n",
      "n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
      "return umap.UMAP(\n",
      "n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
      ").fit_transform(embeddings)\n",
      "ì„ë² ë”© ë°ì´í„°ì— ëŒ€í•´ ì§€ì—­ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ local_cluster_embeddingsë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
      "ì…ë ¥ëœ ì„ë² ë”©(embeddings)ì„ UMAPì„ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ì°¨ì›(dim)ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œí•©ë‹ˆë‹¤.\n",
      "ì°¨ì› ì¶•ì†Œ ê³¼ì •ì—ì„œ ê° ì ì— ëŒ€í•´ ê³ ë ¤í•  ì´ì›ƒì˜ ìˆ˜(num_neighbors)ì™€ ê±°ë¦¬ ì¸¡ì • ë©”íŠ¸ë¦­(metric)ì„ íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "ìµœì¢…ì ìœ¼ë¡œ ì°¨ì›ì´ ì¶•ì†Œëœ ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "def local_cluster_embeddings(\n",
      "embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
      ") -> np.ndarray:\n",
      "\"\"\"\n",
      "ì„ë² ë”©ì— ëŒ€í•´ ì§€ì—­ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì „ì—­ í´ëŸ¬ìŠ¤í„°ë§ ì´í›„ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- embeddings: numpy ë°°ì—´ë¡œì„œì˜ ì…ë ¥ ì„ë² ë”©.\n",
      "- dim: ì¶•ì†Œëœ ê³µê°„ì˜ ëª©í‘œ ì°¨ì› ìˆ˜.\n",
      "- num_neighbors: ê° ì ì— ëŒ€í•´ ê³ ë ¤í•  ì´ì›ƒì˜ ìˆ˜.\n",
      "- metric: UMAPì— ì‚¬ìš©í•  ê±°ë¦¬ ì¸¡ì • ê¸°ì¤€.\n",
      "ë°˜í™˜ê°’:\n",
      "- ì§€ì •ëœ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œëœ ì„ë² ë”©ì˜ numpy ë°°ì—´.\n",
      "\"\"\"\n",
      "return umap.UMAP(\n",
      "n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
      ").fit_transform(embeddings)\n",
      "get_optimal_clusters í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì„ë² ë”© ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(Gaussian Mixture Model)ì„ ì‚¬ìš©í•˜ì—¬ ë² ì´ì§€ì•ˆ ì •ë³´ ê¸°ì¤€(Bayesian Information Criterion, BIC)ì„ ê³„ì‚°í•¨ìœ¼ë¡œì¨ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n",
      "ì…ë ¥ ì„ë² ë”©(embeddings)ì€ numpy ë°°ì—´ë¡œ ì œê³µë©ë‹ˆë‹¤.\n",
      "ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜(max_clusters)ëŠ” ê³ ë ¤í•  í´ëŸ¬ìŠ¤í„°ì˜ ìµœëŒ€ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ 50ì…ë‹ˆë‹¤.\n",
      "ì¬í˜„ì„±ì„ ìœ„í•œ ë‚œìˆ˜ ìƒíƒœ(random_state)ëŠ” ê³ ì •ëœ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "í•¨ìˆ˜ëŠ” ì…ë ¥ ì„ë² ë”©ì— ëŒ€í•´ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì‹œë„í•˜ë©° ê°ê°ì— ëŒ€í•œ BIC ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
      "ìµœì†Œ BIC ê°’ì„ ê°€ì§€ëŠ” í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ ê²°ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "ì´ í•¨ìˆ˜ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ë¬¸ì œì—ì„œ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì°¾ëŠ” ë° ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "def get_optimal_clusters(\n",
      "embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
      ") -> int:\n",
      "\"\"\"\n",
      "ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(Gaussian Mixture Model)ì„ ì‚¬ìš©í•˜ì—¬ ë² ì´ì§€ì•ˆ ì •ë³´ ê¸°ì¤€(BIC)ì„ í†µí•´ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- embeddings: numpy ë°°ì—´ë¡œì„œì˜ ì…ë ¥ ì„ë² ë”©.\n",
      "- max_clusters: ê³ ë ¤í•  ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜.\n",
      "- random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ.\n",
      "ë°˜í™˜ê°’:\n",
      "- ë°œê²¬ëœ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜.\n",
      "\"\"\"\n",
      "max_clusters = min(\n",
      "max_clusters, len(embeddings)\n",
      ")  # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ì™€ ì„ë² ë”©ì˜ ê¸¸ì´ ì¤‘ ì‘ì€ ê°’ì„ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ ì„¤ì •\n",
      "n_clusters = np.arange(1, max_clusters)  # 1ë¶€í„° ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ê¹Œì§€ì˜ ë²”ìœ„ë¥¼ ìƒì„±\n",
      "bics = []  # BIC ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
      "for n in n_clusters:  # ê° í´ëŸ¬ìŠ¤í„° ìˆ˜ì— ëŒ€í•´ ë°˜ë³µ\n",
      "gm = GaussianMixture(\n",
      "n_components=n, random_state=random_state\n",
      ")  # ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ ì´ˆê¸°í™”\n",
      "gm.fit(embeddings)  # ì„ë² ë”©ì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ\n",
      "bics.append(gm.bic(embeddings))  # í•™ìŠµëœ ëª¨ë¸ì˜ BIC ì ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
      "return n_clusters[np.argmin(bics)]  # BIC ì ìˆ˜ê°€ ê°€ì¥ ë‚®ì€ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë°˜í™˜\n",
      "GMM_cluster í•¨ìˆ˜ëŠ” ì„ë² ë”©ì„ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(Gaussian Mixture Model, GMM)ì„ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ í™•ë¥  ì„ê³„ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n",
      "ì…ë ¥ëœ ì„ë² ë”©(embeddings)ì€ numpy ë°°ì—´ë¡œ ì œê³µë©ë‹ˆë‹¤.\n",
      "thresholdëŠ” ì„ë² ë”©ì„ íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•˜ê¸° ìœ„í•œ í™•ë¥  ì„ê³„ê°’ì…ë‹ˆë‹¤.\n",
      "random_stateëŠ” ê²°ê³¼ì˜ ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê°’ì…ë‹ˆë‹¤.\n",
      "ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ get_optimal_clusters í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
      "ê²°ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³ , ì…ë ¥ëœ ì„ë² ë”©ì— ëŒ€í•´ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "ê° ì„ë² ë”©ì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„° í• ë‹¹ í™•ë¥ ì„ ê³„ì‚°í•˜ê³ , ì´ í™•ë¥ ì´ ì£¼ì–´ì§„ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° í•´ë‹¹ ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•©ë‹ˆë‹¤.\n",
      "í•¨ìˆ˜ëŠ” ìµœì¢…ì ìœ¼ë¡œ ì„ë² ë”©ì˜ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ê³¼ ê²°ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ íŠœí”Œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
      "\"\"\"\n",
      "í™•ë¥  ì„ê³„ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(GMM)ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ë§í•©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- embeddings: numpy ë°°ì—´ë¡œì„œì˜ ì…ë ¥ ì„ë² ë”©.\n",
      "- threshold: ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•˜ê¸° ìœ„í•œ í™•ë¥  ì„ê³„ê°’.\n",
      "- random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ.\n",
      "ë°˜í™˜ê°’:\n",
      "- í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ê³¼ ê²°ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” íŠœí”Œ.\n",
      "\"\"\"\n",
      "n_clusters = get_optimal_clusters(embeddings)  # ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ êµ¬í•©ë‹ˆë‹¤.\n",
      "# ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
      "gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
      "gm.fit(embeddings)  # ì„ë² ë”©ì— ëŒ€í•´ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "probs = gm.predict_proba(\n",
      "embeddings\n",
      ")  # ì„ë² ë”©ì´ ê° í´ëŸ¬ìŠ¤í„°ì— ì†í•  í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
      "# ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” í™•ë¥ ì„ ê°€ì§„ í´ëŸ¬ìŠ¤í„°ë¥¼ ë ˆì´ë¸”ë¡œ ì„ íƒí•©ë‹ˆë‹¤.\n",
      "labels = [np.where(prob > threshold)[0] for prob in probs]\n",
      "return labels, n_clusters  # ë ˆì´ë¸”ê³¼ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "perform_clustering í•¨ìˆ˜ëŠ” ì„ë² ë”©ì— ëŒ€í•´ ì°¨ì› ì¶•ì†Œ, ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ì„ ì‚¬ìš©í•œ ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë§, ê·¸ë¦¬ê³  ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œì˜ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "ì…ë ¥ëœ ì„ë² ë”©(embeddings)ì— ëŒ€í•´ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ëŠ” UMAPì„ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ì°¨ì›(dim)ìœ¼ë¡œ ì„ë² ë”©ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "ì°¨ì›ì´ ì¶•ì†Œëœ ì„ë² ë”©ì— ëŒ€í•´ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(GMM)ì„ ì‚¬ìš©í•˜ì—¬ ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° í• ë‹¹ì€ ì£¼ì–´ì§„ í™•ë¥  ì„ê³„ê°’(threshold)ì„ ê¸°ì¤€ìœ¼ë¡œ ê²°ì •ë©ë‹ˆë‹¤.\n",
      "ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ì¶”ê°€ì ì¸ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì„ë² ë”©ë“¤ë§Œì„ ëŒ€ìƒìœ¼ë¡œ ë‹¤ì‹œ ì°¨ì› ì¶•ì†Œ ë° GMM í´ëŸ¬ìŠ¤í„°ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
      "ìµœì¢…ì ìœ¼ë¡œ, ëª¨ë“  ì„ë² ë”©ì— ëŒ€í•´ ê¸€ë¡œë²Œ ë° ë¡œì»¬ í´ëŸ¬ìŠ¤í„° IDë¥¼ í• ë‹¹í•˜ì—¬, ê° ì„ë² ë”©ì´ ì†í•œ í´ëŸ¬ìŠ¤í„° IDë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ë¦¬ìŠ¤íŠ¸ëŠ” ì„ë² ë”©ì˜ ìˆœì„œì— ë”°ë¼ ê° ì„ë² ë”©ì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„° ID ë°°ì—´ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "ì´ í•¨ìˆ˜ëŠ” ê³ ì°¨ì› ë°ì´í„°ì˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ ê¸€ë¡œë²Œ ë° ë¡œì»¬ ì°¨ì›ì—ì„œì˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ê²°í•©í•œ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë” ì„¸ë¶„í™”ëœ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë©°, ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "def perform_clustering(\n",
      "embeddings: np.ndarray,\n",
      "dim: int,\n",
      "threshold: float,\n",
      ") -> List[np.ndarray]:\n",
      "\"\"\"\n",
      "ì„ë² ë”©ì— ëŒ€í•´ ì°¨ì› ì¶•ì†Œ, ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ì„ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§, ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œì˜ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- embeddings: numpy ë°°ì—´ë¡œ ëœ ì…ë ¥ ì„ë² ë”©ì…ë‹ˆë‹¤.\n",
      "- dim: UMAP ì¶•ì†Œë¥¼ ìœ„í•œ ëª©í‘œ ì°¨ì›ì…ë‹ˆë‹¤.\n",
      "- threshold: GMMì—ì„œ ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•˜ê¸° ìœ„í•œ í™•ë¥  ì„ê³„ê°’ì…ë‹ˆë‹¤.\n",
      "ë°˜í™˜ê°’:\n",
      "- ê° ì„ë² ë”©ì˜ í´ëŸ¬ìŠ¤í„° IDë¥¼ í¬í•¨í•˜ëŠ” numpy ë°°ì—´ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
      "\"\"\"\n",
      "if len(embeddings) <= dim + 1:\n",
      "# ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ë•Œ í´ëŸ¬ìŠ¤í„°ë§ì„ í”¼í•©ë‹ˆë‹¤.\n",
      "return [np.array([0]) for _ in range(len(embeddings))]\n",
      "# ê¸€ë¡œë²Œ ì°¨ì› ì¶•ì†Œ\n",
      "reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
      "# ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë§\n",
      "global_clusters, n_global_clusters = GMM_cluster(\n",
      "reduced_embeddings_global, threshold\n",
      ")\n",
      "all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
      "total_clusters = 0\n",
      "# ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë¥¼ ìˆœíšŒí•˜ë©° ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰\n",
      "for i in range(n_global_clusters):\n",
      "# í˜„ì¬ ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ì„ë² ë”© ì¶”ì¶œ\n",
      "global_cluster_embeddings_ = embeddings[\n",
      "np.array([i in gc for gc in global_clusters])\n",
      "]\n",
      "if len(global_cluster_embeddings_) == 0:\n",
      "continue\n",
      "if len(global_cluster_embeddings_) <= dim + 1:\n",
      "# ì‘ì€ í´ëŸ¬ìŠ¤í„°ëŠ” ì§ì ‘ í• ë‹¹ìœ¼ë¡œ ì²˜ë¦¬\n",
      "local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
      "n_local_clusters = 1\n",
      "else:\n",
      "# ë¡œì»¬ ì°¨ì› ì¶•ì†Œ ë° í´ëŸ¬ìŠ¤í„°ë§\n",
      "reduced_embeddings_local = local_cluster_embeddings(\n",
      "global_cluster_embeddings_, dim\n",
      ")\n",
      "local_clusters, n_local_clusters = GMM_cluster(\n",
      "reduced_embeddings_local, threshold\n",
      ")\n",
      "# ë¡œì»¬ í´ëŸ¬ìŠ¤í„° ID í• ë‹¹, ì´ë¯¸ ì²˜ë¦¬ëœ ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì¡°ì •\n",
      "for j in range(n_local_clusters):\n",
      "local_cluster_embeddings_ = global_cluster_embeddings_[\n",
      "np.array([j in lc for lc in local_clusters])\n",
      "]\n",
      "indices = np.where(\n",
      "(embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
      ")[1]\n",
      "for idx in indices:\n",
      "all_local_clusters[idx] = np.append(\n",
      "all_local_clusters[idx], j + total_clusters\n",
      ")\n",
      "total_clusters += n_local_clusters\n",
      "return all_local_clusters\n",
      "í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ëª©ë¡ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ embedë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
      "ì…ë ¥ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ëª©ë¡(texts)ì„ ë°›ìŠµë‹ˆë‹¤.\n",
      "embd ê°ì²´ì˜ embed_documents ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "ìƒì„±ëœ ì„ë² ë”©ì„ numpy.ndarray í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "def embed(texts):\n",
      "# í…ìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "#\n",
      "# ì´ í•¨ìˆ˜ëŠ” `embd` ê°ì²´ê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•˜ë©°, ì´ ê°ì²´ëŠ” í…ìŠ¤íŠ¸ ëª©ë¡ì„ ë°›ì•„ ê·¸ ì„ë² ë”©ì„ ë°˜í™˜í•˜ëŠ” `embed_documents` ë©”ì†Œë“œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "#\n",
      "# ë§¤ê°œë³€ìˆ˜:\n",
      "# - texts: List[str], ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ëª©ë¡ì…ë‹ˆë‹¤.\n",
      "#\n",
      "# ë°˜í™˜ê°’:\n",
      "# - numpy.ndarray: ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì— ëŒ€í•œ ì„ë² ë”© ë°°ì—´ì…ë‹ˆë‹¤.\n",
      "text_embeddings = embd.embed_documents(\n",
      "texts\n",
      ")  # í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "text_embeddings_np = np.array(text_embeddings)  # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
      "return text_embeddings_np  # ì„ë² ë”©ëœ numpy ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "embed_cluster_texts í•¨ìˆ˜ëŠ” í…ìŠ¤íŠ¸ ëª©ë¡ì„ ì„ë² ë”©í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬, ì›ë³¸ í…ìŠ¤íŠ¸, í•´ë‹¹ ì„ë² ë”©, ê·¸ë¦¬ê³  í• ë‹¹ëœ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ í¬í•¨í•˜ëŠ” pandas.DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "ìƒì„±ëœ ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì‚¬ì „ì— ì •ì˜ëœ perform_clustering í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ pandas.DataFrameì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
      "DataFrameì— ì›ë³¸ í…ìŠ¤íŠ¸, ì„ë² ë”© ë¦¬ìŠ¤íŠ¸, í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ ê°ê° ì €ì¥í•©ë‹ˆë‹¤.\n",
      "ì´ í•¨ìˆ˜ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì„ë² ë”© ìƒì„±ê³¼ í´ëŸ¬ìŠ¤í„°ë§ì„ í•˜ë‚˜ì˜ ë‹¨ê³„ë¡œ ê²°í•©í•˜ì—¬, í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ êµ¬ì¡°ì  ë¶„ì„ê³¼ ê·¸ë£¹í™”ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
      "def embed_cluster_texts(texts):\n",
      "\"\"\"\n",
      "í…ìŠ¤íŠ¸ ëª©ë¡ì„ ì„ë² ë”©í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬, í…ìŠ¤íŠ¸, ê·¸ë“¤ì˜ ì„ë² ë”©, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì´ í¬í•¨ëœ DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "ì´ í•¨ìˆ˜ëŠ” ì„ë² ë”© ìƒì„±ê³¼ í´ëŸ¬ìŠ¤í„°ë§ì„ ë‹¨ì¼ ë‹¨ê³„ë¡œ ê²°í•©í•©ë‹ˆë‹¤. ì„ë² ë”©ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ëŠ” `perform_clustering` í•¨ìˆ˜ì˜ ì‚¬ì „ ì •ì˜ëœ ì¡´ì¬ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- texts: List[str], ì²˜ë¦¬ë  í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ëª©ë¡ì…ë‹ˆë‹¤.\n",
      "ë°˜í™˜ê°’:\n",
      "- pandas.DataFrame: ì›ë³¸ í…ìŠ¤íŠ¸, ê·¸ë“¤ì˜ ì„ë² ë”©, ê·¸ë¦¬ê³  í• ë‹¹ëœ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì´ í¬í•¨ëœ DataFrameì…ë‹ˆë‹¤.\n",
      "\"\"\"\n",
      "text_embeddings_np = embed(texts)  # ì„ë² ë”© ìƒì„±\n",
      "cluster_labels = perform_clustering(\n",
      "text_embeddings_np, 10, 0.1\n",
      ")  # ì„ë² ë”©ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰\n",
      "df = pd.DataFrame()  # ê²°ê³¼ë¥¼ ì €ì¥í•  DataFrame ì´ˆê¸°í™”\n",
      "df[\"text\"] = texts  # ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥\n",
      "df[\"embd\"] = list(text_embeddings_np)  # DataFrameì— ë¦¬ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ì €ì¥\n",
      "df[\"cluster\"] = cluster_labels  # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì €ì¥\n",
      "return df\n",
      "fmt_txt í•¨ìˆ˜ëŠ” pandasì˜ DataFrameì—ì„œ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë‹¨ì¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.\n",
      "ì…ë ¥ íŒŒë¼ë¯¸í„°ë¡œ DataFrameì„ ë°›ìœ¼ë©°, ì´ DataFrameì€ í¬ë§·íŒ…í•  í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ í¬í•¨í•œ 'text' ì»¬ëŸ¼ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.\n",
      "ëª¨ë“  í…ìŠ¤íŠ¸ ë¬¸ì„œëŠ” íŠ¹ì • êµ¬ë¶„ì(\"--- --- \\n --- ---\")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê²°ë˜ì–´ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.\n",
      "í•¨ìˆ˜ëŠ” ì—°ê²°ëœ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ í¬í•¨í•˜ëŠ” ë‹¨ì¼ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "def fmt_txt(df: pd.DataFrame) -> str:\n",
      "\"\"\"\n",
      "DataFrameì— ìˆëŠ” í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë‹¨ì¼ ë¬¸ìì—´ë¡œ í¬ë§·í•©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- df: 'text' ì—´ì— í¬ë§·í•  í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ í¬í•¨ëœ DataFrame.\n",
      "ë°˜í™˜ê°’:\n",
      "- ëª¨ë“  í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ íŠ¹ì • êµ¬ë¶„ìë¡œ ê²°í•©ëœ ë‹¨ì¼ ë¬¸ìì—´.\n",
      "\"\"\"\n",
      "unique_txt = df[\"text\"].tolist()  # 'text' ì—´ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
      "return \"--- --- \\n --- --- \".join(\n",
      "unique_txt\n",
      ")  # í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì„ íŠ¹ì • êµ¬ë¶„ìë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜\n",
      "í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ê³ , í´ëŸ¬ìŠ¤í„°ë§í•˜ë©°, ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìœ ì‚¬ì„±ì— ê¸°ë°˜í•œ í´ëŸ¬ìŠ¤í„°ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ df_clusters ë°ì´í„°í”„ë ˆì„ì„ ê²°ê³¼ë¡œ í•©ë‹ˆë‹¤. ì´ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸, ì„ë² ë”©, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° í• ë‹¹ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤.\n",
      "í´ëŸ¬ìŠ¤í„° í• ë‹¹ì„ ì‰½ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ í•­ëª©ì„ í™•ì¥í•©ë‹ˆë‹¤. ê° í–‰ì€ í…ìŠ¤íŠ¸, ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë¥¼ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n",
      "í™•ì¥ëœ ë°ì´í„°í”„ë ˆì„ì—ì„œ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ ì¶”ì¶œí•˜ê³ , ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ìš”ì•½ì€ df_summary ë°ì´í„°í”„ë ˆì„ì— ì €ì¥ë©ë‹ˆë‹¤. ì´ ë°ì´í„°í”„ë ˆì„ì€ ê° í´ëŸ¬ìŠ¤í„°ì˜ ìš”ì•½, ì§€ì •ëœ ì„¸ë¶€ ìˆ˜ì¤€, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "ìµœì¢…ì ìœ¼ë¡œ, í•¨ìˆ˜ëŠ” ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„ì„ í¬í•¨í•˜ëŠ” íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì€ ì›ë³¸ í…ìŠ¤íŠ¸, ì„ë² ë”©, í´ëŸ¬ìŠ¤í„° í• ë‹¹ ì •ë³´ë¥¼ í¬í•¨í•˜ë©°, ë‘ ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì€ ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ìš”ì•½ê³¼ í•´ë‹¹ ì„¸ë¶€ ìˆ˜ì¤€, í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "def embed_cluster_summarize_texts(\n",
      "texts: List[str], level: int\n",
      ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
      "\"\"\"\n",
      "í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë¨¼ì € í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ ,\n",
      "ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•œ ë‹¤ìŒ, í´ëŸ¬ìŠ¤í„° í• ë‹¹ì„ í™•ì¥í•˜ì—¬ ì²˜ë¦¬ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê³  ê° í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- texts: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡ì…ë‹ˆë‹¤.\n",
      "- level: ì²˜ë¦¬ì˜ ê¹Šì´ë‚˜ ì„¸ë¶€ ì‚¬í•­ì„ ì •ì˜í•  ìˆ˜ ìˆëŠ” ì •ìˆ˜ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤.\n",
      "ë°˜í™˜ê°’:\n",
      "- ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„ì„ í¬í•¨í•˜ëŠ” íŠœí”Œ:\n",
      "1. ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„(`df_clusters`)ì€ ì›ë³¸ í…ìŠ¤íŠ¸, ê·¸ë“¤ì˜ ì„ë² ë”©, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° í• ë‹¹ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "2. ë‘ ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„(`df_summary`)ì€ ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ìš”ì•½, ì§€ì •ëœ ì„¸ë¶€ ìˆ˜ì¤€, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "\"\"\"\n",
      "# í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ 'text', 'embd', 'cluster' ì—´ì´ ìˆëŠ” ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "df_clusters = embed_cluster_texts(texts)\n",
      "# í´ëŸ¬ìŠ¤í„°ë¥¼ ì‰½ê²Œ ì¡°ì‘í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ì„ í™•ì¥í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\n",
      "expanded_list = []\n",
      "# ë°ì´í„°í”„ë ˆì„ í•­ëª©ì„ ë¬¸ì„œ-í´ëŸ¬ìŠ¤í„° ìŒìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì²˜ë¦¬ë¥¼ ê°„ë‹¨í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
      "for index, row in df_clusters.iterrows():\n",
      "for cluster in row[\"cluster\"]:\n",
      "expanded_list.append(\n",
      "{\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
      ")\n",
      "# í™•ì¥ëœ ëª©ë¡ì—ì„œ ìƒˆ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "expanded_df = pd.DataFrame(expanded_list)\n",
      "# ì²˜ë¦¬ë¥¼ ìœ„í•´ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
      "all_clusters = expanded_df[\"cluster\"].unique()\n",
      "print(f\"--Generated {len(all_clusters)} clusters--\")\n",
      "# ìš”ì•½\n",
      "template = \"\"\"ì—¬ê¸° LangChain í‘œí˜„ ì–¸ì–´ ë¬¸ì„œì˜ í•˜ìœ„ ì§‘í•©ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "LangChain í‘œí˜„ ì–¸ì–´ëŠ” LangChainì—ì„œ ì²´ì¸ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "ì œê³µëœ ë¬¸ì„œì˜ ìì„¸í•œ ìš”ì•½ì„ ì œê³µí•˜ì‹­ì‹œì˜¤.\n",
      "ë¬¸ì„œ:\n",
      "{context}\n",
      "\"\"\"\n",
      "prompt = ChatPromptTemplate.from_template(template)\n",
      "chain = prompt | model | StrOutputParser()\n",
      "# ê° í´ëŸ¬ìŠ¤í„° ë‚´ì˜ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½ì„ ìœ„í•´ í¬ë§·íŒ…í•©ë‹ˆë‹¤.\n",
      "summaries = []\n",
      "for i in all_clusters:\n",
      "df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
      "formatted_txt = fmt_txt(df_cluster)\n",
      "summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
      "# ìš”ì•½, í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ë° ë ˆë²¨ì„ ì €ì¥í•  ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "df_summary = pd.DataFrame(\n",
      "{\n",
      "\"summaries\": summaries,\n",
      "\"level\": [level] * len(summaries),\n",
      "\"cluster\": list(all_clusters),\n",
      "}\n",
      ")\n",
      "return df_clusters, df_summary\n",
      "í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½í•˜ëŠ” ê³¼ì •ì„ êµ¬í˜„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
      "ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½í•˜ì—¬ ê° ë‹¨ê³„ë³„ë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
      "í•¨ìˆ˜ëŠ” ìµœëŒ€ ì§€ì •ëœ ì¬ê·€ ë ˆë²¨ê¹Œì§€ ì‹¤í–‰ë˜ê±°ë‚˜, ìœ ì¼í•œ í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ê°€ 1ì´ ë  ë•Œê¹Œì§€ ë°˜ë³µë©ë‹ˆë‹¤.\n",
      "ê° ì¬ê·€ ë‹¨ê³„ì—ì„œëŠ” í˜„ì¬ ë ˆë²¨ì˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì™€ ìš”ì•½ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ í˜•íƒœë¡œ ë°˜í™˜í•˜ê³ , ì´ë¥¼ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
      "ë§Œì•½ í˜„ì¬ ë ˆë²¨ì´ ìµœëŒ€ ì¬ê·€ ë ˆë²¨ë³´ë‹¤ ì‘ê³ , ìœ ì¼í•œ í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ê°€ 1ë³´ë‹¤ í¬ë‹¤ë©´, í˜„ì¬ ë ˆë²¨ì˜ ìš”ì•½ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë ˆë²¨ì˜ ì…ë ¥ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ì¬ê·€ì ìœ¼ë¡œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
      "ìµœì¢…ì ìœ¼ë¡œ ê° ë ˆë²¨ë³„ í´ëŸ¬ìŠ¤í„° ë°ì´í„°í”„ë ˆì„ê³¼ ìš”ì•½ ë°ì´í„°í”„ë ˆì„ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "def recursive_embed_cluster_summarize(\n",
      "texts: List[str], level: int = 1, n_levels: int = 3\n",
      ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
      "\"\"\"\n",
      "ì§€ì •ëœ ë ˆë²¨ê¹Œì§€ ë˜ëŠ” ê³ ìœ  í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ê°€ 1ì´ ë  ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§, ìš”ì•½í•˜ì—¬\n",
      "ê° ë ˆë²¨ì—ì„œì˜ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
      "ë§¤ê°œë³€ìˆ˜:\n",
      "- texts: List[str], ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ë“¤.\n",
      "- level: int, í˜„ì¬ ì¬ê·€ ë ˆë²¨ (1ì—ì„œ ì‹œì‘).\n",
      "- n_levels: int, ì¬ê·€ì˜ ìµœëŒ€ ê¹Šì´.\n",
      "ë°˜í™˜ê°’:\n",
      "- Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], ì¬ê·€ ë ˆë²¨ì„ í‚¤ë¡œ í•˜ê³  í•´ë‹¹ ë ˆë²¨ì—ì„œì˜ í´ëŸ¬ìŠ¤í„° DataFrameê³¼ ìš”ì•½ DataFrameì„ í¬í•¨í•˜ëŠ” íŠœí”Œì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ì‚¬ì „.\n",
      "\"\"\"\n",
      "results = {}  # ê° ë ˆë²¨ì—ì„œì˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ì‚¬ì „\n",
      "# í˜„ì¬ ë ˆë²¨ì— ëŒ€í•´ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§, ìš”ì•½ ìˆ˜í–‰\n",
      "df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
      "# í˜„ì¬ ë ˆë²¨ì˜ ê²°ê³¼ ì €ì¥\n",
      "results[level] = (df_clusters, df_summary)\n",
      "# ì¶”ê°€ ì¬ê·€ê°€ ê°€ëŠ¥í•˜ê³  ì˜ë¯¸ê°€ ìˆëŠ”ì§€ ê²°ì •\n",
      "unique_clusters = df_summary[\"cluster\"].nunique()\n",
      "if level < n_levels and unique_clusters > 1:\n",
      "# ë‹¤ìŒ ë ˆë²¨ì˜ ì¬ê·€ ì…ë ¥ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½ ì‚¬ìš©\n",
      "new_texts = df_summary[\"summaries\"].tolist()\n",
      "next_level_results = recursive_embed_cluster_summarize(\n",
      "new_texts, level + 1, n_levels\n",
      ")\n",
      "# ë‹¤ìŒ ë ˆë²¨ì˜ ê²°ê³¼ë¥¼ í˜„ì¬ ê²°ê³¼ ì‚¬ì „ì— ë³‘í•©\n",
      "results.update(next_level_results)\n",
      "return results\n",
      "# ì „ì²´ ë¬¸ì„œì˜ ê°œìˆ˜\n",
      "len(docs_texts)\n",
      "33\n",
      "# íŠ¸ë¦¬ êµ¬ì¶•\n",
      "leaf_texts = docs_texts  # ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¦¬í”„ í…ìŠ¤íŠ¸ë¡œ ì„¤ì •\n",
      "results = recursive_embed_cluster_summarize(\n",
      "leaf_texts, level=1, n_levels=3\n",
      ")  # ì¬ê·€ì ìœ¼ë¡œ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½ì„ ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ìŒ\n",
      "--Generated 6 clusters--\n",
      "LangChain Expression Language (LCEL)ëŠ” LangChainì—ì„œ ì²´ì¸ì„ êµ¬ì„±í•˜ëŠ” ì„ ì–¸ì  ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. LCELì€ í”„ë¡œí† íƒ€ì…ì„ ìƒì‚°ì— íˆ¬ì…í•˜ëŠ” ê²ƒì„ ì§€ì›í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ê°€ì¥ ê°„ë‹¨í•œ \"í”„ë¡¬í”„íŠ¸ + LLM\" ì²´ì¸ë¶€í„° ìˆ˜ë°± ë‹¨ê³„ì˜ ë³µì¡í•œ ì²´ì¸ê¹Œì§€ ì½”ë“œ ë³€ê²½ ì—†ì´ ìƒì‚°ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LCELì„ ì‚¬ìš©í•˜ëŠ” ëª‡ ê°€ì§€ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "- **ìŠ¤íŠ¸ë¦¬ë° ì§€ì›**: LCELë¡œ ì²´ì¸ì„ êµ¬ì¶•í•˜ë©´ ìµœìƒì˜ ì²« ë²ˆì§¸ í† í°ê¹Œì§€ì˜ ì‹œê°„ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¼ë¶€ ì²´ì¸ì—ì„œ LLMì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ íŒŒì„œë¡œ í† í°ì„ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³ , LLM ì œê³µìê°€ ì›ì‹œ í† í°ì„ ì¶œë ¥í•˜ëŠ” ì†ë„ë¡œ íŒŒì‹±ëœ ì¦ë¶„ ì¶œë ¥ ì²­í¬ë¥¼ ë°›ì„ ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
      "- **ë¹„ë™ê¸° ì§€ì›**: LCELë¡œ êµ¬ì¶•ëœ ëª¨ë“  ì²´ì¸ì€ ë™ê¸° API(ì˜ˆ: í”„ë¡œí† íƒ€ì´í•‘ ì¤‘ì¸ Jupyter ë…¸íŠ¸ë¶ì—ì„œ)ì™€ ë¹„ë™ê¸° API(ì˜ˆ: LangServe ì„œë²„ì—ì„œ) ëª¨ë‘ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í”„ë¡œí† íƒ€ì…ê³¼ ìƒì‚°ì—ì„œ ë™ì¼í•œ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ê³ , ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ì œê³µí•˜ë©°, ë™ì¼í•œ ì„œë²„ì—ì„œ ë§ì€ ë™ì‹œ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ìµœì í™”ëœ ë³‘ë ¬ ì‹¤í–‰**: LCEL ì²´ì¸ì— ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë‹¨ê³„ê°€ ìˆëŠ” ê²½ìš°(ì˜ˆ: ì—¬ëŸ¬ ê²€ìƒ‰ê¸°ì—ì„œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ë•Œ), ë™ê¸° ë° ë¹„ë™ê¸° ì¸í„°í˜ì´ìŠ¤ ëª¨ë‘ì—ì„œ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ ê°€ëŠ¥í•œ ê°€ì¥ ì‘ì€ ëŒ€ê¸° ì‹œê°„ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "- **ì¬ì‹œë„ ë° ëŒ€ì²´**: LCEL ì²´ì¸ì˜ ëª¨ë“  ë¶€ë¶„ì— ëŒ€í•´ ì¬ì‹œë„ ë° ëŒ€ì²´ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê·œëª¨ì—ì„œ ì²´ì¸ì„ ë”ìš± ì‹ ë¢°í•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.\n",
      "- **ì¤‘ê°„ ê²°ê³¼ ì ‘ê·¼**: ë” ë³µì¡í•œ ì²´ì¸ì˜ ê²½ìš° ìµœì¢… ì¶œë ¥ì´ ìƒì„±ë˜ê¸° ì „ì— ì¤‘ê°„ ë‹¨ê³„ì˜ ê²°ê³¼ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ê²ƒì´ ë§¤ìš° ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìµœì¢… ì‚¬ìš©ìì—ê²Œ ë¬´ì–¸ê°€ê°€ ì§„í–‰ë˜ê³  ìˆìŒì„ ì•Œë¦¬ê±°ë‚˜ ì²´ì¸ì„ ë””ë²„ê¹…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ì…ë ¥ ë° ì¶œë ¥ ìŠ¤í‚¤ë§ˆ**: ì…ë ¥ ë° ì¶œë ¥ ìŠ¤í‚¤ë§ˆëŠ” ì²´ì¸ì˜ êµ¬ì¡°ë¡œë¶€í„° ì¶”ë¡ ëœ Pydantic ë° JSONSchema ìŠ¤í‚¤ë§ˆë¥¼ ëª¨ë“  LCEL ì²´ì¸ì— ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ ë° ì¶œë ¥ì˜ ìœ íš¨ì„± ê²€ì‚¬ì— ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë©° LangServeì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì…ë‹ˆë‹¤.\n",
      "- **LangSmith ì¶”ì  í†µí•© ë° LangServe ë°°í¬ í†µí•©**: ì²´ì¸ì´ ì ì  ë” ë³µì¡í•´ì§ì— ë”°ë¼ ê° ë‹¨ê³„ì—ì„œ ì •í™•íˆ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”ì§€ ì´í•´í•˜ëŠ” ê²ƒì´ ì ì  ë” ì¤‘ìš”í•´ì§‘ë‹ˆë‹¤. LCELì„ ì‚¬ìš©í•˜ë©´ ëª¨ë“  ë‹¨ê³„ê°€ LangSmithì— ìë™ìœ¼ë¡œ ê¸°ë¡ë˜ì–´ ìµœëŒ€í•œì˜ ê´€ì°° ê°€ëŠ¥ì„±ê³¼ ë””ë²„ê¹… ê°€ëŠ¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "LCELì€ ë‹¤ì–‘í•œ ê³µí†µ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì˜ˆì œ ì½”ë“œë¥¼ ì œê³µí•˜ëŠ” ì¿¡ë¶ë„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì œë“¤ì€ ë‹¤ì–‘í•œ Runnable(í•µì‹¬ LCEL ì¸í„°í˜ì´ìŠ¤) êµ¬ì„± ìš”ì†Œë¥¼ ì¡°í•©í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, LCELì„ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ë”°ë¼ ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì²´ì¸ ë¡œì§ì„ ë™ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” ì‚¬ìš©ì ì§€ì • ë¼ìš°íŒ… ë¡œì§ì„ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¿¼ë¦¬ë¥¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ í”„ë¡¬í”„íŠ¸ë¡œ ë¼ìš°íŒ…í•˜ëŠ” ë° íŠ¹íˆ ìœ ìš©í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.LangChain í‘œí˜„ ì–¸ì–´(LCEL) ë¬¸ì„œëŠ” LangChainì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ë‹¤ì–‘í•œ ê°€ì´ë“œì™€ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” LangChainì˜ ê¸°ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•˜ê³ ì í•˜ëŠ” ê°œë°œìë“¤ì„ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. ì£¼ìš” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "1. **Runnable íƒ€ì…**:\n",
      "- **RunnableParallel**: ë°ì´í„°ë¥¼ ì¡°ì‘í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "- **RunnablePassthrough**: ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "- **RunnableLambda**: ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "- **RunnableBranch**: ì…ë ¥ì— ê¸°ë°˜í•˜ì—¬ ë™ì ìœ¼ë¡œ ë¡œì§ì„ ë¼ìš°íŒ…í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "2. **ëŸ°íƒ€ì„ ì¸ì ë°”ì¸ë”©**: Runnable ì‹œí€€ìŠ¤ ë‚´ì—ì„œ íŠ¹ì • ì¸ìë¥¼ ìƒìˆ˜ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "3. **ëŸ°íƒ€ì„ì—ì„œ ì²´ì¸ ë‚´ë¶€ êµ¬ì„±**: ëŸ°íƒ€ì„ì—ì„œ ì²´ì¸ì˜ ë‚´ë¶€ ì„¤ì •ì„ ì‹¤í—˜í•˜ê±°ë‚˜ ìµœì¢… ì‚¬ìš©ìì—ê²Œ ë…¸ì¶œí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "4. **`@chain` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ runnable ìƒì„±**: ì„ì˜ì˜ í•¨ìˆ˜ë¥¼ ì²´ì¸ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê´€ì°° ê°€ëŠ¥ì„±ì„ í–¥ìƒì‹œí‚¤ê³ , í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ í˜¸ì¶œë˜ëŠ” runnableì„ ì¤‘ì²©ëœ ìì‹ìœ¼ë¡œ ì¶”ì í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
      "5. **Fallback ì¶”ê°€**: LLM(Large Language Model) ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì‹¤íŒ¨ ì§€ì ì— ëŒ€ë¹„í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "6. **ì»¤ìŠ¤í…€ ì œë„ˆë ˆì´í„° í•¨ìˆ˜ ìŠ¤íŠ¸ë¦¬ë°**: `yield` í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜ë¥¼ LCEL íŒŒì´í”„ë¼ì¸ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ëŠ” ì»¤ìŠ¤í…€ ì¶œë ¥ íŒŒì„œë¥¼ êµ¬í˜„í•˜ê±°ë‚˜ ì´ì „ ë‹¨ê³„ì˜ ì¶œë ¥ì„ ìˆ˜ì •í•˜ë©´ì„œ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.\n",
      "7. **Runnable ê²€ì‚¬**: LCELë¡œ ìƒì„±ëœ runnableì„ ê²€ì‚¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "8. **ë©”ì‹œì§€ ê¸°ë¡(ë©”ëª¨ë¦¬) ì¶”ê°€**: íŠ¹ì • runnableì— ë©”ì‹œì§€ ê¸°ë¡ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "ì´ ë¬¸ì„œëŠ” LangChainì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³ ì í•˜ëŠ” ê°œë°œìì—ê²Œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê° ì„¹ì…˜ì€ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…ë˜ì–´ ìˆì–´, ê°œë°œìê°€ LangChainì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì‰½ê²Œ ì´í•´í•˜ê³  ì ìš©í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.ì´ ë¬¸ì„œëŠ” LangChain í‘œí˜„ ì–¸ì–´(LCEL)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì˜ˆì œ ì½”ë“œë¥¼ ì œê³µí•˜ëŠ” ì¿¡ë¶ì…ë‹ˆë‹¤. LCELì€ LangChainì—ì„œ ì²´ì¸ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•˜ë©°, ì´ ì¿¡ë¶ì€ ë‹¤ì–‘í•œ Runnable(í•µì‹¬ LCEL ì¸í„°í˜ì´ìŠ¤) êµ¬ì„± ìš”ì†Œë¥¼ ì¡°í•©í•˜ì—¬ ì—¬ëŸ¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì£¼ì œë“¤ì´ í¬í•¨ë©ë‹ˆë‹¤:\n",
      "1. **Prompt + LLM**: ê°€ì¥ ì¼ë°˜ì ì´ê³  ê°€ì¹˜ ìˆëŠ” êµ¬ì„±ìœ¼ë¡œ, í”„ë¡¬í”„íŠ¸ì™€ LLM(Large Language Model)ì„ ê²°í•©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "2. **RAG**: í”„ë¡¬í”„íŠ¸ì™€ LLMì— ê²€ìƒ‰ ë‹¨ê³„ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
      "3. **Multiple Chains**: Runnableì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì²´ì¸ì„ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "4. **Querying a SQL DB**: Runnableì„ ì‚¬ìš©í•˜ì—¬ SQL ë°ì´í„°ë² ì´ìŠ¤ ì²´ì¸ì„ ë³µì œí•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "5. **Agents**: Runnableì„ ì—ì´ì „íŠ¸ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "6. **Code Writing**: LCELì„ ì‚¬ìš©í•˜ì—¬ Python ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "7. **Routing by Semantic Similarity**: LCELì„ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ë”°ë¼ ë¼ìš°íŒ…ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "8. **Adding Memory**: ì„ì˜ì˜ ì²´ì¸ì— ë©”ëª¨ë¦¬ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "9. **Adding Moderation**: LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ì£¼ë³€ì— ê²€ì—´(ë˜ëŠ” ë‹¤ë¥¸ ì•ˆì „ì¥ì¹˜)ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "10. **Managing Prompt Size**: ì—ì´ì „íŠ¸ê°€ ë™ì ìœ¼ë¡œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³ , ê·¸ ë„êµ¬ í˜¸ì¶œì˜ ê²°ê³¼ê°€ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ë˜ëŠ” ë°©ì‹ì„ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "11. **Using Tools**: Runnableê³¼ í•¨ê»˜ ë„êµ¬ë¥¼ ì‰½ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "ê° ì„¹ì…˜ì€ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ ì˜ˆì œì™€ í•¨ê»˜ êµ¬ì²´ì ì¸ ë°©ë²•ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, \"Adding Moderation\" ì„¹ì…˜ì—ì„œëŠ” OpenAIì˜ ë‚´ìš© ì •ì±…ì„ ìœ„ë°˜í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ë‚´ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ê³ , \"Multiple Chains\" ì„¹ì…˜ì—ì„œëŠ” ì—¬ëŸ¬ ì²´ì¸ì„ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. \"Agents\" ì„¹ì…˜ì—ì„œëŠ” ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ê³¼ì •ì„, \"Code Writing\"ì—ì„œëŠ” LCELì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "ì´ ë¬¸ì„œëŠ” LangChainì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³ ì í•˜ëŠ” ê°œë°œìë“¤ì—ê²Œ ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤ì…ë‹ˆë‹¤. ê° ì˜ˆì œëŠ” êµ¬ì²´ì ì¸ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ë©°, ì´ë¥¼ í†µí•´ ê°œë°œìë“¤ì€ LCELì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì´í•´í•˜ê³  ìì‹ ì˜ í”„ë¡œì íŠ¸ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.The provided documents are part of the LangChain documentation, which details the LangChain Expression Language (LCEL) and its applications. LangChain is a framework designed to facilitate the creation, manipulation, and execution of complex chains of operations, particularly in the context of language models and related tasks. The documentation covers various aspects of using LangChain, including installation, quick start guides, security considerations, and detailed explanations of LCEL's components and capabilities. Here's a summary of the key points from the documents:\n",
      "1. **Introduction to LangChain and LCEL**: LangChain offers a way to build complex chains from basic components, supporting functionalities like streaming, parallelism, and logging. LCEL (LangChain Expression Language) simplifies the process of chaining together prompts, models, and output parsers to perform tasks like generating jokes based on a given topic or conducting retrieval-augmented generation.\n",
      "2. **Basic Examples**: The documentation provides examples of basic use cases, such as chaining a prompt template with a model and an output parser to generate content based on user input. It also demonstrates more complex scenarios like retrieval-augmented generation, where additional context is retrieved and used to inform the generation process.\n",
      "3. **Inspecting Runnables**: LangChain allows users to inspect runnables (components of a chain) to understand their structure and operation better. This can include generating a graph representation of a chain or retrieving the prompts used within a chain.\n",
      "4. **Using Tools with Runnables**: LangChain supports the integration of various tools with runnables. An example provided is using DuckDuckGoSearchRun with a chain to turn user input into a search query and retrieve relevant information.\n",
      "5. **Self-querying Retrievers**: The documentation discusses self-querying retrievers, which can construct structured queries based on natural language input and apply these queries to their underlying VectorStore. This allows for sophisticated retrieval operations based on both semantic similarity and metadata filters.\n",
      "6. **Advanced Features**: LangChain documentation also touches on advanced features like adding memory to chains, managing prompt size, and routing by semantic similarity. These features enable the creation of more sophisticated and efficient chains capable of handling complex tasks.\n",
      "7. **Community and Support**: The documents encourage community engagement and feedback, providing links to community resources like Discord, Twitter, and GitHub. This suggests an active and supportive community around LangChain.\n",
      "Overall, the LangChain documentation provides a comprehensive guide to using the LangChain framework and LCEL for building and executing complex chains of operations involving language models and other components. It covers both basic and advanced use cases, offering practical examples and encouraging community involvement.The provided documents from LangChain cover a range of topics related to the LangChain Expression Language (LCEL) and its applications, including interface design, streaming, dynamic routing, parallel processing, data passing, message history management, and prompt size management. Here's a detailed summary of the key points from each document:\n",
      "### Interface\n",
      "- LangChain introduces a \"Runnable\" protocol to simplify the creation of custom chains.\n",
      "- The standard interface includes methods for streaming, invoking, and batching calls, with asynchronous versions available.\n",
      "- Input and output types vary by component, with schemas provided for inspection.\n",
      "### Streaming\n",
      "- Streaming is crucial for making applications feel responsive.\n",
      "- LangChain supports synchronous and asynchronous streaming, including intermediate steps and final output.\n",
      "- Examples demonstrate streaming with LLMs and chat models, highlighting the importance of handling input streams effectively.\n",
      "### RunnableBranch\n",
      "- RunnableBranch allows for dynamic routing based on input, enabling non-deterministic chains.\n",
      "- Two methods for routing include using a custom function (recommended) or a RunnableBranch.\n",
      "- Examples show how to classify questions and route them to corresponding prompt chains based on the classification.\n",
      "### RunnableParallel\n",
      "- RunnableParallel is used for manipulating data and executing multiple Runnables in parallel.\n",
      "- It can be used to match the output format of one Runnable to the input format of another.\n",
      "- Examples demonstrate parallel execution and the use of itemgetter for shorthand data extraction.\n",
      "### RunnablePassthrough\n",
      "- RunnablePassthrough passes inputs unchanged or with added keys, often used with RunnableParallel.\n",
      "- It allows for the assignment of data to new keys in a map.\n",
      "- An example shows its use in a retrieval chain, passing user input under a specific key.\n",
      "### Add Message History (Memory)\n",
      "- RunnableWithMessageHistory adds message history to chains, managing chat message history.\n",
      "- It supports various input and output formats, including sequences of BaseMessage and dictionaries.\n",
      "- Examples cover in-memory and persistent storage (using Redis) for message histories, demonstrating how to manage and utilize chat histories in chains.\n",
      "### Managing Prompt Size\n",
      "- Managing prompt size is crucial for preventing context window overflow in models.\n",
      "- Custom functionality can be added to LCEL chains for prompt size management.\n",
      "- An example demonstrates a multi-step question with prompt handling logic to condense prompts and ensure the model's context window is not exceeded.\n",
      "These documents collectively provide a comprehensive guide to using LangChain Expression Language for building and managing complex chains, incorporating dynamic routing, parallel processing, and efficient data handling techniques. They emphasize the flexibility and power of LCEL in creating responsive and intelligent applications.The provided documents are part of the LangChain documentation, focusing on the LangChain Expression Language (LCEL), a tool designed to facilitate the construction of complex chains from basic components for language model applications. Here's a detailed summary of the key points from each section:\n",
      "### Why Use LCEL\n",
      "- **Purpose**: LCEL simplifies building complex chains by offering a unified interface and composition primitives.\n",
      "- **Features**:\n",
      "1. **Unified Interface**: Implements the Runnable interface, allowing chains of LCEL objects to support common invocation methods (invoke, batch, stream, etc.).\n",
      "2. **Composition Primitives**: Facilitates composing chains, parallelizing components, adding fallbacks, and more.\n",
      "- **Example**: Demonstrates LCEL's utility through a basic example of creating a prompt + model chain and compares the process with and without LCEL, highlighting LCEL's efficiency and simplicity.\n",
      "### Prompt + LLM\n",
      "- **Common Composition**: Combining a PromptTemplate/ChatPromptTemplate with an LLM/ChatModel and an OutputParser is a fundamental building block in LCEL.\n",
      "- **Simplification and Flexibility**: Shows how to simplify input, attach kwargs, and use different parsers for structured outputs.\n",
      "- **Runnable Parallel**: Introduces RunnableParallel for easier invocation, demonstrating how to streamline the process of creating input dictionaries for prompts.\n",
      "### Add Fallbacks\n",
      "- **Handling Failures**: Discusses using fallbacks to gracefully handle failures at various points, especially useful for LLM API errors.\n",
      "- **Implementation**: Provides examples of implementing fallbacks, including handling specific errors and creating fallbacks for sequences.\n",
      "- **Practical Use**: Offers code snippets to illustrate how fallbacks can be applied to LLMs, showing how to switch between different models or prompts based on runtime configurations.\n",
      "### Configure Chain Internals at Runtime\n",
      "- **Dynamic Configuration**: Explains methods to experiment with or expose different configurations to end-users by adjusting chain internals at runtime.\n",
      "- **Configuration Fields and Alternatives**:\n",
      "- **Fields**: Allows configuring specific fields of a runnable, such as LLM temperature.\n",
      "- **Alternatives**: Enables listing out alternatives for any particular runnable that can be set during runtime, useful for switching between models or prompts.\n",
      "- **Examples**: Provides code examples to demonstrate configuring LLMs and prompts, including saving configured chains as their own objects for reuse.\n",
      "### Quickstart\n",
      "- **Output Parsers**: Introduces output parsers as a means to structure language model responses into more useful formats.\n",
      "- **PydanticOutputParser**: Highlights the use of PydanticOutputParser for defining desired data structures and parsing model outputs into these structures.\n",
      "- **Streaming and Invocation**: Discusses the support for various invocation methods within LCEL and the ability of some parsers to stream through partially parsed objects.\n",
      "Each section of the documentation emphasizes the flexibility, efficiency, and ease of use provided by LCEL, showcasing how it can significantly streamline the process of working with language models by offering a structured approach to building, configuring, and managing complex chains.--Generated 1 clusters--\n",
      "The provided documents offer a comprehensive overview of the LangChain Expression Language (LCEL), a powerful tool designed to facilitate the construction and management of complex operation chains, particularly in the context of language models and related tasks. Here's a consolidated summary of the key points and features highlighted across the documents:\n",
      "1. **Introduction and Purpose**: LCEL is introduced as a declarative method to construct chains within LangChain, aimed at supporting the transition from prototype to production without code changes. It's designed to handle simple to highly complex chains efficiently.\n",
      "2. **Key Features of LCEL**:\n",
      "- **Streaming and Asynchronous Support**: Enables direct streaming of tokens from LLMs to output parsers, supporting both synchronous and asynchronous API calls. This feature is crucial for reducing latency and improving responsiveness.\n",
      "- **Optimized Parallel Execution**: Automatically executes parallelizable steps in a chain to minimize latency, enhancing performance.\n",
      "- **Retry and Fallback Mechanisms**: Offers configurable retry and fallback options for all parts of a chain, increasing reliability at scale.\n",
      "- **Access to Intermediate Results**: Allows access to results from intermediate steps, useful for debugging and providing progress feedback.\n",
      "- **Input and Output Schemas**: Generates Pydantic and JSONSchema schemas from the chain's structure, facilitating input and output validation.\n",
      "- **Integration with LangSmith and LangServe**: Ensures maximum observability and debugging capabilities by automatically logging each step in LangSmith and supporting deployment through LangServe.\n",
      "3. **Runnable Types and Runtime Features**:\n",
      "- Various runnable types such as `RunnableParallel`, `RunnablePassthrough`, `RunnableLambda`, and `RunnableBranch` are introduced, each serving different purposes like data manipulation, dynamic routing, and parallel execution.\n",
      "- Features like runtime argument binding, chain internal configuration at runtime, and the use of the `@chain` decorator to enhance observability and manageability of chains are discussed.\n",
      "4. **Advanced Usage and Examples**:\n",
      "- The documents provide a plethora of examples demonstrating LCEL's versatility, including prompt + LLM chains, adding fallbacks, custom generator function streaming, and managing prompt size.\n",
      "- Specific use cases like routing by semantic similarity, adding memory, and integrating moderation are covered, showcasing LCEL's capability to handle complex logic and dynamic routing based on user input or other criteria.\n",
      "5. **Community and Support**: The documentation emphasizes community engagement and support, encouraging users to contribute feedback and participate in community resources.\n",
      "Overall, the LangChain documentation and the detailed exploration of LCEL highlight its role as a critical tool for developers looking to leverage language models and build sophisticated data processing and transformation pipelines. LCEL's design principles focus on ease of use, flexibility, and efficiency, enabling developers to construct, manage, and scale complex chains with minimal overhead and maximum reliability.\n",
      "ë…¼ë¬¸ì—ì„œëŠ” collapsed tree retrievalì´ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "ì´ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë‹¨ì¼ ê³„ì¸µìœ¼ë¡œ í‰íƒ„í™”í•œ ë‹¤ìŒ, ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ ë™ì‹œì— k-ìµœê·¼ì ‘ ì´ì›ƒ(kNN) ê²€ìƒ‰ì„ ì ìš©í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "ì•„ë˜ì—ì„œ ì´ ê³¼ì •ì„ ê°„ë‹¨íˆ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "Chroma ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ë²¡í„°í™” ë° ê²€ìƒ‰ ê°€ëŠ¥í•œ ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "ì´ˆê¸°ì— leaf_textsì— ì €ì¥ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ all_texts ë³€ìˆ˜ì— ë³µì‚¬í•©ë‹ˆë‹¤.\n",
      "ê²°ê³¼ ë°ì´í„°(results)ë¥¼ ìˆœíšŒí•˜ë©° ê° ë ˆë²¨ì—ì„œ ìš”ì•½ëœ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ all_textsì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
      "ê° ë ˆë²¨ì˜ DataFrameì—ì„œ summaries ì»¬ëŸ¼ì˜ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
      "ì¶”ì¶œëœ ìš”ì•½ë¬¸ì„ all_textsì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
      "ëª¨ë“  í…ìŠ¤íŠ¸ ë°ì´í„°(all_texts)ë¥¼ ì‚¬ìš©í•˜ì—¬ Chroma ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
      "Chroma.from_texts í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ê³ , ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "ìƒì„±ëœ ë²¡í„° ì €ì¥ì†Œë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ .as_retriever() ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ê¸°(retriever)ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
      "ì´ ê³¼ì •ì„ í†µí•´, ë‹¤ì–‘í•œ ë ˆë²¨ì˜ ìš”ì•½ë¬¸ì„ í¬í•¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•œ Chroma ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
      "from langchain_community.vectorstores import FAISS\n",
      "# leaf_textsë¥¼ ë³µì‚¬í•˜ì—¬ all_textsë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
      "all_texts = leaf_texts.copy()\n",
      "# ê° ë ˆë²¨ì˜ ìš”ì•½ì„ ì¶”ì¶œí•˜ì—¬ all_textsì— ì¶”ê°€í•˜ê¸° ìœ„í•´ ê²°ê³¼ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.\n",
      "for level in sorted(results.keys()):\n",
      "# í˜„ì¬ ë ˆë²¨ì˜ DataFrameì—ì„œ ìš”ì•½ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
      "summaries = results[level][1][\"summaries\"].tolist()\n",
      "# í˜„ì¬ ë ˆë²¨ì˜ ìš”ì•½ì„ all_textsì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
      "all_texts.extend(summaries)\n",
      "# ì´ì œ all_textsë¥¼ ì‚¬ìš©í•˜ì—¬ FAISS vectorstoreë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
      "vectorstore = FAISS.from_texts(texts=all_texts, embedding=embd)\n",
      "DB ë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
      "import os\n",
      "DB_INDEX = \"RAPTOR\"\n",
      "# ë¡œì»¬ì— FAISS DB ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ê·¸ë ‡ë‹¤ë©´ ë¡œë“œí•˜ì—¬ vectorstoreì™€ ë³‘í•©í•œ í›„ ì €ì¥í•©ë‹ˆë‹¤.\n",
      "if os.path.exists(DB_INDEX):\n",
      "local_index = FAISS.load_local(DB_INDEX, embd)\n",
      "local_index.merge_from(vectorstore)\n",
      "local_index.save_local(DB_INDEX)\n",
      "else:\n",
      "vectorstore.save_local(folder_path=DB_INDEX)\n",
      "# retriever ìƒì„±\n",
      "retriever = vectorstore.as_retriever()\n",
      "Retrieval Augmented Generation(RAG) ì²´ì¸ì„ ì •ì˜í•˜ê³  íŠ¹ì • ì½”ë“œ ì˜ˆì œë¥¼ ìš”ì²­í•˜ëŠ” ë°©ë²•ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
      "hub.pullì„ ì‚¬ìš©í•˜ì—¬ RAG í”„ë¡¬í”„íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
      "ë¬¸ì„œ í¬ë§·íŒ…ì„ ìœ„í•œ format_docs í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì„ ì—°ê²°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ì²´ì¸ì€ ê²€ìƒ‰ê¸°(retriever)ë¡œë¶€í„° ë¬¸ë§¥ì„ ê°€ì ¸ì˜¤ê³ , format_docs í•¨ìˆ˜ë¡œ í¬ë§·íŒ…í•œ í›„, ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "RunnablePassthrough()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
      "ì²´ì¸ì€ í”„ë¡¬í”„íŠ¸, ëª¨ë¸, ê·¸ë¦¬ê³  StrOutputParser()ë¥¼ í†µí•´ ìµœì¢… ì¶œë ¥ì„ ë¬¸ìì—´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.\n",
      "rag_chain.invoke ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ \"How to define a RAG chain? Give me a specific code example.\"ë¼ëŠ” ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "from langchain import hub\n",
      "from langchain_core.runnables import RunnablePassthrough\n",
      "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
      "prompt = hub.pull(\"rlm/rag-prompt\")\n",
      "# ë¬¸ì„œ í¬ìŠ¤íŠ¸ í”„ë¡œì„¸ì‹±\n",
      "def format_docs(docs):\n",
      "# ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì„ ì´ì–´ë¶™ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
      "# RAG ì²´ì¸ ì •ì˜\n",
      "rag_chain = (\n",
      "# ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•˜ê³  ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
      "| prompt  # í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
      "| model  # ëª¨ë¸ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
      "| StrOutputParser()  # ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
      ")\n",
      "LangSmith ë§í¬ (https://smith.langchain.com/public/178afde0-8dcc-472f-8c47-233fe81cbbad/r)LangSmith ë§í¬\n",
      "# ì¶”ìƒì ì¸ ì§ˆë¬¸ ì‹¤í–‰\n",
      "_ = rag_chain.invoke(\"ì „ì²´ ë¬¸ì„œì˜ í•µì‹¬ ì£¼ì œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
      "LangChain í‘œí˜„ ì–¸ì–´(LCEL) ë¬¸ì„œëŠ” LangChainì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ê°€ì´ë“œì™€ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” ë‹¤ì–‘í•œ Runnable íƒ€ì…, ëŸ°íƒ€ì„ ì¸ì ë°”ì¸ë”©, ì²´ì¸ ë‚´ë¶€ êµ¬ì„±, `@chain` ë°ì½”ë ˆì´í„° ì‚¬ìš©, Fallback ì¶”ê°€, ì»¤ìŠ¤í…€ ì œë„ˆë ˆì´í„° í•¨ìˆ˜ ìŠ¤íŠ¸ë¦¬ë° ë“± LCELì˜ í•µì‹¬ ê¸°ëŠ¥ê³¼ ì‚¬ìš© ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ë˜í•œ, LCELì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ê²°í•©í•˜ëŠ” ë°©ë²•, ê²€ìƒ‰ ë‹¨ê³„ ì¶”ê°€, ì—¬ëŸ¬ ì²´ì¸ ì—°ê²°, SQL DB ì¿¼ë¦¬, ì½”ë“œ ì‘ì„±, ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ë”°ë¥¸ ë¼ìš°íŒ…, ë©”ëª¨ë¦¬ ì¶”ê°€, ê²€ì—´ ì¶”ê°€ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n",
      "LangSmith ë§í¬ (https://smith.langchain.com/public/1d72738b-f378-4676-9961-dd12fe424918/r)LangSmith ë§í¬\n",
      "# Low Level ì§ˆë¬¸ ì‹¤í–‰\n",
      "_ = rag_chain.invoke(\"PydanticOutputParser ì„ í™œìš©í•œ ì˜ˆì‹œ ì½”ë“œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\")\n",
      "from langchain.output_parsers import PydanticOutputParser\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
      "from langchain_openai import OpenAI\n",
      "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
      "class Joke(BaseModel):\n",
      "setup: str = Field(description=\"question to set up a joke\")\n",
      "punchline: str = Field(description=\"answer to resolve the joke\")\n",
      "@validator(\"setup\")\n",
      "def question_ends_with_question_mark(cls, field):\n",
      "if field[-1] != \"?\":\n",
      "raise ValueError(\"Badly formed question!\")\n",
      "return field\n",
      "parser = PydanticOutputParser(pydantic_object=Joke)\n",
      "prompt = PromptTemplate(\n",
      "template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
      "input_variables=[\"query\"],\n",
      "partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
      ")\n",
      "prompt_and_model = prompt | model\n",
      "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
      "parser.invoke(output)\n",
      "```\n",
      "LangSmith ë§í¬ (https://smith.langchain.com/public/bd725efb-d854-4d0e-b34b-52748dddcf4b/r)LangSmith ë§í¬\n",
      "# Low Level ì§ˆë¬¸ ì‹¤í–‰\n",
      "_ = rag_chain.invoke(\"self-querying ë°©ë²•ê³¼ ì˜ˆì‹œ ì½”ë“œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\")\n",
      "Self-querying ë°©ë²•ì€ ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ë°›ì•„ êµ¬ì¡°í™”ëœ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ VectorStoreì— ì ìš©í•˜ì—¬ ë¬¸ì„œì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¹„êµ ë° ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œí•œ í•„í„°ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì‹¤í–‰í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Chroma vector storeë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜í™” ìš”ì•½ ë¬¸ì„œê°€ í¬í•¨ëœ ì‘ì€ ë°ëª¨ ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ í†µí•´ ìì²´ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ìì²´ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ ì½”ë“œì…ë‹ˆë‹¤:\n",
      "```python\n",
      "%pip install --upgrade --quiet  lark chromadb\n",
      "from langchain_community.vectorstores import Chroma\n",
      "from langchain_core.documents import Document\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "docs = [\n",
      "Document(\n",
      "page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
      "metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
      "),\n",
      "# Additional documents...\n",
      "]\n",
      "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n",
      "from langchain.chains.query_constructor.base import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "from langchain_openai import ChatOpenAI\n",
      "metadata_field_info = [\n",
      "AttributeInfo(\n",
      "name=\"genre\",\n",
      "description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
      "type=\"string\",\n",
      "),\n",
      "# Additional metadata fields...\n",
      "]\n",
      "document_content_description = \"Brief summary of a movie\"\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "retriever = SelfQueryRetriever.from_llm(\n",
      "llm,\n",
      "vectorstore,\n",
      "document_content_description,\n",
      "metadata_field_info,\n",
      ")\n",
      "# example usage\n",
      "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in retrieved_docs[:5] :\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    # íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    TextLoader(r\"C:\\Users\\user\\Documents\\GitHub\\Presentation-Agent\\data\\txt\\pt_context.txt\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    # ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  docs ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 50,\n",
    "    separators=['\\n\\n', '\\n', '.', '']\n",
    ")\n",
    "\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
