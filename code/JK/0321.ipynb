{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader('data/Team5 Parking-Genius_Final.pdf')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in docs :\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 컴퓨터 비전의 기본 개념\n",
      "1.1 이미지 표현 방식\n",
      "이미지는 픽셀(Pixel) 값의 집합으로 표현되며, 다양한 방식으로 저장됩니다.\n",
      "\n",
      "픽셀(Pixel): 이미지의 최소 단위. 흑백 이미지는 0~255(단일 채널), 컬러 이미지는 RGB(3채널) 또는 RGBA(4채널)로 표현됨.\n",
      "채널(Channel): 이미지의 색상을 구성하는 정보. RGB(적, 녹, 청), CMYK(청록, 자홍, 노랑, 검정) 등.\n",
      "해상도(Resolution): 이미지의 크기(너비 × 높이)로 결정됨.\n",
      "1.2 이미지 처리 기초 개념\n",
      "히스토그램(Histogram): 픽셀 값의 분포를 나타내는 그래프.\n",
      "컨볼루션(Convolution): 필터(커널, Kernel)를 사용하여 이미지의 특정 특징을 추출하는 연산. 예를 들면 엣지 검출(Edge Detection).\n",
      "필터(Filter): 이미지에서 특정 정보를 강조하거나 억제하는 행렬 연산. 대표적인 예로 가우시안 블러(Gaussian Blur)와 소벨 엣지 검출(Sobel Edge Detection)이 있음.\n",
      "2. 딥러닝을 이용한 컴퓨터 비전 기술\n",
      "딥러닝을 활용한 컴퓨터 비전 기술은 데이터에서 패턴을 학습하고 예측하는데 최적화되어 있습니다.\n",
      "\n",
      "2.1 합성곱 신경망(Convolutional Neural Networks, CNN)\n",
      "CNN은 이미지 인식을 위한 가장 대표적인 딥러닝 모델입니다.\n",
      "\n",
      "CNN의 주요 구조\n",
      "컨볼루션 층(Convolutional Layer): 엣지, 질감, 패턴 등의 특징을 추출.\n",
      "풀링 층(Pooling Layer): 공간적 크기를 줄이고, 중요한 특징을 보존 (Max Pooling, Average Pooling).\n",
      "완전 연결 층(Fully Connected Layer, FC Layer): 최종적으로 이미지 분류 등의 작업 수행.\n",
      "CNN의 대표 모델\n",
      "LeNet-5: 최초의 CNN 모델 중 하나로, 손글씨 숫자 인식을 위해 설계됨.\n",
      "AlexNet: 깊은 신경망을 활용한 최초의 딥러닝 기반 이미지 분류 모델 (ILSVRC 2012 우승).\n",
      "VGGNet: 더 깊은 네트워크 구조를 사용 (16~19층).\n",
      "ResNet: Residual Connection을 도입하여 기울기 소실(Vanishing Gradient) 문제 해결.\n",
      "EfficientNet: 연산량 대비 성능을 최적화한 경량화 모델.\n",
      "2.2 객체 탐지(Object Detection)\n",
      "이미지에서 특정 객체의 위치를 Bounding Box 형태로 감지하고, 어떤 클래스인지 분류하는 기술.\n",
      "\n",
      "대표적인 객체 탐지 모델\n",
      "R-CNN 계열: R-CNN → Fast R-CNN → Faster R-CNN → Mask R-CNN (마스크 추가)\n",
      "YOLO (You Only Look Once): 실시간 객체 탐지에 특화된 모델 (YOLOv1 ~ YOLOv8)\n",
      "SSD (Single Shot MultiBox Detector): YOLO와 유사한 속도와 성능 제공.\n",
      "2.3 이미지 분할(Image Segmentation)\n",
      "각 픽셀이 어느 객체에 속하는지 분류하는 기술.\n",
      "\n",
      "이미지 분할 유형\n",
      "Semantic Segmentation: 픽셀 단위로 객체 클래스를 분류 (예: 배경, 도로, 사람 구분).\n",
      "Instance Segmentation: 같은 클래스 내에서도 개별 객체를 구분 (예: 여러 사람을 개별적으로 구별).\n",
      "Panoptic Segmentation: Semantic Segmentation과 Instance Segmentation을 결합하여 더욱 세밀한 분류 수행.\n",
      "대표적인 이미지 분할 모델\n",
      "FCN (Fully Convolutional Network): CNN을 기반으로 한 최초의 이미지 분할 네트워크.\n",
      "U-Net: 의료 영상 분석(CT, MRI)에서 널리 사용됨.\n",
      "DeepLab (DeepLabV3, DeepLabV3+): 강력한 Semantic Segmentation 성능 제공.\n",
      "2.4 이미지 생성 (Generative Models)\n",
      "딥러닝을 활용하여 새로운 이미지를 생성하는 기술.\n",
      "\n",
      "대표적인 이미지 생성 모델\n",
      "GAN (Generative Adversarial Networks):\n",
      "생성자(Generator)와 판별자(Discriminator)가 서로 경쟁하여 점점 더 사실적인 이미지를 생성.\n",
      "대표 모델: DCGAN, StyleGAN, BigGAN.\n",
      "VAE (Variational Autoencoder): 확률 모델을 이용해 새로운 이미지 생성.\n",
      "Diffusion Model: 최근 대세인 생성 모델 (예: DALL·E 2, Stable Diffusion).\n",
      "2.5 스타일 변환(Style Transfer)\n",
      "이미지의 스타일을 다른 스타일로 변환하는 기술. 예를 들어, 고흐의 화풍을 적용한 이미지 생성이 가능함.\n",
      "\n",
      "대표 모델: Neural Style Transfer (NST).\n",
      "2.6 영상 처리(Video Analysis)\n",
      "영상 데이터에서 의미 있는 패턴을 추출하고 분석하는 기술.\n",
      "\n",
      "주요 기법\n",
      "Optical Flow: 객체의 움직임을 추적.\n",
      "Action Recognition: 행동 인식 (예: 스포츠 동작 분석).\n",
      "Tracking Algorithms: 객체 추적 (예: SORT, DeepSORT).\n",
      "3. 컴퓨터 비전의 실제 응용\n",
      "3.1 자율 주행(Autonomous Driving)\n",
      "객체 탐지(Object Detection): 차선 인식, 보행자 감지.\n",
      "Semantic Segmentation: 도로, 차량, 신호등 등의 분류.\n",
      "3.2 의료 영상 분석(Medical Image Analysis)\n",
      "X-ray, CT, MRI 분석: 암 진단, 병변 탐지.\n",
      "U-Net 기반 세포 분할 모델: 의료 진단에 활용.\n",
      "3.3 얼굴 인식(Face Recognition)\n",
      "FaceNet, DeepFace 등의 딥러닝 모델을 활용하여 얼굴을 인식하고 검출.\n",
      "3.4 스마트 공장 및 산업용 비전\n",
      "제품 결함 탐지(Defect Detection): 공장에서 제품 품질 검사.\n",
      "로봇 비전(Robot Vision): 자동화된 물체 인식 및 분류.\n",
      "4. 최신 트렌드\n",
      "4.1 Transformer 기반 모델\n",
      "CNN을 넘어 Vision Transformer (ViT), DINO, Swin Transformer 등 새로운 구조가 등장.\n",
      "4.2 Diffusion 모델\n",
      "이미지 생성 모델의 새로운 대세로 떠오름 (Stable Diffusion, DALL·E 2).\n",
      "4.3 MLOps & AI 모델 경량화\n",
      "TensorRT, TFLite 등을 활용하여 모델을 최적화하고, 모바일 및 엣지 디바이스에서도 사용 가능하도록 설계.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('data/test_deeplearning.txt')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text = ''\n",
    "for i in docs :\n",
    "    text += i.page_content\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 컴퓨터 비전의 기본 개념\\n1.1 이미지 표현 방식',\n",
       " '이미지는 픽셀(Pixel) 값의 집합으로 표현되며, 다양한 방식으로 저장됩니다.',\n",
       " '픽셀(Pixel): 이미지의 최소 단위',\n",
       " '. 흑백 이미지는 0~255(단일 채널), 컬러 이미지는 RGB(3채널) 또는',\n",
       " '또는 RGBA(4채널)로 표현됨',\n",
       " '.',\n",
       " '채널(Channel): 이미지의 색상을 구성하는 정보',\n",
       " '. RGB(적, 녹, 청), CMYK(청록, 자홍, 노랑, 검정) 등.',\n",
       " '해상도(Resolution): 이미지의 크기(너비 × 높이)로 결정됨.',\n",
       " '1.2 이미지 처리 기초 개념',\n",
       " '히스토그램(Histogram): 픽셀 값의 분포를 나타내는 그래프.',\n",
       " '컨볼루션(Convolution): 필터(커널, Kernel)를 사용하여 이미지의 특정',\n",
       " '이미지의 특정 특징을 추출하는 연산',\n",
       " '. 예를 들면 엣지 검출(Edge Detection).',\n",
       " '필터(Filter): 이미지에서 특정 정보를 강조하거나 억제하는 행렬 연산',\n",
       " '. 대표적인 예로 가우시안 블러(Gaussian Blur)와 소벨 엣지 검출(Sobel',\n",
       " '검출(Sobel Edge Detection)이 있음',\n",
       " '.',\n",
       " '2. 딥러닝을 이용한 컴퓨터 비전 기술',\n",
       " '딥러닝을 활용한 컴퓨터 비전 기술은 데이터에서 패턴을 학습하고 예측하는데 최적화되어',\n",
       " '최적화되어 있습니다',\n",
       " '.',\n",
       " '2.1 합성곱 신경망(Convolutional Neural Networks, CNN)',\n",
       " 'CNN은 이미지 인식을 위한 가장 대표적인 딥러닝 모델입니다.',\n",
       " 'CNN의 주요 구조',\n",
       " '컨볼루션 층(Convolutional Layer): 엣지, 질감, 패턴 등의 특징을 추출',\n",
       " '.',\n",
       " '풀링 층(Pooling Layer): 공간적 크기를 줄이고, 중요한 특징을 보존 (Max',\n",
       " '보존 (Max Pooling, Average Pooling)',\n",
       " '.',\n",
       " '완전 연결 층(Fully Connected Layer, FC Layer): 최종적으로',\n",
       " '최종적으로 이미지 분류 등의 작업 수행',\n",
       " '.',\n",
       " 'CNN의 대표 모델',\n",
       " 'LeNet-5: 최초의 CNN 모델 중 하나로, 손글씨 숫자 인식을 위해 설계됨.',\n",
       " 'AlexNet: 깊은 신경망을 활용한 최초의 딥러닝 기반 이미지 분류 모델 (ILSVRC',\n",
       " '(ILSVRC 2012 우승)',\n",
       " '.',\n",
       " 'VGGNet: 더 깊은 네트워크 구조를 사용 (16~19층).',\n",
       " 'ResNet: Residual Connection을 도입하여 기울기',\n",
       " '도입하여 기울기 소실(Vanishing Gradient) 문제 해결',\n",
       " '.',\n",
       " 'EfficientNet: 연산량 대비 성능을 최적화한 경량화 모델.',\n",
       " '2.2 객체 탐지(Object Detection)',\n",
       " '이미지에서 특정 객체의 위치를 Bounding Box 형태로 감지하고, 어떤 클래스인지',\n",
       " '어떤 클래스인지 분류하는 기술',\n",
       " '.',\n",
       " '대표적인 객체 탐지 모델',\n",
       " 'R-CNN 계열: R-CNN → Fast R-CNN → Faster R-CNN →',\n",
       " 'R-CNN → Mask R-CNN (마스크 추가)',\n",
       " 'YOLO (You Only Look Once): 실시간 객체 탐지에 특화된 모델',\n",
       " '특화된 모델 (YOLOv1 ~ YOLOv8)',\n",
       " 'SSD (Single Shot MultiBox Detector): YOLO와 유사한',\n",
       " 'YOLO와 유사한 속도와 성능 제공',\n",
       " '.',\n",
       " '2.3 이미지 분할(Image Segmentation)',\n",
       " '각 픽셀이 어느 객체에 속하는지 분류하는 기술.',\n",
       " '이미지 분할 유형',\n",
       " 'Semantic Segmentation: 픽셀 단위로 객체 클래스를 분류 (예: 배경,',\n",
       " '(예: 배경, 도로, 사람 구분)',\n",
       " '.',\n",
       " 'Instance Segmentation: 같은 클래스 내에서도 개별 객체를 구분 (예:',\n",
       " '구분 (예: 여러 사람을 개별적으로 구별)',\n",
       " '.',\n",
       " 'Panoptic Segmentation: Semantic Segmentation과',\n",
       " 'Instance Segmentation을 결합하여 더욱 세밀한 분류 수행',\n",
       " '.',\n",
       " '대표적인 이미지 분할 모델',\n",
       " 'FCN (Fully Convolutional Network): CNN을 기반으로 한',\n",
       " '기반으로 한 최초의 이미지 분할 네트워크',\n",
       " '.',\n",
       " 'U-Net: 의료 영상 분석(CT, MRI)에서 널리 사용됨.',\n",
       " 'DeepLab (DeepLabV3, DeepLabV3+): 강력한 Semantic',\n",
       " 'Semantic Segmentation 성능 제공',\n",
       " '.',\n",
       " '2.4 이미지 생성 (Generative Models)',\n",
       " '딥러닝을 활용하여 새로운 이미지를 생성하는 기술.',\n",
       " '대표적인 이미지 생성 모델',\n",
       " 'GAN (Generative Adversarial Networks):',\n",
       " '생성자(Generator)와 판별자(Discriminator)가 서로 경쟁하여 점점 더',\n",
       " '경쟁하여 점점 더 사실적인 이미지를 생성',\n",
       " '.',\n",
       " '대표 모델: DCGAN, StyleGAN, BigGAN.',\n",
       " 'VAE (Variational Autoencoder): 확률 모델을 이용해 새로운 이미지',\n",
       " '새로운 이미지 생성',\n",
       " '.',\n",
       " 'Diffusion Model: 최근 대세인 생성 모델 (예: DALL·E 2,',\n",
       " 'DALL·E 2, Stable Diffusion)',\n",
       " '.',\n",
       " '2.5 스타일 변환(Style Transfer)',\n",
       " '이미지의 스타일을 다른 스타일로 변환하는 기술',\n",
       " '. 예를 들어, 고흐의 화풍을 적용한 이미지 생성이 가능함.',\n",
       " '대표 모델: Neural Style Transfer (NST).',\n",
       " '2.6 영상 처리(Video Analysis)',\n",
       " '영상 데이터에서 의미 있는 패턴을 추출하고 분석하는 기술.',\n",
       " '주요 기법\\nOptical Flow: 객체의 움직임을 추적.',\n",
       " 'Action Recognition: 행동 인식 (예: 스포츠 동작 분석).',\n",
       " 'Tracking Algorithms: 객체 추적 (예: SORT, DeepSORT).',\n",
       " '3. 컴퓨터 비전의 실제 응용\\n3.1 자율 주행(Autonomous Driving)',\n",
       " '객체 탐지(Object Detection): 차선 인식, 보행자 감지.',\n",
       " 'Semantic Segmentation: 도로, 차량, 신호등 등의 분류.',\n",
       " '3.2 의료 영상 분석(Medical Image Analysis)',\n",
       " 'X-ray, CT, MRI 분석: 암 진단, 병변 탐지.',\n",
       " 'U-Net 기반 세포 분할 모델: 의료 진단에 활용.',\n",
       " '3.3 얼굴 인식(Face Recognition)',\n",
       " 'FaceNet, DeepFace 등의 딥러닝 모델을 활용하여 얼굴을 인식하고 검출.',\n",
       " '3.4 스마트 공장 및 산업용 비전',\n",
       " '제품 결함 탐지(Defect Detection): 공장에서 제품 품질 검사.',\n",
       " '로봇 비전(Robot Vision): 자동화된 물체 인식 및 분류.\\n4. 최신 트렌드',\n",
       " '4. 최신 트렌드\\n4.1 Transformer 기반 모델',\n",
       " 'CNN을 넘어 Vision Transformer (ViT), DINO, Swin',\n",
       " 'Swin Transformer 등 새로운 구조가 등장',\n",
       " '.',\n",
       " '4.2 Diffusion 모델',\n",
       " '이미지 생성 모델의 새로운 대세로 떠오름 (Stable Diffusion, DALL·E',\n",
       " 'DALL·E 2)',\n",
       " '.',\n",
       " '4.3 MLOps & AI 모델 경량화',\n",
       " 'TensorRT, TFLite 등을 활용하여 모델을 최적화하고, 모바일 및 엣지',\n",
       " '모바일 및 엣지 디바이스에서도 사용 가능하도록 설계',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 10,\n",
    "    separators = ['\\n\\n', '\\n', '.', ' ']\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성\n",
    "embedding_model_name = \"jhgan/ko-sbert-nli\"  # 임베딩 모델 선택\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 벡터 스토어에 문서 추가\n",
    "test_vector_store = FAISS.from_texts(chunks, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특화된 모델 (YOLOv1 ~ YOLOv8)\n",
      "=========================================================\n",
      "DALL·E 2)\n",
      "=========================================================\n",
      "YOLO (You Only Look Once): 실시간 객체 탐지에 특화된 모델\n",
      "=========================================================\n",
      "Panoptic Segmentation: Semantic Segmentation과\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "retriever = test_vector_store.as_retriever()\n",
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"yolo에 대해 설명해주세요\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPT 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 1에 오디오 추가 완료: ../../data/1000.wav\n",
      "Slide 2에 오디오 추가 완료: ../../data/1000.wav\n",
      "Slide 3에 오디오 추가 완료: ../../data/1000.wav\n",
      "Slide 4에 오디오 추가 완료: ../../data/1000.wav\n",
      "Slide 5에 오디오 추가 완료: ../../data/1000.wav\n",
      "Slide 6에 오디오 추가 완료: ../../data/1000.wav\n",
      "Slide 7에 오디오 추가 완료: ../../data/1000.wav\n",
      "최종 PPT 생성 완료: new.pptx\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def convert_mp3_to_wav(mp3_path, wav_path):\n",
    "    \"\"\"MP3를 PowerPoint 호환성이 높은 WAV 포맷으로 변환\"\"\"\n",
    "    command = [\"ffmpeg\", \"-i\", mp3_path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"44100\", wav_path]\n",
    "    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "def add_audio_to_ppt(ppt_path, num_slides):\n",
    "    \"\"\"원본 PPT에 슬라이드별 발표 음성을 자동 삽입하고, 전환 시 자동 재생 설정\"\"\"\n",
    "    prs = Presentation(ppt_path)\n",
    "\n",
    "    for i in range(num_slides):\n",
    "        if i < len(prs.slides):\n",
    "            slide = prs.slides[i]\n",
    "            mp3_path = f\"speech_slide_{i+1}.mp3\"\n",
    "            wav_path = '../../data/1000.wav'\n",
    "\n",
    "            if os.path.exists(wav_path):\n",
    "                # MP3 → WAV 변환\n",
    "                # convert_mp3_to_wav(mp3_path, wav_path)\n",
    "\n",
    "                # 슬라이드에 오디오 삽입\n",
    "                shape = slide.shapes.add_shape(1, Inches(1), Inches(1), Inches(1), Inches(1))\n",
    "                shape.text = \"발표 음성\"\n",
    "\n",
    "                slide.shapes.add_movie(wav_path, left=Inches(1), top=Inches(1), width=Inches(1), height=Inches(1))\n",
    "\n",
    "                print(f\"Slide {i+1}에 오디오 추가 완료: {wav_path}\")\n",
    "            else:\n",
    "                print(f\"파일 없음: {mp3_path}\")\n",
    "\n",
    "    prs.save(\"../../data/new.pptx\")\n",
    "    print(\"최종 PPT 생성 완료: new.pptx\")\n",
    "\n",
    "# 실행 예제\n",
    "ppt_path = \"../../data/abc.pptx\"\n",
    "num_slides = 10  # PPT의 총 슬라이드 수\n",
    "add_audio_to_ppt(ppt_path, num_slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# 이미지 파일 경로 설정\n",
    "image_path = \"../../data/fight.jpeg\"  # 여기에 실제 이미지 경로 입력\n",
    "\n",
    "# Ollama bakllava 모델을 사용하여 이미지 분석 요청\n",
    "response = ollama.chat(\n",
    "    model=\"bakllava:7b\",  # bakllava 모델은 이미지 설명 가능\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"이 이미지에 대해 설명해줘.\",\n",
    "            \"images\": [image_path]  # 이미지 전달\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"📌 이미지 설명 (bakLLaVA 모델):\")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강조할 단어 :  {'인공지능', 'AI', '기획'}\n",
      "[SSML 미리보기]\n",
      "<speak>이제 최초의 발표하는 <break time=\"300ms\"/><prosody pitch=\"+15%\" rate=\"-5%\" volume=\"+3dB\"></prosody> 모델인 오인용에 대해 소개하겠습니다.</speak>\n",
      "→ output_0.wav\n",
      "\n",
      "output_0.wav 저장 완료\n",
      "\n",
      "[SSML 미리보기]\n",
      "<speak>이 프로젝트는 중요한 내용을 중심으로 발표용 대본을 생성하는 기능을 가지고 있습니다.</speak>\n",
      "→ output_1.wav\n",
      "\n",
      "output_1.wav 저장 완료\n",
      "\n",
      "[SSML 미리보기]\n",
      "<speak><break time=\"300ms\"/><prosody pitch=\"+15%\" rate=\"-5%\" volume=\"+3dB\"></prosody> 음성 합성을 활용하여 자동으로 발표할 수 있는 기능을 제공함으로써, 기업, 연구자, 학생 등이 보다 효율적으로 정보를 활용할 수 있게 됩니다.</speak>\n",
      "→ output_2.wav\n",
      "\n",
      "output_2.wav 저장 완료\n",
      "\n",
      "[SSML 미리보기]\n",
      "<speak>이 시스템은 일관된 발표 퀄리티를 유지하며, 발표에 드는 시간을 단축하는 기대 효과를 가지고 있습니다.</speak>\n",
      "→ output_3.wav\n",
      "\n",
      "output_3.wav 저장 완료\n",
      "\n",
      "[SSML 미리보기]\n",
      "<speak>다음은 프로젝트 기획에 대해 설명드리겠습니다.</speak>\n",
      "→ output_4.wav\n",
      "\n",
      "output_4.wav 저장 완료\n",
      "\n",
      "[SSML 미리보기]\n",
      "<speak>이 슬라이드에서는 프로젝트의 전반적인 <break time=\"300ms\"/><prosody pitch=\"+15%\" rate=\"-5%\" volume=\"+3dB\"></prosody> 방향과 목표를 간략히 정리하겠습니다.</speak>\n",
      "→ output_5.wav\n",
      "\n",
      "output_5.wav 저장 완료\n",
      "\n",
      "[SSML 미리보기]\n",
      "<speak>프로젝트의 기획은 체계적인 진행을 위해 매우 중요합니다.</speak>\n",
      "→ output_6.wav\n",
      "\n",
      "output_6.wav 저장 완료\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from google.cloud import texttospeech_v1 as tts\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 스크립트와 키워드\n",
    "script = \"\"\"이제 최초의 발표하는 인공지능 모델인 오인용에 대해 소개하겠습니다. 이 프로젝트는 중요한 내용을 중심으로 발표용 대본을 생성하는 기능을 가지고 있습니다. AI 음성 합성을 활용하여 자동으로 발표할 수 있는 기능을 제공함으로써, 기업, 연구자, 학생 등이 보다 효율적으로 정보를 활용할 수 있게 됩니다. 이 시스템은 일관된 발표 퀄리티를 유지하며, 발표에 드는 시간을 단축하는 기대 효과를 가지고 있습니다.\n",
    "\n",
    "다음은 프로젝트 기획에 대해 설명드리겠습니다. 이 슬라이드에서는 프로젝트의 전반적인 기획 방향과 목표를 간략히 정리하겠습니다. 프로젝트의 기획은 체계적인 진행을 위해 매우 중요합니다.\"\"\"\n",
    "input_keywords = [\"인공지능\"]\n",
    "\n",
    "# 문장 및 단어 추출\n",
    "sentences = re.split(r'(?<=[.?!])\\s+', script.strip())\n",
    "tokenized_sentences = [re.findall(r'\\w+', s) for s in sentences]\n",
    "unique_words = sorted(set(word for sent in tokenized_sentences for word in sent))\n",
    "\n",
    "# 임베딩\n",
    "embedder = OpenAIEmbeddings()\n",
    "word_embeddings = embedder.embed_documents(unique_words)\n",
    "keyword_embeddings = [embedder.embed_query(k) for k in input_keywords]\n",
    "\n",
    "# 유사도 기반 상위 3단어 추출 \n",
    "similarities = {}\n",
    "for word, w_emb in zip(unique_words, word_embeddings):\n",
    "    sims = [cosine_similarity([w_emb], [k_emb])[0][0] for k_emb in keyword_embeddings]\n",
    "    similarities[word] = max(sims)\n",
    "\n",
    "top_emphasized_words = {word for word, _ in sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:3]}\n",
    "print('강조할 단어 : ', top_emphasized_words)\n",
    "\n",
    "# SSML 생성 (자연스러운 강조)\n",
    "ssml_outputs = []\n",
    "for idx, sent in enumerate(sentences):\n",
    "    words = re.split(r'(\\W+)', sent)\n",
    "    processed = []\n",
    "    for w in words:\n",
    "        if w in top_emphasized_words:\n",
    "            emphasized = (\n",
    "                f'<break time=\"300ms\"/>'\n",
    "                f'<prosody pitch=\"+15%\" rate=\"-5%\" volume=\"+3dB\">'\n",
    "                f'</prosody>'\n",
    "            )\n",
    "            processed.append(emphasized)\n",
    "        else:\n",
    "            processed.append(w)\n",
    "    ssml = f\"<speak>{''.join(processed).strip()}</speak>\"\n",
    "    ssml_outputs.append((ssml, f\"output_{idx}.wav\"))\n",
    "\n",
    "# Google TTS 요청\n",
    "client = tts.TextToSpeechClient()\n",
    "voice = tts.VoiceSelectionParams(language_code=\"ko-KR\", name=\"ko-KR-Neural2-B\") \n",
    "audio_config = tts.AudioConfig(audio_encoding=tts.AudioEncoding.LINEAR16)\n",
    "\n",
    "# WAV 생성\n",
    "for ssml, filename in ssml_outputs:\n",
    "    print(f\"[SSML 미리보기]\\n{ssml}\\n→ {filename}\\n\")\n",
    "    response = client.synthesize_speech(\n",
    "        input=tts.SynthesisInput(ssml=ssml),\n",
    "        voice=voice,\n",
    "        audio_config=audio_config\n",
    "    )\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print(f\"{filename} 저장 완료\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
