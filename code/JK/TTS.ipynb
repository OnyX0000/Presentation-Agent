{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP TTS API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°•ì¡°í•  ë‹¨ì–´ :  {'ê¸°íš', 'ì¸ê³µì§€ëŠ¥', 'AI'}\n",
      "[SSML ë¯¸ë¦¬ë³´ê¸°]\n",
      "<speak>ì´ì œ ìµœì´ˆì˜ ë°œí‘œí•˜ëŠ” <prosody pitch=\"+10%\" volume=\"+2dB\"><emphasis level=\"strong\">ì¸ê³µì§€ëŠ¥!</emphasis></prosody> ëª¨ë¸ì¸ ì˜¤ì¸ìš©ì— ëŒ€í•´ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.</speak>\n",
      "â†’ output_0.wav\n",
      "\n",
      "output_0.wav ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "[SSML ë¯¸ë¦¬ë³´ê¸°]\n",
      "<speak>ì´ í”„ë¡œì íŠ¸ëŠ” ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë°œí‘œìš© ëŒ€ë³¸ì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.</speak>\n",
      "â†’ output_1.wav\n",
      "\n",
      "output_1.wav ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "[SSML ë¯¸ë¦¬ë³´ê¸°]\n",
      "<speak><prosody pitch=\"+10%\" volume=\"+2dB\"><emphasis level=\"strong\">AI!</emphasis></prosody> ìŒì„± í•©ì„±ì„ í™œìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ë°œí‘œí•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•¨ìœ¼ë¡œì¨, ê¸°ì—…, ì—°êµ¬ì, í•™ìƒ ë“±ì´ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.</speak>\n",
      "â†’ output_2.wav\n",
      "\n",
      "output_2.wav ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "[SSML ë¯¸ë¦¬ë³´ê¸°]\n",
      "<speak>ì´ ì‹œìŠ¤í…œì€ ì¼ê´€ëœ ë°œí‘œ í€„ë¦¬í‹°ë¥¼ ìœ ì§€í•˜ë©°, ë°œí‘œì— ë“œëŠ” ì‹œê°„ì„ ë‹¨ì¶•í•˜ëŠ” ê¸°ëŒ€ íš¨ê³¼ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.</speak>\n",
      "â†’ output_3.wav\n",
      "\n",
      "output_3.wav ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "[SSML ë¯¸ë¦¬ë³´ê¸°]\n",
      "<speak>ë‹¤ìŒì€ í”„ë¡œì íŠ¸ ê¸°íšì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</speak>\n",
      "â†’ output_4.wav\n",
      "\n",
      "output_4.wav ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "[SSML ë¯¸ë¦¬ë³´ê¸°]\n",
      "<speak>ì´ ìŠ¬ë¼ì´ë“œì—ì„œëŠ” í”„ë¡œì íŠ¸ì˜ ì „ë°˜ì ì¸ <prosody pitch=\"+10%\" volume=\"+2dB\"><emphasis level=\"strong\">ê¸°íš!</emphasis></prosody> ë°©í–¥ê³¼ ëª©í‘œë¥¼ ê°„ëµíˆ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.</speak>\n",
      "â†’ output_5.wav\n",
      "\n",
      "output_5.wav ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "[SSML ë¯¸ë¦¬ë³´ê¸°]\n",
      "<speak>í”„ë¡œì íŠ¸ì˜ ê¸°íšì€ ì²´ê³„ì ì¸ ì§„í–‰ì„ ìœ„í•´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.</speak>\n",
      "â†’ output_6.wav\n",
      "\n",
      "output_6.wav ì €ì¥ ì™„ë£Œ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from google.cloud import texttospeech_v1 as tts\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ìŠ¤í¬ë¦½íŠ¸ì™€ í‚¤ì›Œë“œ\n",
    "script = \"\"\"ì´ì œ ìµœì´ˆì˜ ë°œí‘œí•˜ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì¸ ì˜¤ì¸ìš©ì— ëŒ€í•´ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” ì¤‘ìš”í•œ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë°œí‘œìš© ëŒ€ë³¸ì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. AI ìŒì„± í•©ì„±ì„ í™œìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ë°œí‘œí•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•¨ìœ¼ë¡œì¨, ê¸°ì—…, ì—°êµ¬ì, í•™ìƒ ë“±ì´ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ì¼ê´€ëœ ë°œí‘œ í€„ë¦¬í‹°ë¥¼ ìœ ì§€í•˜ë©°, ë°œí‘œì— ë“œëŠ” ì‹œê°„ì„ ë‹¨ì¶•í•˜ëŠ” ê¸°ëŒ€ íš¨ê³¼ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒì€ í”„ë¡œì íŠ¸ ê¸°íšì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì´ ìŠ¬ë¼ì´ë“œì—ì„œëŠ” í”„ë¡œì íŠ¸ì˜ ì „ë°˜ì ì¸ ê¸°íš ë°©í–¥ê³¼ ëª©í‘œë¥¼ ê°„ëµíˆ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ì˜ ê¸°íšì€ ì²´ê³„ì ì¸ ì§„í–‰ì„ ìœ„í•´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.\"\"\"\n",
    "input_keywords = [\"ì¸ê³µì§€ëŠ¥\"]\n",
    "\n",
    "# ë¬¸ì¥ ë° ë‹¨ì–´ ì¶”ì¶œ\n",
    "sentences = re.split(r'(?<=[.?!])\\s+', script.strip())\n",
    "tokenized_sentences = [re.findall(r'\\w+', s) for s in sentences]\n",
    "unique_words = sorted(set(word for sent in tokenized_sentences for word in sent))\n",
    "\n",
    "# ì„ë² ë”©\n",
    "embedder = OpenAIEmbeddings()\n",
    "word_embeddings = embedder.embed_documents(unique_words)\n",
    "keyword_embeddings = [embedder.embed_query(k) for k in input_keywords]\n",
    "\n",
    "# ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 3ë‹¨ì–´ ì¶”ì¶œ \n",
    "similarities = {}\n",
    "for word, w_emb in zip(unique_words, word_embeddings):\n",
    "    sims = [cosine_similarity([w_emb], [k_emb])[0][0] for k_emb in keyword_embeddings]\n",
    "    similarities[word] = max(sims)\n",
    "\n",
    "top_emphasized_words = {word for word, _ in sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:3]}\n",
    "print('ê°•ì¡°í•  ë‹¨ì–´ : ', top_emphasized_words)\n",
    "\n",
    "# SSML ìƒì„± (emphasis + prosody)\n",
    "ssml_outputs = []\n",
    "for idx, sent in enumerate(sentences):\n",
    "    words = re.split(r'(\\W+)', sent)\n",
    "    processed = []\n",
    "    for w in words:\n",
    "        if w in top_emphasized_words:\n",
    "            emphasized = (\n",
    "                f'<prosody pitch=\"+10%\" volume=\"+2dB\">'\n",
    "                f'<emphasis level=\"strong\">{w}!</emphasis>'\n",
    "                f'</prosody>'\n",
    "            )\n",
    "            processed.append(emphasized)\n",
    "        else:\n",
    "            processed.append(w)\n",
    "    ssml = f\"<speak>{''.join(processed).strip()}</speak>\"\n",
    "    ssml_outputs.append((ssml, f\"output_{idx}.wav\"))\n",
    "\n",
    "# Google TTS ìš”ì²­\n",
    "client = tts.TextToSpeechClient()\n",
    "voice = tts.VoiceSelectionParams(language_code=\"ko-KR\", name=\"ko-KR-Standard-B\") \n",
    "audio_config = tts.AudioConfig(audio_encoding=tts.AudioEncoding.LINEAR16)\n",
    "\n",
    "# WAV ìƒì„±\n",
    "for ssml, filename in ssml_outputs:\n",
    "    print(f\"[SSML ë¯¸ë¦¬ë³´ê¸°]\\n{ssml}\\nâ†’ {filename}\\n\")\n",
    "    response = client.synthesize_speech(\n",
    "        input=tts.SynthesisInput(ssml=ssml),\n",
    "        voice=voice,\n",
    "        audio_config=audio_config\n",
    "    )\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print(f\"{filename} ì €ì¥ ì™„ë£Œ\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•  ì¼ : ì „ì²´ ëŒ€ë³¸ì„ JSONìœ¼ë¡œ í‚¤ì›Œë“œì™€ í•¨ê¼ ì…ë ¥ë°›ê³  ì „ì²´ ëŒ€ë³¸ì—ì„œ ìœ ì‚¬ì–´ 10ê°œë¥¼ JSONìœ¼ë¡œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube ì˜ìƒì—ì„œ WAV ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def time_convert(time_str):\n",
    "    minutes = time_str // 100  \n",
    "    seconds = time_str % 100   \n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "def download_audio(url, save_dir, clip_idx, start=None, end=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # í™•ì¥ì í¬í•¨ëœ yt-dlp ì €ì¥ìš© ì„ì‹œ íŒŒì¼ ê²½ë¡œ\n",
    "    temp_template = os.path.join(save_dir, f\"CTS_temp_{clip_idx}.%(ext)s\")\n",
    "    temp_wav = os.path.join(save_dir, f\"CTS_temp_{clip_idx}.wav\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': temp_template,\n",
    "        'quiet': True,\n",
    "        'force_ipv4': True,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "    # ìë¥´ê³  ì €ì¥\n",
    "    audio = AudioSegment.from_file(temp_wav, format=\"wav\")\n",
    "    if start and end:\n",
    "        start_ms = time_convert(start) * 1000\n",
    "        end_ms = time_convert(end) * 1000\n",
    "        audio = audio[start_ms:end_ms]\n",
    "\n",
    "    # ìµœì¢… íŒŒì¼ëª…: cheo_1.wav, cheo_2.wav ...\n",
    "    final_path = os.path.join(save_dir, f\"CTS_{clip_idx}.wav\")\n",
    "    audio.export(final_path, format=\"wav\")\n",
    "\n",
    "    os.remove(temp_wav)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://youtu.be/tRn4W5n39U8?si=EGa2PYHHboa2xajA&t=481\"\n",
    "save_dir = \"../../data/test_wav\"\n",
    "audio_data = []\n",
    "start = 0\n",
    "end = 20\n",
    "i = 1 \n",
    "\n",
    "for time in range(480,40000,30):\n",
    "    start += time\n",
    "    end += time\n",
    "    audio = download_audio(\n",
    "        url = url,\n",
    "        save_dir=save_dir,\n",
    "        clip_idx=i,\n",
    "        start=start,\n",
    "        end=end\n",
    "    )\n",
    "    i += 1\n",
    "    audio_data.append(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\GitHub\\zonos\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "import torch._dynamo  # suppress warning if needed\n",
    "\n",
    "# â— phonemizerìš© í™˜ê²½ë³€ìˆ˜ ì„¤ì • (espeak.dll ëŒ€ì‘)\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\eSpeak NG\"\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\espeak.dll\"\n",
    "\n",
    "# â— torch compile ë¹„í™œì„±í™” (C++ ì»´íŒŒì¼ëŸ¬ ì—†ì´ ì‹¤í–‰)\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²½ë¡œ ì„¤ì •\n",
    "path = \"../../data/test_wav\"\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# ì…ë ¥ ìŒì„± ë¡œë“œ\n",
    "wav, sampling_rate = torchaudio.load(os.path.join(path, \"ma_1_test.wav\"))\n",
    "\n",
    "# ìŠ¤í”¼ì»¤ ì„ë² ë”© ìƒì„±\n",
    "speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "\n",
    "text = \"\"\"\n",
    "í˜„ëŒ€ ì‚¬íšŒì—ì„œ *AI* ê¸°ìˆ ì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "ìš°ë¦¬ì˜ 'ë°œí‘œ' ëŠ¥ë ¥ì„ ì§€ì›í•˜ëŠ” ì‹œìŠ¤í…œì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "cond_dict = make_cond_dict(text=text, speaker=speaker, language=\"ko\")\n",
    "conditioning = model.prepare_conditioning(cond_dict)\n",
    "\n",
    "codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "# ì›ë³¸ ì €ì¥\n",
    "torchaudio.save(os.path.join(path, \"sample.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "print(\"ğŸ”Š sample.wav ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "# # í›„ì²˜ë¦¬\n",
    "# sr = model.autoencoder.sampling_rate\n",
    "# audio_np = wavs[0].squeeze().numpy()  # [1, N] â†’ [N] ë³´ì¥\n",
    "\n",
    "# # 1.3ë°° ë¹ ë¥´ê²Œ\n",
    "# y_fast = librosa.effects.time_stretch(audio_np, rate=1.3)\n",
    "\n",
    "# # +2 ë°˜ìŒ\n",
    "# y_shifted = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=2)\n",
    "\n",
    "# # shape ì¡°ì •\n",
    "# if y_shifted.ndim == 1:\n",
    "#     y_shifted = np.expand_dims(y_shifted, axis=0)  # â†’ [1, samples]\n",
    "\n",
    "# # numpy â†’ torch\n",
    "# y_tensor = torch.from_numpy(y_shifted).float()  # float32ë¡œ ë§ì¶¤\n",
    "\n",
    "# # ì €ì¥\n",
    "# output_path = os.path.join(path, \"fast_pitchup.wav\")\n",
    "# torchaudio.save(output_path, y_tensor, sr)\n",
    "\n",
    "# print(\"âš¡ 1.3ë°° ë¹ ë¥´ê²Œ + ğŸµ í”¼ì¹˜ +2ë°˜ìŒ ì ìš© ì™„ë£Œ!\")\n",
    "# print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_shot(path,file,text,name, model):\n",
    "    torch.cuda.empty_cache()\n",
    "    wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "    speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker,\n",
    "        language = \"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_zero_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    # print(\"ğŸ”Š zero_shot.wav ìƒì„± ì™„ë£Œ!\")\n",
    "    # sr = model.autoencoder.sampling_rate\n",
    "    # audio_np = wavs[0].numpy()\n",
    "\n",
    "    # # [1] 1.3ë°° ì†ë„ (tempo)\n",
    "    # y_fast = librosa.effects.time_stretch(audio_np, rate=1.3)\n",
    "\n",
    "    # # [2] +2 ë°˜ìŒ pitch up\n",
    "    # y_shifted = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=2)\n",
    "\n",
    "    # # ì €ì¥\n",
    "    # final_out_path = os.path.join(path, f\"{name}_fast_pitchup.wav\")\n",
    "    # sf.write(final_out_path, y_shifted, sr)\n",
    "\n",
    "\n",
    "def few_shot(path, data, text, name, model):\n",
    "    '''few-shot ë³´ì´ìŠ¤ í´ë¦¬ë‹'''\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    embeddings = []\n",
    "    for file in data:\n",
    "        wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "        emb = model.make_speaker_embedding(wav, sampling_rate)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    speaker_embedding = torch.stack(embeddings).mean(dim=0)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker_embedding,\n",
    "        language=\"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    # ìŒì„± ìƒì„± (ì»´íŒŒì¼ëŸ¬ ë¹„í™œì„±í™”)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_few_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"ğŸ”Š few_shot.wav ìƒì„± ì™„ë£Œ!\")\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/test_wav\"\n",
    "file = \"ma_1_test.wav\"\n",
    "data = [f\"karina_{i}.wav\" for i in range(1,5)]\n",
    "name = \"test01\"\n",
    "text = \"\"\"\n",
    "ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, AI ê¸°ë°˜ ë°œí‘œ ì§€ì› ì‹œìŠ¤í…œì¸ \"ì €í¬ ë°œí‘œ ì•ˆí•©ë‹ˆë‹¤!\" í”„ë¡œì íŠ¸ê°€ ê¸°íšë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "zero_shot(path,file, text,name, model)\n",
    "# few_shot(path,data, text,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\GitHub\\zonos\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "import time\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ğŸ”§ í™˜ê²½ ì„¤ì •\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\eSpeak NG\"\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\espeak.dll\"\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# ğŸ“¦ ëª¨ë¸ ë¡œë”©\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n",
    "\n",
    "# ğŸ“ ê²½ë¡œ ì„¤ì •\n",
    "TEXT_PATH = \"../../data/txt/test_script.txt\"\n",
    "AUDIO_PATH = \"../../data/test_wav\"\n",
    "os.makedirs(AUDIO_PATH, exist_ok=True)\n",
    "\n",
    "# ğŸ”Š ì°¸ì¡° ìŒì„±\n",
    "zero_shot_ref = \"CTS_3.wav\"\n",
    "few_shot_refs = [\"CTS_4.wav\", \"CTS_5.wav\"]\n",
    "\n",
    "# ğŸ“„ í…ìŠ¤íŠ¸ ë¡œë“œ ë° í˜ì´ì§€ ë¶„í• \n",
    "with open(TEXT_PATH, encoding=\"utf-8\") as f:\n",
    "    script = f.read()\n",
    "pages = script.strip().split(\"\\n\\n\")\n",
    "\n",
    "# âœ… speaker ì„ë² ë”© ìºì‹± (ë‹¨ 1íšŒ)\n",
    "zero_wav, zero_sr = torchaudio.load(os.path.join(AUDIO_PATH, zero_shot_ref))\n",
    "zero_shot_speaker = model.make_speaker_embedding(zero_wav, zero_sr)\n",
    "\n",
    "few_embeddings = []\n",
    "for file in few_shot_refs:\n",
    "    wav, sr = torchaudio.load(os.path.join(AUDIO_PATH, file))\n",
    "    emb = model.make_speaker_embedding(wav, sr)\n",
    "    few_embeddings.append(emb)\n",
    "few_shot_speaker = torch.stack(few_embeddings).mean(dim=0)\n",
    "\n",
    "# ğŸ™ Zero-Shot í•©ì„±\n",
    "def zero_shot(text, name):\n",
    "    torch.cuda.empty_cache()\n",
    "    cond = make_cond_dict(text=text, speaker=zero_shot_speaker, language=\"ko\")\n",
    "    conditioning = model.prepare_conditioning(cond)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "    torchaudio.save(os.path.join(AUDIO_PATH, f\"{name}_zero_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "# ğŸ™ Few-Shot í•©ì„±\n",
    "def few_shot(text, name):\n",
    "    torch.cuda.empty_cache()\n",
    "    cond = make_cond_dict(text=text, speaker=few_shot_speaker, language=\"ko\")\n",
    "    conditioning = model.prepare_conditioning(cond)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "    torchaudio.save(os.path.join(AUDIO_PATH, f\"{name}_few_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [01:53<00:00, 22.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 143.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:03<00:00, 21.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 125.55s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:05<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 134.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [01:58<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 119.78s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1937/2588 [01:18<00:26, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 79.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2120/2588 [01:28<00:19, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 89.5s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:06<00:00, 20.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 127.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:05<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 126.37s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:15<00:00, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 137.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:14<00:00, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 136.45s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:03<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 125.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:04<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 129.81s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1791/2588 [01:13<00:32, 24.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 73.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1720/2588 [01:05<00:33, 26.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 66.38s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:15<00:00, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 137.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:14<00:00, 19.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 135.93s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1817/2588 [01:12<00:30, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 73.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1988/2588 [01:20<00:24, 24.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 81.02s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:02<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 124.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:02<00:00, 21.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 133.46s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:06<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 128.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:06<00:00, 20.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 128.11s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:02<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 124.38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:00<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 132.2s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:07<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 129.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:04<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 126.78s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1888/2588 [01:16<00:28, 24.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 77.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [01:57<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 131.24s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2535/2588 [02:02<00:02, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 124.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2588/2588 [02:00<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 133.79s\n",
      "\n",
      "ğŸ¤ ì²˜ë¦¬ ì¤‘: page_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2470/2588 [01:53<00:05, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Zero-Shot ì™„ë£Œ: 122.51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1933/2588 [01:19<00:27, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Few-Shot ì™„ë£Œ: 80.61s\n",
      "\n",
      "â±ï¸ í˜ì´ì§€ë³„ ìƒì„± ì‹œê°„:\n",
      "\n",
      "| page    |   zero_shot |   few_shot |\n",
      "|---------|-------------|------------|\n",
      "| page_0  |      143.52 |     125.55 |\n",
      "| page_1  |      134.99 |     119.78 |\n",
      "| page_2  |       79.26 |      89.5  |\n",
      "| page_3  |      127.08 |     126.37 |\n",
      "| page_4  |      137.22 |     136.45 |\n",
      "| page_5  |      125.05 |     129.81 |\n",
      "| page_6  |       73.77 |      66.38 |\n",
      "| page_7  |      137.52 |     135.93 |\n",
      "| page_8  |       73.29 |      81.02 |\n",
      "| page_9  |      124.1  |     133.46 |\n",
      "| page_10 |      128.26 |     128.11 |\n",
      "| page_11 |      124.38 |     132.2  |\n",
      "| page_12 |      129.16 |     126.78 |\n",
      "| page_13 |       77.17 |     131.24 |\n",
      "| page_14 |      124.6  |     133.79 |\n",
      "| page_15 |      122.51 |      80.61 |\n"
     ]
    }
   ],
   "source": [
    "# ğŸ•’ í•©ì„± ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •\n",
    "results = []\n",
    "for idx, text in enumerate(pages):\n",
    "    \n",
    "    page_name = f\"page_{idx}\"\n",
    "    print(f\"\\nğŸ¤ ì²˜ë¦¬ ì¤‘: {page_name}\")\n",
    "\n",
    "    # Zero-Shot\n",
    "    start = time.time()\n",
    "    try:\n",
    "        zero_shot(text, page_name)\n",
    "        zero_time = round(time.time() - start, 2)\n",
    "        print(f\"âœ… Zero-Shot ì™„ë£Œ: {zero_time}s\")\n",
    "    except Exception as e:\n",
    "        zero_time = f\"Error: {str(e)}\"\n",
    "\n",
    "    # Few-Shot\n",
    "    start = time.time()\n",
    "    try:\n",
    "        few_shot(text, page_name)\n",
    "        few_time = round(time.time() - start, 2)\n",
    "        print(f\"âœ… Few-Shot ì™„ë£Œ: {few_time}s\")\n",
    "    except Exception as e:\n",
    "        few_time = f\"Error: {str(e)}\"\n",
    "\n",
    "    results.append({\n",
    "        \"page\": page_name,\n",
    "        \"zero_shot\": zero_time,\n",
    "        \"few_shot\": few_time\n",
    "    })\n",
    "\n",
    "# ğŸ“Š ê²°ê³¼ ì¶œë ¥\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nâ±ï¸ í˜ì´ì§€ë³„ ìƒì„± ì‹œê°„:\\n\")\n",
    "print(tabulate(df, headers='keys', tablefmt='github', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… speaker ì„ë² ë”© ìºì‹± (ë‹¨ 1íšŒ)\n",
    "zero_wav, zero_sr = torchaudio.load(os.path.join(AUDIO_PATH, zero_shot_ref))\n",
    "zero_shot_speaker = model.make_speaker_embedding(zero_wav, zero_sr)\n",
    "\n",
    "few_embeddings = []\n",
    "for file in few_shot_refs:\n",
    "    wav, sr = torchaudio.load(os.path.join(AUDIO_PATH, file))\n",
    "    emb = model.make_speaker_embedding(wav, sr)\n",
    "    few_embeddings.append(emb)\n",
    "few_shot_speaker = torch.stack(few_embeddings).mean(dim=0)\n",
    "\n",
    "# ğŸ™ Zero-Shot í•©ì„±\n",
    "def zero_shot(text, name):\n",
    "    torch.cuda.empty_cache()\n",
    "    cond = make_cond_dict(text=text, speaker=zero_shot_speaker, language=\"ko\")\n",
    "    conditioning = model.prepare_conditioning(cond)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "    torchaudio.save(os.path.join(AUDIO_PATH, f\"{name}_zero_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "# ğŸ™ Few-Shot í•©ì„±\n",
    "def few_shot(text, name):\n",
    "    torch.cuda.empty_cache()\n",
    "    cond = make_cond_dict(text=text, speaker=few_shot_speaker, language=\"ko\")\n",
    "    conditioning = model.prepare_conditioning(cond)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "    torchaudio.save(os.path.join(AUDIO_PATH, f\"{name}_few_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "# ğŸ•’ í•©ì„± ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •\n",
    "results = []\n",
    "for idx, text in enumerate(pages):\n",
    "    page_name = f\"page_{idx}\"\n",
    "    print(f\"\\nğŸ¤ ì²˜ë¦¬ ì¤‘: {page_name}\")\n",
    "\n",
    "    # Zero-Shot\n",
    "    start = time.time()\n",
    "    try:\n",
    "        zero_shot(text, page_name)\n",
    "        zero_time = round(time.time() - start, 2)\n",
    "        print(f\"âœ… Zero-Shot ì™„ë£Œ: {zero_time}s\")\n",
    "    except Exception as e:\n",
    "        zero_time = f\"Error: {str(e)}\"\n",
    "\n",
    "    # Few-Shot\n",
    "    start = time.time()\n",
    "    try:\n",
    "        few_shot(text, page_name)\n",
    "        few_time = round(time.time() - start, 2)\n",
    "        print(f\"âœ… Few-Shot ì™„ë£Œ: {few_time}s\")\n",
    "    except Exception as e:\n",
    "        few_time = f\"Error: {str(e)}\"\n",
    "\n",
    "    results.append({\n",
    "        \"page\": page_name,\n",
    "        \"zero_shot\": zero_time,\n",
    "        \"few_shot\": few_time\n",
    "    })\n",
    "\n",
    "# ğŸ“Š ê²°ê³¼ ì¶œë ¥\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nâ±ï¸ í˜ì´ì§€ë³„ ìƒì„± ì‹œê°„:\\n\")\n",
    "print(tabulate(df, headers='keys', tablefmt='github', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZONOS ì›¹ì—ì„œ gradioë¡œ ë¶€ë¥´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_client import Client\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "import numpy as np\n",
    "import soundfile as sf  # for saving the audio\n",
    "import numpy as np\n",
    "# 1. Gradio ì•± URL (ì˜ˆ: ë¡œì»¬ ë˜ëŠ” share URL)\n",
    "client = Client(\"http://localhost:7860\")  # ë˜ëŠ” \"https://xxxx.gradio.live\"\n",
    "\n",
    "# 2. generate_audioì— ë§ê²Œ ì…ë ¥ê°’ ì„¤ì •\n",
    "result = client.predict(\n",
    "    \"Zyphra/Zonos-v0.1-transformer\",  # model_choice\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ìˆ˜ì—…í•  ë‚´ìš©ì„ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\",            # text\n",
    "    \"ko\",                          # language\n",
    "    None,                             # speaker_audio (None = ì‚¬ìš© ì•ˆ í•¨)\n",
    "    None,                             # prefix_audio (None = ì‚¬ìš© ì•ˆ í•¨)\n",
    "    1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.2,  # emotion sliders\n",
    "    0.78,                             # vq_single\n",
    "    24000,                            # fmax\n",
    "    45.0,                             # pitch_std\n",
    "    15.0,                             # speaking_rate\n",
    "    4.0,                              # dnsmos\n",
    "    False,                            # speaker_noised\n",
    "    2.0,                              # cfg_scale\n",
    "    0,                              # top_p\n",
    "    0,                                # top_k (min_kë¡œ ì „ë‹¬ë¨)\n",
    "    0,                              # min_p\n",
    "    0.5, 0.4, 0.0,                    # linear, confidence, quadratic\n",
    "    42,                               # seed\n",
    "    False,                            # randomize_seed\n",
    "    [\"emotion\"],                     # unconditional_keys\n",
    "    api_name=\"/generate_audio\"              # ì˜ˆ: í•¨ìˆ˜ê°€ ìë™ ë“±ë¡ëœ ì´ë¦„\n",
    ")\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "src = result[0]\n",
    "dst = \"test.wav\"\n",
    "\n",
    "shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTAPI & STREAMLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import re\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def extract_top_similar_words(text: str, keywords: List[str], top_k: int = 10) -> List[str]:\n",
    "    # í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ í† í° ì¶”ì¶œ\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    word_counts = Counter(words)\n",
    "    unique_words = list(word_counts.keys())\n",
    "\n",
    "    # ì„ë² ë”©\n",
    "    embedder = OpenAIEmbeddings()\n",
    "    word_embeddings = embedder.embed_documents(unique_words)\n",
    "    keyword_embeddings = [embedder.embed_query(k) for k in keywords]\n",
    "\n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = {}\n",
    "    for word, emb in zip(unique_words, word_embeddings):\n",
    "        sim_scores = [cosine_similarity([emb], [k_emb])[0][0] for k_emb in keyword_embeddings]\n",
    "        similarities[word] = max(sim_scores)\n",
    "\n",
    "    # ìƒìœ„ Nê°œ ë‹¨ì–´ ë°˜í™˜\n",
    "    top_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{\"word\": word, \"similarity\": round(score, 4)} for word, score in top_similar]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script_generate.py\n",
    "from utils import extract_top_similar_words\n",
    "\n",
    "def combine_scripts_and_extract_keywords(scripts: List[str], keywords: List[str]):\n",
    "    \"\"\"ì „ì²´ ëŒ€ë³¸ í†µí•© ë° ìœ ì‚¬ ë‹¨ì–´ ì¶”ì¶œ\"\"\"\n",
    "    full_script = \"\\n\\n\".join(scripts)\n",
    "    similar_words = extract_top_similar_words(full_script, keywords, top_k=10)\n",
    "    return {\n",
    "        \"full_script\": full_script,\n",
    "        \"top_similar_words\": similar_words\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# routes.py\n",
    "from fastapi import APIRouter, Body\n",
    "from typing import List\n",
    "from core.script_generate import combine_scripts_and_extract_keywords\n",
    "\n",
    "@router.post(\"/script/combine-and-analyze\")\n",
    "async def combine_and_analyze(\n",
    "    scripts: List[str] = Body(...),\n",
    "    keywords: List[str] = Body(...)\n",
    "):  \n",
    "    try:\n",
    "        result = combine_scripts_and_extract_keywords(scripts, keywords)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zonos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
