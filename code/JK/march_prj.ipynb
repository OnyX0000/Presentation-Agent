{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader('data/Team5 Parking-Genius_Final.pdf')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in docs :\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 컴퓨터 비전의 기본 개념\n",
      "1.1 이미지 표현 방식\n",
      "이미지는 픽셀(Pixel) 값의 집합으로 표현되며, 다양한 방식으로 저장됩니다.\n",
      "\n",
      "픽셀(Pixel): 이미지의 최소 단위. 흑백 이미지는 0~255(단일 채널), 컬러 이미지는 RGB(3채널) 또는 RGBA(4채널)로 표현됨.\n",
      "채널(Channel): 이미지의 색상을 구성하는 정보. RGB(적, 녹, 청), CMYK(청록, 자홍, 노랑, 검정) 등.\n",
      "해상도(Resolution): 이미지의 크기(너비 × 높이)로 결정됨.\n",
      "1.2 이미지 처리 기초 개념\n",
      "히스토그램(Histogram): 픽셀 값의 분포를 나타내는 그래프.\n",
      "컨볼루션(Convolution): 필터(커널, Kernel)를 사용하여 이미지의 특정 특징을 추출하는 연산. 예를 들면 엣지 검출(Edge Detection).\n",
      "필터(Filter): 이미지에서 특정 정보를 강조하거나 억제하는 행렬 연산. 대표적인 예로 가우시안 블러(Gaussian Blur)와 소벨 엣지 검출(Sobel Edge Detection)이 있음.\n",
      "2. 딥러닝을 이용한 컴퓨터 비전 기술\n",
      "딥러닝을 활용한 컴퓨터 비전 기술은 데이터에서 패턴을 학습하고 예측하는데 최적화되어 있습니다.\n",
      "\n",
      "2.1 합성곱 신경망(Convolutional Neural Networks, CNN)\n",
      "CNN은 이미지 인식을 위한 가장 대표적인 딥러닝 모델입니다.\n",
      "\n",
      "CNN의 주요 구조\n",
      "컨볼루션 층(Convolutional Layer): 엣지, 질감, 패턴 등의 특징을 추출.\n",
      "풀링 층(Pooling Layer): 공간적 크기를 줄이고, 중요한 특징을 보존 (Max Pooling, Average Pooling).\n",
      "완전 연결 층(Fully Connected Layer, FC Layer): 최종적으로 이미지 분류 등의 작업 수행.\n",
      "CNN의 대표 모델\n",
      "LeNet-5: 최초의 CNN 모델 중 하나로, 손글씨 숫자 인식을 위해 설계됨.\n",
      "AlexNet: 깊은 신경망을 활용한 최초의 딥러닝 기반 이미지 분류 모델 (ILSVRC 2012 우승).\n",
      "VGGNet: 더 깊은 네트워크 구조를 사용 (16~19층).\n",
      "ResNet: Residual Connection을 도입하여 기울기 소실(Vanishing Gradient) 문제 해결.\n",
      "EfficientNet: 연산량 대비 성능을 최적화한 경량화 모델.\n",
      "2.2 객체 탐지(Object Detection)\n",
      "이미지에서 특정 객체의 위치를 Bounding Box 형태로 감지하고, 어떤 클래스인지 분류하는 기술.\n",
      "\n",
      "대표적인 객체 탐지 모델\n",
      "R-CNN 계열: R-CNN → Fast R-CNN → Faster R-CNN → Mask R-CNN (마스크 추가)\n",
      "YOLO (You Only Look Once): 실시간 객체 탐지에 특화된 모델 (YOLOv1 ~ YOLOv8)\n",
      "SSD (Single Shot MultiBox Detector): YOLO와 유사한 속도와 성능 제공.\n",
      "2.3 이미지 분할(Image Segmentation)\n",
      "각 픽셀이 어느 객체에 속하는지 분류하는 기술.\n",
      "\n",
      "이미지 분할 유형\n",
      "Semantic Segmentation: 픽셀 단위로 객체 클래스를 분류 (예: 배경, 도로, 사람 구분).\n",
      "Instance Segmentation: 같은 클래스 내에서도 개별 객체를 구분 (예: 여러 사람을 개별적으로 구별).\n",
      "Panoptic Segmentation: Semantic Segmentation과 Instance Segmentation을 결합하여 더욱 세밀한 분류 수행.\n",
      "대표적인 이미지 분할 모델\n",
      "FCN (Fully Convolutional Network): CNN을 기반으로 한 최초의 이미지 분할 네트워크.\n",
      "U-Net: 의료 영상 분석(CT, MRI)에서 널리 사용됨.\n",
      "DeepLab (DeepLabV3, DeepLabV3+): 강력한 Semantic Segmentation 성능 제공.\n",
      "2.4 이미지 생성 (Generative Models)\n",
      "딥러닝을 활용하여 새로운 이미지를 생성하는 기술.\n",
      "\n",
      "대표적인 이미지 생성 모델\n",
      "GAN (Generative Adversarial Networks):\n",
      "생성자(Generator)와 판별자(Discriminator)가 서로 경쟁하여 점점 더 사실적인 이미지를 생성.\n",
      "대표 모델: DCGAN, StyleGAN, BigGAN.\n",
      "VAE (Variational Autoencoder): 확률 모델을 이용해 새로운 이미지 생성.\n",
      "Diffusion Model: 최근 대세인 생성 모델 (예: DALL·E 2, Stable Diffusion).\n",
      "2.5 스타일 변환(Style Transfer)\n",
      "이미지의 스타일을 다른 스타일로 변환하는 기술. 예를 들어, 고흐의 화풍을 적용한 이미지 생성이 가능함.\n",
      "\n",
      "대표 모델: Neural Style Transfer (NST).\n",
      "2.6 영상 처리(Video Analysis)\n",
      "영상 데이터에서 의미 있는 패턴을 추출하고 분석하는 기술.\n",
      "\n",
      "주요 기법\n",
      "Optical Flow: 객체의 움직임을 추적.\n",
      "Action Recognition: 행동 인식 (예: 스포츠 동작 분석).\n",
      "Tracking Algorithms: 객체 추적 (예: SORT, DeepSORT).\n",
      "3. 컴퓨터 비전의 실제 응용\n",
      "3.1 자율 주행(Autonomous Driving)\n",
      "객체 탐지(Object Detection): 차선 인식, 보행자 감지.\n",
      "Semantic Segmentation: 도로, 차량, 신호등 등의 분류.\n",
      "3.2 의료 영상 분석(Medical Image Analysis)\n",
      "X-ray, CT, MRI 분석: 암 진단, 병변 탐지.\n",
      "U-Net 기반 세포 분할 모델: 의료 진단에 활용.\n",
      "3.3 얼굴 인식(Face Recognition)\n",
      "FaceNet, DeepFace 등의 딥러닝 모델을 활용하여 얼굴을 인식하고 검출.\n",
      "3.4 스마트 공장 및 산업용 비전\n",
      "제품 결함 탐지(Defect Detection): 공장에서 제품 품질 검사.\n",
      "로봇 비전(Robot Vision): 자동화된 물체 인식 및 분류.\n",
      "4. 최신 트렌드\n",
      "4.1 Transformer 기반 모델\n",
      "CNN을 넘어 Vision Transformer (ViT), DINO, Swin Transformer 등 새로운 구조가 등장.\n",
      "4.2 Diffusion 모델\n",
      "이미지 생성 모델의 새로운 대세로 떠오름 (Stable Diffusion, DALL·E 2).\n",
      "4.3 MLOps & AI 모델 경량화\n",
      "TensorRT, TFLite 등을 활용하여 모델을 최적화하고, 모바일 및 엣지 디바이스에서도 사용 가능하도록 설계.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('data/test_deeplearning.txt')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text = ''\n",
    "for i in docs :\n",
    "    text += i.page_content\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 컴퓨터 비전의 기본 개념\\n1.1 이미지 표현 방식',\n",
       " '이미지는 픽셀(Pixel) 값의 집합으로 표현되며, 다양한 방식으로 저장됩니다.',\n",
       " '픽셀(Pixel): 이미지의 최소 단위',\n",
       " '. 흑백 이미지는 0~255(단일 채널), 컬러 이미지는 RGB(3채널) 또는',\n",
       " '또는 RGBA(4채널)로 표현됨',\n",
       " '.',\n",
       " '채널(Channel): 이미지의 색상을 구성하는 정보',\n",
       " '. RGB(적, 녹, 청), CMYK(청록, 자홍, 노랑, 검정) 등.',\n",
       " '해상도(Resolution): 이미지의 크기(너비 × 높이)로 결정됨.',\n",
       " '1.2 이미지 처리 기초 개념',\n",
       " '히스토그램(Histogram): 픽셀 값의 분포를 나타내는 그래프.',\n",
       " '컨볼루션(Convolution): 필터(커널, Kernel)를 사용하여 이미지의 특정',\n",
       " '이미지의 특정 특징을 추출하는 연산',\n",
       " '. 예를 들면 엣지 검출(Edge Detection).',\n",
       " '필터(Filter): 이미지에서 특정 정보를 강조하거나 억제하는 행렬 연산',\n",
       " '. 대표적인 예로 가우시안 블러(Gaussian Blur)와 소벨 엣지 검출(Sobel',\n",
       " '검출(Sobel Edge Detection)이 있음',\n",
       " '.',\n",
       " '2. 딥러닝을 이용한 컴퓨터 비전 기술',\n",
       " '딥러닝을 활용한 컴퓨터 비전 기술은 데이터에서 패턴을 학습하고 예측하는데 최적화되어',\n",
       " '최적화되어 있습니다',\n",
       " '.',\n",
       " '2.1 합성곱 신경망(Convolutional Neural Networks, CNN)',\n",
       " 'CNN은 이미지 인식을 위한 가장 대표적인 딥러닝 모델입니다.',\n",
       " 'CNN의 주요 구조',\n",
       " '컨볼루션 층(Convolutional Layer): 엣지, 질감, 패턴 등의 특징을 추출',\n",
       " '.',\n",
       " '풀링 층(Pooling Layer): 공간적 크기를 줄이고, 중요한 특징을 보존 (Max',\n",
       " '보존 (Max Pooling, Average Pooling)',\n",
       " '.',\n",
       " '완전 연결 층(Fully Connected Layer, FC Layer): 최종적으로',\n",
       " '최종적으로 이미지 분류 등의 작업 수행',\n",
       " '.',\n",
       " 'CNN의 대표 모델',\n",
       " 'LeNet-5: 최초의 CNN 모델 중 하나로, 손글씨 숫자 인식을 위해 설계됨.',\n",
       " 'AlexNet: 깊은 신경망을 활용한 최초의 딥러닝 기반 이미지 분류 모델 (ILSVRC',\n",
       " '(ILSVRC 2012 우승)',\n",
       " '.',\n",
       " 'VGGNet: 더 깊은 네트워크 구조를 사용 (16~19층).',\n",
       " 'ResNet: Residual Connection을 도입하여 기울기',\n",
       " '도입하여 기울기 소실(Vanishing Gradient) 문제 해결',\n",
       " '.',\n",
       " 'EfficientNet: 연산량 대비 성능을 최적화한 경량화 모델.',\n",
       " '2.2 객체 탐지(Object Detection)',\n",
       " '이미지에서 특정 객체의 위치를 Bounding Box 형태로 감지하고, 어떤 클래스인지',\n",
       " '어떤 클래스인지 분류하는 기술',\n",
       " '.',\n",
       " '대표적인 객체 탐지 모델',\n",
       " 'R-CNN 계열: R-CNN → Fast R-CNN → Faster R-CNN →',\n",
       " 'R-CNN → Mask R-CNN (마스크 추가)',\n",
       " 'YOLO (You Only Look Once): 실시간 객체 탐지에 특화된 모델',\n",
       " '특화된 모델 (YOLOv1 ~ YOLOv8)',\n",
       " 'SSD (Single Shot MultiBox Detector): YOLO와 유사한',\n",
       " 'YOLO와 유사한 속도와 성능 제공',\n",
       " '.',\n",
       " '2.3 이미지 분할(Image Segmentation)',\n",
       " '각 픽셀이 어느 객체에 속하는지 분류하는 기술.',\n",
       " '이미지 분할 유형',\n",
       " 'Semantic Segmentation: 픽셀 단위로 객체 클래스를 분류 (예: 배경,',\n",
       " '(예: 배경, 도로, 사람 구분)',\n",
       " '.',\n",
       " 'Instance Segmentation: 같은 클래스 내에서도 개별 객체를 구분 (예:',\n",
       " '구분 (예: 여러 사람을 개별적으로 구별)',\n",
       " '.',\n",
       " 'Panoptic Segmentation: Semantic Segmentation과',\n",
       " 'Instance Segmentation을 결합하여 더욱 세밀한 분류 수행',\n",
       " '.',\n",
       " '대표적인 이미지 분할 모델',\n",
       " 'FCN (Fully Convolutional Network): CNN을 기반으로 한',\n",
       " '기반으로 한 최초의 이미지 분할 네트워크',\n",
       " '.',\n",
       " 'U-Net: 의료 영상 분석(CT, MRI)에서 널리 사용됨.',\n",
       " 'DeepLab (DeepLabV3, DeepLabV3+): 강력한 Semantic',\n",
       " 'Semantic Segmentation 성능 제공',\n",
       " '.',\n",
       " '2.4 이미지 생성 (Generative Models)',\n",
       " '딥러닝을 활용하여 새로운 이미지를 생성하는 기술.',\n",
       " '대표적인 이미지 생성 모델',\n",
       " 'GAN (Generative Adversarial Networks):',\n",
       " '생성자(Generator)와 판별자(Discriminator)가 서로 경쟁하여 점점 더',\n",
       " '경쟁하여 점점 더 사실적인 이미지를 생성',\n",
       " '.',\n",
       " '대표 모델: DCGAN, StyleGAN, BigGAN.',\n",
       " 'VAE (Variational Autoencoder): 확률 모델을 이용해 새로운 이미지',\n",
       " '새로운 이미지 생성',\n",
       " '.',\n",
       " 'Diffusion Model: 최근 대세인 생성 모델 (예: DALL·E 2,',\n",
       " 'DALL·E 2, Stable Diffusion)',\n",
       " '.',\n",
       " '2.5 스타일 변환(Style Transfer)',\n",
       " '이미지의 스타일을 다른 스타일로 변환하는 기술',\n",
       " '. 예를 들어, 고흐의 화풍을 적용한 이미지 생성이 가능함.',\n",
       " '대표 모델: Neural Style Transfer (NST).',\n",
       " '2.6 영상 처리(Video Analysis)',\n",
       " '영상 데이터에서 의미 있는 패턴을 추출하고 분석하는 기술.',\n",
       " '주요 기법\\nOptical Flow: 객체의 움직임을 추적.',\n",
       " 'Action Recognition: 행동 인식 (예: 스포츠 동작 분석).',\n",
       " 'Tracking Algorithms: 객체 추적 (예: SORT, DeepSORT).',\n",
       " '3. 컴퓨터 비전의 실제 응용\\n3.1 자율 주행(Autonomous Driving)',\n",
       " '객체 탐지(Object Detection): 차선 인식, 보행자 감지.',\n",
       " 'Semantic Segmentation: 도로, 차량, 신호등 등의 분류.',\n",
       " '3.2 의료 영상 분석(Medical Image Analysis)',\n",
       " 'X-ray, CT, MRI 분석: 암 진단, 병변 탐지.',\n",
       " 'U-Net 기반 세포 분할 모델: 의료 진단에 활용.',\n",
       " '3.3 얼굴 인식(Face Recognition)',\n",
       " 'FaceNet, DeepFace 등의 딥러닝 모델을 활용하여 얼굴을 인식하고 검출.',\n",
       " '3.4 스마트 공장 및 산업용 비전',\n",
       " '제품 결함 탐지(Defect Detection): 공장에서 제품 품질 검사.',\n",
       " '로봇 비전(Robot Vision): 자동화된 물체 인식 및 분류.\\n4. 최신 트렌드',\n",
       " '4. 최신 트렌드\\n4.1 Transformer 기반 모델',\n",
       " 'CNN을 넘어 Vision Transformer (ViT), DINO, Swin',\n",
       " 'Swin Transformer 등 새로운 구조가 등장',\n",
       " '.',\n",
       " '4.2 Diffusion 모델',\n",
       " '이미지 생성 모델의 새로운 대세로 떠오름 (Stable Diffusion, DALL·E',\n",
       " 'DALL·E 2)',\n",
       " '.',\n",
       " '4.3 MLOps & AI 모델 경량화',\n",
       " 'TensorRT, TFLite 등을 활용하여 모델을 최적화하고, 모바일 및 엣지',\n",
       " '모바일 및 엣지 디바이스에서도 사용 가능하도록 설계',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 10,\n",
    "    separators = ['\\n\\n', '\\n', '.', ' ']\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성\n",
    "embedding_model_name = \"jhgan/ko-sbert-nli\"  # 임베딩 모델 선택\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 벡터 스토어에 문서 추가\n",
    "test_vector_store = FAISS.from_texts(chunks, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특화된 모델 (YOLOv1 ~ YOLOv8)\n",
      "=========================================================\n",
      "DALL·E 2)\n",
      "=========================================================\n",
      "YOLO (You Only Look Once): 실시간 객체 탐지에 특화된 모델\n",
      "=========================================================\n",
      "Panoptic Segmentation: Semantic Segmentation과\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "retriever = test_vector_store.as_retriever()\n",
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"yolo에 대해 설명해주세요\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPT 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def convert_mp3_to_wav(mp3_path, wav_path):\n",
    "    \"\"\"MP3를 PowerPoint 호환성이 높은 WAV 포맷으로 변환\"\"\"\n",
    "    command = [\"ffmpeg\", \"-i\", mp3_path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"44100\", wav_path]\n",
    "    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "def add_audio_to_ppt(ppt_path, num_slides):\n",
    "    \"\"\"원본 PPT에 슬라이드별 발표 음성을 자동 삽입하고, 전환 시 자동 재생 설정\"\"\"\n",
    "    prs = Presentation(ppt_path)\n",
    "\n",
    "    for i in range(num_slides):\n",
    "        if i < len(prs.slides):\n",
    "            slide = prs.slides[i]\n",
    "            mp3_path = f\"speech_slide_{i+1}.mp3\"\n",
    "            wav_path = f\"speech_slide_{i+1}.wav\"\n",
    "\n",
    "            if os.path.exists(mp3_path):\n",
    "                # MP3 → WAV 변환\n",
    "                convert_mp3_to_wav(mp3_path, wav_path)\n",
    "\n",
    "                # 슬라이드에 오디오 삽입\n",
    "                shape = slide.shapes.add_shape(1, Inches(1), Inches(1), Inches(1), Inches(1))\n",
    "                shape.text = \"발표 음성\"\n",
    "\n",
    "                slide.shapes.add_movie(wav_path, left=Inches(1), top=Inches(1), width=Inches(1), height=Inches(1))\n",
    "\n",
    "                print(f\"Slide {i+1}에 오디오 추가 완료: {wav_path}\")\n",
    "            else:\n",
    "                print(f\"파일 없음: {mp3_path}\")\n",
    "\n",
    "    prs.save(\"final_presentation.pptx\")\n",
    "    print(\"최종 PPT 생성 완료: final_presentation.pptx\")\n",
    "\n",
    "# 실행 예제\n",
    "ppt_path = \"presentation.pptx\"\n",
    "num_slides = 10  # PPT의 총 슬라이드 수\n",
    "add_audio_to_ppt(ppt_path, num_slides)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
