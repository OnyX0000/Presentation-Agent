{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pdf2image import convert_from_path\n",
    "import fitz\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    extract_pdf_data(pdf_path): PDF에서 텍스트 및 이미지 추출\n",
    "    extract_ocr(images): 이미지에서 OCR을 사용하여 텍스트 추출\n",
    "    is_text_insufficient(text): 슬라이드의 정보 부족 여부 판단\n",
    "    compare_text_similarity(text1, text2): 유사도 분석 (TF-IDF 활용)\n",
    "    detect_tables(image_text): OCR 결과에서 표 감지\n",
    "    analyze_slide_for_data(slide_text): 데이터 관련 내용 포함 여부 확인\n",
    "    analyze_image_relevance(image_text, slide_text): OCR 결과와 슬라이드 본문 비교\n",
    "    generate_presentation_script(slides): 발표 대본 자동 생성\n",
    "    main(pdf_path): 전체 흐름 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_data(pdf_path):\n",
    "    '''PDF에서 텍스트와 이미지를 추출'''\n",
    "    docs = fitz.open(pdf_path)\n",
    "    page_data = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.get_text(\"text\").strip()\n",
    "        images = doc.get_images(full=True)\n",
    "\n",
    "        image_data = []\n",
    "        for img in images:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_data.append(image_bytes) \n",
    "\n",
    "        page_data.append({\"text\": text, \"images\": image_data})\n",
    "\n",
    "    return page_data\n",
    "\n",
    "def extract_ocr(images):\n",
    "    \"\"\"OCR을 활용하여 이미지에서 텍스트 추출\"\"\"\n",
    "    extracted_texts = []\n",
    "    \n",
    "    for image in images:\n",
    "        text = pytesseract.image_to_string(image, lang=\"kor+eng\")\n",
    "        extracted_texts.append(text.strip())\n",
    "    \n",
    "    return extracted_texts\n",
    "\n",
    "def is_text_insufficient(text):\n",
    "    \"\"\"텍스트 부족 여부 판단\"\"\"\n",
    "    # 기준 1: 텍스트 밀도 분석\n",
    "    if len(text.split()) < 3:\n",
    "        return True\n",
    "    \n",
    "    # model_name = \"\"  \n",
    "    # keywords = [] \n",
    "    # if not any(keyword in text for keyword in keywords):\n",
    "    #     return True\n",
    "    \n",
    "    # 기준 2: 문장구조를 분석하는 NLP모델을 사용하여 문장 구조 분석 \n",
    "    is_valid_sentence = True \n",
    "    if not is_valid_sentence:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def compare_text_similarity(text1, text2):\n",
    "    \"\"\"텍스트 유사도 분석\"\"\"\n",
    "    vectorizer = TfidfVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity = cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
    "    return similarity\n",
    "\n",
    "def detect_tables(image_text):\n",
    "    \"\"\"OCR 결과에서 표를 감지 (단순 행렬 구조 확인)\"\"\"\n",
    "    lines = image_text.split(\"\\n\")\n",
    "    table_like_lines = [line for line in lines if re.search(r'\\d+[\\s|,|\\t]+\\d+', line)] \n",
    "    return len(table_like_lines) > 2  # 일정 개수 이상의 행이 감지되면 표로 간주\n",
    "\n",
    "def analyze_slide_for_data(slide_text):\n",
    "    \"\"\"슬라이드에서 데이터 관련 용어가 포함되었는지 확인\"\"\"\n",
    "    data_keywords = [\"통계\", \"데이터\", \"비율\", \"퍼센트\", \"%\", \"결과\", \"수치\", \"값\", \"평균\", \"표\"]\n",
    "    return any(keyword in slide_text for keyword in data_keywords)\n",
    "\n",
    "def analyze_image_relevance(image_text, slide_text):\n",
    "    \"\"\"이미지 내 텍스트(OCR 결과)와 슬라이드 본문 비교\"\"\"\n",
    "    similarity = compare_text_similarity(image_text, slide_text)\n",
    "    return similarity >= 0.8  # 유사도가 80% 이상이면 관련 있음으로 판단\n",
    "\n",
    "def detect_image_caption(image, slide_text):\n",
    "    \"\"\"이미지 설명 문구(캡션) 감지 및 연관성 평가\"\"\"\n",
    "    pass\n",
    "\n",
    "def analyze_table_in_slide(image_text, slide_text):\n",
    "    \"\"\"표와 슬라이드 데이터 관련성 분석\"\"\"\n",
    "    if detect_tables(image_text) and analyze_slide_for_data(slide_text):\n",
    "        return True \n",
    "    return False\n",
    "\n",
    "def analyze_slide_text(text, next_slide_text):\n",
    "    \"\"\"현재 슬라이드의 내용이 부족할 경우 후속 슬라이드 참조\"\"\"\n",
    "    if is_text_insufficient(text):\n",
    "        return f\"현재 슬라이드의 정보가 부족하여 다음 슬라이드 내용을 참고합니다: {next_slide_text}\"\n",
    "    return text\n",
    "\n",
    "def generate_presentation_script(slides):\n",
    "    \"\"\"발표 대본 생성 (후속 슬라이드 참조)\"\"\"\n",
    "    pass\n",
    "\n",
    "def main(pdf_path):\n",
    "    \"\"\"전체 흐름 제어\"\"\"\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\wanted\\Lang\\lang\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\wanted\\Lang\\lang\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--google--t5-v1_1-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../../model/t5_model\\\\tokenizer_config.json',\n",
       " '../../model/t5_model\\\\special_tokens_map.json',\n",
       " '../../model/t5_model\\\\spiece.model',\n",
       " '../../model/t5_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"google/t5-v1_1-base\"  # 사용할 모델 선택\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 로컬에 저장\n",
    "model.save_pretrained(\"../../model/t5_model\")\n",
    "tokenizer.save_pretrained(\"../../model/t5_model\")\n",
    "# 더 가벼운거\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "# timeout=60 (기본 10초 → 60초로 증가)\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = Blip2Processor.from_pretrained(model_name, timeout=60)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# 로컬에 저장\n",
    "model.save_pretrained(\"../../model/blip2\")\n",
    "tokenizer.save_pretrained(\"../../model/blip2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP 모델이 ../../model/blip_model에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"  # 사용할 BLIP 모델 선택\n",
    "save_path = \"../../model/blip_model\"  # 저장할 폴더 지정\n",
    "\n",
    "# 모델 다운로드 및 로컬 저장\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "processor.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "print(f\"BLIP 모델이 {save_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import easyocr\n",
    "import paddleocr\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import io\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# 모델 경로를 JSON 파일에서 로드\n",
    "def load_model_paths(config_path=\"model_config.json\"):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    return config[\"t5_model_path\"], config[\"blip_model_path\"]\n",
    "\n",
    "# 사전 학습된 모델을 로컬에 저장 후 로드\n",
    "def load_t5_model(model_path):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    text_generation_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "    return tokenizer, model, text_generation_pipeline\n",
    "\n",
    "def load_blip_model(model_path):\n",
    "    processor = BlipProcessor.from_pretrained(model_path)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
    "    return processor, model\n",
    "\n",
    "# PDF에서 페이지별 텍스트 및 이미지 추출\n",
    "def extract_content_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_content = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        # 텍스트 추출\n",
    "        text = doc[page_num].get_text(\"text\").strip()\n",
    "        if not text:\n",
    "            text = \"[페이지에 텍스트 없음]\"\n",
    "        \n",
    "        # 이미지 추출\n",
    "        images = []\n",
    "        for img_index, img in enumerate(doc[page_num].get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            images.append(image)\n",
    "        \n",
    "        # 페이지별 텍스트 및 이미지 저장\n",
    "        pages_content.append({\"text\": text, \"images\": images})\n",
    "    \n",
    "    return pages_content\n",
    "\n",
    "# OCR을 사용하여 이미지에서 텍스트 추출\n",
    "def extract_text_from_image(image, use_easyocr=True):\n",
    "    image_np = np.array(image)\n",
    "    ocr_reader = easyocr.Reader(['en']) if use_easyocr else paddleocr.OCR()\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    if use_easyocr:\n",
    "        results = ocr_reader.readtext(image_np)\n",
    "        extracted_text = \" \".join([text[1] for text in results])\n",
    "    else:\n",
    "        results = ocr_reader.ocr(image_np)\n",
    "        extracted_text = \" \".join([text[1][0] for text in results])\n",
    "    \n",
    "    if not extracted_text.strip():\n",
    "        extracted_text = \"[OCR 인식 불가]\"\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "# BLIP-2를 사용하여 이미지 설명 생성 및 번역\n",
    "def generate_caption(image, blip_processor, blip_model):\n",
    "    image = image.convert(\"RGB\")\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "    caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    if caption.count(\"black background\") > 2:\n",
    "        caption = \"[이미지 설명 불가능]\"\n",
    "    else:\n",
    "        caption = GoogleTranslator(source='en', target='ko').translate(caption)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# 문장 유사도 판별\n",
    "def is_text_similar(text1, text2, threshold=0.7):\n",
    "    model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "    return similarity >= threshold, similarity\n",
    "\n",
    "# T5 모델을 LangChain을 사용하여 발표 대본 생성\n",
    "def generate_script(text, text_generation_pipeline):\n",
    "    if len(text.strip()) < 10:\n",
    "        return \"[대본 생성 불가: 입력 데이터 부족]\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"\n",
    "        당신은 전문 발표자입니다. 주어진 내용을 기반으로 청중이 이해하기 쉬운 발표 대본을 작성하세요.\n",
    "        다음은 슬라이드의 내용입니다:\n",
    "        {text}\n",
    "        \n",
    "        이에 대한 발표 대본을 생성하세요.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run(text=text)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 전체 발표 대본 생성 과정\n",
    "def generate_presentation_script(pdf_path, config_path=\"model_config.json\"):\n",
    "    t5_model_path, blip_model_path = load_model_paths(config_path)\n",
    "    pages_content = extract_content_from_pdf(pdf_path)\n",
    "    tokenizer, t5_model, text_generation_pipeline = load_t5_model(t5_model_path)\n",
    "    blip_processor, blip_model = load_blip_model(blip_model_path)\n",
    "    \n",
    "    scripts = []\n",
    "    \n",
    "    for idx, page in enumerate(pages_content):\n",
    "        text = page[\"text\"]\n",
    "        ocr_text = \"\"\n",
    "        caption_text = \"\"\n",
    "        \n",
    "        for image in page[\"images\"]:\n",
    "            ocr_text += extract_text_from_image(image) + \" \"\n",
    "            caption = generate_caption(image, blip_processor, blip_model)\n",
    "            is_similar, similarity = is_text_similar(text, caption)\n",
    "            if is_similar:\n",
    "                caption_text += caption + \" \"\n",
    "        \n",
    "        combined_text = f\"{text} {caption_text}\".strip()\n",
    "        script = generate_script(combined_text, text_generation_pipeline)\n",
    "        scripts.append(f\"슬라이드 {idx+1} 발표 대본: {script}\")\n",
    "    \n",
    "    return scripts\n",
    "\n",
    "# PDF 파일 경로\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "config_path = \"model_config.json\"\n",
    "\n",
    "# 발표 대본 생성 실행\n",
    "scripts = generate_presentation_script(pdf_path, config_path)\n",
    "\n",
    "# 결과 출력\n",
    "for i, script in enumerate(scripts):\n",
    "    print(f\"슬라이드 {i+1} 발표 대본:\\n{script}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import easyocr\n",
    "import paddleocr\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import io\n",
    "from deep_translator import GoogleTranslator\n",
    "from langchain.memory import ConversationBufferMemory    # 메모리 부여\n",
    "from langchain.chains import ConversationChain  # 메모리 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_content = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        # 텍스트 추출\n",
    "        text = doc[page_num].get_text(\"text\").strip()\n",
    "        if not text:\n",
    "            text = \"[페이지에 텍스트 없음]\"\n",
    "        \n",
    "        # 이미지 추출\n",
    "        images = []\n",
    "        for img_index, img in enumerate(doc[page_num].get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            images.append(image)\n",
    "        \n",
    "        # 페이지별 텍스트 및 이미지 저장\n",
    "        pages_content.append({\"text\": text, \"images\": images})\n",
    "    return pages_content\n",
    "\n",
    "# OCR을 사용하여 이미지에서 텍스트 추출\n",
    "def extract_text_from_image(image, use_easyocr=True):\n",
    "    image_np = np.array(image)\n",
    "    ocr_reader = easyocr.Reader(['en']) if use_easyocr else paddleocr.OCR()\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    if use_easyocr:\n",
    "        results = ocr_reader.readtext(image_np)\n",
    "        extracted_text = \" \".join([text[1] for text in results])\n",
    "    else:\n",
    "        results = ocr_reader.ocr(image_np)\n",
    "        extracted_text = \" \".join([text[1][0] for text in results])\n",
    "    \n",
    "    if not extracted_text.strip():\n",
    "        extracted_text = \"[OCR 인식 불가]\"\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "def generate_caption(image, blip_processor, blip_model):\n",
    "    image = image.convert(\"RGB\")\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "    caption_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    if caption.count(\"black background\") > 2:\n",
    "        caption = \"[이미지 설명 불가능]\"\n",
    "    else:\n",
    "        caption = GoogleTranslator(source='en', target='ko').translate(caption)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "def is_text_similar(text1, text2, threshold=0.7):\n",
    "    model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "    return similarity >= threshold, similarity\n",
    "\n",
    "def generate_script(text):\n",
    "\n",
    "    llm = ChatOllama(model='mistral:7b', temperature=0.4)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"\n",
    "        당신은 전문 발표자입니다. \n",
    "        주어진 내용을 기반으로 청중이 이해하기 쉬운 발표 대본을 작성하세요.\n",
    "        발표 대본은 문단이 하나입니다.\n",
    "        한글로만 말하세요.\n",
    "        다음은 슬라이드의 내용입니다:\n",
    "        {text}\n",
    "        \n",
    "        이에 대한 발표 대본을 생성하세요.\n",
    "        \"\"\"\n",
    "    )\n",
    "    memory = ConversationBufferMemory()\n",
    "\n",
    "    chain = ConversationChain()\n",
    "\n",
    "    result = chain.invoke({'text' : text})\n",
    "    return result\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "con_chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "ppt_content = extract_content_from_pdf(pdf_path)\n",
    "image = ppt_content[20]['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image[0].show()\n",
    "# len(image)\n",
    "for i in image:\n",
    "    i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page : 2 , text : HTP 검사란?\n",
      "주제 소개\n",
      "House(집), Tree(나무), Person(사람)의 세\n",
      "가지 그림을 그리게 하여 개인의 성격, 정서 상\n",
      "태, 대인 관계 등을 평가하는 투사적 심리 검사\n",
      "특히 아동 및 청소년의 심리 상태를 파악하거나,\n",
      "성인의 무의식적 감정과 스트레스를 이해하는\n",
      "데 유용하게 사용\n",
      "01. 프로젝트 개요\n",
      "OCR : HTP HTP, 해석 : 나무와 연필이있는 집의 그림\n",
      "ppt에 필요한 이미지인가? : (False, 0.41756924986839294)\n",
      "대본 \n",
      "  \"안녕하세요, 모두! 오늘은 우리는 'HTP 검사'라는 시각적인 심리적 평가 도구에 대해 알아보도록 합니다. 슬라이드에 나와 있듯이, 이 검사는 '집', '나무', '사람' 세 그림을 그리게 하여 개인의 성격, 정서 상태, 대인관계 등을 평가합니다. 특히 아동과 청소년의 심리 상황에 유용하며, 성인들이 무의식적 감정과 스트레스를 이해하는데 도움이 되기도 합니다. 오늘은 'HTP 검사' 프로젝트에 대한 개요를 알아보겠습니다. 감상하시고 이해하시면 됩니다! 지금부터!\"\n",
      "========================================================================================================================\n",
      "page : 3 , text : HTP 검사의 한계\n",
      "주제 선정 배경\n",
      "주관적 해석의 가능성\n",
      "표준화 부족\n",
      "문화적 차이\n",
      "피검사자의 의도적 왜곡\n",
      "기술적 한계\n",
      "01. 프로젝트 개요\n",
      "OCR : DRAWING TEST HTP ORCEZOCi gntON PERSON TREE HOUSE, 해석 : 그 위에 그림이있는 책상에 앉아있는 남자\n",
      "ppt에 필요한 이미지인가? : (False, 0.3413795232772827)\n",
      "대본 \n",
      "  Title Slide: Limitations of HTP Testing\n",
      "\n",
      "   Welcome everyone, today I will discuss the limitations of the Homeostatic Thyroid Prophyruron (HTP) test, a commonly used diagnostic tool in thyroid function assessment. Despite its widespread use, this test has several drawbacks that can affect its accuracy and reliability. These limitations include subjective interpretation, lack of standardization, cultural variations, intentional misinterpretation by the patient, and technical issues. It's crucial to understand these pitfalls to ensure proper diagnosis and effective treatment of thyroid disorders. Let's delve deeper into each of these points during our discussion. Thank you!\n",
      "========================================================================================================================\n",
      "page : 11 , text : 성능 개선\n",
      "02. Project Flow\n",
      "OCR : Mountain 0.97 House 0.961 Sun 0.97 Wall 0.97 Pond 098 Winadw Door 0.88 Fence 0.915 Fence 0.89 0D Tree 0.97 Flower 0.92, 해석 : 네 가지 색상의 집의 그림\n",
      "ppt에 필요한 이미지인가? : (False, 0.2807292938232422)\n",
      "대본 \n",
      " 제가 말하고자 하는 주제는 '성과 향상'입니다. 이 프로젝트의 흐름에 대해서 알아보겠습니다. 슬라이드를 통해 구분되어 있지만, 일련의 단계들이 서서히 개선된 결과를 가져올 수 있는 것을 보여드리고 있습니다. 각 단계에는 우리가 어떻게 개선하는지, 그리고 이로 인해 얻은 이점에 대해 설명할 것입니다. 이해가 되신가요? 계속해서 진행하겠습니다!\n",
      "========================================================================================================================\n",
      "page : 11 , text : 성능 개선\n",
      "02. Project Flow\n",
      "OCR : Mountoin 0961 House 0.96} Grass (Sun 0.951 Wall 0.96 Pond 097 Windl Door 0.93 Grass083 Fence_0.9 Fence 0.87 Fence 0.86 D Iree 0.96 Flowver 0.92, 해석 : 색상이 다른 집의 그림\n",
      "ppt에 필요한 이미지인가? : (False, 0.28918588161468506)\n",
      "대본 \n",
      " 안녕하세요! 오늘은 우리 프로젝트의 흐름과 함께 성능 개선에 대해 공유하게 되어 있습니다.\n",
      "\n",
      "이번 프로젝트는 성능을 개선하기 위한 절차를 설명합니다. 우리는 초기 분석부터 시작하여, 문제의 근본을 찾아내고 해결책을 선택함으로써 성능 개선에 나아갈 것입니다. 이러한 단계는 다음과 같습니다:\n",
      "\n",
      "1. 시스템 분석 - 현재 프로젝트의 성능을 분석하여 문제점을 식별합니다.\n",
      "2. 원인 파악 - 분석결과를 바탕으로 문제의 근본을 찾습니다.\n",
      "3. 해결책 선택 - 우리는 범위에 맞게 적절한 해결책을 선택합니다.\n",
      "4. 구현 - 선택된 해결책을 실제 시스템에 적용합니다.\n",
      "5. 테스트 - 구현 후 성능 개선의 효과를 검증하고, 필요에 따라 조정합니다.\n",
      "6. 리펙토링 - 효율적이지 못한 부분을 개선하여 더욱 좋은 성능을 유지합니다.\n",
      "\n",
      "우리는 이러한 단계를 통해 프로젝트의 성능을 개선시키고자 노력합니다. 예상되는 결과로는 시스템의 속도 향상, 메모리 효율 개선 등이 있습니다. 또한 우리는 프로젝트를 진행하면서 발생할 수 있는 문제에 대처하고 성공적인 프로젝트를 완료할 것입니다.\n",
      "\n",
      "많은 기대와 지내주시고, 협력해 주신 모든 분께 감사합니다!\n",
      "========================================================================================================================\n",
      "page : 12 , text : 특징 추출 및 해석\n",
      "결과 해석\n",
      "어떤 개체(클래스)가 그려졌는지\n",
      "몇 개나 그려졌는지\n",
      "전체 이미지 크기 대비 어느정도인지\n",
      "해당 개체가 오브젝트에서 어느정도 크기인지\n",
      "위 결과들을 해석하는 레퍼런스 탐색 및 구현\n",
      "02. Project Flow\n",
      "OCR : WonG MTID dcbioiogto Vtavn Norod HTP HTP HOUSE-TREE-PERSON ML Tre PLASCN DE: ILED ANALYSIS Wuotanetout Wnontdt lneurehadinmuciincgelauuuuidu MMn Jun Wiu Ee eoROOOdt\"e LOm;JIn[T - Pn AGR Oyonlo / Ovuaiml= OVER OVERVIEW DETAILED ANALYSIS DEATIED ANALYSIS 05d Wtzoliiiquin'4ig ooin {eiFmom Gixai MelsWu MLEn Itodato{ \"OcEn tt LaArt Iot con?t unduoroit {U4anaei 'AJhi cacimij Dafo'lase ma F YIMu Wic Ioie5W7tM D Ei7in Stneetai @nount OIacs anl DaA[1114 00n c 0177 uit n eh / DETAIED ANLYEIS ocmircquv Rio7 Ol 6; Cjii L _AahIIm Gu ceni puertuhte (C0 F CT GEFIFCIDW COHPATEL AKALYCMTS COPARATVE REPORT UEER FTTJBACK Mcoguo icrac, 해석 : 파란색과 흰색 그래픽이있는 화이트 보드\n",
      "ppt에 필요한 이미지인가? : (False, 0.4207049310207367)\n",
      "대본 \n",
      "  Today, we'll delve into the results of our Feature Extraction and Interpretation process. Here's what we discovered:\n",
      "\n",
      "   - We identified specific objects (classes) within the image.\n",
      "   - The number of these objects in the image was also determined.\n",
      "   - The proportion of these objects within the overall image size was noted.\n",
      "   - The size of each object relative to the entire object was calculated.\n",
      "\n",
      "Next, we interpreted these results using a reference for analysis and implementation. This brings us to our Project Flow:\n",
      "\n",
      "   - Step 1: Data Collection\n",
      "   - Step 2: Preprocessing\n",
      "   - Step 3: Feature Extraction and Interpretation\n",
      "   - Step 4: Result Interpretation\n",
      "   - Step 5: Reference Model Selection\n",
      "   - Step 6: Model Training\n",
      "   - Step 7: Evaluation and Optimization\n",
      "   - Step 8: Deployment\n",
      "\n",
      "Let's move forward with this exciting journey of understanding our data!\n",
      "========================================================================================================================\n",
      "page : 14 , text : 기대 효과\n",
      "객관적이고 일관된 분석 가능\n",
      "자동화된 이미지 해석을 통한 시간 단축 \n",
      "데이터 기반 해석 가능\n",
      "전문가와 보완적인 역할 기대\n",
      "03. 서비스 구현\n",
      "OCR : House-Tree-Person HTP House Person, 해석 : 집이있는 태블릿을 들고있는 사람\n",
      "ppt에 필요한 이미지인가? : (False, 0.13909068703651428)\n",
      "대본 \n",
      "  \"Today, I'd like to discuss the anticipated benefits of our new image analysis service:\n",
      "\n",
      "1. **Consistent and Objective Analysis**: Our service provides a consistent and objective approach to analyzing images, ensuring reliable results every time.\n",
      "\n",
      "2. **Time-Saving Automation**: By automating the image analysis process, we can significantly reduce the time required for this task, allowing you to focus on other important aspects of your work.\n",
      "\n",
      "3. **Data-Driven Insights**: The service offers data-based insights, enabling you to make informed decisions backed by robust data analysis.\n",
      "\n",
      "4. **Complementing Expertise**: While our service complements the expertise of professionals, it also empowers those without specialized training to derive meaningful insights from images.\n",
      "\n",
      "Lastly, let's transition to the topic of 'Service Implementation' on slide 03. Thank you.\"\n",
      "========================================================================================================================\n",
      "page : 16 , text : 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[페이지 진입 시 초기 화면]\n",
      "[테마 선택 → 이미지 업로드]\n",
      "OCR : app localhost 8501 88 Gmail Web DeepL 89: 41Aq _ chatGPT Perplexity Dataiku Sludy [QIE 914013_ Dxpg Deploy DeePrint AI HTP 3*xl8 4HIA 9254384352 4e4ot4ir, 4E4 NIT/ 92e4 7442r5 4elatxie:, 해석 : 코드 코드의 코드는 코드 코드에 표시됩니다.\n",
      "ppt에 필요한 이미지인가? : (False, 0.3170565962791443)\n",
      "대본 \n",
      " 안녕하세요! 오늘은 우리의 서비스 - [이름]에 대해 간단히 알아보도록 하겠습니다.\n",
      "\n",
      "이 프로젝트에는 '이미지 업로드 및 해석' 기능을 포함하고 있으며, 이번 슬라이드에서는 이 기능에 대해 알아보도록 합니다.\n",
      "\n",
      "먼저, 페이지를 진입한 시점에서는 초기 화면이 나타납니다. 그리고, 이미지 업로드와 관련된 옵션을 선택하시면 됩니다.\n",
      "\n",
      "다음으로, 테마를 선택할 수 있습니다. 이미지 업로드를 통해 이러한 테마에 맞게 해석된 결과가 나타납니다.\n",
      "\n",
      "여기서 끝입니다. 잠시후 다음 슬라이드로 진행하겠습니다. 감사합니다!\n",
      "========================================================================================================================\n",
      "page : 16 , text : 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[페이지 진입 시 초기 화면]\n",
      "[테마 선택 → 이미지 업로드]\n",
      "OCR : app localhost 8501 0 | 88 Gmail Web DeepL 89: 41Aq _ chatGPT Perplexity Dataiku Sludy [QIE 914013_ Dxpg Deploy DeePrint AI HTP 3*xl8 4HIA 4254784253 4e4o4i8, Jooitis 92544%e4ck 43842E(ipg/ jpeg / png) drop file here Browse files Limit 2OOMB per file JPG, PNG, JPEG oloitiz %2E*x18. Drag : and, 해석 : 브라우저의 스크린 샷 앱\n",
      "ppt에 필요한 이미지인가? : (False, 0.4359034299850464)\n",
      "대본 \n",
      " 안녕하세요! 오늘은 서비스 구현 섹션에 대해 알아보겠습니다. 첫 번째 단계로, 페이지를 진입할 때 초기 화면이 나타나는데요. 그리고, 이미지를 업로드하고 선택한 테마로 변환시킬 수 있습니다.\n",
      "\n",
      "이 프로세스를 통해 사용자는 원하는 이미지를 업로드하여 서비스의 테마를 바꿀 수 있습니다. 이렇게 되면 서비스에 개인적이고 자신감 있는 느낌을 주실 것입니다.\n",
      "\n",
      "이제, 테마를 선택하고 이미지를 업로드하여 서비스의 특별한 모습을 만들어 보세요!\n",
      "========================================================================================================================\n",
      "page : 17 , text : 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[탐지 모드 설정]\n",
      "[선택 보기 → 탐색 객체 선택]\n",
      "OCR : app localhost 8501 0 | 88 |  Gmail D Web DeepL 89: 41Aq _ chatGPT Perplexity Tool Dataiku Sludy [QIE 914013_ Dxpg Deploy Jooitis 92544%e4ck 43842E(ipg/ jpeg / png) drop file here Browse files Limit 2OOMB per file JPG, PNG, JPEG House_8_F_01855.jpg 87.3KB ojogtiz %2E%ME4ch 42801#42 322 4elolxia, 5547 22940/ NeideLct: 8xl 34 4AI 243440l82| Drag : and, 해석 : 웹의 스크린 샷 앱\n",
      "ppt에 필요한 이미지인가? : (False, 0.4087625741958618)\n",
      "대본 \n",
      " 안녕하세요! 오늘은 슬라이드의 내용인 \"서비스 구현\" 부분에 대해 설명합니다.\n",
      "\n",
      "이 시간에는 우리의 서비스가 어떻게 작동하는지 알아보겠습니다. 첫번째로, \"탐지 모드\"를 설정합니다. 그런 다음, \"선택 보기\"에서 \"객체 탐색\"을 선택해주세요.\n",
      "\n",
      "이렇게 하면, 시스템은 화면에 있는 이미지를 자동으로 탐지하고, 필요한 객체를 선택할 수 있습니다. 예를 들어, 인간, 캔디나 기타와 같은 특정 객체를 찾을 수 있습니다!\n",
      "\n",
      "이렇게 작업하면 사용자는 시스템으로부터 필요한 정보만 받아올 수 있고, 더 쉽게 작업을 처리할 수 있습니다.\n",
      "\n",
      "감사합니다! 이제 퀴즈를 통해 어떤 객체가 탐지되었는지 테스트해보세요!\n",
      "========================================================================================================================\n",
      "page : 17 , text : 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[탐지 모드 설정]\n",
      "[선택 보기 → 탐색 객체 선택]\n",
      "OCR : app localhost 8501 88 Gmail Web DeepL 89: 41Aq _ chatGPT Perplexity Tool Dataiku Sludy [QIE 914013_ Dxpg Deploy 47882E (jpg/ jpeg / png) and drop file here Browse files Limit 2OOMB per file JPG, PNG, JPEG House_8_F_01855.jpg 87.3KB olu[xiz 8259MELCt1 47301 #42 852 4e4ot4la 4el 47| E4e 858 4eal4la. Hutl * T8 * 44 * 4e4:83:[YNtI,*I3,34] 8i 34 9AI 243440l821 Drag :, 해석 : 텍스트가있는 스크린 샷 화면 '' '및' '\n",
      "ppt에 필요한 이미지인가? : (False, 0.37757670879364014)\n",
      "대본 \n",
      " 안녕하세요! 오늘 주제로는 서비스 구현에 대해 알아보도록 하겠습니다. 슬라이드 3은 이미지 업로드 및 해석 화면입니다. 이 화면에서 우리는 [탐지 모드 설정]을 할 수 있으며, 이 때 필요한 객체를 [선택 보기 → 탐색 객체 선택]하여 정확한 결과를 얻어낼 수 있습니다. 감상하실 내용이라면, 이미지 업로드와 해석이 일어나는 화면에 대해 자세히 알아보시리라는 의도입니다. 감상해 주셔서 감사합니다!\n",
      "========================================================================================================================\n",
      "page : 18 , text : 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[모두 보기 모드]\n",
      "OCR : app localhost 8501 0 | & 88 Gmail Web DeepL 89: 41Aq _ chatGPT Perplexity Tool Dataiku Sludy O (QIE 97/4013_ Dxpg oc AENA AEEnEDI 2541 229401 4145181844. 8437 %n1434 8218t21 4zarr 144qda 071 Juq430m9 81xji *IElc41R+17R4.0i8 R8.42484,8430,9848 14232394%4. 1n4844J 871  44Id44s 3r444 87 357181844. 144400 071 44biqxs 3r444 87 357181844. 8040 821 44biqxs 3r444 87 357181844. 182404 871 44biqxs 3r444 87 357181844. 24444 871 Jucachiningy 64015)1 TRUE ci= 88,4e8 348 1444238 eic1. 9121444 821 Juc4Smoke 6404)1 TRuE Oi8 88,428 848 144323# elc. 842440 *71 Juc4Fcree 2401474TRUE DI=44152, oidance,412U 24,4452t, 968844432544 94. 13042 021 Juc4fcree- 240147TRUE DIE 44452,Aoidance,42824,3452,26884443258 24. 787444 871 Juq4Pord 9240147TRUE 0I=88,428 87,3430844442584 914. 40l4u 021 Juqountain 640147FTRUE 0=44422, enaganor 42824,84528 140425# + 94. 143440 871 Juq44rr4 24447TAUE_OI244321,aoldance,428 24,34528 4443125# 214. Juq44rr4 244471 TAUE_OI244321,aoldance,423 84,34528 444325# 21474444 9u449*178*4+184424. 0183438 44u4244 + %c. 7 844e 57 44ultd 8 38440 071 357t8l844. 1u444d 7 ge/2n 44ultd 8 3r440 071 357t8l844. 4424571, 해석 : 사이트의 레이아웃을 보여주는 스크린 샷 화면\n",
      "ppt에 필요한 이미지인가? : (False, 0.46743836998939514)\n",
      "대본 \n",
      " 오늘은 \"이미지 업로드 및 해석\" 시스템의 구현 과정에 관심이 있는 분들을 위해 서비스 구현을 담담히 설명하도록 합니다.\n",
      "\n",
      "당연히 우리 프로젝트는 \"모두 보기 모드\"를 지원하여 이미지를 업로드한 결과 및 해석 결과를 쉽게 확인할 수 있습니다.\n",
      "\n",
      "이제, 어떤 이미지를 사용하고, 그 이미지의 무엇을 해석할지 결정하는 방법부터 시작합니다. 그리고 우리 시스템은 이미지 업로드, 해석, 결과 전달 등 단계적으로 진행됩니다.\n",
      "\n",
      "또한, 우리는 사용자들이 이러한 과정에서 발생할 수 있는 오류 및 문제를 최소화하기 위해 안정성과 신뢰성을 귀중시키고 있습니다.\n",
      "\n",
      "즉, 우리의 시스템은 이미지 업로드 및 해석 작업에 대한 간편하고 안정적인 환경을 제공합니다. 이를 활용하여 당신도 쉽게 이해할 수 있는 결과를 얻어볼 수 있습니다!\n",
      "\n",
      "따라서, 이제 업로드된 이미지의 해석 결과를 확인하고, 필요에 따라 사용자들이 추가적인 요구에 맞게 개선할 수 있습니다.\n",
      "\n",
      "감사합니다! 더 질문이 있으시면 언제든지 질문해 주세요.\n",
      "========================================================================================================================\n",
      "page : 19 , text : 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[선택 모드]\n",
      "OCR : app localhost 8501 0 | & 88 Gmail D Web DeepL 89: 41Aq _ chatGPT Perplexity Tool Dataiku Sludy [QIE 914013_ Dxpg oc 044388494484 Htlu < 78 * 44 * 25 * 271 * 48 4488*.(3ut,78,34,24,071,42] 04349n5444421821 Nzeer 74u\"44d 871 Juq4934991*171 *Ed 4844914.0= 88,42884,3432,9588 4403i25# $ %c. 14184140 071 44ulud4= 3r4144 871 35712l844. 444144 071 44ulud4= 3r4144 871 35712l844. 25444 071 Juc4chinincy 6401471TRue %=88,428 8748 4444838 glc. 01214144 071 Juc4Smoke 240471TruE 0=88,428848444325# 9l4. J4344437 744449r4 64447 TRUe ol84432,Aokancc,4ede444ae84uu:1942 744449r4 24447 TRUE_olz 4432L,4okdance,42384,34388 #4021048 214 74c44 9u449*178*4+184424. 0183438 44u4244 + %c. 48r571 AAM, 해석 : 사이트의 레이아웃을 보여주는 화면\n",
      "ppt에 필요한 이미지인가? : (False, 0.36234962940216064)\n",
      "대본 \n",
      " 안녕하세요! 오늘은 슬라이드 03을 소개해 드리게 됩니다. 우리의 서비스는 이미지를 업로드하고, 해당 이미지를 자동으로 해석하여 분석하는 기능을 제공합니다. 이 기능은 선택 모드에서만 사용할 수 있으며, 이 기능을 통해 쉽고 빠르게 이미지를 분석하고 활용할 수 있습니다. 자신의 데이터나 내부 시스템에서 이미지를 업로드하고, 우리 서비스가 해당 이미지를 분석하여 원하는 정보를 얻어볼 수 있습니다. 이제부터 더 자세한 설명과 예시를 들어 봅시다!\n",
      "========================================================================================================================\n",
      "page : 20 , text : 이미지 업로드 및  해석 화면\n",
      "03. 서비스 구현\n",
      "[전체 요약 보기]\n",
      "OCR : app #4g 48 42/8N %% D Smart Swtch 8 & 48I= localhost 8501 0 88 | Gmail D Web DeepL 89: 41A0_ chatGPT Perplexity Tool Dataiku Study [IQIE 914013_ Dxpg Deploy Avoidance,7lz?t 21l,34382 #glet 3022+%ct. J3014424119 2*172401tl?t %ct 0= 24 42#08305E+%ct.  #Oll cHet %71 ZcI ZCI e4th 4xiollx= 0 g201chel %7/357&E4ct. THcIoll cHet %z elth 4xiolx= 0 gE0ll cHet !71 357&e4c1: V Egtol clet %1 8x/34 21th 4xiolx= 0 gG01cHel 97 357824ct. Xtl 84571 Al Tltl 8934 Foiti [IoEnte2=7447114014g2\"ollzloaeyc: 7a484405LEHLIZ \" 8 #ul834923+acl\"ziz #%l9943713 48TIFTH4I9 3@ol cHol get2 482 0121 818/7414,33 9 442 #elole CI41lgtol %8244t + %e4c. 0= 583,*l3 =4,E= 42148017/1l2 04471330/ %e4c. 44FOt7}, #8le 8+4 9l4 371Xltlz} L8xle 4214 2430/44E4149372 4+8345 %e4c1., 해석 : 텍스트 '' '가있는 웹 페이지의 화면\n",
      "ppt에 필요한 이미지인가? : (False, 0.38148748874664307)\n",
      "대본 \n",
      "  \"Ladies and Gentlemen, today we are going to discuss the 'Service Implementation' phase of our project, which is highlighted in Slide 3. This stage involves the implementation of the image upload and interpretation feature that you saw on the previous slide. Our team has been working tirelessly to ensure that this feature not only works seamlessly but also delivers accurate results. We believe this addition will greatly enhance the overall user experience for our service, making it more intuitive and efficient. Let's take a moment to review the main points:\n",
      "\n",
      "1. Image Upload: The platform allows users to effortlessly upload images for interpretation.\n",
      "2. Interpretation: Our advanced machine learning algorithms analyze the uploaded image to provide accurate results.\n",
      "3. User-friendly Interface: The interface is designed to be intuitive and easy-to-use, ensuring a smooth user experience.\n",
      "\n",
      "We invite you to explore these features in more detail as we continue our presentation. Thank you!\"\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# texts\n",
    "processor = BlipProcessor.from_pretrained(\"../../model/blip_model\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"../../model/blip_model\")\n",
    "\n",
    "for page, content in enumerate(ppt_content):\n",
    "    for image in content['images']:\n",
    "        if extract_text_from_image(image) != \"[OCR 인식 불가]\":\n",
    "            print(f\"page : {page} , text : {content['text']}\\nOCR : {extract_text_from_image(image)}, 해석 : {generate_caption(image, processor, model)}\")\n",
    "            relation = is_text_similar(\n",
    "                content['text'],\n",
    "                generate_caption(image, processor, model) + extract_text_from_image(image)\n",
    "            )\n",
    "            print(f\"ppt에 필요한 이미지인가? : {relation}\")\n",
    "        print(F\"대본 \\n {generate_script(content['text']).content}\")\n",
    "        print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import easyocr\n",
    "import torch\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from PIL import Image\n",
    "\n",
    "# ✅ PDF 경로\n",
    "pdf_path = \"../../data/pdf/presentation_agent.pdf\"\n",
    "\n",
    "# ✅ OCR 모델 (EasyOCR)\n",
    "ocr_reader = easyocr.Reader([\"en\", \"ko\"])  # 한국어 + 영어 지원\n",
    "\n",
    "# ✅ BLIP-2 모델 (이미지 설명 AI)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 1번 GPU만 사용\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "\n",
    "# ✅ OpenCV를 활용하여 PIL 이미지를 NumPy 배열로 변환\n",
    "def image_to_numpy(image_pil):\n",
    "    \"\"\"PIL 이미지를 NumPy 배열로 변환 (EasyOCR 입력용)\"\"\"\n",
    "    image_cv = np.array(image_pil.convert(\"RGB\"))  # RGB 변환\n",
    "    image_gray = cv2.cvtColor(image_cv, cv2.COLOR_RGB2GRAY)  # ✅ 흑백 변환 (OCR 성능 향상)\n",
    "    return image_gray  # ✅ EasyOCR는 grayscale을 선호함\n",
    "\n",
    "# ✅ PDF 분석 및 페이지 분류 + 이미지 처리\n",
    "def extract_page(pdf_path):\n",
    "    loader = PDFPlumberLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    pages = []\n",
    "    total_pages = len(documents)\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, doc_page in enumerate(documents):\n",
    "            page = pdf.pages[page_num]\n",
    "            text = doc_page.page_content.strip() if doc_page.page_content else \"\"\n",
    "\n",
    "            # ✅ 이미지 처리\n",
    "            images = []\n",
    "            for img in page.images:\n",
    "                x0, y0, x1, y1 = img[\"x0\"], img[\"top\"], img[\"x1\"], img[\"bottom\"]\n",
    "                xyxy = x0, y0, x1, y1\n",
    "                images.append(xyxy)\n",
    "                # ✅ 이미지 크롭 후 OCR 적용\n",
    "                page_image = page.to_image()\n",
    "                full_img = page_image.annotated\n",
    "                img_crop = full_img.crop((x0, y0, x1, y1))\n",
    "                img_bytes = io.BytesIO()\n",
    "                img_crop.save(img_bytes, format=\"PNG\")\n",
    "                img_pil = Image.open(img_bytes)\n",
    "\n",
    "                # # ✅ OCR 적용 (EasyOCR)\n",
    "                # ocr_input = image_to_numpy(img_pil)  # OpenCV 변환 후 OCR 실행\n",
    "                # ocr_text = ocr_reader.readtext(ocr_input, detail=0)\n",
    "                # ocr_result = \" \".join(ocr_text) if ocr_text else \"No OCR text found.\"\n",
    "\n",
    "                # ✅ 이미지 설명 (BLIP-2)\n",
    "                img_pil = img_pil.convert(\"RGB\")  # BLIP-2에서 RGB 변환 필수\n",
    "                inputs = processor(img_pil, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    caption = model.generate(**inputs)\n",
    "                    blip_caption = processor.decode(caption[0], skip_special_tokens=True)\n",
    "\n",
    "                images.append({\n",
    "                    \"image_id\": len(images) + 1,\n",
    "                    \"ocr_text\": xyxy,\n",
    "                    \"blip_caption\": blip_caption\n",
    "                })\n",
    "\n",
    "            pages.append({\n",
    "                \"page\": page_num + 1,\n",
    "                \"text\": text,\n",
    "                \"images\": images\n",
    "            })\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "page_info = extract_page(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 1,\n",
       " 'text': 'Team D.P. Wanted PotenUp\\n3rd Project\\n딥러닝 기반 아동 미술 심리 진단\\nDeepPrint\\n김지민, 박형빈, 정재식\\n2025.03.05.',\n",
       " 'images': [(-422.643150389868,\n",
       "   -227.99541783018992,\n",
       "   1440.356771985132,\n",
       "   1013.254650451055),\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1863x1241>,\n",
       "  {'image_id': 3,\n",
       "   'ocr_text': (-422.643150389868,\n",
       "    -227.99541783018992,\n",
       "    1440.356771985132,\n",
       "    1013.254650451055),\n",
       "   'blip_caption': 'there is a blackboard with a drawing of a bird and a flower',\n",
       "   'image_pil': <PIL.Image.Image image mode=RGB size=1863x1241>}]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info[0]['images'][2]['image_pil'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
