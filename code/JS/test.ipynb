{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path, extract_images=True)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 정보 출력\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"Page {idx + 1}:\")\n",
    "    print(\"Text:\", doc.page_content)  # 페이지의 텍스트 출력\n",
    "    if \"images\" in doc.metadata:\n",
    "        print(\"Images:\", doc.metadata[\"images\"])  # 이미지 정보 출력\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# PDF 파일 로드 및 페이지 분할\n",
    "loader = PyPDFLoader(\"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\")  # PDF 파일 경로 설정\n",
    "pages = loader.load_and_split()  # 페이지별로 텍스트 추출\n",
    "\n",
    "# **2. 벡터 스토어 생성**\n",
    "vectorstore = Chroma.from_documents(pages, embedding=OpenAIEmbeddings())  # 임베딩 생성 및 저장\n",
    "retriever = vectorstore.as_retriever()  # 검색기 생성\n",
    "\n",
    "# **3. 검색된 문서 데이터를 문자열로 변환하는 함수**\n",
    "def format_context(docs):\n",
    "    \"\"\"검색된 문서 리스트를 하나의 문자열로 병합\"\"\"\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# **4. 프롬프트 템플릿 정의**\n",
    "template = '''다음 컨텍스트를 기반으로 발표 대본을 작성하세요:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# **5. LLM 모델 초기화**\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# **6. RAG 체인 구성**\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}  # 컨텍스트와 질문 연결\n",
    "    | prompt  # 프롬프트 적용\n",
    "    | model   # 언어 모델 실행\n",
    "    | StrOutputParser()  # 결과 파싱\n",
    ")\n",
    "\n",
    "# **7. 체인 실행**\n",
    "retrieved_docs = retriever.get_relevant_documents(\"이 문서를 기반으로 발표 대본을 작성해주세요.\")  # 문서 검색\n",
    "formatted_context = format_context(retrieved_docs)  # 검색된 문서를 문자열로 변환\n",
    "\n",
    "result = rag_chain.invoke({\n",
    "    \"context\": formatted_context,  # 문자열 형태의 context 전달\n",
    "    \"question\": \"페이지별로 발표 대본을 작성해주세요.\"\n",
    "})\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# **1. PDF 텍스트 추출**\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"  # 각 페이지의 텍스트를 추가\n",
    "    return text.strip()\n",
    "\n",
    "# PDF 파일 경로 설정\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# **2. 프롬프트 템플릿 정의**\n",
    "template = '''다음 PDF 내용을 기반으로 발표 대본을 작성하세요:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# **3. LLM 모델 초기화**\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# **4. 프롬프트 실행**\n",
    "question = \"페이지별로 발표 대본을 작성해주세요.\"\n",
    "formatted_prompt = prompt.format(context=pdf_text, question=question)\n",
    "\n",
    "result = model(formatted_prompt)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def time_convert(time_str):\n",
    "    minutes, seconds = map(int, time_str.split(':'))\n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "def download_audio(url, save_dir, clip_idx, start=None, end=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 확장자 포함된 yt-dlp 저장용 임시 파일 경로\n",
    "    temp_template = os.path.join(save_dir, f\"cheo_temp_{clip_idx}.%(ext)s\")\n",
    "    temp_wav = os.path.join(save_dir, f\"cheo_temp_{clip_idx}.wav\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': temp_template,\n",
    "        'quiet': True,\n",
    "        'force_ipv4': True,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "    # 자르고 저장\n",
    "    audio = AudioSegment.from_file(temp_wav, format=\"wav\")\n",
    "    if start and end:\n",
    "        start_ms = time_convert(start) * 1000\n",
    "        end_ms = time_convert(end) * 1000\n",
    "        audio = audio[start_ms:end_ms]\n",
    "\n",
    "    # 최종 파일명: cheo_1.wav, cheo_2.wav ...\n",
    "    final_path = os.path.join(save_dir, f\"winter_{clip_idx}.wav\")\n",
    "    audio.export(final_path, format=\"wav\")\n",
    "\n",
    "    os.remove(temp_wav)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=4vS0Bhe5zjA\"\n",
    "save_dir = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "audio_data = []\n",
    "for i in range(5):\n",
    "    start = f\"{12+i}:00\"\n",
    "    end = f\"{12+i+1}:00\"\n",
    "    saved = download_audio(url, save_dir, clip_idx=i+1, start=start, end=end)\n",
    "    audio_data.append(saved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zonos.model import Zonos\n",
    "\n",
    "print(\"Zonos import 성공\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\eSpeak NG\"\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\espeak.dll\"\n",
    "\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "backend = EspeakBackend(language='ko')\n",
    "print(backend.phonemize([\"야 이제 되냐 안 되냐\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "import torch._dynamo  # suppress warning if needed\n",
    "\n",
    "# ❗ phonemizer용 환경변수 설정 (espeak.dll 대응)\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\eSpeak NG\"\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\espeak.dll\"\n",
    "\n",
    "# ❗ torch compile 비활성화 (C++ 컴파일러 없이 실행)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# 경로 설정\n",
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "\n",
    "# 모델 불러오기\n",
    "device = \"cuda\"  # 또는 \"cpu\"\n",
    "model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n",
    "\n",
    "# 입력 음성 로드\n",
    "wav, sampling_rate = torchaudio.load(os.path.join(path, \"cheo_1.wav\"))\n",
    "\n",
    "# 스피커 임베딩 생성 (제로샷 보이스 클로닝)\n",
    "speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "text = \"\"\"\n",
    "현대 사회에서 *AI* 기술은 빠르게 발전하고 있습니다.\n",
    "우리의 '발표' 능력을 지원하는 시스템이 필요합니다.\n",
    "\"\"\"\n",
    "# 텍스트와 스피커로 조건 딕셔너리 구성\n",
    "cond_dict = make_cond_dict(text=text, speaker=speaker, language=\"ko\")\n",
    "conditioning = model.prepare_conditioning(cond_dict)\n",
    "\n",
    "# 음성 생성 (컴파일러 비활성화)\n",
    "codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "# 결과 저장\n",
    "torchaudio.save(os.path.join(path, \"sample.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "print(\"🔊 sample.wav 생성 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "import torch._dynamo  # suppress warning if needed\n",
    "\n",
    "def zero_shot(path,file,text,name):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n",
    "    wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "    speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker,\n",
    "        language = \"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # 결과 저장\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_zero_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"🔊 zero_shot.wav 생성 완료!\")\n",
    "    return codes\n",
    "\n",
    "def few_shot(path, data, text, name):\n",
    "    '''few-shot 보이스 클리닝'''\n",
    "    torch.cuda.empty_cache()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n",
    "\n",
    "    embeddings = []\n",
    "    for file in data:\n",
    "        wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "        emb = model.make_speaker_embedding(wav, sampling_rate)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    speaker_embedding = torch.stack(embeddings).mean(dim=0)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker_embedding,\n",
    "        language=\"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    # 음성 생성 (컴파일러 비활성화)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # 결과 저장\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_few_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"🔊 few_shot.wav 생성 완료!\")\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "file = \"winter_2.wav\"\n",
    "data = [f\"winter_{i}.wav\" for i in range(2,6)]\n",
    "name = \"winter\"\n",
    "text = \"\"\"\n",
    "현대 사회에서 *AI* 기술은 빠르게 발전하고 있습니다.\n",
    "우리의 '발표' 능력을 지원하는 시스템이 필요합니다.\n",
    "\"\"\"\n",
    "\n",
    "zero_shot(path,file, text,name)\n",
    "few_shot(path,data, text,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "synthesiser = pipeline(\"text-to-speech\", \"suno/bark\")\n",
    "\n",
    "speech = synthesiser(\"현대 사회에서 발표는 필수적인 활동입니다.\", forward_params={\"do_sample\": True})\n",
    "\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from IPython.display import Audio\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# download and load all models\n",
    "preload_models(\n",
    "    text_use_gpu=False,\n",
    "    text_use_small=True,\n",
    "    coarse_use_gpu=True,\n",
    "    fine_use_gpu=True,\n",
    "    codec_use_gpu=True\n",
    ")\n",
    "\n",
    "\n",
    "# generate audio from text\n",
    "text_prompt = \"\"\"현대 사회에서 발표는 필수적인 활동입니다.\n",
    "하지만 많은 사람들은 내성적인 성격, 무대 공포증, 실수에 대한 두려움 등으로 인해 어려움을 겪습니다.\n",
    "이러한 문제를 해결하기 위해, AI 기반 발표 지원 시스템인 \"저희 발표 안합니다!\" 프로젝트가 기획되었습니다.\"\"\"\n",
    "speech_array = generate_audio(text_prompt)\n",
    "\n",
    "# play text in notebook\n",
    "Audio(speech_array, rate=SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 경로 설정 (MeloTTS 코드 추가)\n",
    "import sys, os\n",
    "sys.path.append(\"C:/wanted/Lang/MeloTTS\")  # 너의 경로에 맞게 조정\n",
    "\n",
    "# 📌 PyTorch 확인\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.exists(\"C:/wanted/Lang/MeloTTS/melo/api.py\"))\n",
    "print(os.listdir(\"C:/wanted/Lang/MeloTTS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/wanted/Lang/MeloTTS/melo\")  # ← melo 폴더 자체를 직접 추가\n",
    "\n",
    "from api import TTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Jupyter에서 재생 (IPython 오디오 위젯)\n",
    "from IPython.display import Audio\n",
    "Audio(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melo.api import TTS\n",
    "\n",
    "# Speed is adjustable\n",
    "speed = 1.0\n",
    "device = 'cpu' # or cuda:0\n",
    "\n",
    "text = \"안녕하세요! 오늘은 날씨가 정말 좋네요.\"\n",
    "model = TTS(language='KR', device=device)\n",
    "speaker_ids = model.hps.data.spk2id\n",
    "\n",
    "output_path = 'kr.wav'\n",
    "model.tts_to_file(text, speaker_ids['KR'], output_path, speed=speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에요 여기 구글은 여기입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text():\n",
    "    \"\"\"Synthesizes speech from the input string of text.\"\"\"\n",
    "    from google.cloud import texttospeech\n",
    "\n",
    "    text = \"과연 성공인것인가 실패인 것인가 나는 누구인가 우린 어디로 가는 것인가 성공은 가능한 것인가가.\"\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # Note: the voice can also be specified by name.\n",
    "    # Names of voices can be retrieved with client.list_voices().\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ko-KR\"\n",
    "    )\n",
    "\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    response = client.synthesize_speech(\n",
    "        input=input_text,\n",
    "        voice=voice,\n",
    "        audio_config=audio_config,\n",
    "    )\n",
    "\n",
    "    # The response's audio_content is binary.\n",
    "    with open(\"output.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print('Audio content written to file \"output.mp3\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to file \"output.mp3\"\n"
     ]
    }
   ],
   "source": [
    "synthesize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
