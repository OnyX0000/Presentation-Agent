{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path, extract_images=True)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 정보 출력\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"Page {idx + 1}:\")\n",
    "    print(\"Text:\", doc.page_content)  # 페이지의 텍스트 출력\n",
    "    if \"images\" in doc.metadata:\n",
    "        print(\"Images:\", doc.metadata[\"images\"])  # 이미지 정보 출력\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# PDF 파일 로드 및 페이지 분할\n",
    "loader = PyPDFLoader(\"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\")  # PDF 파일 경로 설정\n",
    "pages = loader.load_and_split()  # 페이지별로 텍스트 추출\n",
    "\n",
    "# **2. 벡터 스토어 생성**\n",
    "vectorstore = Chroma.from_documents(pages, embedding=OpenAIEmbeddings())  # 임베딩 생성 및 저장\n",
    "retriever = vectorstore.as_retriever()  # 검색기 생성\n",
    "\n",
    "# **3. 검색된 문서 데이터를 문자열로 변환하는 함수**\n",
    "def format_context(docs):\n",
    "    \"\"\"검색된 문서 리스트를 하나의 문자열로 병합\"\"\"\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# **4. 프롬프트 템플릿 정의**\n",
    "template = '''다음 컨텍스트를 기반으로 발표 대본을 작성하세요:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# **5. LLM 모델 초기화**\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# **6. RAG 체인 구성**\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}  # 컨텍스트와 질문 연결\n",
    "    | prompt  # 프롬프트 적용\n",
    "    | model   # 언어 모델 실행\n",
    "    | StrOutputParser()  # 결과 파싱\n",
    ")\n",
    "\n",
    "# **7. 체인 실행**\n",
    "retrieved_docs = retriever.get_relevant_documents(\"이 문서를 기반으로 발표 대본을 작성해주세요.\")  # 문서 검색\n",
    "formatted_context = format_context(retrieved_docs)  # 검색된 문서를 문자열로 변환\n",
    "\n",
    "result = rag_chain.invoke({\n",
    "    \"context\": formatted_context,  # 문자열 형태의 context 전달\n",
    "    \"question\": \"페이지별로 발표 대본을 작성해주세요.\"\n",
    "})\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# **1. PDF 텍스트 추출**\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"  # 각 페이지의 텍스트를 추가\n",
    "    return text.strip()\n",
    "\n",
    "# PDF 파일 경로 설정\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# **2. 프롬프트 템플릿 정의**\n",
    "template = '''다음 PDF 내용을 기반으로 발표 대본을 작성하세요:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# **3. LLM 모델 초기화**\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# **4. 프롬프트 실행**\n",
    "question = \"페이지별로 발표 대본을 작성해주세요.\"\n",
    "formatted_prompt = prompt.format(context=pdf_text, question=question)\n",
    "\n",
    "result = model(formatted_prompt)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def time_convert(time_str):\n",
    "    minutes = time_str // 100  \n",
    "    seconds = time_str % 100   \n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "def download_audio(url, save_dir, clip_idx, start=None, end=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 확장자 포함된 yt-dlp 저장용 임시 파일 경로\n",
    "    temp_template = os.path.join(save_dir, f\"karina_temp_{clip_idx}.%(ext)s\")\n",
    "    temp_wav = os.path.join(save_dir, f\"karina_temp_{clip_idx}.wav\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': temp_template,\n",
    "        'quiet': True,\n",
    "        'force_ipv4': True,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "    # 자르고 저장\n",
    "    audio = AudioSegment.from_file(temp_wav, format=\"wav\")\n",
    "    if start and end:\n",
    "        start_ms = time_convert(start) * 1000\n",
    "        end_ms = time_convert(end) * 1000\n",
    "        audio = audio[start_ms:end_ms]\n",
    "\n",
    "    # 최종 파일명: cheo_1.wav, cheo_2.wav ...\n",
    "    final_path = os.path.join(save_dir, f\"karina_{clip_idx}.wav\")\n",
    "    audio.export(final_path, format=\"wav\")\n",
    "\n",
    "    os.remove(temp_wav)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=QXlm7RXDnMI&t=11s\"\n",
    "save_dir = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "audio_data = []\n",
    "start = 0\n",
    "end = 20\n",
    "i = 1 \n",
    "\n",
    "for time in range(100,4000,1000):\n",
    "    start += time\n",
    "    end += time\n",
    "    audio = download_audio(\n",
    "        url = url,\n",
    "        save_dir=save_dir,\n",
    "        clip_idx=i,\n",
    "        start=start,\n",
    "        end=end\n",
    "    )\n",
    "    i += 1\n",
    "    audio_data.append(audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zonos.model import Zonos\n",
    "\n",
    "print(\"Zonos import 성공\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zonos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\wanted\\Lang\\lang\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "import torch._dynamo  # suppress warning if needed\n",
    "\n",
    "# ❗ phonemizer용 환경변수 설정 (espeak.dll 대응)\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\eSpeak NG\"\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\espeak.dll\"\n",
    "\n",
    "# ❗ torch compile 비활성화 (C++ 컴파일러 없이 실행)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# 경로 설정\n",
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "\n",
    "# 모델 불러오기\n",
    "device = \"cuda\"  # 또는 \"cpu\"\n",
    "model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "\n",
    "# 모델 불러오기\n",
    "device = 'cuda' if torch.cuda.is_available() else \n",
    "model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  33%|███▎      | 855/2588 [00:23<00:48, 35.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔊 sample.wav 생성 완료!\n",
      "⚡ 1.3배 빠르게 + 🎵 피치 +2반음 적용 완료!\n",
      "📁 저장 위치: C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\\fast_pitchup.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# 입력 음성 로드\n",
    "wav, sampling_rate = torchaudio.load(os.path.join(path, \"karina_1.wav\"))\n",
    "\n",
    "# 스피커 임베딩 생성\n",
    "speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "\n",
    "text = \"\"\"\n",
    "현대 사회에서 *AI* 기술은 빠르게 발전하고 있습니다.\n",
    "우리의 '발표' 능력을 지원하는 시스템이 필요합니다.\n",
    "\"\"\"\n",
    "\n",
    "cond_dict = make_cond_dict(text=text, speaker=speaker, language=\"ko\")\n",
    "conditioning = model.prepare_conditioning(cond_dict)\n",
    "\n",
    "codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "# 원본 저장\n",
    "torchaudio.save(os.path.join(path, \"sample.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "print(\"🔊 sample.wav 생성 완료!\")\n",
    "\n",
    "# # 후처리\n",
    "# sr = model.autoencoder.sampling_rate\n",
    "# audio_np = wavs[0].squeeze().numpy()  # [1, N] → [N] 보장\n",
    "\n",
    "# # 1.3배 빠르게\n",
    "# y_fast = librosa.effects.time_stretch(audio_np, rate=1.3)\n",
    "\n",
    "# # +2 반음\n",
    "# y_shifted = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=2)\n",
    "\n",
    "# # shape 조정\n",
    "# if y_shifted.ndim == 1:\n",
    "#     y_shifted = np.expand_dims(y_shifted, axis=0)  # → [1, samples]\n",
    "\n",
    "# # numpy → torch\n",
    "# y_tensor = torch.from_numpy(y_shifted).float()  # float32로 맞춤\n",
    "\n",
    "# # 저장\n",
    "# output_path = os.path.join(path, \"fast_pitchup.wav\")\n",
    "# torchaudio.save(output_path, y_tensor, sr)\n",
    "\n",
    "# print(\"⚡ 1.3배 빠르게 + 🎵 피치 +2반음 적용 완료!\")\n",
    "# print(f\"📁 저장 위치: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최종 y_tensor.shape: torch.Size([1, 333588])\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ 최종 y_tensor.shape:\", y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_shot(path,file,text,name, model):\n",
    "    torch.cuda.empty_cache()\n",
    "    wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "    speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker,\n",
    "        language = \"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # 결과 저장\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_zero_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"🔊 zero_shot.wav 생성 완료!\")\n",
    "    sr = model.autoencoder.sampling_rate\n",
    "    audio_np = wavs[0].numpy()\n",
    "\n",
    "    # [1] 1.3배 속도 (tempo)\n",
    "    y_fast = librosa.effects.time_stretch(audio_np, rate=1.3)\n",
    "\n",
    "    # [2] +2 반음 pitch up\n",
    "    y_shifted = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=2)\n",
    "\n",
    "    # 저장\n",
    "    final_out_path = os.path.join(path, f\"{name}_fast_pitchup.wav\")\n",
    "    sf.write(final_out_path, y_shifted, sr)\n",
    "\n",
    "\n",
    "def few_shot(path, data, text, name, model):\n",
    "    '''few-shot 보이스 클리닝'''\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    embeddings = []\n",
    "    for file in data:\n",
    "        wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "        emb = model.make_speaker_embedding(wav, sampling_rate)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    speaker_embedding = torch.stack(embeddings).mean(dim=0)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker_embedding,\n",
    "        language=\"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    # 음성 생성 (컴파일러 비활성화)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # 결과 저장\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_few_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"🔊 few_shot.wav 생성 완료!\")\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "file = \"karina_2.wav\"\n",
    "data = [f\"karina_{i}.wav\" for i in range(1,5)]\n",
    "name = \"karina2\"\n",
    "text = \"\"\"\n",
    "이러한 문제를 해결하기 위해, AI 기반 발표 지원 시스템인 \"저희 발표 안합니다!\" 프로젝트가 기획되었습니다.\n",
    "\"\"\"\n",
    "\n",
    "zero_shot(path,file, text,name, model)\n",
    "# few_shot(path,data, text,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "synthesiser = pipeline(\"text-to-speech\", \"suno/bark\")\n",
    "\n",
    "speech = synthesiser(\"현대 사회에서 발표는 필수적인 활동입니다.\", forward_params={\"do_sample\": True})\n",
    "\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from IPython.display import Audio\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# download and load all models\n",
    "preload_models(\n",
    "    text_use_gpu=False,\n",
    "    text_use_small=True,\n",
    "    coarse_use_gpu=True,\n",
    "    fine_use_gpu=True,\n",
    "    codec_use_gpu=True\n",
    ")\n",
    "\n",
    "\n",
    "# generate audio from text\n",
    "text_prompt = \"\"\"현대 사회에서 발표는 필수적인 활동입니다.\n",
    "하지만 많은 사람들은 내성적인 성격, 무대 공포증, 실수에 대한 두려움 등으로 인해 어려움을 겪습니다.\n",
    "이러한 문제를 해결하기 위해, AI 기반 발표 지원 시스템인 \"저희 발표 안합니다!\" 프로젝트가 기획되었습니다.\"\"\"\n",
    "speech_array = generate_audio(text_prompt)\n",
    "\n",
    "# play text in notebook\n",
    "Audio(speech_array, rate=SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 경로 설정 (MeloTTS 코드 추가)\n",
    "import sys, os\n",
    "sys.path.append(\"C:/wanted/Lang/MeloTTS\")  # 너의 경로에 맞게 조정\n",
    "\n",
    "# 📌 PyTorch 확인\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.exists(\"C:/wanted/Lang/MeloTTS/melo/api.py\"))\n",
    "print(os.listdir(\"C:/wanted/Lang/MeloTTS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/wanted/Lang/MeloTTS/melo\")  # ← melo 폴더 자체를 직접 추가\n",
    "\n",
    "from api import TTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Jupyter에서 재생 (IPython 오디오 위젯)\n",
    "from IPython.display import Audio\n",
    "Audio(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melo.api import TTS\n",
    "\n",
    "# Speed is adjustable\n",
    "speed = 1.0\n",
    "device = 'cpu' # or cuda:0\n",
    "\n",
    "text = \"안녕하세요! 오늘은 날씨가 정말 좋네요.\"\n",
    "model = TTS(language='KR', device=device)\n",
    "speaker_ids = model.hps.data.spk2id\n",
    "\n",
    "output_path = 'kr.wav'\n",
    "model.tts_to_file(text, speaker_ids['KR'], output_path, speed=speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "def ssml_to_audio(ssml_text: str) -> None:\n",
    "    \"\"\"\n",
    "    SSML(Speech Synthesis Markup Language)을 기반으로 음성을 생성하여 MP3 파일로 저장하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        ssml_text (str): SSML 형식의 문자열\n",
    "    \"\"\"\n",
    "\n",
    "    # Text-to-Speech 클라이언트 객체 생성\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    # SSML 텍스트를 음성 합성 입력으로 설정\n",
    "    synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)\n",
    "\n",
    "    # 사용할 목소리 설정\n",
    "    # 언어는 영어(\"en-US\"), 성별은 남성(MALE)으로 지정\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ko-KR\",  # 영어 음성\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.MALE  # 남성 목소리\n",
    "    )\n",
    "\n",
    "    # 출력할 오디오 형식 설정: MP3 파일\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    # 음성 합성 요청 수행\n",
    "    response = client.synthesize_speech(\n",
    "        input=synthesis_input,   # SSML 기반 입력\n",
    "        voice=voice,             # 목소리 설정\n",
    "        audio_config=audio_config  # 오디오 설정\n",
    "    )\n",
    "\n",
    "    # 합성된 오디오 데이터를 MP3 파일로 저장\n",
    "    with open(\"../../data/train_wav/test5_example.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print(\"Audio content written to file \" + \"test_example.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to file test_example.mp3\n"
     ]
    }
   ],
   "source": [
    "test1 = \"\"\"<speak> 안녕하세요오 만나서 반가워요오 나는 정재식입니다</speak>\"\"\"\n",
    "test2 = \"\"\"<speak> 발표하는 모델 <say-as interpret-as=\"characters\">AI</say-as>오 인 용 입니다 만나서 반가워요</speak>\"\"\"\n",
    "test3 = \"\"\"<speak> 오늘 날씨 <mark name=\"오늘 날씨\"/> 더럽게 <mark name=\"춥네요\"/> 춥네요!!!!</speak>\"\"\"\n",
    "test4 = \"\"\"<speak> \n",
    "누구보다 빠르게 남들과는 다르게 색다르게 <prosody rate=\"fast\" pitch=\"+2st\">누구보다 빠르게 남들과는 다르게 색다르게</prosody>\n",
    "<prosody rate=\"fast\" pitch=\"+4st\">누구보다 빠르게 남들과는 다르게 색다르게</prosody>\n",
    "<prosody rate=\"fast\" pitch=\"+6st\">누구보다 빠르게 남들과는 다르게 색다르게</prosody>\n",
    "<prosody rate=\"fast\" pitch=\"+8st\">누구보다 빠르게 남들과는 다르게 색다르게</prosody>\n",
    "</speak>\"\"\"\n",
    "test5 = \"\"\"<speak> \n",
    "<emphasis level=\"strong\">내가 그린 기린 그림은 잘 그린 기린 그림이고 니가 그린 기린 그림은 못 그린 기림 그림이다</emphasis>\n",
    "</speak>\"\"\"\n",
    "ssml_to_audio(test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text(text):\n",
    "    \"\"\"입력된 문자열을 음성으로 합성하여 MP3 파일로 저장하는 함수입니다.\"\"\"\n",
    "    \n",
    "    # Google Cloud의 Text-to-Speech 클라이언트 라이브러리를 import\n",
    "    from google.cloud import texttospeech\n",
    "    # Text-to-Speech API 클라이언트 객체 생성\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    # 합성할 텍스트를 API 입력 형식에 맞게 변환\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # 사용할 음성 설정 (언어 코드만 지정하면 기본 한국어 음성이 사용됨)\n",
    "    # 보다 세부적으로 설정하려면 voice name을 지정할 수도 있음\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ko-KR\",  # 한국어(Korean) 설정\n",
    "        name = \"ko-KR-Chirp3-HD-Leda\"\n",
    "    )\n",
    "\n",
    "    # 오디오 출력 형식 설정 (여기서는 MP3로 설정)\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    # 실제로 음성 합성을 요청하는 부분\n",
    "    response = client.synthesize_speech(\n",
    "        input=input_text,        # 입력 텍스트\n",
    "        voice=voice,             # 음성 설정\n",
    "        audio_config=audio_config  # 오디오 포맷 설정\n",
    "    )\n",
    "\n",
    "    # 응답의 오디오 콘텐츠(audio_content)는 바이너리 형식으로 전달됨\n",
    "    # MP3 파일로 저장\n",
    "    with open(\"../../data/train_wav/output.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print('Audio content written to file \"output.mp3\"')  # 완료 메시지 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to file \"output.mp3\"\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "이전 슬라이드에서는 \"발표 안 합니다!\" 서비스 구현에 사용된 핵심 기술 스택 – FastAPI, Streamlit, LangChain, Ollama, Hugging Face, ChromaDB, OpenAI – 에 대해 자세히 알아보았습니다. 이러한 최첨단 기술들이 어떻게 조화롭게 작동하여 강력한 발표 자동화 시스템을 구축하는지 기억하시죠? 이번 슬라이드에서는 저희 프로젝트의 미래, 즉 \"발표 안 합니다!\" 서비스의 발전 방향에 대해 이야기해보겠습니다.  프로젝트 방향성 03 슬라이드에서 확인할 수 있듯이, 저희는 단순히 현재에 안주하지 않고 끊임없이 발전하는 서비스를 제공하고자 합니다.\n",
    "현재 \"발표 안 합니다!\"는  발표 자료를 기반으로 대본을 생성하고 음성 합성을 통해 발표를 진행하는 핵심 기능을 제공합니다. 하지만 저희는 여기서 멈추지 않고, 사용자에게 더욱 자연스럽고 풍부한 발표 경험을 제공하기 위해 몇 가지 핵심 기능들을 추가할 계획입니다.  실시간 상호작용 기능을 통해 청중의 질문에 즉각적으로 답변하고,  마치 사람처럼 자연스럽게 말하는 기능을 구현하여 발표의 몰입도를 높일 것입니다. 또한, 사용자별 맞춤 발표 스타일 적용 기능을 통해 개인의 발표 스타일에 맞춘 최적화된 대본을 제공하고, 디지털 아바타를 활용한 발표 기능을 추가하여 시각적인 요소까지 더욱 풍부하게 만들어갈 예정입니다.\n",
    "더 나아가, 현재는 일반적인 발표 자료에 초점을 맞추고 있지만,  향후에는 특정 도메인에 특화된 발표 대본 생성 기능을 추가하여 다양한 분야에서 활용될 수 있도록 확장할 계획입니다.  이러한 발전 방향을 통해 \"발표 안 합니다!\"는 단순한 발표 도구를 넘어,  누구나 쉽고 효과적으로 자신의 아이디어를 전달하고 소통할 수 있도록 돕는  혁신적인 플랫폼으로 성장할 것입니다.\n",
    "\"\"\"\n",
    "synthesize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud ml speech voices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "ssml_text = \"\"\"\n",
    "<speak>\n",
    "    오늘은 <emphasis level=\"strong\">중요한 발표</emphasis>가 있습니다.\n",
    "    <break time=\"500ms\"/> 모두 집중해 주세요.\n",
    "</speak>\n",
    "\"\"\"\n",
    "\n",
    "synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)\n",
    "\n",
    "voice = texttospeech.VoiceSelectionParams(\n",
    "    language_code=\"ko-KR\",\n",
    "    name=\"ko-KR-Wavenet-B\"\n",
    ")\n",
    "\n",
    "audio_config = texttospeech.AudioConfig(\n",
    "    audio_encoding=texttospeech.AudioEncoding.MP3\n",
    ")\n",
    "\n",
    "response = client.synthesize_speech(\n",
    "    input=synthesis_input,\n",
    "    voice=voice,\n",
    "    audio_config=audio_config\n",
    ")\n",
    "\n",
    "with open(\"output.mp3\", \"wb\") as out:\n",
    "    out.write(response.audio_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
