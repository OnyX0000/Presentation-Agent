{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path, extract_images=True)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# ë¬¸ì„œ ì •ë³´ ì¶œë ¥\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"Page {idx + 1}:\")\n",
    "    print(\"Text:\", doc.page_content)  # í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶œë ¥\n",
    "    if \"images\" in doc.metadata:\n",
    "        print(\"Images:\", doc.metadata[\"images\"])  # ì´ë¯¸ì§€ ì •ë³´ ì¶œë ¥\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# PDF íŒŒì¼ ë¡œë“œ ë° í˜ì´ì§€ ë¶„í• \n",
    "loader = PyPDFLoader(\"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\")  # PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "pages = loader.load_and_split()  # í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "\n",
    "# **2. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±**\n",
    "vectorstore = Chroma.from_documents(pages, embedding=OpenAIEmbeddings())  # ì„ë² ë”© ìƒì„± ë° ì €ì¥\n",
    "retriever = vectorstore.as_retriever()  # ê²€ìƒ‰ê¸° ìƒì„±\n",
    "\n",
    "# **3. ê²€ìƒ‰ëœ ë¬¸ì„œ ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜**\n",
    "def format_context(docs):\n",
    "    \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³‘í•©\"\"\"\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# **4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜**\n",
    "template = '''ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•˜ì„¸ìš”:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# **5. LLM ëª¨ë¸ ì´ˆê¸°í™”**\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# **6. RAG ì²´ì¸ êµ¬ì„±**\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}  # ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ ì—°ê²°\n",
    "    | prompt  # í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "    | model   # ì–¸ì–´ ëª¨ë¸ ì‹¤í–‰\n",
    "    | StrOutputParser()  # ê²°ê³¼ íŒŒì‹±\n",
    ")\n",
    "\n",
    "# **7. ì²´ì¸ ì‹¤í–‰**\n",
    "retrieved_docs = retriever.get_relevant_documents(\"ì´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\")  # ë¬¸ì„œ ê²€ìƒ‰\n",
    "formatted_context = format_context(retrieved_docs)  # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "result = rag_chain.invoke({\n",
    "    \"context\": formatted_context,  # ë¬¸ìì—´ í˜•íƒœì˜ context ì „ë‹¬\n",
    "    \"question\": \"í˜ì´ì§€ë³„ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "})\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# **1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ**\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"  # ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€\n",
    "    return text.strip()\n",
    "\n",
    "# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# **2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜**\n",
    "template = '''ë‹¤ìŒ PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•˜ì„¸ìš”:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# **3. LLM ëª¨ë¸ ì´ˆê¸°í™”**\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# **4. í”„ë¡¬í”„íŠ¸ ì‹¤í–‰**\n",
    "question = \"í˜ì´ì§€ë³„ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "formatted_prompt = prompt.format(context=pdf_text, question=question)\n",
    "\n",
    "result = model(formatted_prompt)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def time_convert(time_str):\n",
    "    minutes = time_str // 100  \n",
    "    seconds = time_str % 100   \n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "def download_audio(url, save_dir, clip_idx, start=None, end=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # í™•ì¥ì í¬í•¨ëœ yt-dlp ì €ì¥ìš© ì„ì‹œ íŒŒì¼ ê²½ë¡œ\n",
    "    temp_template = os.path.join(save_dir, f\"karina_temp_{clip_idx}.%(ext)s\")\n",
    "    temp_wav = os.path.join(save_dir, f\"karina_temp_{clip_idx}.wav\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': temp_template,\n",
    "        'quiet': True,\n",
    "        'force_ipv4': True,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "    # ìë¥´ê³  ì €ì¥\n",
    "    audio = AudioSegment.from_file(temp_wav, format=\"wav\")\n",
    "    if start and end:\n",
    "        start_ms = time_convert(start) * 1000\n",
    "        end_ms = time_convert(end) * 1000\n",
    "        audio = audio[start_ms:end_ms]\n",
    "\n",
    "    # ìµœì¢… íŒŒì¼ëª…: cheo_1.wav, cheo_2.wav ...\n",
    "    final_path = os.path.join(save_dir, f\"karina_{clip_idx}.wav\")\n",
    "    audio.export(final_path, format=\"wav\")\n",
    "\n",
    "    os.remove(temp_wav)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=QXlm7RXDnMI&t=11s\"\n",
    "save_dir = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "audio_data = []\n",
    "start = 0\n",
    "end = 20\n",
    "i = 1 \n",
    "\n",
    "for time in range(100,4000,1000):\n",
    "    start += time\n",
    "    end += time\n",
    "    audio = download_audio(\n",
    "        url = url,\n",
    "        save_dir=save_dir,\n",
    "        clip_idx=i,\n",
    "        start=start,\n",
    "        end=end\n",
    "    )\n",
    "    i += 1\n",
    "    audio_data.append(audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zonos.model import Zonos\n",
    "\n",
    "print(\"Zonos import ì„±ê³µ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zonos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\wanted\\Lang\\lang\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "import torch._dynamo  # suppress warning if needed\n",
    "\n",
    "# â— phonemizerìš© í™˜ê²½ë³€ìˆ˜ ì„¤ì • (espeak.dll ëŒ€ì‘)\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\eSpeak NG\"\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\espeak.dll\"\n",
    "\n",
    "# â— torch compile ë¹„í™œì„±í™” (C++ ì»´íŒŒì¼ëŸ¬ ì—†ì´ ì‹¤í–‰)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "device = \"cuda\"  # ë˜ëŠ” \"cpu\"\n",
    "model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²½ë¡œ ì„¤ì •\n",
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "device = 'cuda' if torch.cuda.is_available() else \n",
    "model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  33%|â–ˆâ–ˆâ–ˆâ–      | 855/2588 [00:23<00:48, 35.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Š sample.wav ìƒì„± ì™„ë£Œ!\n",
      "âš¡ 1.3ë°° ë¹ ë¥´ê²Œ + ğŸµ í”¼ì¹˜ +2ë°˜ìŒ ì ìš© ì™„ë£Œ!\n",
      "ğŸ“ ì €ì¥ ìœ„ì¹˜: C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\\fast_pitchup.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# ì…ë ¥ ìŒì„± ë¡œë“œ\n",
    "wav, sampling_rate = torchaudio.load(os.path.join(path, \"karina_1.wav\"))\n",
    "\n",
    "# ìŠ¤í”¼ì»¤ ì„ë² ë”© ìƒì„±\n",
    "speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "\n",
    "text = \"\"\"\n",
    "í˜„ëŒ€ ì‚¬íšŒì—ì„œ *AI* ê¸°ìˆ ì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "ìš°ë¦¬ì˜ 'ë°œí‘œ' ëŠ¥ë ¥ì„ ì§€ì›í•˜ëŠ” ì‹œìŠ¤í…œì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "cond_dict = make_cond_dict(text=text, speaker=speaker, language=\"ko\")\n",
    "conditioning = model.prepare_conditioning(cond_dict)\n",
    "\n",
    "codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "# ì›ë³¸ ì €ì¥\n",
    "torchaudio.save(os.path.join(path, \"sample.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "print(\"ğŸ”Š sample.wav ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "# # í›„ì²˜ë¦¬\n",
    "# sr = model.autoencoder.sampling_rate\n",
    "# audio_np = wavs[0].squeeze().numpy()  # [1, N] â†’ [N] ë³´ì¥\n",
    "\n",
    "# # 1.3ë°° ë¹ ë¥´ê²Œ\n",
    "# y_fast = librosa.effects.time_stretch(audio_np, rate=1.3)\n",
    "\n",
    "# # +2 ë°˜ìŒ\n",
    "# y_shifted = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=2)\n",
    "\n",
    "# # shape ì¡°ì •\n",
    "# if y_shifted.ndim == 1:\n",
    "#     y_shifted = np.expand_dims(y_shifted, axis=0)  # â†’ [1, samples]\n",
    "\n",
    "# # numpy â†’ torch\n",
    "# y_tensor = torch.from_numpy(y_shifted).float()  # float32ë¡œ ë§ì¶¤\n",
    "\n",
    "# # ì €ì¥\n",
    "# output_path = os.path.join(path, \"fast_pitchup.wav\")\n",
    "# torchaudio.save(output_path, y_tensor, sr)\n",
    "\n",
    "# print(\"âš¡ 1.3ë°° ë¹ ë¥´ê²Œ + ğŸµ í”¼ì¹˜ +2ë°˜ìŒ ì ìš© ì™„ë£Œ!\")\n",
    "# print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì¢… y_tensor.shape: torch.Size([1, 333588])\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… ìµœì¢… y_tensor.shape:\", y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_shot(path,file,text,name, model):\n",
    "    torch.cuda.empty_cache()\n",
    "    wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "    speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker,\n",
    "        language = \"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_zero_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"ğŸ”Š zero_shot.wav ìƒì„± ì™„ë£Œ!\")\n",
    "    sr = model.autoencoder.sampling_rate\n",
    "    audio_np = wavs[0].numpy()\n",
    "\n",
    "    # [1] 1.3ë°° ì†ë„ (tempo)\n",
    "    y_fast = librosa.effects.time_stretch(audio_np, rate=1.3)\n",
    "\n",
    "    # [2] +2 ë°˜ìŒ pitch up\n",
    "    y_shifted = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=2)\n",
    "\n",
    "    # ì €ì¥\n",
    "    final_out_path = os.path.join(path, f\"{name}_fast_pitchup.wav\")\n",
    "    sf.write(final_out_path, y_shifted, sr)\n",
    "\n",
    "\n",
    "def few_shot(path, data, text, name, model):\n",
    "    '''few-shot ë³´ì´ìŠ¤ í´ë¦¬ë‹'''\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    embeddings = []\n",
    "    for file in data:\n",
    "        wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "        emb = model.make_speaker_embedding(wav, sampling_rate)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    speaker_embedding = torch.stack(embeddings).mean(dim=0)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker_embedding,\n",
    "        language=\"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    # ìŒì„± ìƒì„± (ì»´íŒŒì¼ëŸ¬ ë¹„í™œì„±í™”)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_few_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"ğŸ”Š few_shot.wav ìƒì„± ì™„ë£Œ!\")\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "file = \"karina_2.wav\"\n",
    "data = [f\"karina_{i}.wav\" for i in range(1,5)]\n",
    "name = \"karina2\"\n",
    "text = \"\"\"\n",
    "ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, AI ê¸°ë°˜ ë°œí‘œ ì§€ì› ì‹œìŠ¤í…œì¸ \"ì €í¬ ë°œí‘œ ì•ˆí•©ë‹ˆë‹¤!\" í”„ë¡œì íŠ¸ê°€ ê¸°íšë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "zero_shot(path,file, text,name, model)\n",
    "# few_shot(path,data, text,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "synthesiser = pipeline(\"text-to-speech\", \"suno/bark\")\n",
    "\n",
    "speech = synthesiser(\"í˜„ëŒ€ ì‚¬íšŒì—ì„œ ë°œí‘œëŠ” í•„ìˆ˜ì ì¸ í™œë™ì…ë‹ˆë‹¤.\", forward_params={\"do_sample\": True})\n",
    "\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from IPython.display import Audio\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# download and load all models\n",
    "preload_models(\n",
    "    text_use_gpu=False,\n",
    "    text_use_small=True,\n",
    "    coarse_use_gpu=True,\n",
    "    fine_use_gpu=True,\n",
    "    codec_use_gpu=True\n",
    ")\n",
    "\n",
    "\n",
    "# generate audio from text\n",
    "text_prompt = \"\"\"í˜„ëŒ€ ì‚¬íšŒì—ì„œ ë°œí‘œëŠ” í•„ìˆ˜ì ì¸ í™œë™ì…ë‹ˆë‹¤.\n",
    "í•˜ì§€ë§Œ ë§ì€ ì‚¬ëŒë“¤ì€ ë‚´ì„±ì ì¸ ì„±ê²©, ë¬´ëŒ€ ê³µí¬ì¦, ì‹¤ìˆ˜ì— ëŒ€í•œ ë‘ë ¤ì›€ ë“±ìœ¼ë¡œ ì¸í•´ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.\n",
    "ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, AI ê¸°ë°˜ ë°œí‘œ ì§€ì› ì‹œìŠ¤í…œì¸ \"ì €í¬ ë°œí‘œ ì•ˆí•©ë‹ˆë‹¤!\" í”„ë¡œì íŠ¸ê°€ ê¸°íšë˜ì—ˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "speech_array = generate_audio(text_prompt)\n",
    "\n",
    "# play text in notebook\n",
    "Audio(speech_array, rate=SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ ê²½ë¡œ ì„¤ì • (MeloTTS ì½”ë“œ ì¶”ê°€)\n",
    "import sys, os\n",
    "sys.path.append(\"C:/wanted/Lang/MeloTTS\")  # ë„ˆì˜ ê²½ë¡œì— ë§ê²Œ ì¡°ì •\n",
    "\n",
    "# ğŸ“Œ PyTorch í™•ì¸\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âœ… Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.exists(\"C:/wanted/Lang/MeloTTS/melo/api.py\"))\n",
    "print(os.listdir(\"C:/wanted/Lang/MeloTTS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/wanted/Lang/MeloTTS/melo\")  # â† melo í´ë” ìì²´ë¥¼ ì§ì ‘ ì¶”ê°€\n",
    "\n",
    "from api import TTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ Jupyterì—ì„œ ì¬ìƒ (IPython ì˜¤ë””ì˜¤ ìœ„ì ¯)\n",
    "from IPython.display import Audio\n",
    "Audio(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melo.api import TTS\n",
    "\n",
    "# Speed is adjustable\n",
    "speed = 1.0\n",
    "device = 'cpu' # or cuda:0\n",
    "\n",
    "text = \"ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.\"\n",
    "model = TTS(language='KR', device=device)\n",
    "speaker_ids = model.hps.data.spk2id\n",
    "\n",
    "output_path = 'kr.wav'\n",
    "model.tts_to_file(text, speaker_ids['KR'], output_path, speed=speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "def ssml_to_audio(ssml_text: str) -> None:\n",
    "    \"\"\"\n",
    "    SSML(Speech Synthesis Markup Language)ì„ ê¸°ë°˜ìœ¼ë¡œ ìŒì„±ì„ ìƒì„±í•˜ì—¬ MP3 íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        ssml_text (str): SSML í˜•ì‹ì˜ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "\n",
    "    # Text-to-Speech í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„±\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    # SSML í…ìŠ¤íŠ¸ë¥¼ ìŒì„± í•©ì„± ì…ë ¥ìœ¼ë¡œ ì„¤ì •\n",
    "    synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)\n",
    "\n",
    "    # ì‚¬ìš©í•  ëª©ì†Œë¦¬ ì„¤ì •\n",
    "    # ì–¸ì–´ëŠ” ì˜ì–´(\"en-US\"), ì„±ë³„ì€ ë‚¨ì„±(MALE)ìœ¼ë¡œ ì§€ì •\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ko-KR\",  # ì˜ì–´ ìŒì„±\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.MALE  # ë‚¨ì„± ëª©ì†Œë¦¬\n",
    "    )\n",
    "\n",
    "    # ì¶œë ¥í•  ì˜¤ë””ì˜¤ í˜•ì‹ ì„¤ì •: MP3 íŒŒì¼\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    # ìŒì„± í•©ì„± ìš”ì²­ ìˆ˜í–‰\n",
    "    response = client.synthesize_speech(\n",
    "        input=synthesis_input,   # SSML ê¸°ë°˜ ì…ë ¥\n",
    "        voice=voice,             # ëª©ì†Œë¦¬ ì„¤ì •\n",
    "        audio_config=audio_config  # ì˜¤ë””ì˜¤ ì„¤ì •\n",
    "    )\n",
    "\n",
    "    # í•©ì„±ëœ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ MP3 íŒŒì¼ë¡œ ì €ì¥\n",
    "    with open(\"../../data/train_wav/test5_example.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print(\"Audio content written to file \" + \"test_example.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to file test_example.mp3\n"
     ]
    }
   ],
   "source": [
    "test1 = \"\"\"<speak> ì•ˆë…•í•˜ì„¸ìš”ì˜¤ ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”ì˜¤ ë‚˜ëŠ” ì •ì¬ì‹ì…ë‹ˆë‹¤</speak>\"\"\"\n",
    "test2 = \"\"\"<speak> ë°œí‘œí•˜ëŠ” ëª¨ë¸ <say-as interpret-as=\"characters\">AI</say-as>ì˜¤ ì¸ ìš© ì…ë‹ˆë‹¤ ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”</speak>\"\"\"\n",
    "test3 = \"\"\"<speak> ì˜¤ëŠ˜ ë‚ ì”¨ <mark name=\"ì˜¤ëŠ˜ ë‚ ì”¨\"/> ë”ëŸ½ê²Œ <mark name=\"ì¶¥ë„¤ìš”\"/> ì¶¥ë„¤ìš”!!!!</speak>\"\"\"\n",
    "test4 = \"\"\"<speak> \n",
    "ëˆ„êµ¬ë³´ë‹¤ ë¹ ë¥´ê²Œ ë‚¨ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ ìƒ‰ë‹¤ë¥´ê²Œ <prosody rate=\"fast\" pitch=\"+2st\">ëˆ„êµ¬ë³´ë‹¤ ë¹ ë¥´ê²Œ ë‚¨ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ ìƒ‰ë‹¤ë¥´ê²Œ</prosody>\n",
    "<prosody rate=\"fast\" pitch=\"+4st\">ëˆ„êµ¬ë³´ë‹¤ ë¹ ë¥´ê²Œ ë‚¨ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ ìƒ‰ë‹¤ë¥´ê²Œ</prosody>\n",
    "<prosody rate=\"fast\" pitch=\"+6st\">ëˆ„êµ¬ë³´ë‹¤ ë¹ ë¥´ê²Œ ë‚¨ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ ìƒ‰ë‹¤ë¥´ê²Œ</prosody>\n",
    "<prosody rate=\"fast\" pitch=\"+8st\">ëˆ„êµ¬ë³´ë‹¤ ë¹ ë¥´ê²Œ ë‚¨ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ ìƒ‰ë‹¤ë¥´ê²Œ</prosody>\n",
    "</speak>\"\"\"\n",
    "test5 = \"\"\"<speak> \n",
    "<emphasis level=\"strong\">ë‚´ê°€ ê·¸ë¦° ê¸°ë¦° ê·¸ë¦¼ì€ ì˜ ê·¸ë¦° ê¸°ë¦° ê·¸ë¦¼ì´ê³  ë‹ˆê°€ ê·¸ë¦° ê¸°ë¦° ê·¸ë¦¼ì€ ëª» ê·¸ë¦° ê¸°ë¦¼ ê·¸ë¦¼ì´ë‹¤</emphasis>\n",
    "</speak>\"\"\"\n",
    "ssml_to_audio(test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text(text):\n",
    "    \"\"\"ì…ë ¥ëœ ë¬¸ìì—´ì„ ìŒì„±ìœ¼ë¡œ í•©ì„±í•˜ì—¬ MP3 íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # Google Cloudì˜ Text-to-Speech í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ import\n",
    "    from google.cloud import texttospeech\n",
    "    # Text-to-Speech API í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„±\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    # í•©ì„±í•  í…ìŠ¤íŠ¸ë¥¼ API ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # ì‚¬ìš©í•  ìŒì„± ì„¤ì • (ì–¸ì–´ ì½”ë“œë§Œ ì§€ì •í•˜ë©´ ê¸°ë³¸ í•œêµ­ì–´ ìŒì„±ì´ ì‚¬ìš©ë¨)\n",
    "    # ë³´ë‹¤ ì„¸ë¶€ì ìœ¼ë¡œ ì„¤ì •í•˜ë ¤ë©´ voice nameì„ ì§€ì •í•  ìˆ˜ë„ ìˆìŒ\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ko-KR\",  # í•œêµ­ì–´(Korean) ì„¤ì •\n",
    "        name = \"ko-KR-Chirp3-HD-Leda\"\n",
    "    )\n",
    "\n",
    "    # ì˜¤ë””ì˜¤ ì¶œë ¥ í˜•ì‹ ì„¤ì • (ì—¬ê¸°ì„œëŠ” MP3ë¡œ ì„¤ì •)\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    # ì‹¤ì œë¡œ ìŒì„± í•©ì„±ì„ ìš”ì²­í•˜ëŠ” ë¶€ë¶„\n",
    "    response = client.synthesize_speech(\n",
    "        input=input_text,        # ì…ë ¥ í…ìŠ¤íŠ¸\n",
    "        voice=voice,             # ìŒì„± ì„¤ì •\n",
    "        audio_config=audio_config  # ì˜¤ë””ì˜¤ í¬ë§· ì„¤ì •\n",
    "    )\n",
    "\n",
    "    # ì‘ë‹µì˜ ì˜¤ë””ì˜¤ ì½˜í…ì¸ (audio_content)ëŠ” ë°”ì´ë„ˆë¦¬ í˜•ì‹ìœ¼ë¡œ ì „ë‹¬ë¨\n",
    "    # MP3 íŒŒì¼ë¡œ ì €ì¥\n",
    "    with open(\"../../data/train_wav/output.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print('Audio content written to file \"output.mp3\"')  # ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to file \"output.mp3\"\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "ì´ì „ ìŠ¬ë¼ì´ë“œì—ì„œëŠ” \"ë°œí‘œ ì•ˆ í•©ë‹ˆë‹¤!\" ì„œë¹„ìŠ¤ êµ¬í˜„ì— ì‚¬ìš©ëœ í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ â€“ FastAPI, Streamlit, LangChain, Ollama, Hugging Face, ChromaDB, OpenAI â€“ ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ìµœì²¨ë‹¨ ê¸°ìˆ ë“¤ì´ ì–´ë–»ê²Œ ì¡°í™”ë¡­ê²Œ ì‘ë™í•˜ì—¬ ê°•ë ¥í•œ ë°œí‘œ ìë™í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ”ì§€ ê¸°ì–µí•˜ì‹œì£ ? ì´ë²ˆ ìŠ¬ë¼ì´ë“œì—ì„œëŠ” ì €í¬ í”„ë¡œì íŠ¸ì˜ ë¯¸ë˜, ì¦‰ \"ë°œí‘œ ì•ˆ í•©ë‹ˆë‹¤!\" ì„œë¹„ìŠ¤ì˜ ë°œì „ ë°©í–¥ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³´ê² ìŠµë‹ˆë‹¤.  í”„ë¡œì íŠ¸ ë°©í–¥ì„± 03 ìŠ¬ë¼ì´ë“œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´, ì €í¬ëŠ” ë‹¨ìˆœíˆ í˜„ì¬ì— ì•ˆì£¼í•˜ì§€ ì•Šê³  ëŠì„ì—†ì´ ë°œì „í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³ ì í•©ë‹ˆë‹¤.\n",
    "í˜„ì¬ \"ë°œí‘œ ì•ˆ í•©ë‹ˆë‹¤!\"ëŠ”  ë°œí‘œ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ë³¸ì„ ìƒì„±í•˜ê³  ìŒì„± í•©ì„±ì„ í†µí•´ ë°œí‘œë¥¼ ì§„í–‰í•˜ëŠ” í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì €í¬ëŠ” ì—¬ê¸°ì„œ ë©ˆì¶”ì§€ ì•Šê³ , ì‚¬ìš©ìì—ê²Œ ë”ìš± ìì—°ìŠ¤ëŸ½ê³  í’ë¶€í•œ ë°œí‘œ ê²½í—˜ì„ ì œê³µí•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ ì¶”ê°€í•  ê³„íšì…ë‹ˆë‹¤.  ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš© ê¸°ëŠ¥ì„ í†µí•´ ì²­ì¤‘ì˜ ì§ˆë¬¸ì— ì¦‰ê°ì ìœ¼ë¡œ ë‹µë³€í•˜ê³ ,  ë§ˆì¹˜ ì‚¬ëŒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ì—¬ ë°œí‘œì˜ ëª°ì…ë„ë¥¼ ë†’ì¼ ê²ƒì…ë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìë³„ ë§ì¶¤ ë°œí‘œ ìŠ¤íƒ€ì¼ ì ìš© ê¸°ëŠ¥ì„ í†µí•´ ê°œì¸ì˜ ë°œí‘œ ìŠ¤íƒ€ì¼ì— ë§ì¶˜ ìµœì í™”ëœ ëŒ€ë³¸ì„ ì œê³µí•˜ê³ , ë””ì§€í„¸ ì•„ë°”íƒ€ë¥¼ í™œìš©í•œ ë°œí‘œ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ì‹œê°ì ì¸ ìš”ì†Œê¹Œì§€ ë”ìš± í’ë¶€í•˜ê²Œ ë§Œë“¤ì–´ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "ë” ë‚˜ì•„ê°€, í˜„ì¬ëŠ” ì¼ë°˜ì ì¸ ë°œí‘œ ìë£Œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆì§€ë§Œ,  í–¥í›„ì—ëŠ” íŠ¹ì • ë„ë©”ì¸ì— íŠ¹í™”ëœ ë°œí‘œ ëŒ€ë³¸ ìƒì„± ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë  ìˆ˜ ìˆë„ë¡ í™•ì¥í•  ê³„íšì…ë‹ˆë‹¤.  ì´ëŸ¬í•œ ë°œì „ ë°©í–¥ì„ í†µí•´ \"ë°œí‘œ ì•ˆ í•©ë‹ˆë‹¤!\"ëŠ” ë‹¨ìˆœí•œ ë°œí‘œ ë„êµ¬ë¥¼ ë„˜ì–´,  ëˆ„êµ¬ë‚˜ ì‰½ê³  íš¨ê³¼ì ìœ¼ë¡œ ìì‹ ì˜ ì•„ì´ë””ì–´ë¥¼ ì „ë‹¬í•˜ê³  ì†Œí†µí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”  í˜ì‹ ì ì¸ í”Œë«í¼ìœ¼ë¡œ ì„±ì¥í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "synthesize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud ml speech voices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "ssml_text = \"\"\"\n",
    "<speak>\n",
    "    ì˜¤ëŠ˜ì€ <emphasis level=\"strong\">ì¤‘ìš”í•œ ë°œí‘œ</emphasis>ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "    <break time=\"500ms\"/> ëª¨ë‘ ì§‘ì¤‘í•´ ì£¼ì„¸ìš”.\n",
    "</speak>\n",
    "\"\"\"\n",
    "\n",
    "synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)\n",
    "\n",
    "voice = texttospeech.VoiceSelectionParams(\n",
    "    language_code=\"ko-KR\",\n",
    "    name=\"ko-KR-Wavenet-B\"\n",
    ")\n",
    "\n",
    "audio_config = texttospeech.AudioConfig(\n",
    "    audio_encoding=texttospeech.AudioEncoding.MP3\n",
    ")\n",
    "\n",
    "response = client.synthesize_speech(\n",
    "    input=synthesis_input,\n",
    "    voice=voice,\n",
    "    audio_config=audio_config\n",
    ")\n",
    "\n",
    "with open(\"output.mp3\", \"wb\") as out:\n",
    "    out.write(response.audio_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
