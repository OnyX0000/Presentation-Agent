{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path, extract_images=True)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# ë¬¸ì„œ ì •ë³´ ì¶œë ¥\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"Page {idx + 1}:\")\n",
    "    print(\"Text:\", doc.page_content)  # í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶œë ¥\n",
    "    if \"images\" in doc.metadata:\n",
    "        print(\"Images:\", doc.metadata[\"images\"])  # ì´ë¯¸ì§€ ì •ë³´ ì¶œë ¥\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# PDF íŒŒì¼ ë¡œë“œ ë° í˜ì´ì§€ ë¶„í• \n",
    "loader = PyPDFLoader(\"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\")  # PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "pages = loader.load_and_split()  # í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "\n",
    "# **2. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±**\n",
    "vectorstore = Chroma.from_documents(pages, embedding=OpenAIEmbeddings())  # ì„ë² ë”© ìƒì„± ë° ì €ì¥\n",
    "retriever = vectorstore.as_retriever()  # ê²€ìƒ‰ê¸° ìƒì„±\n",
    "\n",
    "# **3. ê²€ìƒ‰ëœ ë¬¸ì„œ ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜**\n",
    "def format_context(docs):\n",
    "    \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³‘í•©\"\"\"\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# **4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜**\n",
    "template = '''ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•˜ì„¸ìš”:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# **5. LLM ëª¨ë¸ ì´ˆê¸°í™”**\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# **6. RAG ì²´ì¸ êµ¬ì„±**\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}  # ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ ì—°ê²°\n",
    "    | prompt  # í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "    | model   # ì–¸ì–´ ëª¨ë¸ ì‹¤í–‰\n",
    "    | StrOutputParser()  # ê²°ê³¼ íŒŒì‹±\n",
    ")\n",
    "\n",
    "# **7. ì²´ì¸ ì‹¤í–‰**\n",
    "retrieved_docs = retriever.get_relevant_documents(\"ì´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\")  # ë¬¸ì„œ ê²€ìƒ‰\n",
    "formatted_context = format_context(retrieved_docs)  # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "result = rag_chain.invoke({\n",
    "    \"context\": formatted_context,  # ë¬¸ìì—´ í˜•íƒœì˜ context ì „ë‹¬\n",
    "    \"question\": \"í˜ì´ì§€ë³„ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "})\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# **1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ**\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"  # ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€\n",
    "    return text.strip()\n",
    "\n",
    "# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "pdf_path = \"C:/wanted/Lang/Presentation-Agent/data/pdf/DeePrint.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# **2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜**\n",
    "template = '''ë‹¤ìŒ PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•˜ì„¸ìš”:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# **3. LLM ëª¨ë¸ ì´ˆê¸°í™”**\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# **4. í”„ë¡¬í”„íŠ¸ ì‹¤í–‰**\n",
    "question = \"í˜ì´ì§€ë³„ë¡œ ë°œí‘œ ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "formatted_prompt = prompt.format(context=pdf_text, question=question)\n",
    "\n",
    "result = model(formatted_prompt)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def time_convert(time_str):\n",
    "    minutes, seconds = map(int, time_str.split(':'))\n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "def download_audio(url, save_dir, clip_idx, start=None, end=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # í™•ì¥ì í¬í•¨ëœ yt-dlp ì €ì¥ìš© ì„ì‹œ íŒŒì¼ ê²½ë¡œ\n",
    "    temp_template = os.path.join(save_dir, f\"cheo_temp_{clip_idx}.%(ext)s\")\n",
    "    temp_wav = os.path.join(save_dir, f\"cheo_temp_{clip_idx}.wav\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': temp_template,\n",
    "        'quiet': True,\n",
    "        'force_ipv4': True,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "    # ìë¥´ê³  ì €ì¥\n",
    "    audio = AudioSegment.from_file(temp_wav, format=\"wav\")\n",
    "    if start and end:\n",
    "        start_ms = time_convert(start) * 1000\n",
    "        end_ms = time_convert(end) * 1000\n",
    "        audio = audio[start_ms:end_ms]\n",
    "\n",
    "    # ìµœì¢… íŒŒì¼ëª…: cheo_1.wav, cheo_2.wav ...\n",
    "    final_path = os.path.join(save_dir, f\"winter_{clip_idx}.wav\")\n",
    "    audio.export(final_path, format=\"wav\")\n",
    "\n",
    "    os.remove(temp_wav)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=4vS0Bhe5zjA\"\n",
    "save_dir = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "audio_data = []\n",
    "for i in range(5):\n",
    "    start = f\"{12+i}:00\"\n",
    "    end = f\"{12+i+1}:00\"\n",
    "    saved = download_audio(url, save_dir, clip_idx=i+1, start=start, end=end)\n",
    "    audio_data.append(saved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zonos.model import Zonos\n",
    "\n",
    "print(\"Zonos import ì„±ê³µ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\eSpeak NG\"\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\espeak.dll\"\n",
    "\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "backend = EspeakBackend(language='ko')\n",
    "print(backend.phonemize([\"ì•¼ ì´ì œ ë˜ëƒ ì•ˆ ë˜ëƒ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "import torch._dynamo  # suppress warning if needed\n",
    "\n",
    "# â— phonemizerìš© í™˜ê²½ë³€ìˆ˜ ì„¤ì • (espeak.dll ëŒ€ì‘)\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\eSpeak NG\"\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\espeak.dll\"\n",
    "\n",
    "# â— torch compile ë¹„í™œì„±í™” (C++ ì»´íŒŒì¼ëŸ¬ ì—†ì´ ì‹¤í–‰)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "device = \"cuda\"  # ë˜ëŠ” \"cpu\"\n",
    "model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n",
    "\n",
    "# ì…ë ¥ ìŒì„± ë¡œë“œ\n",
    "wav, sampling_rate = torchaudio.load(os.path.join(path, \"cheo_1.wav\"))\n",
    "\n",
    "# ìŠ¤í”¼ì»¤ ì„ë² ë”© ìƒì„± (ì œë¡œìƒ· ë³´ì´ìŠ¤ í´ë¡œë‹)\n",
    "speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "text = \"\"\"\n",
    "í˜„ëŒ€ ì‚¬íšŒì—ì„œ *AI* ê¸°ìˆ ì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "ìš°ë¦¬ì˜ 'ë°œí‘œ' ëŠ¥ë ¥ì„ ì§€ì›í•˜ëŠ” ì‹œìŠ¤í…œì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "# í…ìŠ¤íŠ¸ì™€ ìŠ¤í”¼ì»¤ë¡œ ì¡°ê±´ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±\n",
    "cond_dict = make_cond_dict(text=text, speaker=speaker, language=\"ko\")\n",
    "conditioning = model.prepare_conditioning(cond_dict)\n",
    "\n",
    "# ìŒì„± ìƒì„± (ì»´íŒŒì¼ëŸ¬ ë¹„í™œì„±í™”)\n",
    "codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "torchaudio.save(os.path.join(path, \"sample.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "print(\"ğŸ”Š sample.wav ìƒì„± ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "import torch._dynamo  # suppress warning if needed\n",
    "\n",
    "def zero_shot(path,file,text,name):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n",
    "    wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "    speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker,\n",
    "        language = \"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_zero_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"ğŸ”Š zero_shot.wav ìƒì„± ì™„ë£Œ!\")\n",
    "    return codes\n",
    "\n",
    "def few_shot(path, data, text, name):\n",
    "    '''few-shot ë³´ì´ìŠ¤ í´ë¦¬ë‹'''\n",
    "    torch.cuda.empty_cache()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=device)\n",
    "\n",
    "    embeddings = []\n",
    "    for file in data:\n",
    "        wav, sampling_rate = torchaudio.load(os.path.join(path, file))\n",
    "        emb = model.make_speaker_embedding(wav, sampling_rate)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    speaker_embedding = torch.stack(embeddings).mean(dim=0)\n",
    "\n",
    "    cond_dict = make_cond_dict(\n",
    "        text = text,\n",
    "        speaker = speaker_embedding,\n",
    "        language=\"ko\"\n",
    "    )\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "    # ìŒì„± ìƒì„± (ì»´íŒŒì¼ëŸ¬ ë¹„í™œì„±í™”)\n",
    "    codes = model.generate(conditioning, disable_torch_compile=True)\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    torchaudio.save(os.path.join(path, f\"{name}_few_shot.wav\"), wavs[0], model.autoencoder.sampling_rate)\n",
    "\n",
    "    print(\"ğŸ”Š few_shot.wav ìƒì„± ì™„ë£Œ!\")\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\wanted\\Lang\\Presentation-Agent\\data\\train_wav\"\n",
    "file = \"winter_2.wav\"\n",
    "data = [f\"winter_{i}.wav\" for i in range(2,6)]\n",
    "name = \"winter\"\n",
    "text = \"\"\"\n",
    "í˜„ëŒ€ ì‚¬íšŒì—ì„œ *AI* ê¸°ìˆ ì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "ìš°ë¦¬ì˜ 'ë°œí‘œ' ëŠ¥ë ¥ì„ ì§€ì›í•˜ëŠ” ì‹œìŠ¤í…œì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "zero_shot(path,file, text,name)\n",
    "few_shot(path,data, text,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "synthesiser = pipeline(\"text-to-speech\", \"suno/bark\")\n",
    "\n",
    "speech = synthesiser(\"í˜„ëŒ€ ì‚¬íšŒì—ì„œ ë°œí‘œëŠ” í•„ìˆ˜ì ì¸ í™œë™ì…ë‹ˆë‹¤.\", forward_params={\"do_sample\": True})\n",
    "\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from IPython.display import Audio\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# download and load all models\n",
    "preload_models(\n",
    "    text_use_gpu=False,\n",
    "    text_use_small=True,\n",
    "    coarse_use_gpu=True,\n",
    "    fine_use_gpu=True,\n",
    "    codec_use_gpu=True\n",
    ")\n",
    "\n",
    "\n",
    "# generate audio from text\n",
    "text_prompt = \"\"\"í˜„ëŒ€ ì‚¬íšŒì—ì„œ ë°œí‘œëŠ” í•„ìˆ˜ì ì¸ í™œë™ì…ë‹ˆë‹¤.\n",
    "í•˜ì§€ë§Œ ë§ì€ ì‚¬ëŒë“¤ì€ ë‚´ì„±ì ì¸ ì„±ê²©, ë¬´ëŒ€ ê³µí¬ì¦, ì‹¤ìˆ˜ì— ëŒ€í•œ ë‘ë ¤ì›€ ë“±ìœ¼ë¡œ ì¸í•´ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.\n",
    "ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, AI ê¸°ë°˜ ë°œí‘œ ì§€ì› ì‹œìŠ¤í…œì¸ \"ì €í¬ ë°œí‘œ ì•ˆí•©ë‹ˆë‹¤!\" í”„ë¡œì íŠ¸ê°€ ê¸°íšë˜ì—ˆìŠµë‹ˆë‹¤.\"\"\"\n",
    "speech_array = generate_audio(text_prompt)\n",
    "\n",
    "# play text in notebook\n",
    "Audio(speech_array, rate=SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ ê²½ë¡œ ì„¤ì • (MeloTTS ì½”ë“œ ì¶”ê°€)\n",
    "import sys, os\n",
    "sys.path.append(\"C:/wanted/Lang/MeloTTS\")  # ë„ˆì˜ ê²½ë¡œì— ë§ê²Œ ì¡°ì •\n",
    "\n",
    "# ğŸ“Œ PyTorch í™•ì¸\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âœ… Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.exists(\"C:/wanted/Lang/MeloTTS/melo/api.py\"))\n",
    "print(os.listdir(\"C:/wanted/Lang/MeloTTS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/wanted/Lang/MeloTTS/melo\")  # â† melo í´ë” ìì²´ë¥¼ ì§ì ‘ ì¶”ê°€\n",
    "\n",
    "from api import TTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ Jupyterì—ì„œ ì¬ìƒ (IPython ì˜¤ë””ì˜¤ ìœ„ì ¯)\n",
    "from IPython.display import Audio\n",
    "Audio(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melo.api import TTS\n",
    "\n",
    "# Speed is adjustable\n",
    "speed = 1.0\n",
    "device = 'cpu' # or cuda:0\n",
    "\n",
    "text = \"ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.\"\n",
    "model = TTS(language='KR', device=device)\n",
    "speaker_ids = model.hps.data.spk2id\n",
    "\n",
    "output_path = 'kr.wav'\n",
    "model.tts_to_file(text, speaker_ids['KR'], output_path, speed=speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì—ìš” ì—¬ê¸° êµ¬ê¸€ì€ ì—¬ê¸°ì…ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text():\n",
    "    \"\"\"Synthesizes speech from the input string of text.\"\"\"\n",
    "    from google.cloud import texttospeech\n",
    "\n",
    "    text = \"ê³¼ì—° ì„±ê³µì¸ê²ƒì¸ê°€ ì‹¤íŒ¨ì¸ ê²ƒì¸ê°€ ë‚˜ëŠ” ëˆ„êµ¬ì¸ê°€ ìš°ë¦° ì–´ë””ë¡œ ê°€ëŠ” ê²ƒì¸ê°€ ì„±ê³µì€ ê°€ëŠ¥í•œ ê²ƒì¸ê°€ê°€.\"\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # Note: the voice can also be specified by name.\n",
    "    # Names of voices can be retrieved with client.list_voices().\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ko-KR\"\n",
    "    )\n",
    "\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    response = client.synthesize_speech(\n",
    "        input=input_text,\n",
    "        voice=voice,\n",
    "        audio_config=audio_config,\n",
    "    )\n",
    "\n",
    "    # The response's audio_content is binary.\n",
    "    with open(\"output.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print('Audio content written to file \"output.mp3\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to file \"output.mp3\"\n"
     ]
    }
   ],
   "source": [
    "synthesize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
